{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def read_token(token_file):\n",
    "    with open(token_file, 'r') as f:\n",
    "        token = f.read()\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingqi/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/yingqi/miniconda3/envs/llm/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, OPTConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import os\n",
    "\n",
    "# Configurations\n",
    "## Primary Configs\n",
    "TOKEN = read_token(\"/home/yingqi/repo/HMT-pytorch/huggingface_token.txt\")\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
    "CACHE_DIR = os.environ.get(\"HF_HOME\")\n",
    "SEGMENT_LENGTH = 1024\n",
    "BPTT_DEPTH = 6\n",
    "NUM_SENSORY = 32\n",
    "BATCH_SIZE = 1\n",
    "NUM_SEG_SAVE = 8\n",
    "CHECKPOINT = \"/home/yingqi/repo/HMT-pytorch/tmp/model_weights_0_lv_2.pth\"\n",
    "\n",
    "## Minor Configs\n",
    "USE_LORA = False\n",
    "BASELINE_ONLY = False\n",
    "\n",
    "## Check Config Validity\n",
    "assert CACHE_DIR is not None, \"Please set the HF_HOME environment variable to a directory where the model can be cached\"\n",
    "\n",
    "# Intionialize model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, token=TOKEN, cache_dir=CACHE_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=TOKEN, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Get the word embedding dimension \n",
    "if isinstance(model.config, OPTConfig):\n",
    "    word_emb_dim = model.config.word_embed_proj_dim\n",
    "else:\n",
    "    word_emb_dim = model.config.hidden_size\n",
    "\n",
    "if USE_LORA:\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False, \n",
    "        # target_modules=['embed_tokens', 'gate_proj', 'up_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "        r=8, \n",
    "        lora_alpha=32, \n",
    "        lora_dropout=0.1\n",
    "        )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "# Compute Sizes\n",
    "input_size = SEGMENT_LENGTH\n",
    "memory_size = 1\n",
    "n_segments = BPTT_DEPTH\n",
    "\n",
    "if BASELINE_ONLY:\n",
    "    memory_size = 0\n",
    "    n_segments = 2\n",
    "\n",
    "batch_size = BATCH_SIZE\n",
    "\n",
    "block_size = input_size\n",
    "block_size -= 2 * memory_size\n",
    "block_size -= NUM_SENSORY\n",
    "history_size = (n_segments - 1) * block_size\n",
    "\n",
    "mask_size = block_size\n",
    "\n",
    "block_size_2 = input_size - (2*memory_size) - NUM_SENSORY//2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.\n",
      "num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.\n",
      "num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.\n"
     ]
    }
   ],
   "source": [
    "# Load Datasets\n",
    "from tools.data_processing.qmsum import load_qmsum_test\n",
    "\n",
    "datapoints = load_qmsum_test(40000, 40000, block_size=block_size, tokenizer=tokenizer, split='test[:2]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['answer_length', 'input_ids', 'attention_mask', 'labels', 'mask_size'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapoints[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models\n",
    "from tools.models import load_model\n",
    "\n",
    "class SimpleArgs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "args = SimpleArgs(rmt_only=False, \n",
    "                  baseline_only=False, \n",
    "                  num_seg_save=NUM_SEG_SAVE, \n",
    "                  num_sensory=NUM_SENSORY, \n",
    "                  segment_alignment=None, \n",
    "                  hmt_stage_2=False, \n",
    "                  load_from_ckpt=CHECKPOINT,\n",
    "                  hmt_stage_1=False,\n",
    "                  mem_recall_context=100,\n",
    "                  mem_recall_hidden_dim=4864)\n",
    "\n",
    "model = load_model(args=args, model=model, memory_size=memory_size, block_size=block_size, \n",
    "                   n_segments=n_segments, mask_size=mask_size,\n",
    "                    word_emb_dim=word_emb_dim, is_qa_task=False, cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecurrentWrapper(\n",
       "  (memory_cell): MemoryCell(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 896)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2SdpaAttention(\n",
       "              (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "              (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "              (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "              (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "              (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "              (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm()\n",
       "            (post_attention_layernorm): Qwen2RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       "    )\n",
       "    (mem_map): MemoryMap(\n",
       "      (linear): Linear(in_features=896, out_features=896, bias=False)\n",
       "      (inv_linear): Linear(in_features=896, out_features=896, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (cross_attn): CrossAttentionMemory(\n",
       "    (wq): Linear(in_features=896, out_features=4864, bias=False)\n",
       "    (wk): Linear(in_features=896, out_features=4864, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "RecurrentWrapper.generate() missing 3 required positional arguments: 'input_ids', 'attention_mask', and 'segment_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate an output\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: RecurrentWrapper.generate() missing 3 required positional arguments: 'input_ids', 'attention_mask', and 'segment_size'"
     ]
    }
   ],
   "source": [
    "# Generate an output\n",
    "model.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
