{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SentencePiece model is used to tokenize the input strings and decode the output tokens. You can create your own model with the google/sentencepiece library, or use our default one at t5.data.DEFAULT_SPM_PATH. If you create your own, you must use the flags --pad_id=0 --eos_id=1 --unk_id=2 --bos_id=-1 with spm_train to be compatible with our model code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. preprocess such that one sentence per one line\n",
    "\n",
    "\n",
    "speaker 1, speaker 2 tokens: or just put them into source text? second option is ok as they are already in data\n",
    "\n",
    "https://github.com/google/sentencepiece/blob/master/doc/special_symbols.md\n",
    "\n",
    "2. run training\n",
    "\n",
    "how to train: https://github.com/google/sentencepiece#train-sentencepiece-model\n",
    "\n",
    "pass multiple files: https://github.com/google/sentencepiece/issues/489#issuecomment-631556141\n",
    "\n",
    "all options: https://github.com/google/sentencepiece/blob/master/doc/options.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess ods_data\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "from ru_sent_tokenize import ru_sent_tokenize\n",
    "\n",
    "path = Path('/home/kuratov/data/ods_shards/ods_shards/')\n",
    "\n",
    "save_path = Path('/home/kuratov/data/ods_shards/merged_txt')\n",
    "if not Path(save_path).exists():\n",
    "    Path(save_path).mkdir(parents=True)\n",
    "\n",
    "files = sorted(list(path.glob('*json')))\n",
    "\n",
    "for f in tqdm(files):\n",
    "    sentences = []\n",
    "    dialogs = json.load(f.open('r'))\n",
    "    for d in dialogs:\n",
    "        dialog = ' '.join(d) \n",
    "        dialog = dialog.replace('<speaker1>', '<speaker1> ').replace('<speaker2>', '<speaker2> ')\n",
    "        if len(dialog) == 0:\n",
    "            continue\n",
    "        sentences += ru_sent_tokenize(dialog)\n",
    "    with (save_path / (f.stem + '.txt')).open('w') as fout:\n",
    "        for sent in sentences:\n",
    "            fout.write(sent + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path('/home/kuratov/data/ods_shards/merged_txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.SentencePieceTrainer.Train(input=list(save_path.glob('*.txt')), vocab_size=50259,\n",
    "                              pad_id=0, eos_id=1, unk_id=2, bos_id=-1,\n",
    "                              model_prefix='ods_data_50259_sp_1M_speaker_tokens',\n",
    "                              user_defined_symbols=['<speaker1>','<speaker2>'],\n",
    "                              train_extremely_large_corpus=True, # to run on full data\n",
    "                              input_sentence_size=1000000, # to use less RAM, ~630 Gb needed for full data\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_tokenizer = sp.SentencePieceProcessor(model_file='./ods_data_50259_sp_dgx3_spkr_tokens.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 655, 11]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_tokenizer.tokenize('<speaker1> Привет!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<speaker1> Привет!'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_tokenizer.decode_ids([5, 3, 655, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0",
   "language": "python",
   "name": "tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
