{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import PreTrainedModel, AutoModelForSequenceClassification, AutoConfig\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import datasets\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import BertForSequenceClassification\n",
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from modeling_rmt import RMTEncoderForSequenceClassification\n",
    "from modeling_rmt_enc_dec import RMTEncoderDecoderForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /home/bulatov/bulatov/rmt_internship/finetune/contract_nli/bert-base-cased/lr1e-05_constant_with_warmup_adamw_wd1e-03_1452_mem25_sum_loss/run_3/model_best.pth\n",
    "# !ls /home/bulatov/bulatov/rmt_internship/finetune/contract_nli/bert-base-cased/lr1e-05_linear_adamw_wd1e-03_968_mem25_sum_loss/run_3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cpt_path = \"/cephfs/home/bulatov/bulatov/rmt_internship/finetune/contract_nli/t5-base/lr1e-05_linear_adamw_wd1e-03_972_mem25_sum_loss/run_1/model_best.pth\"\n",
    "\n",
    "model_name = 'bert-base-cased'\n",
    "\n",
    "experiment_path = \"/home/bulatov/bulatov/rmt_internship/finetune/contract_nli/bert-base-cased/lr1e-05_linear_adamw_wd1e-03_968_mem25_sum_loss/run_3/\"\n",
    "cpt_path = os.path.join(experiment_path, \"model_best.pth\")\n",
    "config_path = os.path.join(experiment_path, \"config.json\")\n",
    "cpt = torch.load(cpt_path, map_location='cpu')\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    d = json.load(f)\n",
    "\n",
    "rmt = RMTEncoderForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "set_params_kwargs = {k:v for k,v in d.items() if k in rmt.set_params.__code__.co_varnames}\n",
    "set_params_kwargs['segment_ordering'] = 'regular'\n",
    "set_params_kwargs['inter_layer_memory'] = False\n",
    "set_params_kwargs['tokenizer'] = tokenizer        \n",
    "\n",
    "rmt.set_params(**set_params_kwargs)\n",
    "rmt.load_state_dict(cpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'bert-base-cased'\n",
    "\n",
    "# experiment_path = \"/home/bulatov/bulatov/runs/finetune/debug/contract_nli/bert-base-cased/lr1e-05_linear_adamw_wd1e-03_512_mem/run_1/\"\n",
    "# cpt_path = os.path.join(experiment_path, \"model_best.pth\")\n",
    "# config_path = os.path.join(experiment_path, \"config.json\")\n",
    "# cpt = torch.load(cpt_path, map_location='cpu')\n",
    "\n",
    "# with open(config_path, 'r') as f:\n",
    "#     d = json.load(f)\n",
    "\n",
    "# baseline = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# baseline.load_state_dict(cpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder. cnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = d['input_seq_len']\n",
    "target_seq_len = 2\n",
    "batch_size = 2\n",
    "\n",
    "device = torch.device(0)\n",
    "\n",
    "encode_plus_kwargs = {'max_length': input_seq_len,\n",
    "                              'truncation': True,\n",
    "                              'padding': 'longest',\n",
    "                              'pad_to_multiple_of': 1}\n",
    "generate_kwargs = {}\n",
    "labels_map = {'Contradiction': 0, 'Entailment': 1, 'Not mentioned': 2}\n",
    "num_labels = len(labels_map)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # cut too long strings because they may slow down tokenization\n",
    "    inputs = [b['input'][:input_seq_len * 10] for b in batch]\n",
    "    labels = [b['output'][:target_seq_len * 10] for b in batch]\n",
    "    features = tokenizer.batch_encode_plus(list(inputs), return_tensors='pt', **encode_plus_kwargs)\n",
    "    labels = np.array([labels_map[t] for t in labels])\n",
    "    features['labels'] = torch.from_numpy(labels)\n",
    "    features['id'] = [b['id'] for b in batch]\n",
    "    features['pid'] = [b['pid'] for b in batch]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/contract_nli/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653cfc9bf386493db0927193fa518b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('tau/scrolls', 'contract_nli')\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset,)\n",
    "kwargs = {'pin_memory': True, 'num_workers': 0}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "valid_dataset = dataset['validation']\n",
    "valid_sampler = RandomSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7191, 1037)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset['pid']), len(valid_dataset['pid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictions from all segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, input_ids, return_all_segments=False, **kwargs):\n",
    "    memory = self.set_memory()\n",
    "    segmented = self.pad_and_segment(input_ids)\n",
    "    segmented = list(zip(*segmented))\n",
    "\n",
    "    if self.segment_ordering in {'regular', 'last_memory_only'}:\n",
    "        pass\n",
    "    elif self.segment_ordering == 'reversed':\n",
    "        segmented = segmented[::-1]\n",
    "    elif self.segment_ordering == 'bidirectional':\n",
    "        segmented = segmented + segmented[::-1][1:]\n",
    "    elif self.segment_ordering == 'repeat_first':\n",
    "        segmented = segmented + segmented[:1]\n",
    "    else:\n",
    "        raise ValueError(f'Unknown segment ordering: {self.segment_ordering}')\n",
    "\n",
    "    self.memory_storage = {'num_mem_tokens': self.num_mem_tokens}\n",
    "    outputs = []\n",
    "    for seg_num, segment_data in enumerate(segmented):\n",
    "        input_ids, attention_mask, token_type_ids = segment_data\n",
    "        if memory.ndim == 2:\n",
    "            memory = memory.repeat(input_ids.shape[0], 1, 1)\n",
    "        if (self.bptt_depth > -1) and (len(segmented) - seg_num > self.bptt_depth): \n",
    "            memory = memory.detach()\n",
    "\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "        if self.drop_empty_segments:\n",
    "            non_empty_mask = [not torch.equal(input_ids[i], self.empty) for i in range(len(input_ids))]\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            input_ids = input_ids[non_empty_mask]\n",
    "            attention_mask = attention_mask[non_empty_mask]\n",
    "            token_type_ids = token_type_ids[non_empty_mask]\n",
    "            seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "\n",
    "            inputs_embeds = self.base_model.embeddings.word_embeddings(input_ids)\n",
    "            inputs_embeds[:, 1:1+self.num_mem_tokens] = memory[non_empty_mask]\n",
    "        else:\n",
    "            inputs_embeds = self.base_model.embeddings.word_embeddings(input_ids)\n",
    "            inputs_embeds[:, 1:1+self.num_mem_tokens] = memory\n",
    "\n",
    "        seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "        seg_kwargs['attention_mask'] = attention_mask\n",
    "        seg_kwargs['token_type_ids'] = token_type_ids\n",
    "\n",
    "        out = self.model.forward(**seg_kwargs, output_hidden_states=True)\n",
    "        outputs.append(out)\n",
    "\n",
    "        if self.drop_empty_segments:\n",
    "            memory[non_empty_mask] = out.hidden_states[-1][:, :self.num_mem_tokens]\n",
    "            out['non_empty_mask'] = non_empty_mask\n",
    "        else:\n",
    "            memory = out.hidden_states[-1][:, :self.num_mem_tokens]\n",
    "\n",
    "    for i, o in enumerate(outputs):\n",
    "        out[f'loss_{i}'] = o['loss'].mean()\n",
    "\n",
    "    if self.sum_loss:\n",
    "        out['loss'] = torch.stack([o['loss'] for o in outputs]).sum(dim=-1)\n",
    "\n",
    "    if return_all_segments:\n",
    "        return out, outputs\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def evaluate(output, sample):\n",
    "    labels = sample['labels']\n",
    "    logits = o['logits']\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    correct_mask = preds == labels[output['non_empty_mask']]\n",
    "    return correct_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'pid', 'input', 'output'],\n",
       "    num_rows: 7191\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label_train = dict(zip(train_dataset['id'], train_dataset['output']))\n",
    "id2label_valid = dict(zip(valid_dataset['id'], valid_dataset['output']))\n",
    "\n",
    "id2text_train = dict(zip(train_dataset['id'], train_dataset['input']))\n",
    "id2text_valid = dict(zip(valid_dataset['id'], valid_dataset['input']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = RandomSampler(valid_dataset)\n",
    "dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "gen = iter(dataloader)\n",
    "sample = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'pid', 'input', 'output'],\n",
       "    num_rows: 7191\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id, pid = sample.pop('id'), sample.pop('pid')\n",
    "out = rmt(**sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = out['loss'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/graphviz/backend/execute.py:79\u001b[0m, in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mPIPE\n\u001b[0;32m---> 79\u001b[0m     proc \u001b[38;5;241m=\u001b[39m \u001b[43m_run_input_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_lines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/graphviz/backend/execute.py:99\u001b[0m, in \u001b[0;36m_run_input_lines\u001b[0;34m(cmd, input_lines, kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_input_lines\u001b[39m(cmd, input_lines, \u001b[38;5;241m*\u001b[39m, kwargs):\n\u001b[0;32m---> 99\u001b[0m     popen \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     stdin_write \u001b[38;5;241m=\u001b[39m popen\u001b[38;5;241m.\u001b[39mstdin\u001b[38;5;241m.\u001b[39mwrite\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:858\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    856\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 858\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:1704\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1703\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: PosixPath('dot')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/IPython/core/formatters.py:973\u001b[0m, in \u001b[0;36mMimeBundleFormatter.__call__\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    970\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 973\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/graphviz/jupyter_integration.py:98\u001b[0m, in \u001b[0;36mJupyterIntegration._repr_mimebundle_\u001b[0;34m(self, include, exclude, **_)\u001b[0m\n\u001b[1;32m     96\u001b[0m include \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(include) \u001b[38;5;28;01mif\u001b[39;00m include \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jupyter_mimetype}\n\u001b[1;32m     97\u001b[0m include \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(exclude \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {mimetype: \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, method_name)()\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m mimetype, method_name \u001b[38;5;129;01min\u001b[39;00m MIME_TYPES\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mimetype \u001b[38;5;129;01min\u001b[39;00m include}\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/graphviz/jupyter_integration.py:98\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     96\u001b[0m include \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(include) \u001b[38;5;28;01mif\u001b[39;00m include \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jupyter_mimetype}\n\u001b[1;32m     97\u001b[0m include \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(exclude \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {mimetype: \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m mimetype, method_name \u001b[38;5;129;01min\u001b[39;00m MIME_TYPES\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mimetype \u001b[38;5;129;01min\u001b[39;00m include}\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/graphviz/jupyter_integration.py:112\u001b[0m, in \u001b[0;36mJupyterIntegration._repr_image_svg_xml\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_repr_image_svg_xml\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the rendered graph as SVG string.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msvg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSVG_ENCODING\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/graphviz/piping.py:104\u001b[0m, in \u001b[0;36mPipe.pipe\u001b[0;34m(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipe\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     56\u001b[0m          \u001b[38;5;28mformat\u001b[39m: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     57\u001b[0m          renderer: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m          engine: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     62\u001b[0m          encoding: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mUnion[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the source piped through the Graphviz layout command.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m        '<?xml version='\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipe_legacy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mneato_no_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneato_no_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/graphviz/_tools.py:171\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     wanted \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    163\u001b[0m                        \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m deprecated\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m    164\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe signature of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be reduced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msupported_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m positional args\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(supported)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: pass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwanted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as keyword arg(s)\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    168\u001b[0m                   stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    169\u001b[0m                   category\u001b[38;5;241m=\u001b[39mcategory)\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/graphviz/piping.py:121\u001b[0m, in \u001b[0;36mPipe._pipe_legacy\u001b[0;34m(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@_tools\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecate_positional_args(supported_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pipe_legacy\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    114\u001b[0m                  \u001b[38;5;28mformat\u001b[39m: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m                  engine: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    120\u001b[0m                  encoding: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mUnion[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipe_future\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mneato_no_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneato_no_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/graphviz/piping.py:149\u001b[0m, in \u001b[0;36mPipe._pipe_future\u001b[0;34m(self, format, renderer, formatter, neato_no_op, quiet, engine, encoding)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mlookup(encoding) \u001b[38;5;129;01mis\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding):\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;66;03m# common case: both stdin and stdout need the same encoding\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pipe_lines_string\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m         raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_lines(\u001b[38;5;241m*\u001b[39margs, input_encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/graphviz/backend/piping.py:212\u001b[0m, in \u001b[0;36mpipe_lines_string\u001b[0;34m(engine, format, input_lines, encoding, renderer, formatter, neato_no_op, quiet)\u001b[0m\n\u001b[1;32m    206\u001b[0m cmd \u001b[38;5;241m=\u001b[39m dot_command\u001b[38;5;241m.\u001b[39mcommand(engine, \u001b[38;5;28mformat\u001b[39m,\n\u001b[1;32m    207\u001b[0m                           renderer\u001b[38;5;241m=\u001b[39mrenderer,\n\u001b[1;32m    208\u001b[0m                           formatter\u001b[38;5;241m=\u001b[39mformatter,\n\u001b[1;32m    209\u001b[0m                           neato_no_op\u001b[38;5;241m=\u001b[39mneato_no_op)\n\u001b[1;32m    210\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_lines\u001b[39m\u001b[38;5;124m'\u001b[39m: input_lines, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding}\n\u001b[0;32m--> 212\u001b[0m proc \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstdout\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/graphviz/backend/execute.py:84\u001b[0m, in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOENT:\n\u001b[0;32m---> 84\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ExecutableNotFound(cmd) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mstderr:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f7dcc80aa00>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(loss, params = dict(rmt.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_df.to_csv('tables/cnli-train-bert.csv', index=False)\n",
    "# res_df.to_csv('tables/cnli-valid-bert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt_df = pd.read_csv('tables/cnli-train-rm-bert-968-25.csv')\n",
    "baseline_df = pd.read_csv('tables/cnli-train-bert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt_df.pred_seg_0.value_counts(), rmt_df.pred_seg_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt_df['correct_seg_0'] = rmt_df.pred_seg_0 == rmt_df.labels_seg_0\n",
    "rmt_df['correct_seg_1'] = rmt_df.pred_seg_1 == rmt_df.labels_seg_1\n",
    "\n",
    "baseline_df['correct_seg'] = baseline_df.pred_seg == baseline_df.labels_seg\n",
    "\n",
    "rmt_correct_ids_seg_0 = set(rmt_df[rmt_df.correct_seg_0].ids)\n",
    "rmt_correct_ids_seg_1 = set(rmt_df[rmt_df.correct_seg_1].ids)\n",
    "bl_correct_ids = set(baseline_df[baseline_df.correct_seg].ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt_lose_seg_0 = bl_correct_ids.difference(rmt_correct_ids_seg_0)\n",
    "rmt_lose_seg_1 = bl_correct_ids.difference(rmt_correct_ids_seg_1)\n",
    "baseline_lose = rmt_correct_ids_seg_1.difference(bl_correct_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'rmt loses: {len(rmt_lose_seg_1)}, baseline loses: {len(baseline_lose)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline \n",
    "\n",
    "accuracy = (baseline_df.pred_seg == baseline_df.labels_seg).mean()\n",
    "conf_mat = confusion_matrix(y_pred=baseline_df.pred_seg, y_true=baseline_df.labels_seg, )\n",
    "conf_mat_normalized = (conf_mat / conf_mat.sum(axis=1).reshape(-1, 1)).round(2)\n",
    "print(f'Accuracy: {accuracy}\\n\\nConfusion matrix: \\n{conf_mat}\\nNormalized: \\n{conf_mat_normalized}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt\n",
    "\n",
    "df = rmt_df \n",
    "for l in [0, 1]:\n",
    "    accuracy = (df[f'pred_seg_{l}'] == df[f'labels_seg_{l}']).mean()\n",
    "    conf_mat = confusion_matrix(y_pred=df[f'pred_seg_{l}'], y_true=df[f'labels_seg_{l}'])\n",
    "    conf_mat_normalized = (conf_mat / conf_mat.sum(axis=1).reshape(-1, 1)).round(2)\n",
    "    print(f'\\nSegment {l}\\nAccuracy: {accuracy}\\n\\nConfusion matrix: \\n{conf_mat}\\nNormalized: \\n{conf_mat_normalized}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt_df = pd.read_csv('tables/cnli-valid-rm-bert-968-25.csv')\n",
    "baseline_df = pd.read_csv('tables/cnli-valid-bert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt_df['correct_seg_0'] = rmt_df.pred_seg_0 == rmt_df.labels_seg_0\n",
    "rmt_df['correct_seg_1'] = rmt_df.pred_seg_1 == rmt_df.labels_seg_1\n",
    "\n",
    "baseline_df['correct_seg'] = baseline_df.pred_seg == baseline_df.labels_seg\n",
    "\n",
    "rmt_correct_ids_seg_0 = set(rmt_df[rmt_df.correct_seg_0].ids)\n",
    "rmt_correct_ids_seg_1 = set(rmt_df[rmt_df.correct_seg_1].ids)\n",
    "bl_correct_ids = set(baseline_df[baseline_df.correct_seg].ids)\n",
    "\n",
    "rmt_lose_seg_0 = bl_correct_ids.difference(rmt_correct_ids_seg_0)\n",
    "rmt_lose_seg_1 = bl_correct_ids.difference(rmt_correct_ids_seg_1)\n",
    "baseline_lose = rmt_correct_ids_seg_1.difference(bl_correct_ids)\n",
    "baseline_lose_seg_0 = rmt_correct_ids_seg_0.difference(bl_correct_ids)\n",
    "\n",
    "print(f'rmt loses: seg_0 - {len(rmt_lose_seg_0)}, seg_1 - {len(rmt_lose_seg_1)}, baseline loses: {len(baseline_lose)} (to seg 0: {len(baseline_lose_seg_0)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline \n",
    "\n",
    "accuracy = (baseline_df.pred_seg == baseline_df.labels_seg).mean()\n",
    "conf_mat = confusion_matrix(y_pred=baseline_df.pred_seg, y_true=baseline_df.labels_seg, )\n",
    "conf_mat_normalized = (conf_mat / conf_mat.sum(axis=1).reshape(-1, 1)).round(2)\n",
    "print(f'Accuracy: {accuracy}\\n\\nConfusion matrix: \\n{conf_mat}\\nNormalized: \\n{conf_mat_normalized}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt\n",
    "\n",
    "df = rmt_df \n",
    "for l in [0, 1]:\n",
    "    accuracy = (df[f'pred_seg_{l}'] == df[f'labels_seg_{l}']).mean()\n",
    "    conf_mat = confusion_matrix(y_pred=df[f'pred_seg_{l}'], y_true=df[f'labels_seg_{l}'])\n",
    "    conf_mat_normalized = (conf_mat / conf_mat.sum(axis=1).reshape(-1, 1)).round(2)\n",
    "    print(f'\\nSegment {l}\\nAccuracy: {accuracy}\\n\\nConfusion matrix: \\n{conf_mat}\\nNormalized: \\n{conf_mat_normalized}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(conf_mat, index=labels_map.keys(), columns=labels_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt\n",
    "\n",
    "df = rmt_df[rmt_df.ids.isin(rmt_lose_seg_1)]\n",
    "for l in [0, ]:\n",
    "    accuracy = (df[f'pred_seg_{l}'] == df[f'labels_seg_{l}']).mean()\n",
    "    conf_mat = confusion_matrix(y_pred=df[f'pred_seg_{l}'], y_true=df[f'labels_seg_{l}'])\n",
    "    conf_mat_normalized = (conf_mat / conf_mat.sum(axis=1).reshape(-1, 1)).round(2)\n",
    "    print(f'\\nSegment {l}\\nAccuracy: {accuracy}\\n\\nConfusion matrix: \\n{conf_mat}\\nNormalized: \\n{conf_mat_normalized}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(conf_mat, index=labels_map.keys(), columns=labels_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt\n",
    "\n",
    "df = rmt_df[rmt_df.ids.isin(rmt_lose_seg_0)]\n",
    "for l in [0, 1]:\n",
    "    accuracy = (df[f'pred_seg_{l}'] == df[f'labels_seg_{l}']).mean().round(2)\n",
    "    conf_mat = confusion_matrix(y_pred=df[f'pred_seg_{l}'], y_true=df[f'labels_seg_{l}'])\n",
    "    conf_mat_normalized = (conf_mat / conf_mat.sum(axis=1).reshape(-1, 1)).round(2)\n",
    "    print(f'\\nSegment {l}\\nAccuracy: {accuracy}\\n\\nConfusion matrix: \\n{conf_mat}\\nNormalized: \\n{conf_mat_normalized}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline \n",
    "df = baseline_df[baseline_df.ids.isin(baseline_lose)]\n",
    "accuracy = (df.pred_seg == df.labels_seg).mean()\n",
    "conf_mat = confusion_matrix(y_pred=df.pred_seg, y_true=df.labels_seg, )\n",
    "conf_mat_normalized = (conf_mat / conf_mat.sum(axis=1).reshape(-1, 1)).round(2)\n",
    "print(f'Accuracy: {accuracy}\\n\\nConfusion matrix: \\n{conf_mat}\\nNormalized: \\n{conf_mat_normalized}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(conf_mat, index=labels_map.keys(), columns=labels_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('What label model distinguishes worse than its rival?\\n RMT-seg-1, RMT-seg-0, baseline')\n",
    "# rmt_df[rmt_df.ids.isin(rmt_lose_seg_1)].labels_seg_1.value_counts()/rmt_df.labels_seg_1.value_counts(), rmt_df[rmt_df.ids.isin(rmt_lose_seg_0)].labels_seg_0.value_counts()/rmt_df.labels_seg_0.value_counts(), baseline_df[baseline_df.ids.isin(baseline_lose)].labels_seg.value_counts()/baseline_df.labels_seg.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode(text, **encode_plus_kwargs)\n",
    "n_segments = 2\n",
    "def split(text, n_segments=n_segments):\n",
    "    premise = text.split('.')[0]\n",
    "    encoded = tokenizer.encode(text, **encode_plus_kwargs, add_special_tokens=False)\n",
    "    segments = np.split(np.array(encoded), n_segments)    \n",
    "    texts = [tokenizer.decode(s) for s in segments]\n",
    "    \n",
    "    \n",
    "    return [premise] + texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 10\n",
    "\n",
    "idx = list(rmt_lose_seg_1)[i]\n",
    "label = id2label_valid[idx] \n",
    "rmt_preds = ', '.join(rmt_df[rmt_df.ids == idx][['pred_seg_0', 'pred_seg_1']].astype(str).values[0])\n",
    "baseline_pred = baseline_df[baseline_df.ids == idx].pred_seg.values[0]\n",
    "print(f'idx: {idx}\\nlabel: {label, labels_map[label]}\\nrmt predictions: {rmt_preds}\\nbaseline_prediction: {baseline_pred}\\n\\n')\n",
    "\n",
    "text = id2text_valid[idx]\n",
    "texts = split(text)\n",
    "\n",
    "print('\\n\\n\\n'.join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "\n",
    "idx = list(baseline_lose)[i]\n",
    "label = id2label_valid[idx] \n",
    "rmt_preds = ', '.join(rmt_df[rmt_df.ids == idx][['pred_seg_0', 'pred_seg_1']].astype(str).values[0])\n",
    "baseline_pred = baseline_df[baseline_df.ids == idx].pred_seg.values[0]\n",
    "print(f'idx: {idx}\\nlabel: {label, labels_map[label]}\\nrmt predictions: {rmt_preds}\\nbaseline_prediction: {baseline_pred}\\n\\n')\n",
    "\n",
    "text = id2text_valid[idx]\n",
    "texts = split(text)\n",
    "\n",
    "print('\\n\\n\\n'.join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "i = 2\n",
    "\n",
    "ids = rmt_df[(rmt_df.pred_seg_0 == rmt_df.labels_seg_0) & ( rmt_df.pred_seg_1 != rmt_df.labels_seg_1)].ids.values\n",
    "idx = ids[i]\n",
    "label = id2label_valid[idx] \n",
    "rmt_preds = ', '.join(rmt_df[rmt_df.ids == idx][['pred_seg_0', 'pred_seg_1']].astype(str).values[0])\n",
    "baseline_pred = baseline_df[baseline_df.ids == idx].pred_seg.values[0]\n",
    "print(f'idx: {idx}\\nlabel: {label, labels_map[label]}\\nrmt predictions: {rmt_preds}\\nbaseline_prediction: {baseline_pred}\\n\\n')\n",
    "\n",
    "text = id2text_valid[idx]\n",
    "texts = split(text)\n",
    "\n",
    "print('\\n\\n\\n'.join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt_df[(rmt_df.pred_seg_0 == rmt_df.labels_seg_0) & ( rmt_df.pred_seg_1 != rmt_df.labels_seg_1)].labels_seg_1.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mmaybe finetune only memory weights\n",
    "use mem outputs to decode previous segment with a separate decoder\n",
    "orr pool answer to task not from cls but from mem tokens\n",
    "check if  gradient flows through memory\n",
    "decode using conccatenated memory, not last segment\n",
    "train baseline without question on qasper\n",
    "\n",
    "how do we improve remembering using memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15ebdd31b1273fe4d2b1fe1822219a570cf61693f7cab545dbe286c10cf9691f"
  },
  "kernelspec": {
   "display_name": "hvdenv",
   "language": "python",
   "name": "hvdenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
