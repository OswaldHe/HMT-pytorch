{
 "cells": [
   {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n",
    "\n",
    "def _forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        output_hidden_states: Optional[bool] = False,\n",
    "        return_dict: Optional[bool] = True,\n",
    "        memory_storage = None\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                # if use_cache:\n",
    "                #     logger.warning(\n",
    "                #         \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                #     )\n",
    "                #     use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                num_mem = memory_storage['num_mem_tokens']\n",
    "                if i in memory_storage:\n",
    "                    layer_memory = memory_storage[i]\n",
    "                    for j, h in enumerate(hidden_states):\n",
    "                        hidden_states[j][:layer_memory[j].shape[0]] = layer_memory[j]\n",
    "\n",
    "                print(f'hidden states shape: {len(hidden_states), hidden_states[0].shape}\\n memory storage:{memory_storage.keys()}')\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "                \n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if i in memory_storage:\n",
    "                print(f'replacing ms[i] {memory_storage[i][0][0][:10]}... to {[h[:num_mem] for h in hidden_states][0][0][:10]}')\n",
    "            memory_storage[i] = [h[:num_mem] for h in hidden_states]\n",
    "\n",
    "            # memory_storage['success'] = True\n",
    "            # print(f'Overrided method message: hidden states shape: {len(hidden_states), hidden_states[0].shape}\\n memory storage:{memory_storage}')\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import types\n",
    "# def create_memory_forward(module, memory_storage):\n",
    "#     def memory_forward(*args, **kwargs):\n",
    "#         return module(*args, **kwargs, memory_storage=memory_storage)\n",
    "\n",
    "#     return memory_forward\n",
    "# self.base_model.encoder.forward = types.MethodType(create_memory_forward(self.base_model.encoder.forward, memory_storage), self.base_model.encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "self = rmt\n",
    "\n",
    "memory_storage = {'num_mem_tokens': 10}\n",
    "self.base_model.encoder.forward = types.MethodType(lambda *args, **kwargs: _forward(*args, **kwargs, memory_storage=memory_storage), self.base_model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states shape: (2, torch.Size([512, 256]))\n",
      " memory storage:dict_keys(['num_mem_tokens'])\n",
      "hidden states shape: (2, torch.Size([512, 256]))\n",
      " memory storage:dict_keys(['num_mem_tokens', 0])\n",
      "hidden states shape: (2, torch.Size([512, 256]))\n",
      " memory storage:dict_keys(['num_mem_tokens', 0, 1])\n",
      "hidden states shape: (2, torch.Size([512, 256]))\n",
      " memory storage:dict_keys(['num_mem_tokens', 0, 1, 2])\n",
      "\n",
      "\n",
      "found memory!!\n",
      "hidden states shape: (2, torch.Size([512, 256]))\n",
      " memory storage:dict_keys(['num_mem_tokens', 0, 1, 2, 3])\n",
      "replacing ms[i] tensor([ 0.4674, -0.2865,  0.6345,  0.4214,  0.9536,  0.4036, -0.4627,  0.2310,\n",
      "        -5.6048, -0.0293], grad_fn=<SliceBackward0>)... to tensor([ 0.7673, -0.3799,  0.3761,  0.2463,  0.4095, -0.2411, -0.8653, -0.4908,\n",
      "        -3.7968, -0.1928], grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "found memory!!\n",
      "hidden states shape: (2, torch.Size([512, 256]))\n",
      " memory storage:dict_keys(['num_mem_tokens', 0, 1, 2, 3])\n",
      "replacing ms[i] tensor([ 3.3768e-01, -3.1988e-01,  5.4384e-01,  4.3457e-01,  8.9003e-01,\n",
      "        -2.9533e-01, -3.6630e-01, -2.4385e-03, -6.9026e+00,  2.0820e-01],\n",
      "       grad_fn=<SliceBackward0>)... to tensor([ 0.5029, -0.1903,  0.3713,  0.4202,  0.8091, -0.5679,  0.0548,  0.1266,\n",
      "        -8.8174,  0.2342], grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "found memory!!\n",
      "hidden states shape: (2, torch.Size([512, 256]))\n",
      " memory storage:dict_keys(['num_mem_tokens', 0, 1, 2, 3])\n",
      "replacing ms[i] tensor([ 0.2493, -0.2427,  0.4581,  0.0305,  0.9501, -0.2016, -0.6997,  0.0157,\n",
      "        -3.9045, -0.2146], grad_fn=<SliceBackward0>)... to tensor([-0.7593, -0.0639,  0.4152, -0.6003,  1.1929,  0.2724, -1.2205, -0.1848,\n",
      "        -2.5121, -0.3173], grad_fn=<SliceBackward0>)\n",
      "\n",
      "\n",
      "found memory!!\n",
      "hidden states shape: (2, torch.Size([512, 256]))\n",
      " memory storage:dict_keys(['num_mem_tokens', 0, 1, 2, 3])\n",
      "replacing ms[i] tensor([-0.2909, -0.4081,  0.4398, -0.3557,  1.0469,  0.0998, -0.7792,  0.5558,\n",
      "        -3.1731,  0.1406], grad_fn=<SliceBackward0>)... to tensor([-0.1704, -0.5169,  0.1084, -0.8437,  1.1095, -0.4232, -0.8593,  0.3689,\n",
      "        -1.8543,  0.1048], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids = sample['input_ids']\n",
    "\n",
    "memory = self.set_memory()\n",
    "segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "outputs = []\n",
    "for seg_num, segment_data in enumerate(zip(*segmented)):\n",
    "    input_ids, attention_mask, token_type_ids = segment_data\n",
    "    if memory.ndim == 2:\n",
    "        memory = memory.repeat(input_ids.shape[0], 1, 1)\n",
    "    if (self.bptt_depth > -1) and (len(segmented) - seg_num > self.bptt_depth): \n",
    "        memory = memory.detach()\n",
    "\n",
    "    seg_kwargs = dict(**kwargs)\n",
    "    if self.drop_empty_segments:\n",
    "\n",
    "        non_empty_mask = [not torch.equal(input_ids[i], self.empty) for i in range(len(input_ids))]\n",
    "        if sum(non_empty_mask) == 0:\n",
    "            continue\n",
    "        input_ids = input_ids[non_empty_mask]\n",
    "        attention_mask = attention_mask[non_empty_mask]\n",
    "        token_type_ids = token_type_ids[non_empty_mask]\n",
    "        seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "\n",
    "        inputs_embeds = self.base_model.embeddings.word_embeddings(input_ids)\n",
    "        inputs_embeds[:, 1:1+self.num_mem_tokens] = memory[non_empty_mask]\n",
    "    else:\n",
    "        inputs_embeds = self.base_model.embeddings.word_embeddings(input_ids)\n",
    "        inputs_embeds[:, 1:1+self.num_mem_tokens] = memory\n",
    "\n",
    "    seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "    seg_kwargs['attention_mask'] = attention_mask\n",
    "    seg_kwargs['token_type_ids'] = token_type_ids\n",
    "    \n",
    "    out = self.model.forward(**seg_kwargs, output_hidden_states=True)\n",
    "    outputs.append(out)\n",
    "\n",
    "    if self.drop_empty_segments:\n",
    "        memory[non_empty_mask] = out.hidden_states[-1][:, :self.num_mem_tokens]\n",
    "    else:\n",
    "        memory = out.hidden_states[-1][:, :self.num_mem_tokens]\n",
    "\n",
    "if self.sum_loss:\n",
    "    out['loss'] = torch.stack([o['loss'] for o in outputs]).sum(dim=-1)"
   ]
  }]
}
