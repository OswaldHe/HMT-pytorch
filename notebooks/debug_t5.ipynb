{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import BertForSequenceClassification, BartForConditionalGeneration, T5ForConditionalGeneration\n",
    "\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset scrolls (/home/booydar/.cache/huggingface/datasets/tau___scrolls/contract_nli/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n",
      "100%|██████████| 3/3 [00:00<00:00, 65.21it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"tau/scrolls\", 'contract_nli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset scrolls/quality to /home/booydar/.cache/huggingface/datasets/tau___scrolls/quality/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 31.3M/31.3M [00:27<00:00, 1.16MB/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset scrolls downloaded and prepared to /home/booydar/.cache/huggingface/datasets/tau___scrolls/quality/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 277.96it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"tau/scrolls\", 'quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '105_nda-7',\n",
       " 'pid': '105_nda-7_0',\n",
       " 'input': 'Receiving Party may share some Confidential Information with some third-parties (including consultants, agents and professional advisors).\\n\\nNON-DISCLOSURE AGREEMENT\\nTHIS NON-DISCLOSURE AGREEMENT (hereinafter referred to as the \"Agreement\") is entered into by and between Excelerate, Inc. a corporation organized and existing under the laws of the State of Alabama, (“Excelerate”) having its principal place of business at 1230 Slaughter Road, Suite F, Madison, AL 35758 U.S.A., and Burton Technical Group (\"Second Party\"), with its corporate office located at Company Address and each or both of which shall also hereinafter be referred to as the \"Party\" or \"Parties,\" respectively.\\nWHEREAS, the Parties represent that they control or may in the future control and have in their possession or may in the future possess valuable proprietary, confidential information as described in Paragraph 1 of this Agreement;\\nWHEREAS, in order for the Parties each to evaluate its interest in participating in a future business relationship it appears necessary that the Parties disclose to each other certain information; and\\nWHEREAS, the Parties are willing to disclose and receive such information pursuant to the terms and conditions of this Agreement and neither Party has an obligation to supply PROPRIETARY Information;\\nNOW, THEREFORE, the Parties agree as follows:\\n1. \"PROPRIETARY Information\" shall mean, in the case of Excelerate and the Second Party, proprietary or confidential information that is owned or controlled by each party relating to any solicitation or request for proposal. Each Party\\'s Proprietary Information may include, but is not limited to, patents, copyrights, design methods, ideas, concepts, data, formulas, manufacturing techniques, know-how, business plans, customer lists, solicitation response strategies, technical solutions to client requirements, system architectures, proposal preparation techniques and pricing policies, software, methodologies, technologies, processes, financial information, and sales and marketing information. Edit the highlighted portion of the first sentence in this paragraph to reflect the activities that are part of the NDA then delete this sentence.\\n2. Each Party represents that to the best of its knowledge it has the right to disclose its Proprietary Information to the other without conflict with, or violation of the rights of, any third party.\\n3. Each Party receiving Proprietary Information will require that all third parties to this Agreement, if any, to which it may give such proprietary information protect the same in accordance with the provisions contained herein. The third party shall be required to execute an agreement with the same provisions as contained herein.\\n4. Except as provided in Paragraphs 6 and 7 hereof, Proprietary Information disclosed to a receiving Party shall for a period of three (3) years from the effective date of this Agreement be held in confidence by the receiving Party and not be disclosed to others or used except for the purposes set forth above, without the prior written approval of the disclosing Party.\\n5. Disclosure of Proprietary Information to a receiving Party may be either oral or in writing. If an oral disclosure occurs, it will be confirmed within fifteen (15) days following initial disclosure by a written communication stating at least the date and circumstances under which the disclosure occurred and the general nature of the information disclosed. When a writing contains Proprietary Information the writing will, prior to disclosure to the receiving Party, be marked by the disclosing Party with a suitable legend (such as \"Proprietary Information\") to indicate its Proprietary status. The Parties shall utilize no less than reasonable care in protecting Proprietary Information received from the disclosing party; such care shall be the same degree of care as though it were to protect its own confidential or proprietary information.\\n6. The conditions of Paragraph 4 hereof shall not apply to information which:\\na. Was in the public domain or generally available to the public prior to receipt thereof by the receiving Party from the disclosing Party, or which subsequently becomes part of the public domain or generally available to the public except by wrongful act of the receiving Party or an employee or agent of the receiving Party; or\\nb. Was (i) in the possession of the receiving Party prior to receipt from the disclosing Party, or (ii) is later received by the receiving Party from a third party, unless the receiving Party knows or has reason to know of an obligation of secrecy of the third party to the disclosing Party with respect to such information; or (iii) is developed by the receiving Party independent of such information received from the disclosing Party; or\\nc. Is generally disclosed by the disclosing Party to third parties without obligation of secrecy.\\n7. Notwithstanding anything to the contrary in Paragraph 4 hereof, Proprietary Information may be disclosed by a receiving Party to those of its employees and consultants who require knowledge thereof in connection with their duties in conducting the aforesaid purpose of this Agreement and who are obligated by written agreement to hold such Proprietary Information in confidence and restrict its use consistent with the receiving Party\\'s obligations under this Agreement; and Proprietary Information may be disclosed to a legislative, judicial, or regulatory body requiring its disclosure, provided that, prior to such disclosure, the receiving Party has notified the disclosing Party of the requirement with an opportunity for the disclosing Party to object or seek an appropriate protective order.\\n8. Upon written request of a Party who has disclosed Proprietary Information to a receiving Party, the receiving Party shall promptly return all Proprietary Information except that one copy may be retained by legal counsel of the receiving Party as evidence of what was disclosed.\\n9. No title, license, or any other right of ownership or use shall be granted (expressly, by implication, or by estoppels) to the receiving party under any patent, trademark, copyright, or trade secret owned or controlled by the disclosing party by the disclosure of Proprietary Information. This Agreement shall not be construed to grant to either Party any patent license, use license, know-how license, or any other rights except as specifically provided herein.\\n10. This Agreement shall be effective on the date of its full execution by the Parties.\\na. This Agreement shall terminate at the end of the period of years as provided for in paragraph 4 above, or upon the delivery of written notice of termination by a Party to the other Party; however, the obligations of a receiving Party pursuant to Paragraph 4 shall remain in effect for the term specified therein.\\nb. Notwithstanding the termination of this agreement, the supplied data must be maintained and protected in accordance with its provisions for three (3) years following the termination of this agreement. At the conclusion of this agreement/contract for which data is exchanged, proprietary data shall be returned to the provider or destroyed with a certification to that effect provided to the other party, except the one (1) copy retained by legal counsel as provided in paragraph 8 above.\\n11. Except as specifically provided herein, neither Party makes any warranty, express or implied, oral or written, with respect to products, services, or information supplied hereunder, including any warranties related to patent, trademark, or copyright infringements. In no event shall either Party be liable to the other for indirect, special, consequential, punitive, exemplary, or incidental damages including, but not limited to damages for loss of use of facilities or equipment, loss of revenue, loss of profits or loss of goodwill regardless of (a) the negligence (either sole or concurrent) of either party; and (b) whether either party has been informed of the possibility of such damages.\\n12. Each Party acknowledges that the Proprietary Information disclosed hereunder may be subject to export control, and that compliance with all appropriate Government regulations such as the International Traffic in Arms Regulations (ITAR), the Export Administration Regulations (EAR), etc., may be necessary to obtain required approvals before disclosing Proprietary Information to foreign nationals, businesses or governments. The receiving Party shall obtain the written consent of the disclosing Party prior to submitting any request for authority to export any such Proprietary Information. The receiving Party shall indemnify and hold the disclosing Party harmless from all claims, demands, damages, costs, fines, penalties, attorneys’ fees, and all other expenses arising from failure of the receiving Party to comply with this clause or the ITAR or the EAR.\\n13. The exclusive points of contact (POC) for the Parties with respect to the exchange of Proprietary Information are as follows:\\nExcelerate, Inc. Second Party\\nAttention: Michael Doubleday Name\\n1230 Slaughter Road, Suite F Address\\nAddress:\\nMadison, AL 35759 City, State Zip\\nPhone: (256) 325-4050 Phone\\nFax: (256) 325-4052 Fax\\nEmail: Inquiry@Excelerate-Inc.com Email\\nThe parties may change their Point of Contact (POC) by written notice to the others.\\n14. The parties acknowledge and agree that due to the unique nature of the Proprietary Information, and breach of this agreement by the receiving party would cause irreparable harm to the disclosing party and that the disclosing party shall therefore be entitled to equitable relief in addition to all other remedies at law. The parties agree that, in the event of breach, or threatened breach of the terms of this agreement, the originating party may seek an injunction prohibiting such breach. Both parties hereby waiver any requirement to post bond for attempts to obtain such injunctive relief. Any such relief shall be in addition to and not in lieu of any appropriate relief in the way of monetary damages. Each party shall pay its own attorney’s fees, costs, and expenses for any dispute under the agreement.\\n15. This Agreement contains the full and complete understanding of the Parties with respect to the subject matter hereof and supersedes all prior representations and understandings, whether oral or written. This Agreement may not be modified in any manner except by written amendment executed by both Parties.\\n16. If any provision of this Agreement is found to be unenforceable, the remainder of this Agreement shall be enforced to the extent permitted by law.\\nIN WITNESS WHEREOF, the Parties have executed this Agreement in duplicate counterparts on the date(s) set forth below.\\nEXCELERATE, INC. SECOND PARTY\\nMichael K. Doubleday Name\\nPresident & CEO Title\\nDate Date\\n',\n",
       " 'output': 'Entailment'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Receiving Party shall not reverse engineer any objects which embody Disclosing Party's Confidential Information.\\n\\nNON-DISCLOSURE AND CONFIDENTIALITY AGREEMENT\\nThis NON-DISCLOSURE AND CONFIDENTIALITY AGREEMENT (“Agreement”) is made by and between:\\n(i) the Office of the United Nations High Commissioner for Refugees, having its headquarters located at 94 rue de Montbrillant, 1202 Geneva, Switzerland (hereinafter “UNHCR” or the “Discloser”); and\\n(ii) ________________________ , a company established in accordance with the laws of ________________________ and having its principal offices located at ________________________________________________ (hereinafter the “Bidder” or the “Recipient”).\\nThe Discloser and Recipient are also referred to collectively as the “Parties” and individually as a “Party”.\\nRECITALS\\nWHEREAS in connection with RFP/2014/620, Request for Proposal for the provision Off-the-shelf Soft-skill, IT Online and HR specific E-learning Courses (the “RFP”), it is advantageous to share certain data and information with the Bidder participating in the RFP;\\nWHEREAS UNHCR agrees to provide such data and information to the Bidder for the sole purpose of preparing its Proposal under said RFP;\\nWHEREAS the Bidder is willing to ensure that UNHCR’s data and information will be held in strict confidence and only used for the permitted purpose;\\nNOW, THEREFORE, the Parties agree as follows:\\n1. “Confidential Information”, whenever used in this Agreement, shall mean any data, document, specification and other information or material, that is delivered or disclosed by UNHCR to the Recipient in any form whatsoever, whether orally, visually in writing or otherwise (including computerized form), and that, at the time of disclosure to the Recipient, is designated as confidential.\\n2. The Confidential Information that is delivered or otherwise disclosed by the Discloser to the Recipient shall be held in trust and confidence by the Recipient and shall be handled as follows:\\n2.1 The Recipient shall use the same care and discretion to avoid disclosure, publication or dissemination of the Confidential Information as it uses with its own similar information that it does not wish to disclose, publish or disseminate;\\n2.2 The Recipient shall use the Confidential Information solely for the purpose for which it was disclosed;\\n2.3 Provided that the Recipient has a written agreement with the following persons or entities requiring them to treat the Confidential Information in accordance with this Agreement, the Recipient may disclose the Confidential Information to:\\n2.3.1 Any other party with the Discloser’s prior written consent; and\\n2.3.2 the Recipient’s employees, officials, representatives and agents who have a strict need to know the contents of the Confidential Information, and employees, officials, representatives and agents of any legal entity that it controls, controls it, or with which it is under common control, who have a similar need to know the contents of the Confidential Information, provided that, for these purposes a controlled legal entity means:\\n2.3.2.1 a corporate entity in which the Party owns or otherwise controls, whether directly or indirectly, over fifty percent (50%) of voting shares thereof; or,\\n2.3.2.2 any entity over which the Party exercises effective managerial control; or,\\n2.3.2.3 for UNHCR, a principal or subsidiary organ of the United Nations established in accordance with the Charter of the United Nations.\\n2.4 The Recipient may disclose the Confidential Information to the extent required by law, provided that, subject to and without any waiver of the privileges and immunities of UNHCR, the Recipient will give UNHCR sufficient prior notice of a request for the disclosure of the Confidential Information in order to allow UNHCR to have a reasonable opportunity to take protective measures or such other action as may be appropriate before any such disclosure is made.\\n2.5 The Recipient shall not be precluded from disclosing the Confidential Information that is (i) obtained by the Recipient without restriction from a third party who is not in breach of any obligation as to confidentiality to the owner of such Confidential Information or any other person, or (ii) disclosed by the Discloser to a third party without any obligation of confidentiality, or (iii) previously known by the Recipient, or (iv) at any time is developed by the Recipient completely independently of any disclosures hereunder.\\n2.6 The Recipient will not copy or reproduce the Confidential Information except as reasonably required for the purposes contemplated in this Agreement, and will ensure that any confidentiality or other proprietary rights notices on the Confidential Information are reproduced on all copies.\\n3. The Recipient acknowledges that UNHCR hereto makes no any representation or warranty, express or implied, as to the accuracy or completeness of the Confidential Information.\\n4. Nothing in this Agreement is to be construed as granting the Recipient, by implication or otherwise, any right whatsoever with respect to the Confidential Information or part thereof.\\n5. All Confidential Information in any form and any medium, including all copies thereof, disclosed to the Recipient shall be returned to UNHCR or destroyed: (a) if a business relationship is not entered into with UNHCR on or before the date which is three (3) months after the date both Parties have signed the Agreement; or (b) promptly upon request by the UNHCR at any time.\\n6. The Recipient agrees to indemnify UNHCR in respect of any expenses, losses, damages, costs, claims or liability UNHCR may suffer or incur as a result of an act or omission by the Recipient or its employees, consultants and agents in connection with the Confidential Information and the Recipient’s obligations under this Agreement.\\n7. Nothing in this Agreement shall be construed as obligating any Party to continue any discussions or to enter into a business relationship.\\n8. This Agreement shall enter into force on the date it is signed by both Parties. Either Party may terminate the working relationship contemplated by this Agreement by providing written notice to the other, provided, however, that the obligations and restrictions hereunder regarding the Confidential Information shall remain effective following any such termination or any other termination or expiration of this Agreement.\\n9. Any dispute, controversy or claim between the Parties arising out of, this Agreement or the breach, termination or invalidity thereof, unless settled amicably within twenty (20) days after receipt by one Party of the other Party's request for such amicable settlement, shall be referred by either Party to arbitration in accordance with the UNCITRAL Arbitration Rules then obtaining, including provisions on applicable law. The arbitral tribunal shall have no authority to award punitive damages. In addition, unless otherwise expressly provided in this Agreement, the arbitral tribunal shall have no authority to award interest. The Parties shall be bound by any arbitration award rendered as a result of such arbitration as the final adjudication of any such controversy, claim or dispute.\\n10. Nothing in or relating to this Agreement shall be deemed a waiver, express or implied, of any of the privileges and immunities of the United Nations, including UNHCR as its subsidiary organ.\\n11. The Recipient shall not advertise or otherwise make public the fact that it has a confidential relationship with UNHCR, nor shall the Recipient, in any manner whatsoever use the name, emblem, or official seal of the United Nations or UNHCR, or any abbreviation of the name of the United Nations or UNHCR in connection with its business or otherwise.\\n12. If any provision of this Agreement shall be held to be invalid, illegal or unenforceable, the validity, legality and enforceability of the remaining provisions shall not in any way be affected or impaired.\\n13. This Agreement constitutes the entire agreement concerning the subject matter hereof above and supersedes all prior representations, agreements and understandings, whether written or oral, by and between the Parties on the subject hereof.\\n14. The Parties acknowledge and agree that their representatives who have signed this Agreement had full authority to do so and to fully bind the Party being represented by doing so.\\nIN WITNESS WHEREOF, the Parties, acting through their authorized representatives, have caused this Agreement to be signed on the dates set forth below:\\nFor and on behalf of UNHCR: For and on behalf of the Bidder:\\n________________________ ________________________\\n Name: Name:\\nTitle: Title:\\n Date: Date:\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = [dataset['train'][i]['input'].find('\\n\\n\\n') for i in range(500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 40., 104., 115., 131.,  85.,  18.,   4.,   2.,   0.,   1.]),\n",
       " array([ 29. , 119.2, 209.4, 299.6, 389.8, 480. , 570.2, 660.4, 750.6,\n",
       "        840.8, 931. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('quality_example.txt', 'r') as f:\n",
    "    t = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.where(torch.zeros(10) == 0)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = t[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [363, 656, 2846, 2370, 77, 46, 91, 3299, 16, 8, 915, 239, 58, 41, 188, 61, 216, 19, 231, 2749, 145, 8, 880, 13, 8, 2074, 5, 41, 279, 61, 216, 9460, 7, 126, 2673, 24, 228, 1172, 112, 533, 5, 41, 254, 61, 978, 809, 19, 341, 1676, 6, 11, 3, 88, 2620, 614, 161, 5, 41, 308, 61, 216, 341, 2112, 7, 9832, 11, 701, 4820, 114, 8, 2045, 1605, 787, 12, 925, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(slice[:d_pos + dot_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_pos = slice.find('(D)')\n",
    "dot_pos = slice[d_pos:].find('.')\n",
    "slice[d_pos + dot_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ' '.join(t.split(' ')[:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer.encode(t, max_length=1024, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1], [1], [41, 308, 61, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('\\n\\n\\n'), tokenizer.encode(''), tokenizer.encode('(D)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_id = tokenizer.encode('.')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized.index(dot_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[363,\n",
       " 656,\n",
       " 2846,\n",
       " 2370,\n",
       " 77,\n",
       " 46,\n",
       " 91,\n",
       " 3299,\n",
       " 16,\n",
       " 8,\n",
       " 915,\n",
       " 239,\n",
       " 58,\n",
       " 41,\n",
       " 188,\n",
       " 61,\n",
       " 216,\n",
       " 19,\n",
       " 231,\n",
       " 2749,\n",
       " 145,\n",
       " 8,\n",
       " 880,\n",
       " 13,\n",
       " 8,\n",
       " 2074,\n",
       " 5,\n",
       " 41,\n",
       " 279,\n",
       " 61,\n",
       " 216,\n",
       " 9460,\n",
       " 7,\n",
       " 126,\n",
       " 2673,\n",
       " 24,\n",
       " 228,\n",
       " 1172,\n",
       " 112,\n",
       " 533,\n",
       " 5,\n",
       " 41,\n",
       " 254,\n",
       " 61,\n",
       " 978,\n",
       " 809,\n",
       " 19,\n",
       " 341,\n",
       " 1676,\n",
       " 6]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4829"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 4), (3, 3), (2, 2), (1, 1), (0, 0)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(range(5)))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/booydar/Desktop/MIPT/memory_experiments/debug_t5.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug_t5.ipynb#ch0000009?line=0'>1</a>\u001b[0m tokenizer\u001b[39m.\u001b[39mdecode(tokenized)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'was grinning inwardly. There wasn\\'t anything they could do. He had them now. He had enough Basic to keep him comfortably, by his standards, for the rest of his life. He was never going to subject himself to space cafard again. Just thinking about it, now, set the tic to going at the side of his mouth. They could count down and blast off, for all he gave a damn. The gold watch idea had been that of Lofting Gubelin, which was typical, he being in the way of a living anachronism himself. In fact, Academician Gubelin was possibly the only living man on North America who still wore spectacles. His explanation was that a phobia against having his eyes touched prohibited either surgery to remould his eyeballs and cure his myopia, or contact lenses. That was only an alibi so far as his closest associate, Hans Girard-Perregaux, was concerned. Doctor Girard-Perregaux was convinced Gubelin would have even worn facial hair, had he but a touch more courage. Gubelin longed for yesteryear, a seldom found phenomenon under the Ultrawelfare State. Slumped in an autochair in the escape room of his Floridian home, Lofting Gubelin scowled at his friend. He said, acidly, \"Any more bright schemes, Hans? I presume you now acknowledge that appealing to the cloddy\\'s patriotism, sentiment and desire for public acclaim have miserably failed.\" Girard-Perregaux said easily, \"I wouldn\\'t call Seymour Pond a cloddy. In his position, I am afraid I would do the same thing he has.\" \"That\\'s nonsense, Hans. Zoroaster! Either you or I would gladly take Pond\\'s place were we capable of performing the duties for which he has been trained. There aren\\'t two men on North America—there aren\\'t two men in the world!—who better realize the urgency of continuing our delving into space.\" Gubelin snapped his fingers. \"Like that, either of us would give our lives to prevent man from completely abandoning the road to his destiny.\" His friend said drily, \"Either of us could have volunteered for pilot training forty years ago, Lofting. We didn\\'t.\" \"At that time there wasn\\'t such a blistering percentage of funkers throughout this whole blistering Ultrawelfare State! Who could foresee that eventually our whole program would face ending due to lack of courageous young men willing to take chances, willing to face adventure, willing to react to the stimulus of danger in the manner our ancestors did?\" Girard-Perregaux grunted his sarcasm and dialed a glass of iced tea and tequila. He said, \"Nevertheless, both you and I conform with the present generation in finding it far more pleasant to follow one\\'s way of life in the comfort of one\\'s home than to be confronted with the unpleasantness of facing nature\\'s dangers in more adventurous pastimes.\" Gubelin, half angry at his friend\\'s argument, leaned forward to snap rebuttal, but the other was wagging a finger at him negatively. \"Face reality, Lofting. Don\\'t require or expect from Seymour Pond more than is to be found there. He is an average young man. Born in our Ultrawelfare State, he was guaranteed his fundamental'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(t.split(' ')[512:1024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "# pretrained_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model.encoder.block[0].layer[0].SelfAttention.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model.model.encoder.embed_tokens.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50265, 768, padding_idx=1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.model.shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0396,  0.1533, -0.0428,  ..., -0.0316,  0.1415,  0.0479],\n",
       "        [ 0.0189, -0.0148,  0.1171,  ...,  0.0793,  0.0303,  0.0619],\n",
       "        [ 0.0919,  0.0036,  0.0454,  ..., -0.0061,  0.0180, -0.0290],\n",
       "        ...,\n",
       "        [ 0.0406,  0.0953,  0.0361,  ..., -0.0677, -0.0262, -0.0538],\n",
       "        [ 0.0719,  0.1156, -0.0101,  ...,  0.0600,  0.0103, -0.0804],\n",
       "        [ 0.1002, -0.1005, -0.0149,  ..., -0.0311,  0.0263,  0.0912]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.model.encoder.layers[0].self_attn.k_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0125,  0.0014, -0.0096,  ...,  0.0022,  0.1057,  0.0103],\n",
       "        [-0.0114, -0.0169, -0.0184,  ..., -0.0131, -0.0043, -0.0053],\n",
       "        [ 0.0842, -0.0389,  0.0096,  ...,  0.0583,  0.0082,  0.0357],\n",
       "        ...,\n",
       "        [ 0.0141, -0.0241, -0.0207,  ..., -0.0046,  0.0241, -0.0071],\n",
       "        [ 0.0071, -0.0391, -0.0273,  ...,  0.0055,  0.0131, -0.0042],\n",
       "        [ 0.0012,  0.0042, -0.0230,  ...,  0.0065, -0.0083,  0.0180]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.model.encoder.get_input_embeddings().weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseReluDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset yelp_review_full (/home/booydar/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n",
      "100%|██████████| 2/2 [00:00<00:00, 13.59it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(31230, 768)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.encoder.resize_token_embeddings(31230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(31230, 768)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.encoder.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love rats.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode_plus('I love rats.', add_special_tokens=True, padding='max_length', max_length=20)['input_ids']\n",
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 333, 20063, 5, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_segments = 2\n",
    "num_mem_tokens = 10\n",
    "\n",
    "tokenizer.model_max_length  = (tokenizer.model_max_length - num_mem_tokens) * num_segments\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/booydar/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-b83c89ca994b964f.arrow\n",
      "Loading cached processed dataset at /home/booydar/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-79cae018cf33be2d.arrow\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/booydar/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-13eb503c718d339c.arrow\n",
      "Loading cached shuffled indices for dataset at /home/booydar/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-0de892a126228ab3.arrow\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(small_eval_dataset)\n",
    "src = next(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo\n",
    "* mb store smth for bptt?\n",
    "* does altering embedding weights parameter work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from typing import List, Optional, Tuple, Union\n",
    "# from transformers import BertForSequenceClassification\n",
    "\n",
    "# class RMBertForSequenceClassification(BertForSequenceClassification):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "\n",
    "#     def set_mem_tokens(self, num_mem_tokens):\n",
    "#         if num_mem_tokens is not None:\n",
    "#             embedding_weights = self.bert.embeddings.word_embeddings.weight.data\n",
    "#             embedding_weights = F.pad(embedding_weights, (0, 0, 0, num_mem_tokens), \"constant\", 0)\n",
    "#             embedding_weights[-num_mem_tokens:] = torch.rand(num_mem_tokens, *embedding_weights.shape[1:]) * 2 - 0.5\n",
    "#             self.bert.embeddings.word_embeddings.weight.data = embedding_weights\n",
    "            \n",
    "#             if self.bert.embeddings.word_embeddings.weight.grad is not None:\n",
    "#                 embedding_grad = self.bert.embeddings.word_embeddings.weight.grad.data\n",
    "#                 embedding_grad = F.pad(embedding_grad, (0, 0, 0, num_mem_tokens), \"constant\", 0)\n",
    "#                 self.bert.embeddings.word_embeddings.grad.data = embedding_grad\n",
    "\n",
    "\n",
    "#     def __call__(self, **kwargs):\n",
    "#         seg_num = 2\n",
    "#         segmented_kwargs = {}\n",
    "#         for arg, value in kwargs.items():\n",
    "#             if isinstance(value, torch.Tensor) and value.ndim > 1:\n",
    "#                 segmented_kwargs[arg] = torch.chunk(value, seg_num, dim=-1) \n",
    "#                 print(segmented_kwargs[arg][0].shape)\n",
    "#             else:\n",
    "#                 segmented_kwargs[arg] = [value] * seg_num\n",
    "\n",
    "#         for seg_values in list(zip(*segmented_kwargs.values())):\n",
    "#             seg_kwargs = dict(zip(segmented_kwargs.keys(), seg_values))\n",
    "#             print('forwarding')\n",
    "#             # self.forward( **seg_kwargs)\n",
    "#         return self.forward( **seg_kwargs)\n",
    "\n",
    "# def __call__(self, return_memory=False, **kwargs):\n",
    "#         if self.memory is None:\n",
    "#             self.set_memory()\n",
    "\n",
    "#         segmented_kwargs = {}\n",
    "#         for arg, value in kwargs.items():\n",
    "#             if isinstance(value, torch.Tensor) and value.ndim > 1:\n",
    "#                 segmented_kwargs[arg] = torch.chunk(value, self.n_segment, dim=-1) \n",
    "#                 # print(segmented_kwargs[arg][0].shape)\n",
    "#             else:\n",
    "#                 segmented_kwargs[arg] = [value] * self.n_segment\n",
    "\n",
    "#         # print('segmented_kwargs ', [(i,len(j), j[0].shape) for i,j in segmented_kwargs.items()])\n",
    "#         for seg_values in list(zip(*segmented_kwargs.values())):\n",
    "#             seg_kwargs = dict(zip(segmented_kwargs.keys(), seg_values))\n",
    "            \n",
    "            \n",
    "#             if seg_kwargs['input_ids'] is not None:\n",
    "#                 input_embeds = self.net.embeddings.word_embeddings(seg_kwargs.pop('input_ids'))\n",
    "#                 # print('input_embeds.shape ', input_embeds.shape)\n",
    "#             else:\n",
    "#                 input_embeds = seg_kwargs.pop('input_embeds')\n",
    "            \n",
    "#             if self.memory.ndim == 2:\n",
    "#                 self.memory = self.memory.repeat(input_embeds.shape[0], 1, 1)\n",
    "#             seg_kwargs['inputs_embeds'] = torch.hstack((self.memory, input_embeds))\n",
    "            \n",
    "#             type_ids = seg_kwargs['token_type_ids']\n",
    "#             type_ids_pad = torch.ones(type_ids.shape[0], self.num_mem_tokens, dtype=type_ids.dtype, device=type_ids.device)\n",
    "#             seg_kwargs['token_type_ids'] = torch.hstack((type_ids_pad, type_ids))\n",
    "\n",
    "#             attn_mask = seg_kwargs['attention_mask']\n",
    "#             attn_mask_pad = torch.ones(attn_mask.shape[0], self.num_mem_tokens, dtype=attn_mask.dtype, device=attn_mask.device)\n",
    "#             seg_kwargs['attention_mask'] = torch.hstack((attn_mask_pad, attn_mask))\n",
    "\n",
    "#             # print(seg_kwargs)\n",
    "#             out = self.model.forward(**seg_kwargs)\n",
    "#             print('Out ', out, out.hidden_states[0].shape, out.hidden_states[0][:self.num_mem_tokens].shape)\n",
    "#             print('self memory ', self.memory.shape)\n",
    "\n",
    "#         if return_memory:\n",
    "#             return out, self.memory\n",
    "#         self.reset_memory()\n",
    "        \n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.tokenize('Hello![SEP][MEM11]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import PreTrainedModel, AutoModelForSequenceClassification\n",
    "\n",
    "import math\n",
    "\n",
    "SEGMENTABLE = ['input_ids', 'inputs_embeds', 'token_type_ids', 'position_ids', 'attention_mask']\n",
    "PAD_ZEROS = ['token_type_ids', 'attention_mask']\n",
    "\n",
    "NET_ATTRIBUTE = 'bert'\n",
    "class RMTEncoderForSequenceClassification():\n",
    "    def __init__(self, config=None, base_model=None):\n",
    "        if config is not None:\n",
    "            self.model = AutoModelForSequenceClassification(config)\n",
    "        \n",
    "        if base_model is not None:\n",
    "            self.model = base_model\n",
    "\n",
    "        self.net = getattr(self.model, NET_ATTRIBUTE)\n",
    "        self.num_mem_tokens = 0\n",
    "\n",
    "\n",
    "    def from_pretrained(from_pretrained):\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(from_pretrained)\n",
    "        rmt = RMTEncoderForSequenceClassification(base_model=base_model)\n",
    "        return rmt\n",
    "        \n",
    "\n",
    "    def set_mem_tokens(self, num_mem_tokens):\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        self.extend_word_embeddings()\n",
    "\n",
    "\n",
    "    def set_memory(self, memory=None):\n",
    "        if memory is None:\n",
    "            mem_token_ids = self.mem_token_ids.to(device=self.device)\n",
    "            # print('mem_token_ids', mem_token_ids.shape)\n",
    "            memory = self.net.embeddings.word_embeddings(mem_token_ids)\n",
    "        return memory\n",
    "\n",
    "\n",
    "    def extend_word_embeddings(self):\n",
    "        vocab_size = self.net.embeddings.word_embeddings.weight.shape[0]\n",
    "        extended_vocab_size = vocab_size + self.num_mem_tokens + 1\n",
    "        self.mem_token_ids = torch.arange(vocab_size, vocab_size + self.num_mem_tokens)\n",
    "        self.pad_token_id = vocab_size + self.num_mem_tokens\n",
    "        self.net.resize_token_embeddings(extended_vocab_size)\n",
    "\n",
    "\n",
    "    def __call__(self, memory=None, return_memory=False, **kwargs):\n",
    "        # if self.num_mem_tokens == 0:\n",
    "        #     return self.model.forward(**kwargs)\n",
    "        \n",
    "        memory = self.set_memory(memory)\n",
    "        \n",
    "        segmented_kwargs = self.pad_and_segment(**kwargs)\n",
    "        for seg_kwargs in segmented_kwargs:\n",
    "            # print('mem_tokens gradient ', self.net.embeddings.word_embeddings.weight.shape, self.net.embeddings.word_embeddings.weight.grad[:, -self.num_mem_tokens:].sum(dim=0))\n",
    "            if seg_kwargs['input_ids'] is not None:\n",
    "                saved_input_ids = seg_kwargs['input_ids']\n",
    "                input_embeds = self.net.embeddings.word_embeddings(seg_kwargs.pop('input_ids'))\n",
    "            else:\n",
    "                input_embeds = seg_kwargs.pop('input_embeds')\n",
    "            \n",
    "            if memory.ndim == 2:\n",
    "                memory = memory.repeat(input_embeds.shape[0], 1, 1)\n",
    "            # print('inputs_embed shape', input_embeds.shape)\n",
    "            # print('memory shape', memory.shape)\n",
    "            seg_kwargs['inputs_embeds'] = torch.hstack((memory, input_embeds))\n",
    "            # print('seg kwargs keys', seg_kwargs.keys())\n",
    "            # print('seg_kwargs[inputs_embeds] shape', seg_kwargs['inputs_embeds'].shape)\n",
    "            \n",
    "            out = self.model.forward(**seg_kwargs, output_hidden_states=True)\n",
    "            memory = out.hidden_states[-1][:, :self.num_mem_tokens]\n",
    "\n",
    "        if return_memory:\n",
    "            return out, memory\n",
    "        # print(\"end of task\")\n",
    "        return out\n",
    "\n",
    "    def pad_and_segment(self, **kwargs):\n",
    "\n",
    "        context_size = self.net.embeddings.position_embeddings.weight.shape[0] - self.num_mem_tokens\n",
    "\n",
    "        if 'input_ids' in kwargs:\n",
    "            sequence_len = kwargs['input_ids'].shape[1]\n",
    "        elif 'input_embeds' in kwargs:\n",
    "            sequence_len = kwargs['input_embeds'].shape[1]\n",
    "        else:\n",
    "            raise(ValueError)\n",
    "\n",
    "        n_segments = math.ceil(sequence_len / context_size)\n",
    "        # print('sequence_len ', sequence_len)\n",
    "        # print('context_size ', context_size)\n",
    "        # print(f'split to {n_segments} segments')\n",
    "\n",
    "        segmented_kwargs = {}\n",
    "        for label, value in kwargs.items():\n",
    "            if label in SEGMENTABLE and isinstance(value, torch.Tensor):\n",
    "                pad_length = n_segments * context_size - value.shape[1]\n",
    "                padded_value = self.pad(label, value, pad_length)\n",
    "\n",
    "                segmented_value = list(torch.chunk(padded_value, n_segments, dim=1) )\n",
    "                \n",
    "                if label not in {'input_ids', 'input_embeds'}:\n",
    "                    for i, seg in enumerate(segmented_value):\n",
    "                        segmented_value[i] = self.pad(label, seg, self.num_mem_tokens)\n",
    "                segmented_kwargs[label] = segmented_value\n",
    "            else:\n",
    "                segmented_kwargs[label] = [value] * n_segments\n",
    "        \n",
    "        segmented_kwargs = [dict(zip(segmented_kwargs.keys(), seg_values)) for seg_values in zip(*segmented_kwargs.values())]\n",
    "        # print('segmented_kwargs', segmented_kwargs)\n",
    "        return segmented_kwargs\n",
    "\n",
    "\n",
    "    def pad(self, label, value, pad_length):\n",
    "        # if pad_length == 0:\n",
    "        #     return value\n",
    "\n",
    "        if label in PAD_ZEROS:\n",
    "            pad_value = 0\n",
    "        elif label == 'input_ids':\n",
    "            pad_value = self.pad_token_id\n",
    "        elif label == 'input_embeds':\n",
    "            pad_value = self.net.embeddings.word_embeddings(self.pad_token_id.to(device=self.device))\n",
    "\n",
    "        padded_value = F.pad(value, (pad_length, 0, 0, 0), \"constant\", pad_value)\n",
    "        return padded_value\n",
    "\n",
    "\n",
    "    def to(self, device):\n",
    "        self.model = self.model.to(device)\n",
    "        \n",
    "    \n",
    "    def cuda(self):\n",
    "        self.model.cuda()\n",
    "\n",
    "\n",
    "    def __getattr__(self, attribute):\n",
    "        return getattr(self.model, attribute)\n",
    "\n",
    "\n",
    "    def parameters(self, **kwargs):\n",
    "        return self.model.parameters(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kw = rmt.last_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kw.pop('labels')\n",
    "# kw.pop('inputs_embeds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt.model.generate(**rmt.last_kwargs, max_length=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.17k/1.17k [00:00<00:00, 216kB/s]\n",
      "Downloading: 100%|██████████| 850M/850M [02:00<00:00, 7.39MB/s] \n"
     ]
    }
   ],
   "source": [
    "pretrained_model = AutoModel.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32128, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model.encoder.embed_tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 768])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = pretrained_model.bert.embeddings.position_embeddings.weight.data.clone()\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0015,  0.0140,  0.0098],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0059,  0.0046, -0.0055],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0062,  0.0007, -0.0105],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0096,  0.0225,  0.0138],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0240, -0.0016, -0.0129],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0018, -0.0146, -0.0002]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.pad(t, (10, 0, 0, 0), \"constant\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.pad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretrained_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/booydar/Desktop/MIPT/memory_experiments/debug.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug.ipynb#ch0000024?line=0'>1</a>\u001b[0m rmt \u001b[39m=\u001b[39m RMTForSequenceClassification(pretrained_model, pretrained_model\u001b[39m.\u001b[39mbert, num_mem_tokens\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, n_segment\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pretrained_model' is not defined"
     ]
    }
   ],
   "source": [
    "rmt = RMTForSequenceClassification(pretrained_model, pretrained_model.bert, num_mem_tokens=10, n_segment=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.item>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ceil(torch.Tensor([1.35]))[0].item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(per_device_train_batch_size=2, per_device_eval_batch_size=1, output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", no_cuda=True, max_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "The following columns in the training set  don't have a corresponding argument in `RMTForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RMTForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels torch.Size([2])\n",
      "token_type_ids torch.Size([2, 512])\n",
      "attention_mask torch.Size([2, 512])\n",
      "inputs_embeds torch.Size([2, 512, 768])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/booydar/Desktop/MIPT/memory_experiments/debug.ipynb Cell 29'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug.ipynb#ch0000025?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug.ipynb#ch0000025?line=1'>2</a>\u001b[0m     model\u001b[39m=\u001b[39mrmt,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug.ipynb#ch0000025?line=2'>3</a>\u001b[0m     \u001b[39m# model=classic_bert,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug.ipynb#ch0000025?line=6'>7</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug.ipynb#ch0000025?line=7'>8</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug.ipynb#ch0000025?line=8'>9</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py:1400\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1397'>1398</a>\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1398'>1399</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1399'>1400</a>\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1401'>1402</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1402'>1403</a>\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1403'>1404</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1404'>1405</a>\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1405'>1406</a>\u001b[0m ):\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1406'>1407</a>\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1407'>1408</a>\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py:2002\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1999'>2000</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=2000'>2001</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=2001'>2002</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=2003'>2004</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=rmt,\n",
    "    # model=classic_bert,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = SequentialSampler(small_train_dataset)\n",
    "dl = DataLoader(small_train_dataset, sampler=sampler, batch_size=4)\n",
    "gen = dl.__iter__()\n",
    "s = next(gen)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15ebdd31b1273fe4d2b1fe1822219a570cf61693f7cab545dbe286c10cf9691f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('dpenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
