{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import PreTrainedModel, AutoModelForSequenceClassification\n",
    "\n",
    "import math\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import BertForSequenceClassification\n",
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# from modeling_rmt import RMTEncoderForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class RMTEncoderForSequenceClassification():\n",
    "    def __init__(self, config=None, base_model=None, **kwargs):\n",
    "        if config is not None:\n",
    "            self.model = AutoModelForSequenceClassification(config, **kwargs)\n",
    "        \n",
    "        if base_model is not None:\n",
    "            self.model = base_model\n",
    "\n",
    "\n",
    "    def from_pretrained(from_pretrained, **kwargs):\n",
    "        base_model = AutoModelForSequenceClassification.from_pretrained(from_pretrained, **kwargs)\n",
    "        rmt = RMTEncoderForSequenceClassification(base_model=base_model)\n",
    "        return rmt\n",
    "        \n",
    "\n",
    "    def set_params(self, \n",
    "                drop_empty_segments=True,\n",
    "                sum_loss=False,\n",
    "                input_size=None, \n",
    "                input_seg_size=None, \n",
    "                backbone_cls=None,\n",
    "                num_mem_tokens=0, \n",
    "                bptt_depth=-1, \n",
    "                pad_token_id=0, \n",
    "                eos_token_id=1,\n",
    "                cls_token_id=101, \n",
    "                sep_token_id=102):\n",
    "        if input_size is not None:\n",
    "            self.input_size = input_size\n",
    "        else:\n",
    "            self.input_size =  self.base_model.embeddings.position_embeddings.weight.shape[0]\n",
    "        self.input_seg_size = input_seg_size\n",
    "\n",
    "        self.bptt_depth = bptt_depth\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.cls_token = torch.tensor([cls_token_id])\n",
    "        self.sep_token = torch.tensor([sep_token_id])\n",
    "        self.num_mem_tokens = num_mem_tokens\n",
    "        self.drop_empty_segments = drop_empty_segments\n",
    "        self.sum_loss = sum_loss\n",
    "        self.extend_word_embeddings()\n",
    "\n",
    "\n",
    "    def set_memory(self, memory=None):\n",
    "        if memory is None:\n",
    "            mem_token_ids = self.mem_token_ids.to(device=self.device)\n",
    "            memory = self.base_model.embeddings.word_embeddings(mem_token_ids)\n",
    "        return memory\n",
    "    \n",
    "    def extend_word_embeddings(self):\n",
    "        vocab_size = self.base_model.embeddings.word_embeddings.weight.shape[0]\n",
    "        extended_vocab_size = vocab_size + self.num_mem_tokens\n",
    "        self.mem_token_ids = torch.arange(vocab_size, vocab_size + self.num_mem_tokens)\n",
    "        self.base_model.resize_token_embeddings(extended_vocab_size)\n",
    "\n",
    "\n",
    "    def __call__(self, input_ids, **kwargs):\n",
    "        memory = self.set_memory()\n",
    "        segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "        outputs = []\n",
    "        for seg_num, segment_data in enumerate(zip(*segmented)):\n",
    "            input_ids, attention_mask, token_type_ids = segment_data\n",
    "            if memory.ndim == 2:\n",
    "                memory = memory.repeat(input_ids.shape[0], 1, 1)\n",
    "            if (self.bptt_depth > -1) and (len(segmented) - seg_num > self.bptt_depth): \n",
    "                memory = memory.detach()\n",
    "\n",
    "            seg_kwargs = dict(**kwargs)\n",
    "            if self.drop_empty_segments:\n",
    "\n",
    "                non_empty_mask = [not torch.equal(input_ids[i], self.empty) for i in range(len(input_ids))]\n",
    "                if sum(non_empty_mask) == 0:\n",
    "                    continue\n",
    "                input_ids = input_ids[non_empty_mask]\n",
    "                attention_mask = attention_mask[non_empty_mask]\n",
    "                token_type_ids = token_type_ids[non_empty_mask]\n",
    "                seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "\n",
    "                inputs_embeds = self.base_model.embeddings.word_embeddings(input_ids)\n",
    "                inputs_embeds[:, 1:1+self.num_mem_tokens] = memory[non_empty_mask]\n",
    "            else:\n",
    "                inputs_embeds = self.base_model.embeddings.word_embeddings(input_ids)\n",
    "                inputs_embeds[:, 1:1+self.num_mem_tokens] = memory\n",
    "\n",
    "            seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "            seg_kwargs['attention_mask'] = attention_mask\n",
    "            seg_kwargs['token_type_ids'] = token_type_ids\n",
    "            \n",
    "            out = self.model.forward(**seg_kwargs, output_hidden_states=True)\n",
    "            outputs.append(out)\n",
    "\n",
    "            if self.drop_empty_segments:\n",
    "                memory[non_empty_mask] = out.hidden_states[-1][:, :self.num_mem_tokens]\n",
    "            else:\n",
    "                memory = out.hidden_states[-1][:, :self.num_mem_tokens]\n",
    "\n",
    "        if self.sum_loss:\n",
    "            out['loss'] = torch.stack([o['loss'] for o in outputs]).sum(dim=-1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def pad_and_segment(self, input_ids):\n",
    "        \n",
    "        sequence_len = input_ids.shape[1]\n",
    "        input_seg_size = self.input_size - self.num_mem_tokens - 3 \n",
    "        if self.input_seg_size is not None and self.input_seg_size < input_seg_size:\n",
    "            input_seg_size = self.input_seg_size\n",
    "            \n",
    "        n_segments = math.ceil(sequence_len / input_seg_size)\n",
    "\n",
    "        augmented_inputs = []\n",
    "        for input in input_ids:\n",
    "            input = input[input != self.pad_token_id][1:-1]\n",
    "\n",
    "            seg_sep_inds = [0] + list(range(len(input), 0, -input_seg_size))[::-1] # chunk so that first segment has various size\n",
    "            input_segments = [input[s:e] for s, e in zip(seg_sep_inds, seg_sep_inds[1:])]\n",
    "\n",
    "            def pad_add_special_tokens(tensor, seg_size):\n",
    "                tensor = torch.cat([self.cls_token.to(device=self.device),\n",
    "                                    self.mem_token_ids.to(device=self.device),\n",
    "                                    self.sep_token.to(device=self.device),\n",
    "                                    tensor.to(device=self.device),\n",
    "                                    self.sep_token.to(device=self.device)])\n",
    "                pad_size = seg_size - tensor.shape[0]\n",
    "                if pad_size > 0:\n",
    "                    tensor = F.pad(tensor, (0, pad_size))\n",
    "                return tensor\n",
    "\n",
    "            input_segments = [pad_add_special_tokens(t, self.input_size) for t in input_segments]\n",
    "            empty = torch.Tensor([]).int()\n",
    "            self.empty = pad_add_special_tokens(empty, self.input_size)\n",
    "            empty_segments = [self.empty for i in range(n_segments - len(input_segments))]\n",
    "            input_segments = empty_segments + input_segments\n",
    "\n",
    "            augmented_input = torch.cat(input_segments)\n",
    "            augmented_inputs.append(augmented_input)\n",
    "            \n",
    "        augmented_inputs = torch.stack(augmented_inputs)\n",
    "        attention_mask = torch.ones_like(augmented_inputs)\n",
    "        attention_mask[augmented_inputs == self.pad_token_id] = 0\n",
    "\n",
    "        token_type_ids = torch.zeros_like(attention_mask)\n",
    "\n",
    "        input_segments = torch.chunk(augmented_inputs, n_segments, dim=1)\n",
    "        attention_mask = torch.chunk(attention_mask, n_segments, dim=1)\n",
    "        token_type_ids = torch.chunk(token_type_ids, n_segments, dim=1)\n",
    "    \n",
    "        return input_segments, attention_mask, token_type_ids\n",
    "\n",
    "\n",
    "    def to(self, device):\n",
    "        self.model = self.model.to(device)\n",
    "        \n",
    "    \n",
    "    def cuda(self):\n",
    "        self.model.cuda()\n",
    "\n",
    "\n",
    "    def __getattr__(self, attribute):\n",
    "        return getattr(self.model, attribute)\n",
    "\n",
    "\n",
    "    def parameters(self, **kwargs):\n",
    "        return self.model.parameters(**kwargs)\n",
    "\n",
    "    def named_parameters(self, **kwargs):\n",
    "        return self.model.named_parameters(**kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/bert_uncased_L-4_H-256_A-4\"\n",
    "# model_name = 'google/electra-base-discriminator'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_segments = 2\n",
    "num_mem_tokens = 10\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.model_max_length  = (tokenizer.model_max_length - num_mem_tokens) * num_segments\n",
    "tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea1ea2d201f4965b9cd61f71aba82e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/43.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-4_H-256_A-4 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-256_A-4 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "rmt = RMTEncoderForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt.set_params(\n",
    "                drop_empty_segments=True,\n",
    "                sum_loss=False,\n",
    "                input_size=None, \n",
    "                input_seg_size=None, \n",
    "                backbone_cls=None,\n",
    "                num_mem_tokens=0, \n",
    "                bptt_depth=-1, \n",
    "                pad_token_id=0, \n",
    "                eos_token_id=1,\n",
    "                cls_token_id=101, \n",
    "                sep_token_id=102)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = 512\n",
    "target_seq_len = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_plus_kwargs = {'max_length': input_seq_len,\n",
    "                              'truncation': True,\n",
    "                              'padding': 'longest',\n",
    "                              'pad_to_multiple_of': 64}\n",
    "generate_kwargs = {}\n",
    "labels_map = {'Contradiction': 0, 'Entailment': 1, 'Not mentioned': 2}\n",
    "num_labels = len(labels_map)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # cut too long strings because they may slow down tokenization\n",
    "    inputs = [b['input'][:input_seq_len * 10] for b in batch]\n",
    "    labels = [b['output'][:target_seq_len * 10] for b in batch]\n",
    "    features = tokenizer.batch_encode_plus(list(inputs), return_tensors='pt', **encode_plus_kwargs)\n",
    "    labels = np.array([labels_map[t] for t in labels])\n",
    "    features['labels'] = torch.from_numpy(labels)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n",
      "Reusing dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/contract_nli/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597f2608740b4f459dd21c5c0a42c6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_dataset('tau/scrolls', 'contract_nli')\n",
    "train_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_dataset,)\n",
    "kwargs = {'pin_memory': True, 'num_workers': 0}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(train_dataloader)\n",
    "sample = next(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Override forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n",
    "\n",
    "def _forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        output_hidden_states: Optional[bool] = False,\n",
    "        return_dict: Optional[bool] = True,\n",
    "        memory_storage = None\n",
    "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                # if use_cache:\n",
    "                #     logger.warning(\n",
    "                #         \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                #     )\n",
    "                #     use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                num_mem = memory_storage['num_mem_tokens']\n",
    "                if i in memory_storage:\n",
    "                    layer_memory = memory_storage[i]\n",
    "                    for j, h in enumerate(hidden_states):\n",
    "                        hidden_states[j][:layer_memory[j].shape[0]] = layer_memory[j]\n",
    "\n",
    "                print(f'hidden states shape: {len(hidden_states), hidden_states[0].shape}\\n memory storage:{memory_storage.keys()}')\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "                \n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if i in memory_storage:\n",
    "                print(f'replacing ms[i] {memory_storage[i][0][0][:10]}... to {[h[:num_mem] for h in hidden_states][0][0][:10]}')\n",
    "            memory_storage[i] = [h[:num_mem] for h in hidden_states]\n",
    "\n",
    "            # memory_storage['success'] = True\n",
    "            # print(f'Overrided method message: hidden states shape: {len(hidden_states), hidden_states[0].shape}\\n memory storage:{memory_storage}')\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import types\n",
    "self = rmt\n",
    "\n",
    "memory_storage = {'num_mem_tokens': 10}\n",
    "self.base_model.encoder.forward = types.MethodType(lambda *args, **kwargs: _forward(*args, **kwargs, memory_storage=memory_storage), self.base_model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  4909,  2283,  ...,  1017,  1012,   102],\n",
       "         [  101,  2070, 14422,  ...,  2025,  3024,   102]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'labels': tensor([1, 2])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pin_memory': True, 'num_workers': 0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = sample.copy()\n",
    "# kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states shape: (2, torch.Size([512, 256]))\n",
      " memory storage:dict_keys(['num_mem_tokens'])\n",
      "hidden states shape: (2, torch.Size([512, 256]))\n",
      " memory storage:dict_keys(['num_mem_tokens', 0])\n",
      "hidden states shape: (2, torch.Size([512, 256]))\n",
      " memory storage:dict_keys(['num_mem_tokens', 0, 1])\n",
      "hidden states shape: (2, torch.Size([512, 256]))\n",
      " memory storage:dict_keys(['num_mem_tokens', 0, 1, 2])\n",
      "hidden states shape: (2, torch.Size([512, 256]))\n",
      " memory storage:dict_keys(['num_mem_tokens', 0, 1, 2, 3])\n",
      "replacing ms[i] tensor([ 0.4910, -0.3252,  0.6379,  0.4366,  0.9436,  0.3721, -0.4503,  0.2484,\n",
      "        -5.6303,  0.0416], grad_fn=<SliceBackward>)... to tensor([ 0.5875, -0.4431,  0.4703,  0.1823,  0.2762, -0.0980, -0.7847, -0.3769,\n",
      "        -4.0026, -0.0786], grad_fn=<SliceBackward>)\n",
      "hidden states shape: (2, torch.Size([512, 256]))\n",
      " memory storage:dict_keys(['num_mem_tokens', 0, 1, 2, 3])\n",
      "replacing ms[i] tensor([ 0.4131, -0.2734,  0.5131,  0.4519,  0.8615, -0.4164, -0.4695,  0.1083,\n",
      "        -7.1332,  0.2477], grad_fn=<SliceBackward>)... to tensor([ 0.4684, -0.1915,  0.3924,  0.3985,  0.8635, -0.7359, -0.1106,  0.1712,\n",
      "        -8.9686,  0.3769], grad_fn=<SliceBackward>)\n",
      "hidden states shape: (2, torch.Size([512, 256]))\n",
      " memory storage:dict_keys(['num_mem_tokens', 0, 1, 2, 3])\n",
      "replacing ms[i] tensor([ 0.2953, -0.1400,  0.4266,  0.0419,  0.8344, -0.2630, -0.7411, -0.0187,\n",
      "        -3.9098, -0.2587], grad_fn=<SliceBackward>)... to tensor([-0.7230,  0.0042,  0.3788, -0.5297,  1.0977,  0.0802, -1.1541, -0.1407,\n",
      "        -2.3975, -0.3298], grad_fn=<SliceBackward>)\n",
      "hidden states shape: (2, torch.Size([512, 256]))\n",
      " memory storage:dict_keys(['num_mem_tokens', 0, 1, 2, 3])\n",
      "replacing ms[i] tensor([ 0.4336, -0.2249, -0.2261, -0.3031,  1.1261, -0.1303, -0.8440,  0.6503,\n",
      "        -3.5424,  0.2903], grad_fn=<SliceBackward>)... to tensor([ 0.4518, -0.6125,  0.3085, -0.5292,  1.3546, -0.4337, -0.9142,  0.5836,\n",
      "        -2.3023, -0.2561], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "input_ids = kwargs.pop('input_ids')\n",
    "\n",
    "memory = self.set_memory()\n",
    "segmented = self.pad_and_segment(input_ids)\n",
    "\n",
    "outputs = []\n",
    "for seg_num, segment_data in enumerate(zip(*segmented)):\n",
    "    input_ids, attention_mask, token_type_ids = segment_data\n",
    "    if memory.ndim == 2:\n",
    "        memory = memory.repeat(input_ids.shape[0], 1, 1)\n",
    "    if (self.bptt_depth > -1) and (len(segmented) - seg_num > self.bptt_depth): \n",
    "        memory = memory.detach()\n",
    "\n",
    "    seg_kwargs = dict(**kwargs)\n",
    "    if self.drop_empty_segments:\n",
    "\n",
    "        non_empty_mask = [not torch.equal(input_ids[i], self.empty) for i in range(len(input_ids))]\n",
    "        if sum(non_empty_mask) == 0:\n",
    "            continue\n",
    "        input_ids = input_ids[non_empty_mask]\n",
    "        attention_mask = attention_mask[non_empty_mask]\n",
    "        token_type_ids = token_type_ids[non_empty_mask]\n",
    "        seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "\n",
    "        inputs_embeds = self.base_model.embeddings.word_embeddings(input_ids)\n",
    "        inputs_embeds[:, 1:1+self.num_mem_tokens] = memory[non_empty_mask]\n",
    "    else:\n",
    "        inputs_embeds = self.base_model.embeddings.word_embeddings(input_ids)\n",
    "        inputs_embeds[:, 1:1+self.num_mem_tokens] = memory\n",
    "\n",
    "    seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "    seg_kwargs['attention_mask'] = attention_mask\n",
    "    seg_kwargs['token_type_ids'] = token_type_ids\n",
    "    \n",
    "    out = self.model.forward(**seg_kwargs, output_hidden_states=True)\n",
    "    outputs.append(out)\n",
    "\n",
    "    if self.drop_empty_segments:\n",
    "        memory[non_empty_mask] = out.hidden_states[-1][:, :self.num_mem_tokens]\n",
    "    else:\n",
    "        memory = out.hidden_states[-1][:, :self.num_mem_tokens]\n",
    "\n",
    "if self.sum_loss:\n",
    "    out['loss'] = torch.stack([o['loss'] for o in outputs]).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = self.model.forward(**seg_kwargs, output_hidden_states=True)\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out['hidden_states']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits', 'hidden_states'])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = forward(self.model, **seg_kwargs, output_hidden_states=True, use_cache=True)\n",
    "out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(per_device_train_batch_size=2, per_device_eval_batch_size=1, output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", no_cuda=True, max_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented_inputs = np.load('augmented_inputs.npy', allow_pickle=True)\n",
    "# attn_masks = np.load('attention_masks.npy', allow_pickle=True)\n",
    "# tokenizer.decode(augmented_inputs[0][512:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "The following columns in the training set  don't have a corresponding argument in `RMTEncoderForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RMTEncoderForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[  101, 29206, 29207,  ...,     0,     0,     0],\n",
      "        [  101, 29206, 29207,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [21:23<?, ?it/s]\n",
      "  0%|          | 0/5 [20:56<?, ?it/s]\n",
      "  0%|          | 0/5 [14:10<?, ?it/s]\n",
      "  0%|          | 0/5 [11:58<?, ?it/s]\n",
      "  0%|          | 0/5 [07:14<?, ?it/s]\n",
      "  0%|          | 0/5 [06:41<?, ?it/s]\n",
      "  0%|          | 0/5 [04:25<?, ?it/s]\n",
      "  0%|          | 0/5 [03:41<?, ?it/s]\n",
      "  0%|          | 0/5 [02:55<?, ?it/s]\n",
      "  0%|          | 0/5 [02:18<?, ?it/s]\n",
      "  0%|          | 0/5 [01:49<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/booydar/Desktop/MIPT/memory_experiments/debug_rmt.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug_rmt.ipynb#ch0000035?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug_rmt.ipynb#ch0000035?line=1'>2</a>\u001b[0m     model\u001b[39m=\u001b[39mrmt,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug_rmt.ipynb#ch0000035?line=2'>3</a>\u001b[0m     \u001b[39m# model=classic_bert,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug_rmt.ipynb#ch0000035?line=6'>7</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug_rmt.ipynb#ch0000035?line=7'>8</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug_rmt.ipynb#ch0000035?line=8'>9</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py:1400\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1397'>1398</a>\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1398'>1399</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1399'>1400</a>\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1401'>1402</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1402'>1403</a>\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1403'>1404</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1404'>1405</a>\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1405'>1406</a>\u001b[0m ):\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1406'>1407</a>\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1407'>1408</a>\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py:1984\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1980'>1981</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1982'>1983</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocast_smart_context_manager():\n\u001b[0;32m-> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1983'>1984</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1985'>1986</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=1986'>1987</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py:2016\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=2013'>2014</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=2014'>2015</a>\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=2015'>2016</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=2016'>2017</a>\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=2017'>2018</a>\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/trainer.py?line=2018'>2019</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/home/booydar/Desktop/MIPT/memory_experiments/debug_rmt.ipynb Cell 16'\u001b[0m in \u001b[0;36mRMTEncoderForSequenceClassification.__call__\u001b[0;34m(self, input_ids, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug_rmt.ipynb#ch0000018?line=73'>74</a>\u001b[0m     seg_kwargs[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m attention_mask\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug_rmt.ipynb#ch0000018?line=74'>75</a>\u001b[0m     seg_kwargs[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m token_type_ids\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug_rmt.ipynb#ch0000018?line=76'>77</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mseg_kwargs, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug_rmt.ipynb#ch0000018?line=77'>78</a>\u001b[0m     memory \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mhidden_states[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][:, :\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_mem_tokens]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/booydar/Desktop/MIPT/memory_experiments/debug_rmt.ipynb#ch0000018?line=79'>80</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1545\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1536'>1537</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1537'>1538</a>\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1538'>1539</a>\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1539'>1540</a>\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1540'>1541</a>\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1541'>1542</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1542'>1543</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1544'>1545</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1545'>1546</a>\u001b[0m     input_ids,\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1546'>1547</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1547'>1548</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1548'>1549</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1549'>1550</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1550'>1551</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1551'>1552</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1552'>1553</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1553'>1554</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1554'>1555</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1556'>1557</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1558'>1559</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=986'>987</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=988'>989</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=989'>990</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=990'>991</a>\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=993'>994</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=994'>995</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=995'>996</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=996'>997</a>\u001b[0m     embedding_output,\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=997'>998</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=998'>999</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=999'>1000</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1000'>1001</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1001'>1002</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1002'>1003</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1003'>1004</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1004'>1005</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1005'>1006</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1006'>1007</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1007'>1008</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1008'>1009</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=575'>576</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=576'>577</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=577'>578</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=581'>582</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=582'>583</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=583'>584</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=584'>585</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=585'>586</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=586'>587</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=587'>588</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=588'>589</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=589'>590</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=590'>591</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=591'>592</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=592'>593</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=594'>595</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=595'>596</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:513\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=509'>510</a>\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=510'>511</a>\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=512'>513</a>\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=513'>514</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=514'>515</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=515'>516</a>\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=517'>518</a>\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/modeling_utils.py:2472\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/modeling_utils.py?line=2468'>2469</a>\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/modeling_utils.py?line=2469'>2470</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m-> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/modeling_utils.py?line=2471'>2472</a>\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:525\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=523'>524</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[0;32m--> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=524'>525</a>\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=525'>526</a>\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=526'>527</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:426\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=424'>425</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m--> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=425'>426</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=426'>427</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=427'>428</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/booydar/anaconda3/envs/dpenv/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=rmt,\n",
    "    # model=classic_bert,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = SequentialSampler(small_train_dataset)\n",
    "dl = DataLoader(small_train_dataset, sampler=sampler, batch_size=4)\n",
    "gen = dl.__iter__()\n",
    "s = next(gen)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15ebdd31b1273fe4d2b1fe1822219a570cf61693f7cab545dbe286c10cf9691f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
