{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import PreTrainedModel, AutoModelForSequenceClassification, AutoConfig\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import datasets\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import BertForSequenceClassification\n",
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from modeling_rmt import RMTEncoderForSequenceClassification\n",
    "from modeling_rmt_enc_dec import RMTEncoderDecoderForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /home/bulatov/bulatov/rmt_internship/finetune/contract_nli/bert-base-cased/lr1e-05_constant_with_warmup_adamw_wd1e-03_1452_mem25_sum_loss/run_3/model_best.pth\n",
    "# !ls /home/bulatov/bulatov/rmt_internship/finetune/contract_nli/bert-base-cased/lr1e-05_linear_adamw_wd1e-03_968_mem25_sum_loss/run_3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cpt_path = \"/cephfs/home/bulatov/bulatov/rmt_internship/finetune/contract_nli/t5-base/lr1e-05_linear_adamw_wd1e-03_972_mem25_sum_loss/run_1/model_best.pth\"\n",
    "\n",
    "model_name = 'bert-base-cased'\n",
    "\n",
    "experiment_path = \"/home/bulatov/bulatov/rmt_internship/finetune/contract_nli/bert-base-cased/lr1e-05_linear_adamw_wd1e-03_968_mem25_sum_loss/run_3/\"\n",
    "cpt_path = os.path.join(experiment_path, \"model_best.pth\")\n",
    "config_path = os.path.join(experiment_path, \"config.json\")\n",
    "cpt = torch.load(cpt_path, map_location='cpu')\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    d = json.load(f)\n",
    "\n",
    "rmt = RMTEncoderForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "set_params_kwargs = {k:v for k,v in d.items() if k in rmt.set_params.__code__.co_varnames}\n",
    "set_params_kwargs['segment_ordering'] = 'regular'\n",
    "set_params_kwargs['inter_layer_memory'] = False\n",
    "set_params_kwargs['tokenizer'] = tokenizer        \n",
    "\n",
    "rmt.set_params(**set_params_kwargs)\n",
    "rmt.load_state_dict(cpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'bert-base-cased'\n",
    "\n",
    "experiment_path = \"/home/bulatov/bulatov/runs/finetune/debug/contract_nli/bert-base-cased/lr1e-05_linear_adamw_wd1e-03_512_mem/run_1/\"\n",
    "cpt_path = os.path.join(experiment_path, \"model_best.pth\")\n",
    "config_path = os.path.join(experiment_path, \"config.json\")\n",
    "cpt = torch.load(cpt_path, map_location='cpu')\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    d = json.load(f)\n",
    "\n",
    "baseline = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "baseline.load_state_dict(cpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder. cnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = d['input_seq_len'] * 10\n",
    "target_seq_len = 2\n",
    "batch_size = 2\n",
    "\n",
    "device = torch.device(0)\n",
    "\n",
    "encode_plus_kwargs = {'max_length': input_seq_len,\n",
    "                              'truncation': True,\n",
    "                              'padding': 'longest',\n",
    "                              'pad_to_multiple_of': 1}\n",
    "generate_kwargs = {}\n",
    "labels_map = {'Contradiction': 0, 'Entailment': 1, 'Not mentioned': 2}\n",
    "num_labels = len(labels_map)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # cut too long strings because they may slow down tokenization\n",
    "    inputs = [b['input'][:input_seq_len * 10] for b in batch]\n",
    "    labels = [b['output'][:target_seq_len * 10] for b in batch]\n",
    "    features = tokenizer.batch_encode_plus(list(inputs), return_tensors='pt', **encode_plus_kwargs)\n",
    "    labels = np.array([labels_map[t] for t in labels])\n",
    "    features['labels'] = torch.from_numpy(labels)\n",
    "    features['id'] = [b['id'] for b in batch]\n",
    "    features['pid'] = [b['pid'] for b in batch]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/contract_nli/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3771872bc9945dcb30a34edd1d78ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('tau/scrolls', 'contract_nli')\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset,)\n",
    "kwargs = {'pin_memory': True, 'num_workers': 0}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "valid_dataset = dataset['validation']\n",
    "valid_sampler = RandomSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7191, 1037)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset['pid']), len(valid_dataset['pid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2496])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(gen)\n",
    "sample['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(sample['attention_mask'][0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictions from all segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, input_ids, return_all_segments=False, **kwargs):\n",
    "    memory = self.set_memory()\n",
    "    segmented = self.pad_and_segment(input_ids)\n",
    "    segmented = list(zip(*segmented))\n",
    "\n",
    "    if self.segment_ordering in {'regular', 'last_memory_only'}:\n",
    "        pass\n",
    "    elif self.segment_ordering == 'reversed':\n",
    "        segmented = segmented[::-1]\n",
    "    elif self.segment_ordering == 'bidirectional':\n",
    "        segmented = segmented + segmented[::-1][1:]\n",
    "    elif self.segment_ordering == 'repeat_first':\n",
    "        segmented = segmented + segmented[:1]\n",
    "    else:\n",
    "        raise ValueError(f'Unknown segment ordering: {self.segment_ordering}')\n",
    "\n",
    "    self.memory_storage = {'num_mem_tokens': self.num_mem_tokens}\n",
    "    outputs = []\n",
    "    for seg_num, segment_data in enumerate(segmented):\n",
    "        input_ids, attention_mask, token_type_ids = segment_data\n",
    "        if memory.ndim == 2:\n",
    "            memory = memory.repeat(input_ids.shape[0], 1, 1)\n",
    "        if (self.bptt_depth > -1) and (len(segmented) - seg_num > self.bptt_depth): \n",
    "            memory = memory.detach()\n",
    "\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "        if self.drop_empty_segments:\n",
    "            non_empty_mask = [not torch.equal(input_ids[i], self.empty) for i in range(len(input_ids))]\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            input_ids = input_ids[non_empty_mask]\n",
    "            attention_mask = attention_mask[non_empty_mask]\n",
    "            token_type_ids = token_type_ids[non_empty_mask]\n",
    "            seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "\n",
    "            inputs_embeds = self.base_model.embeddings.word_embeddings(input_ids)\n",
    "            inputs_embeds[:, 1:1+self.num_mem_tokens] = memory[non_empty_mask]\n",
    "        else:\n",
    "            inputs_embeds = self.base_model.embeddings.word_embeddings(input_ids)\n",
    "            inputs_embeds[:, 1:1+self.num_mem_tokens] = memory\n",
    "\n",
    "        seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "        seg_kwargs['attention_mask'] = attention_mask\n",
    "        seg_kwargs['token_type_ids'] = token_type_ids\n",
    "\n",
    "        out = self.model.forward(**seg_kwargs, output_hidden_states=True)\n",
    "        outputs.append(out)\n",
    "\n",
    "        if self.drop_empty_segments:\n",
    "            memory[non_empty_mask] = out.hidden_states[-1][:, :self.num_mem_tokens]\n",
    "            out['non_empty_mask'] = non_empty_mask\n",
    "        else:\n",
    "            memory = out.hidden_states[-1][:, :self.num_mem_tokens]\n",
    "\n",
    "    for i, o in enumerate(outputs):\n",
    "        out[f'loss_{i}'] = o['loss'].mean()\n",
    "\n",
    "    if self.sum_loss:\n",
    "        out['loss'] = torch.stack([o['loss'] for o in outputs]).sum(dim=-1)\n",
    "\n",
    "    if return_all_segments:\n",
    "        return out, outputs\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def evaluate(output, sample):\n",
    "    labels = sample['labels']\n",
    "    logits = o['logits']\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    correct_mask = preds == labels[output['non_empty_mask']]\n",
    "    return correct_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'pid', 'input', 'output'],\n",
       "    num_rows: 7191\n",
       "})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label_train = dict(zip(train_dataset['id'], train_dataset['output']))\n",
    "id2label_valid = dict(zip(valid_dataset['id'], valid_dataset['output']))\n",
    "\n",
    "id2text_train = dict(zip(train_dataset['id'], train_dataset['input']))\n",
    "id2text_valid = dict(zip(valid_dataset['id'], valid_dataset['input']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, max_it=10000,):\n",
    "    it = 0\n",
    "    \n",
    "    res_df = pd.DataFrame()\n",
    "    gen = iter(dataloader)\n",
    "    for sample in gen:\n",
    "        ids, pids = sample.pop('id'), sample.pop('pid')\n",
    "        for key in sample:\n",
    "            sample[key] = sample[key].to(device)\n",
    "        out, outputs = __call__(model, **sample, return_all_segments=True)    \n",
    "\n",
    "        preds, labels = [], []\n",
    "        for i, o in enumerate(outputs):\n",
    "            logits = o['logits']\n",
    "\n",
    "            seg_labels = [s.item() for s in sample['labels']]\n",
    "            seg_preds = [p.item() for p in torch.argmax(logits, dim=1)]\n",
    "            labels.append(seg_labels)\n",
    "            preds.append(seg_preds)\n",
    "\n",
    "        res_dict = {'ids': ids}\n",
    "\n",
    "        res_dict.update({f'pred_seg_{i}':v for i,v in enumerate(preds)})\n",
    "        res_dict.update({f'labels_seg_{i}':v for i,v in enumerate(labels)})\n",
    "\n",
    "        res_df = res_df.append(pd.DataFrame(res_dict), ignore_index=True)\n",
    "\n",
    "\n",
    "        it += 1\n",
    "        if it > max_it:\n",
    "            break\n",
    "            \n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.3 s, sys: 420 ms, total: 53.7 s\n",
      "Wall time: 49.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "it = 0\n",
    "max_it = 10000\n",
    "\n",
    "rmt.to(device=device)\n",
    "rmt.drop_empty_segments = False\n",
    "# sampler = RandomSampler(train_dataset)\n",
    "# dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler,\n",
    "#                                 collate_fn=collate_fn, **kwargs)\n",
    "sampler = RandomSampler(valid_dataset)\n",
    "dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "rmt_res_df = evaluate_model(rmt, dataloader, max_it=max_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = iter(dataloader)\n",
    "# sample = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 968])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt_res_df.to_csv('tables/cnli-train-rm-bert-968-25.csv', index=False)\n",
    "rmt_res_df.to_csv('tables/cnli-valid-rm-bert-968-25.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = baseline\n",
    "model.to(device=device)\n",
    "model.drop_empty_segments = False\n",
    "\n",
    "# sampler = RandomSampler(train_dataset)\n",
    "# dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler,\n",
    "#                                 collate_fn=collate_fn, **kwargs)\n",
    "sampler = RandomSampler(valid_dataset)\n",
    "dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "# baseline_df = evaluate_model(model, dataloader, max_it=100, forward_func=model.__call__)\n",
    "\n",
    "it = 0\n",
    "max_it=10000\n",
    "    \n",
    "res_df = pd.DataFrame()\n",
    "gen = iter(dataloader)\n",
    "for sample in gen:\n",
    "    ids, pids = sample.pop('id'), sample.pop('pid')\n",
    "    for key in sample:\n",
    "        sample[key] = sample[key].to(device)\n",
    "    o = model.forward(**sample)    \n",
    "\n",
    "    preds, labels = [], []\n",
    "\n",
    "    logits = o['logits']\n",
    "\n",
    "    seg_labels = [s.item() for s in sample['labels']]\n",
    "    seg_preds = [p.item() for p in torch.argmax(logits, dim=1)]\n",
    "    labels.append(seg_labels)\n",
    "    preds.append(seg_preds)\n",
    "\n",
    "    res_dict = {'ids': ids}\n",
    "\n",
    "    res_dict.update({'pred_seg':v for i,v in enumerate(preds)})\n",
    "    res_dict.update({'labels_seg':v for i,v in enumerate(labels)})\n",
    "\n",
    "    res_df = res_df.append(pd.DataFrame(res_dict), ignore_index=True)\n",
    "\n",
    "\n",
    "    it += 1\n",
    "    if it > max_it:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_df.to_csv('tables/cnli-train-bert.csv', index=False)\n",
    "# res_df.to_csv('tables/cnli-valid-bert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt_df = pd.read_csv('tables/cnli-train-rm-bert-968-25.csv')\n",
    "baseline_df = pd.read_csv('tables/cnli-train-bert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt_df.pred_seg_0.value_counts(), rmt_df.pred_seg_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt_df['correct_seg_0'] = rmt_df.pred_seg_0 == rmt_df.labels_seg_0\n",
    "rmt_df['correct_seg_1'] = rmt_df.pred_seg_1 == rmt_df.labels_seg_1\n",
    "\n",
    "baseline_df['correct_seg'] = baseline_df.pred_seg == baseline_df.labels_seg\n",
    "\n",
    "rmt_correct_ids_seg_0 = set(rmt_df[rmt_df.correct_seg_0].ids)\n",
    "rmt_correct_ids_seg_1 = set(rmt_df[rmt_df.correct_seg_1].ids)\n",
    "bl_correct_ids = set(baseline_df[baseline_df.correct_seg].ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt_lose_seg_0 = bl_correct_ids.difference(rmt_correct_ids_seg_0)\n",
    "rmt_lose_seg_1 = bl_correct_ids.difference(rmt_correct_ids_seg_1)\n",
    "baseline_lose = rmt_correct_ids_seg_1.difference(bl_correct_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmt loses: 558, baseline loses: 380\n"
     ]
    }
   ],
   "source": [
    "print(f'rmt loses: {len(rmt_lose_seg_1)}, baseline loses: {len(baseline_lose)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8050340703657349\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 645   75  121]\n",
      " [ 125 3074  331]\n",
      " [ 193  557 2070]]\n",
      "Normalized: \n",
      "[[0.77 0.09 0.14]\n",
      " [0.04 0.87 0.09]\n",
      " [0.07 0.2  0.73]]\n"
     ]
    }
   ],
   "source": [
    "# baseline \n",
    "\n",
    "accuracy = (baseline_df.pred_seg == baseline_df.labels_seg).mean()\n",
    "conf_mat = confusion_matrix(y_pred=baseline_df.pred_seg, y_true=baseline_df.labels_seg, )\n",
    "conf_mat_normalized = (conf_mat / conf_mat.sum(axis=1).reshape(-1, 1)).round(2)\n",
    "print(f'Accuracy: {accuracy}\\n\\nConfusion matrix: \\n{conf_mat}\\nNormalized: \\n{conf_mat_normalized}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segment 0\n",
      "Accuracy: 0.7916840495063273\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 648   58  135]\n",
      " [ 124 2792  614]\n",
      " [ 203  364 2253]]\n",
      "Normalized: \n",
      "[[0.77 0.07 0.16]\n",
      " [0.04 0.79 0.17]\n",
      " [0.07 0.13 0.8 ]]\n",
      "\n",
      "Segment 1\n",
      "Accuracy: 0.7802809066889167\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 647   54  140]\n",
      " [ 132 2734  664]\n",
      " [ 180  410 2230]]\n",
      "Normalized: \n",
      "[[0.77 0.06 0.17]\n",
      " [0.04 0.77 0.19]\n",
      " [0.06 0.15 0.79]]\n"
     ]
    }
   ],
   "source": [
    "# rmt\n",
    "\n",
    "df = rmt_df \n",
    "for l in [0, 1]:\n",
    "    accuracy = (df[f'pred_seg_{l}'] == df[f'labels_seg_{l}']).mean()\n",
    "    conf_mat = confusion_matrix(y_pred=df[f'pred_seg_{l}'], y_true=df[f'labels_seg_{l}'])\n",
    "    conf_mat_normalized = (conf_mat / conf_mat.sum(axis=1).reshape(-1, 1)).round(2)\n",
    "    print(f'\\nSegment {l}\\nAccuracy: {accuracy}\\n\\nConfusion matrix: \\n{conf_mat}\\nNormalized: \\n{conf_mat_normalized}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmt_df = pd.read_csv('tables/cnli-valid-rm-bert-968-25.csv')\n",
    "baseline_df = pd.read_csv('tables/cnli-valid-bert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmt loses: seg_0 - 75, seg_1 - 67, baseline loses: 76 (to seg 0: 58)\n"
     ]
    }
   ],
   "source": [
    "rmt_df['correct_seg_0'] = rmt_df.pred_seg_0 == rmt_df.labels_seg_0\n",
    "rmt_df['correct_seg_1'] = rmt_df.pred_seg_1 == rmt_df.labels_seg_1\n",
    "\n",
    "baseline_df['correct_seg'] = baseline_df.pred_seg == baseline_df.labels_seg\n",
    "\n",
    "rmt_correct_ids_seg_0 = set(rmt_df[rmt_df.correct_seg_0].ids)\n",
    "rmt_correct_ids_seg_1 = set(rmt_df[rmt_df.correct_seg_1].ids)\n",
    "bl_correct_ids = set(baseline_df[baseline_df.correct_seg].ids)\n",
    "\n",
    "rmt_lose_seg_0 = bl_correct_ids.difference(rmt_correct_ids_seg_0)\n",
    "rmt_lose_seg_1 = bl_correct_ids.difference(rmt_correct_ids_seg_1)\n",
    "baseline_lose = rmt_correct_ids_seg_1.difference(bl_correct_ids)\n",
    "baseline_lose_seg_0 = rmt_correct_ids_seg_0.difference(bl_correct_ids)\n",
    "\n",
    "print(f'rmt loses: seg_0 - {len(rmt_lose_seg_0)}, seg_1 - {len(rmt_lose_seg_1)}, baseline loses: {len(baseline_lose)} (to seg 0: {len(baseline_lose_seg_0)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7097396335583414\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 66  13  16]\n",
      " [ 30 416  73]\n",
      " [ 48 121 254]]\n",
      "Normalized: \n",
      "[[0.69 0.14 0.17]\n",
      " [0.06 0.8  0.14]\n",
      " [0.11 0.29 0.6 ]]\n"
     ]
    }
   ],
   "source": [
    "# baseline \n",
    "\n",
    "accuracy = (baseline_df.pred_seg == baseline_df.labels_seg).mean()\n",
    "conf_mat = confusion_matrix(y_pred=baseline_df.pred_seg, y_true=baseline_df.labels_seg, )\n",
    "conf_mat_normalized = (conf_mat / conf_mat.sum(axis=1).reshape(-1, 1)).round(2)\n",
    "print(f'Accuracy: {accuracy}\\n\\nConfusion matrix: \\n{conf_mat}\\nNormalized: \\n{conf_mat_normalized}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segment 0\n",
      "Accuracy: 0.6933461909353905\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 65  11  19]\n",
      " [ 28 372 119]\n",
      " [ 56  85 282]]\n",
      "Normalized: \n",
      "[[0.68 0.12 0.2 ]\n",
      " [0.05 0.72 0.23]\n",
      " [0.13 0.2  0.67]]\n",
      "\n",
      "Segment 1\n",
      "Accuracy: 0.7184185149469624\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 64  13  18]\n",
      " [ 22 389 108]\n",
      " [ 44  87 292]]\n",
      "Normalized: \n",
      "[[0.67 0.14 0.19]\n",
      " [0.04 0.75 0.21]\n",
      " [0.1  0.21 0.69]]\n"
     ]
    }
   ],
   "source": [
    "# rmt\n",
    "\n",
    "df = rmt_df \n",
    "for l in [0, 1]:\n",
    "    accuracy = (df[f'pred_seg_{l}'] == df[f'labels_seg_{l}']).mean()\n",
    "    conf_mat = confusion_matrix(y_pred=df[f'pred_seg_{l}'], y_true=df[f'labels_seg_{l}'])\n",
    "    conf_mat_normalized = (conf_mat / conf_mat.sum(axis=1).reshape(-1, 1)).round(2)\n",
    "    print(f'\\nSegment {l}\\nAccuracy: {accuracy}\\n\\nConfusion matrix: \\n{conf_mat}\\nNormalized: \\n{conf_mat_normalized}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contradiction</th>\n",
       "      <th>Entailment</th>\n",
       "      <th>Not mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Contradiction</th>\n",
       "      <td>64</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entailment</th>\n",
       "      <td>22</td>\n",
       "      <td>389</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not mentioned</th>\n",
       "      <td>44</td>\n",
       "      <td>87</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Contradiction  Entailment  Not mentioned\n",
       "Contradiction             64          13             18\n",
       "Entailment                22         389            108\n",
       "Not mentioned             44          87            292"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(conf_mat, index=labels_map.keys(), columns=labels_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segment 0\n",
      "Accuracy: 0.3582089552238806\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 3  2  2]\n",
      " [ 2 13 31]\n",
      " [ 3  3  8]]\n",
      "Normalized: \n",
      "[[0.43 0.29 0.29]\n",
      " [0.04 0.28 0.67]\n",
      " [0.21 0.21 0.57]]\n"
     ]
    }
   ],
   "source": [
    "# rmt\n",
    "\n",
    "df = rmt_df[rmt_df.ids.isin(rmt_lose_seg_1)]\n",
    "for l in [0, ]:\n",
    "    accuracy = (df[f'pred_seg_{l}'] == df[f'labels_seg_{l}']).mean()\n",
    "    conf_mat = confusion_matrix(y_pred=df[f'pred_seg_{l}'], y_true=df[f'labels_seg_{l}'])\n",
    "    conf_mat_normalized = (conf_mat / conf_mat.sum(axis=1).reshape(-1, 1)).round(2)\n",
    "    print(f'\\nSegment {l}\\nAccuracy: {accuracy}\\n\\nConfusion matrix: \\n{conf_mat}\\nNormalized: \\n{conf_mat_normalized}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contradiction</th>\n",
       "      <th>Entailment</th>\n",
       "      <th>Not mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Contradiction</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entailment</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not mentioned</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Contradiction  Entailment  Not mentioned\n",
       "Contradiction              3           2              2\n",
       "Entailment                 2          13             31\n",
       "Not mentioned              3           3              8"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(conf_mat, index=labels_map.keys(), columns=labels_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segment 0\n",
      "Accuracy: 0.0\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 0  2  2]\n",
      " [ 6  0 50]\n",
      " [ 9  6  0]]\n",
      "Normalized: \n",
      "[[0.   0.5  0.5 ]\n",
      " [0.11 0.   0.89]\n",
      " [0.6  0.4  0.  ]]\n",
      "\n",
      "Segment 1\n",
      "Accuracy: 0.43\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 0  3  1]\n",
      " [ 2 23 31]\n",
      " [ 3  3  9]]\n",
      "Normalized: \n",
      "[[0.   0.75 0.25]\n",
      " [0.04 0.41 0.55]\n",
      " [0.2  0.2  0.6 ]]\n"
     ]
    }
   ],
   "source": [
    "# rmt\n",
    "\n",
    "df = rmt_df[rmt_df.ids.isin(rmt_lose_seg_0)]\n",
    "for l in [0, 1]:\n",
    "    accuracy = (df[f'pred_seg_{l}'] == df[f'labels_seg_{l}']).mean().round(2)\n",
    "    conf_mat = confusion_matrix(y_pred=df[f'pred_seg_{l}'], y_true=df[f'labels_seg_{l}'])\n",
    "    conf_mat_normalized = (conf_mat / conf_mat.sum(axis=1).reshape(-1, 1)).round(2)\n",
    "    print(f'\\nSegment {l}\\nAccuracy: {accuracy}\\n\\nConfusion matrix: \\n{conf_mat}\\nNormalized: \\n{conf_mat_normalized}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "\n",
      "Confusion matrix: \n",
      "[[ 0  2  3]\n",
      " [ 8  0 11]\n",
      " [10 42  0]]\n",
      "Normalized: \n",
      "[[0.   0.4  0.6 ]\n",
      " [0.42 0.   0.58]\n",
      " [0.19 0.81 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# baseline \n",
    "df = baseline_df[baseline_df.ids.isin(baseline_lose)]\n",
    "accuracy = (df.pred_seg == df.labels_seg).mean()\n",
    "conf_mat = confusion_matrix(y_pred=df.pred_seg, y_true=df.labels_seg, )\n",
    "conf_mat_normalized = (conf_mat / conf_mat.sum(axis=1).reshape(-1, 1)).round(2)\n",
    "print(f'Accuracy: {accuracy}\\n\\nConfusion matrix: \\n{conf_mat}\\nNormalized: \\n{conf_mat_normalized}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Contradiction</th>\n",
       "      <th>Entailment</th>\n",
       "      <th>Not mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Contradiction</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entailment</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not mentioned</th>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Contradiction  Entailment  Not mentioned\n",
       "Contradiction              0           2              3\n",
       "Entailment                 8           0             11\n",
       "Not mentioned             10          42              0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(conf_mat, index=labels_map.keys(), columns=labels_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('What label model distinguishes worse than its rival?\\n RMT-seg-1, RMT-seg-0, baseline')\n",
    "# rmt_df[rmt_df.ids.isin(rmt_lose_seg_1)].labels_seg_1.value_counts()/rmt_df.labels_seg_1.value_counts(), rmt_df[rmt_df.ids.isin(rmt_lose_seg_0)].labels_seg_0.value_counts()/rmt_df.labels_seg_0.value_counts(), baseline_df[baseline_df.ids.isin(baseline_lose)].labels_seg.value_counts()/baseline_df.labels_seg.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode(text, **encode_plus_kwargs)\n",
    "n_segments = 2\n",
    "def split(text, n_segments=n_segments):\n",
    "    premise = text.split('.')[0]\n",
    "    encoded = tokenizer.encode(text, **encode_plus_kwargs, add_special_tokens=False)\n",
    "    segments = np.split(np.array(encoded), n_segments)    \n",
    "    texts = [tokenizer.decode(s) for s in segments]\n",
    "    \n",
    "    \n",
    "    return [premise] + texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id2label_valid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [88]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      3\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(rmt_lose_seg_1)[i]\n\u001b[0;32m----> 4\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mid2label_valid\u001b[49m[idx] \n\u001b[1;32m      5\u001b[0m rmt_preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(rmt_df[rmt_df\u001b[38;5;241m.\u001b[39mids \u001b[38;5;241m==\u001b[39m idx][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_seg_0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_seg_1\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      6\u001b[0m baseline_pred \u001b[38;5;241m=\u001b[39m baseline_df[baseline_df\u001b[38;5;241m.\u001b[39mids \u001b[38;5;241m==\u001b[39m idx]\u001b[38;5;241m.\u001b[39mpred_seg\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'id2label_valid' is not defined"
     ]
    }
   ],
   "source": [
    "i = 10\n",
    "\n",
    "idx = list(rmt_lose_seg_1)[i]\n",
    "label = id2label_valid[idx] \n",
    "rmt_preds = ', '.join(rmt_df[rmt_df.ids == idx][['pred_seg_0', 'pred_seg_1']].astype(str).values[0])\n",
    "baseline_pred = baseline_df[baseline_df.ids == idx].pred_seg.values[0]\n",
    "print(f'idx: {idx}\\nlabel: {label, labels_map[label]}\\nrmt predictions: {rmt_preds}\\nbaseline_prediction: {baseline_pred}\\n\\n')\n",
    "\n",
    "text = id2text_valid[idx]\n",
    "texts = split(text)\n",
    "\n",
    "print('\\n\\n\\n'.join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 590_nda-16\n",
      "label: ('Not mentioned', 2)\n",
      "rmt predictions: 1, 2\n",
      "baseline_prediction: 1\n",
      "\n",
      "\n",
      "Receiving Party shall destroy or return some Confidential Information upon the termination of Agreement\n",
      "\n",
      "\n",
      "Receiving Party shall destroy or return some Confidential Information upon the termination of Agreement. Exhibit ( d ) ( 3 ) NON - DISCLOSURE AGREEMENT In connection with a potential transaction ( “ Proposed Transaction ” ) between 3M Company ( “ Interested Party ” or “ Receiving Party ” ), and Cogent, Inc., a Delaware corporation ( “ Company ” or “ Disclosing Party ” ), the parties wish to protect and preserve the confidential and / or proprietary nature of certain information and materials of the Company that may be disclosed or made available to the Interested Party or its Representatives ( as defined below ) in connection with certain discussions, negotiations or dealings between the parties relating to the Proposed Transaction. In consideration of the foregoing and the rights and obligations set forth herein, both parties hereby agree as of July 31, 2008 ( the “ Effective Date ” ) : 1. PROPRIETARY INFORMATION AND OTHER DEFINITIONS. 1. 1 “ Proprietary Information ” means any and all information and material disclosed by Disclosing Party or any of its Representatives to Receiving Party or any of its Representatives in connection with the Proposed Transaction or in the course of the parties ’ evaluation and negotiation of the Proposed Transaction, together with all communications, data, reports, analyses, compilations, studies, interpretations, records, notes, lists, financial statements or other materials or information prepared by Receiving Party or any of its Representatives that contain or otherwise reflect or are based upon, in whole or in part, any Proprietary Information of Disclosing Party or that reflect the review of, interest in, or evaluation of all or any portion of the Proposed Transaction or Disclosing Party ’ s business ( collectively, “ Derived Information ” ), whether tangible or intangible, furnished or prepared in writing or in oral, graphic, electronic or any other form or manner. In addition, Proprietary Information shall include ( x ) the fact that discussions or negotiations are taking place concerning the Proposed Transaction or that Interested Party has made or may make an offer to acquire Company ’ s stock or assets or that any Proprietary Information has been shared between the parties and their respective\n",
      "\n",
      "\n",
      "Representatives in connection therewith, ( y ) the proposed terms and conditions of the Proposed Transaction ( including any financial terms and conditions ) and the status thereof, and ( z ) the existence, context, and scope of this Agreement. Proprietary Information shall not include information that : ( i ) is or becomes generally available to the public other than as a result of any disclosure or other action or inaction by Receiving Party in breach of this Agreement ( including any disclosure or other action or inaction by Representatives of Receiving Party that would constitute a breach of this Agreement if undertaken by Receiving Party itself ) ; ( ii ) is or becomes known or avai1able to Receiving Party or any of its Representatives on a non - confidential basis from a source ( other than Disclosing Party or any of its subsidiaries, affiliates or Representatives ) that, to the best of the knowledge of Receiving Party, is not prohibited from disclosing such Proprietary Information to Receiving Party by a contractual, legal or fiduciary obligation ; or ( iii ) is or was independently developed by Receiving Party or any of its Representatives without violation of any obligation under this Agreement. 1. 2 “ Representatives ” means as to any person, its directors, officers, employees, agents and advisors ( including, without limitation, financial advisors, financing sources, attorneys, accountants and their respective Representatives ). 1. 3 “ person ” shall be broadly interpreted to include, without limitation, any corporation, company, partnership, other entity or individual. 2. NON - DISCLOSURE AND LIMITED USE. 2. 1 Non - Disclosure. Without the written consent of Disclosing Party and except as otherwise required by applicable law, Receiving Party shall keep, and shall cause its Representatives to keep, all Proprietary Information confidential and shall not disclose or reveal, and shall cause its Representatives not to disclose or reveal, in any manner whatsoever, in whole or in part, any Proprietary Information to any person, other than to its Representatives who are actively and directly participating in its evaluation of the Proposed Transaction or who otherwise need to know the Proprietary Information for the purpose of evaluating the Proposed Transaction and who are bound by restrictions regarding the disclosure and use of such Proprietary Information\n"
     ]
    }
   ],
   "source": [
    "i = 4\n",
    "\n",
    "idx = list(baseline_lose)[i]\n",
    "label = id2label_valid[idx] \n",
    "rmt_preds = ', '.join(rmt_df[rmt_df.ids == idx][['pred_seg_0', 'pred_seg_1']].astype(str).values[0])\n",
    "baseline_pred = baseline_df[baseline_df.ids == idx].pred_seg.values[0]\n",
    "print(f'idx: {idx}\\nlabel: {label, labels_map[label]}\\nrmt predictions: {rmt_preds}\\nbaseline_prediction: {baseline_pred}\\n\\n')\n",
    "\n",
    "text = id2text_valid[idx]\n",
    "texts = split(text)\n",
    "\n",
    "print('\\n\\n\\n'.join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx: 70_nda-13\n",
      "label: ('Not mentioned', 2)\n",
      "rmt predictions: 2, 1\n",
      "baseline_prediction: 2\n",
      "\n",
      "\n",
      "Receiving Party may acquire information similar to Confidential Information from a third party\n",
      "\n",
      "\n",
      "Receiving Party may acquire information similar to Confidential Information from a third party. CONFIDENTIALITY, NON - DISCLOSURE & APPROPRIATE USE AGREEMENT FAU has a legal responsibility to safeguard the confidentiality and security of our patients ’ protected health information ( “ PHI ” ) as well as operational, proprietary, and student and employee information ( collectively “ FAU Confidential Information ” ). This information may include, but is not limited to, patient health records, as well as information regarding human resources, payroll, fiscal matters, research, and strategic planning, and may exist in any form, including electronic, video, spoken, or written. This Agreement applies to all members of the workforce, including but not limited to, employees, volunteers, students, faculty, physicians, and third parties, whether temporary or permanent, paid or not paid, visiting, or designated as affiliates or associates, who are employed by, contracted with, or under the direct control of FAU. This Agreement also applies to all users who are granted access to FAU - issued computing and technology resources ( e. g., desktops, laptops, tablets, mobile phones, printers, etc. ), application systems or access to the FAU network, whether the user is affiliated with FAU or not, and whether access to or use of the information systems occurs locally or from remote locations. I hereby agree as follows : I will maintain patient privacy and protect and safeguard the confidentiality and security of PHI and FAU Confidential Information in accordance with Florida and federal laws and FAU policies and procedures. I understand that access to health information created, received, or maintained by FAU or its affiliates is limited to those who have a valid business, medical, or professional need to know the information. I understand that FAU has implemented administrative, technical, and physical safeguards to protect the confidentiality and security of PHI, and I agree not to bypass or disable these safeguards. I will not disclose any PHI to any individual or third party, except as specifically authorized by FAU policies and procedures, and upon receiving a written authorization from the patient ( unless otherwise required by applicable law ), and then only on a need -\n",
      "\n",
      "\n",
      "to - know basis. I will not use any PHI in an inappropriate, unethical, detrimental or unauthorized manner. I will not discuss any information regarding FAU or patients in an area where unauthorized individuals may overhear such information, including waiting rooms, hallways, elevators and other public areas. I understand that it is strictly prohibited to discuss any FAU Confidential Information or PHI in public areas even if a patient ’ s name is not used. I understand that I will be given a unique User ID and password to access electronic health, operational, proprietary, student or employee or other confidential information. I understand that my User ID and password are confidential, may only be used by me, that I am responsible for safekeeping my password, that I am also responsible for any activity initiated by my User ID and password, and that in certain circumstances my User ID and password may be equivalent to my legal signature. If I suspect that my User ID or password has been compromised, I should immediately contact FAU ’ s Office of Information Technology ( “ FAU OIT ” ). I have no expectation of privacy when using FAU information systems. FAU shall have the right to record, audit, log, and / or monitor access to or use of its information systems that is attributed to my User ID. I agree to practice good workstation security measures on any computing device that uses or accesses a FAU information system. Good security measures include, but are not limited to, maintaining physical security of electronic devices, never leaving a device unattended while in use, and adequately shielding the screen from unauthorized viewing by others. I understand that only encrypted and password protected devices may be used to transport PHI or other Restricted Data ( defined below ). I understand that smartphones and other mobile devices used to access FAU information systems must be configured to encrypt any Restricted or Sensitive Data ( defined below ), including photographs and videos, in persistent storage. I understand that I may access and / or use FAU Confidential Information, Restricted Data and Sensitive Data only as necessary to perform my job - related duties and that I may disclose ( i. e., share ) FAU Confidential Information,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "i = 2\n",
    "\n",
    "ids = rmt_df[(rmt_df.pred_seg_0 == rmt_df.labels_seg_0) & ( rmt_df.pred_seg_1 != rmt_df.labels_seg_1)].ids.values\n",
    "idx = ids[i]\n",
    "label = id2label_valid[idx] \n",
    "rmt_preds = ', '.join(rmt_df[rmt_df.ids == idx][['pred_seg_0', 'pred_seg_1']].astype(str).values[0])\n",
    "baseline_pred = baseline_df[baseline_df.ids == idx].pred_seg.values[0]\n",
    "print(f'idx: {idx}\\nlabel: {label, labels_map[label]}\\nrmt predictions: {rmt_preds}\\nbaseline_prediction: {baseline_pred}\\n\\n')\n",
    "\n",
    "text = id2text_valid[idx]\n",
    "texts = split(text)\n",
    "\n",
    "print('\\n\\n\\n'.join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    21\n",
       "1    15\n",
       "0     5\n",
       "Name: labels_seg_1, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt_df[(rmt_df.pred_seg_0 == rmt_df.labels_seg_0) & ( rmt_df.pred_seg_1 != rmt_df.labels_seg_1)].labels_seg_1.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mmaybe finetune only memory weights\n",
    "use mem outputs to decode previous segment with a separate decoder\n",
    "orr pool answer to task not from cls but from mem tokens\n",
    "check if  gradient flows through memory\n",
    "decode using conccatenated memory, not last segment\n",
    "train baseline without question on qasper\n",
    "\n",
    "how do we improve remembering using memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15ebdd31b1273fe4d2b1fe1822219a570cf61693f7cab545dbe286c10cf9691f"
  },
  "kernelspec": {
   "display_name": "hvdenv",
   "language": "python",
   "name": "hvdenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
