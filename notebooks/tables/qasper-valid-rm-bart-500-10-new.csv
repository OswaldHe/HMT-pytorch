ids,f1_seg_0,preds_seg_0,preds_seg_0_tokens,target_text,labels
c09a92e25e6a81369fcc4ae6045491f2690ccc10,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1000 term groups based on the number of total annotations(200 term groups with 2 total annotations, 200 term groups with 3 total annotations, and so on up to term groups with 6 total annotations)","[[    0 20078  1385  1134   716    15     5   346     9   746 47234  1640
   2619  1385  1134    19   132   746 47234     6  1878  1385  1134    19
    155   746 47234     6     8    98    15    62     7  1385  1134    19
    231   746 47234    43     2]]"
de53af4eddbc30c808d90b8a11a29217d377569e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.","[[    0 19360  5532     6  3480     6  4944     6   188   469  1513     6
   3421  4320     6 27436  1869 39804   491     6    20  8137     6 41453
   3658     6 32284  5737     6  2193 32284 22080     6  3289 16370     6
  24558  4636   261     6 41809   428  2413     6  4672     4     2]]"
e2db361ae9ad9dbaa9a85736c5593eb3a471983d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder.","[[    0 48929     4  4573   139 30660 33183   417  1033     6 43730     4
   1769    12 29015 33183   417  1033     6 43730     4   163 18854 33183
    417  1033     6   163 18854 45026    12 48219     6    96  6646 35212
    111  4573   139 30660     8  9973 12169  4086 14813 15362     4     2]]"
41b70699514703820435b00efbc3aac4dd67560a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],divorce ,[[    0 26037 34260  1437     2]]
4944cd597b836b62616a4e37c045ce48de8c82ca,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.

CR: Sentiment prediction of customer product reviews BIBREF26.

SUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.

MPQA: Phrase level opinion polarity classification from newswire BIBREF28.

SST: Stanford Sentiment Treebank with binary labels BIBREF29.

TREC: Fine grained question-type classification from TREC BIBREF30.

MRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.","[[    0 12642    35 12169  8913 16782    13  1569  6173 39976    15    10
    292   386  3189   163  8863 45935  1244     4 50118 50118  9822    35
  12169  8913 16782     9  2111  1152  6173   163  8863 45935  2481     4
  50118 50118   104 12027   863    35 36994  9866 16782     9 11305    31
   1569  6173     8  6197 32933  5119   163  8863 45935  2518     4 50118
  50118  7629  1864   250    35  4129 34338   672  2979  8385 21528 20257
     31   340 11208   163  8863 45935  2517     4 50118 50118   104  4014
     35  8607 12169  8913 11077  5760    19 32771 14105   163  8863 45935
   2890     4 50118 50118   565 40698    35 14321  4435  7153   864    12
  12528 20257    31   255 40698   163  8863 45935   541     4 50118 50118
  12642  4794    35  3709  1624  2884  8258 34338 28556    31 12980   340
   1715   163  8863 45935  2983     4     2]]"
74fb77a624ea9f1821f58935a52cca3086bb0981,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"14,100 tweets",[[   0 1570    6 1866 6245    2]]
729694a9fe1e05d329b7a4078a596fe606bc5a95,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"F-1 score on the OntoNotes is 88%, and it is 53% on Wiki (gold).","[[    0   597    12   134  1471    15     5 13302   139 44691    16  7953
   4234     8    24    16  4268   207    15 45569    36 16472   322     2]]"
f428618ca9c017e0c9c2a23515dab30a7660f65f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN) and Random Forest (RF)","[[    0 46064    12 48159  2595 16771  2839    36 10537   510   238  7300
   2088  1501   293  4210 24072    36 19542   238  7737 40419 14969    36
    104 20954   238 24257  4843 25802   154  4210 24072    36   534  3573
    238   312  4306 11599 24257  4843  4762  6342    36 33020   495   238
    229  3864 18759 16853 22482    36   530    12 20057    43     8 34638
   5761    36 30455    43     2]]"
cf171fad0bea5ab985c53d11e48e7883c23cdc44,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"one of the Twitter datasets is about Turkish mobile network operators, there are positive, neutral and negative labels and provide the total amount plus the distribution of labels","[[    0  1264     9     5   599 42532    16    59  4423  1830  1546  5990
      6    89    32  1313     6  7974     8  2430 14105     8   694     5
    746  1280  2704     5  3854     9 14105     2]]"
1beb4a590fa6127a138f4ed1dd13d5d51cc96809,0.0,Entailment,[[    2     0 30495  3760  1757     2]], The features extracted from CNN.,[[    0    20  1575 27380    31  3480     4     2]]
c5b0ed5db65051eebd858beaf303809aa815e8e5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],small BERT,[[    0 23115   163 18854     2]]
b7381927764536bd97b099b6a172708125364954,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings.","[[    0   170    67 10516    70   292  3092    15 18561  8558    31     5
  39800   717  6486 10606   163  8863 45935  1558  2156   634   129     5
   8558    13    61  1058     8 10437   414    16 13215   577    35 15836
    154     6  5702     8   864 20257     6     8  1632  2777 10614    36
    487 27049   322    20  6814  9629    31     5 10606    32   341     6
     53    52   422   129     5  4460  9629     6   147     5 33183   417
   1033  1235    32    45 27115   868 17294     9     5  3092     6  6437
      5   467     7   304   129     5   335   416    11     5 33183   417
   1033     4     2]]"
d9412dda3279729e95fcb35cbed09e61577a896e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"precision, recall, F1 , accuracy ",[[    0  5234 37938     6  6001     6   274   134  2156  8611  1437     2]]
d3dbb5c22ef204d85707d2d24284cc77fa816b6c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BNA, DocQA, R.M-Reader, R.M-Reader + Verifier, DocQA + ELMo, R.M-Reader+Verifier+ELMo","[[    0   387  4444     6 19761  1864   250     6   248     4   448    12
  46347     6   248     4   448    12 46347  2055  3060 24072     6 19761
   1864   250  2055 17678 17357     6   248     4   448    12 46347  2744
  21119 24072  2744  3721 17357     2]]"
c6a0b9b5dabcefda0233320dd1548518a0ae758e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CJFA encoder ,[[    0   347   863  5944  9689 15362  1437     2]]
1d197cbcac7b3f4015416f0152a6692e881ada6c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Using the OpenIE toolbox and applying heuristic rules to select the most relevant relation.,"[[    0 36949     5  2117  7720  3944  8304     8  9889    37 46177  1492
      7  5163     5   144  4249  9355     4     2]]"
d9b6c61fc6d29ad399d27b931b6cb7b1117b314a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The question generation model provides each candidate answer with a score by measuring semantic relevance between the question and the generated question based on the semantics of the candidate answer. ,"[[    0   133   864  2706  1421  1639   349  1984  1948    19    10  1471
     30 14978 46195 21623   227     5   864     8     5  5129   864   716
     15     5 46264     9     5  1984  1948     4  1437     2]]"
a3efe43a72b76b8f5e5111b54393d00e6a5c97ab,0.0,Entailment,[[    2     0 30495  3760  1757     2]],around 332k questions,[[    0 13837 37811   330  1142     2]]
044f922604b4b3f42ae381419fd5cd5624fa0637,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"most word alignments only involve one or a few words, attention can be distributed more freely","[[    0  7877  2136 12432  2963   129  6877    65    50    10   367  1617
      6  1503    64    28  7664    55 13215     2]]"
a5505e25ee9ae84090e1442034ddbb3cedabcf04,0.0,Entailment,[[    2     0 30495  3760  1757     2]],F1 score result of 0.8099,[[   0  597  134 1471  898    9  321    4 2940 2831    2]]
6c91d44d5334a4ac80100eead4e105d34e99a284,0.0,Entailment,[[    2     0 30495  3760  1757     2]], Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0,"[[    0  5428 22098   163  8863 45935   134  1421     8     5  9164   248
  20057    12 39954  1421   163  8863 45935   288     2]]"
b1a068c1050e2bed12d5c9550c73e59cd5b1f78d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d6191c4643201262a770947fc95a613f57bedb6b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],DIP corpus BIBREF37,[[    0   495  3808 42168   163  8863 45935  3272     2]]
007b13f05d234d37966d1aa7d85b5fd78564ff45,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
a3bb9a936f61bafb509fa12ac0a61f91abcc5106,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ARC , TREC, GARD , MLBioMedLAT ","[[    0 11969  2156   255 40698     6   272 11250  2156  9536  1020 21243
    574  2571  1437     2]]"
2317ca8d475b01f6632537b95895608dc40c4415,0.0,Entailment,[[    2     0 30495  3760  1757     2]],chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account,"[[    0   611  6435 10726     9    10 24713 13931     9  6245 16274    30
      5  6929     9    63 12337  1316     2]]"
bc730e4d964b6a66656078e2da130310142ab641,0.0,Entailment,[[    2     0 30495  3760  1757     2]],probabilistic model,[[    0  4892   428 14148  5580  1421     2]]
9bb7ae50bff91571a945c1af025ed2e67714a788,0.0,Entailment,[[    2     0 30495  3760  1757     2]],hLSTM,[[   0  298  574 4014  448    2]]
ce0e2a8675055a5468c4c54dbb099cfd743df8a7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse","[[    0 41245     4  6924 11817     6 17638     4 23851 11817     6 17794
  23827     6 38207 39053   636 10179   620 21130   918     6   211  6285
    493     6 23384     6  2717   337 44137     6 40931     6 43254 12876
      8 37506 23827     2]]"
7e38e0279a620d3df05ab9b5e2795044f18d4471,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"racism, sexism, personal attack, not specifically about any single topic","[[    0 13249  1809     6 26032     6  1081   908     6    45  4010    59
    143   881  5674     2]]"
6c91d44d5334a4ac80100eead4e105d34e99a284,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Transformer, RNN-Search model",[[    0 19163 22098     6   248 20057    12 39954  1421     2]]
abe2393415e533cb06311e74ed1c5674cff8571f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU, NIST, METEOR, ROUGE-L, CIDEr","[[    0 30876   791     6   234 11595     6 30782   717  3411     6   248
   5061  8800    12   574     6   230  2688 28012     2]]"
10d450960907091f13e0be55f40bcb96f44dd074,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
fabf6fdcfb4c4c7affaa1e4336658c1e6635b1bf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Semeval 2014 BIBREF34 Twitter Sentiment Analysis Dataset ,  dataset was created by BIBREF8,  English dataset from BIBREF8,  dataset from The Sarcasm Detector","[[    0 37504 28017   777   163  8863 45935  3079   599 12169  8913  5213
  16673   281   594  2156  1437 41616    21  1412    30   163  8863 45935
    398     6  1437  2370 41616    31   163  8863 45935   398     6  1437
  41616    31    20   208  9636 16836 11185 26997     2]]"
71bd5db79635d48a0730163a9f2e8ef19a86cd66,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Restrictivity , Factivity , Coreference ",[[    0 31921 12127  9866  2156 18454  9866  2156  9025 23861  1437     2]]
2c7e94a65f5f532aa31d3e538dcab0468a43b264,0.0,Entailment,[[    2     0 30495  3760  1757     2]],intents are annotated manually with guidance from queries collected using a scoping crowdsourcing task,"[[    0  2544  4189    32 45068  1070 24704    19  3824    31 22680  4786
    634    10  2850 12232  8817 27824  3685     2]]"
33554065284110859a8ea3ca7346474ab2cab100,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1,873 Twitter conversation threads, roughly 14k tweets","[[    0   134     6   398  5352   599  1607 25816     6  3667   501   330
   6245     2]]"
500a8ec1c56502529d6e59ba6424331f797f31f0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],700 ,[[   0 5987 1437    2]]
113d791df6fcfc9cecfb7b1bebaf32cc2e4402ab,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Using file size on disk,[[    0 36949  2870  1836    15 21675     2]]
97466a37525536086ed5d6e5ed143df085682318,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Answer with content missing: (Baseline Method section) We implemented a simple approach inspired by previous work on concept map generation and keyphrase extraction.,"[[    0 33683    19  1383  1716    35    36 40258  7012 16410  2810    43
    166  6264    10  2007  1548  4083    30   986   173    15  4286  5456
   2706     8   762 40726 23226     4     2]]"
af34051bf3e628c1e2a00b110bb84e5f018b419f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train","[[    0 34230  4699  4062 18043     6  5048    12 32530 11909 38630     6
  19268    12 45025 11909 38630     6  1876    12   560    12 19827  2744
   5234    12 32530     6 28802  2744  5234    12 21714     2]]"
32a3c248b928d4066ce00bbb0053534ee62596e7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The task of predicting MSD tags: V, PST, V.PCTP, PASS.","[[    0   133  3685     9 15924  6253   495 19445    35   468     6 13388
      6   468     4   510  7164   510     6 38798     4     2]]"
4477bb513d56e57732fba126944073d414d1f75f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],clinical notes from the CE task in 2010 i2b2/VA ,"[[    0 31903  2775    31     5 16327  3685    11  1824   939   176   428
    176    73  9788  1437     2]]"
22714f6cad2d5c54c28823e7285dc85e8d6bc109,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Reduction, Selection, Rank",[[    0 15638 27345     6 30418     6 17816     2]]
18942ab8c365955da3fd8fc901dfb1a3b65c1be1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],TripAdvisor,[[    0   565 10706  9167 27880     2]]
29c014baf99fb9f40b5171aab3e2c7f12a748f79,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"popularity-based, similarity-based, hybrid",[[    0 15076 42664    12   805     6 37015    12   805     6  9284     2]]
1bb7eb5c3d029d95d1abf9f2892c1ec7b6eef306,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Their best model achieved a 2.49% Character Error Rate.,"[[    0 16837   275  1421  4824    10   132     4  3414   207 35177 37943
  14064     4     2]]"
4eaf9787f51cd7cdc45eb85cf223d752328c6ee4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary","[[    0   627 23899 21257   221  2839 28981 41243   163  8863 45935  1092
      6     5  7268 41586 44919 42168  4786    30  1935   118  9029   571
   8645   700  1794  2156   910  1253 43324 27380    31 40823 24659  1766
      2]]"
6ecb69360449bb9915ac73c0a816c8ac479cbbfc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"text, speech, image, click, etc",[[    0 29015     6  1901     6  2274     6  3753     6  4753     2]]
b5bc34e1e381dbf972d0b594fe8c66ff75305d71,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively","[[    0 35007 42532   217   944   487  6006 35153     8 13302   139 44691
    245     4   288     6    84  1850  5448  9980 33334   163 18854    12
    448  5199  5383   387 45935  3170    30  2055   288     4  2890     8
   2055   288     4  5607  4067     6  1111 42532     6  9499   274   134
   5139    30  2055   288     4  6750     8  2055   176     4  3367    15
   6253  4396     8 13302   139 44691   306     4   288     6  4067     2]]"
0737954caf66f2b4c898b356d2a3c43748b9706b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
a267d620af319b48e56c191aa4c433ea3870f6fb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],car ,[[   0 5901 1437    2]]
483a699563efcb8804e1861b18809279f21c7610,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
6bf5620f295b5243230bc97b340fae6e92304595,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"We use the same baseline as used by lang2011unsupervised which has been shown to be difficult to outperform. This baseline assigns a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head.","[[    0   170   304     5   276 18043    25   341    30 22682 22748   879
  16101 25376    61    34    57  2343     7    28  1202     7  9980  3899
      4   152 18043 42091    10 46195   774     7    10 31350   716    15
     63 45774 28201  5043     6   939     4   242     4     5 31492  9355
      7    63   471     4     2]]"
440faf8d0af8291d324977ad0f68c8d661fe365e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Reuters-8 dataset without stop words,[[    0  1251    12   398 41616   396   912  1617     2]]
41b2355766a4260f41b477419d44c3fd37f3547d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],sampling tweets from specific keywords create systematic and substancial racial biases in datasets,"[[    0 33243 20418  6245    31  2167 32712  1045 20552     8 32493 47142
   6689 31681    11 42532     2]]"
26327ccebc620a73ba37a95aabe968864e3392b2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The time devoted to self-coverage, opponent-coverage, and the number of adopted discussion points.","[[    0   133    86 11213     7  1403    12   876 33943     6  5254    12
    876 33943     6     8     5   346     9  5091  3221   332     4     2]]"
d28d86524292506d4b24ae2d486725a6d57a3db3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"33.33 average of ROUGE-1, ROUGE-2 and ROUGE-L ","[[   0 3103    4 3103  674    9  248 5061 8800   12  134    6  248 5061
  8800   12  176    8  248 5061 8800   12  574 1437    2]]"
003d6f9722ddc2ee13e879fefafc315fb8e87cb9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
867290103f762e1ddfa6f2ea30dd0a327f595182,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB),"[[    0 33683    19  1383  1716    35    36 30383  2810    43  1111    19
   1732   195     4   134     9     5  1111  5953 11077  5760    36  7164
    387    43     2]]"
81a35b9572c9d574a30cc2164f47750716157fc8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, Waseem et al. BIBREF10","[[    0   771  3175   991     8   289 35664   163  8863 45935   245     6
  10553  4400  1076     4   163  8863 45935   466     6   305  3175   991
   4400  1076     4   163  8863 45935   698     2]]"
72f7ef55e150e16dcf97fe443aff9971a32414ef,0.0,Entailment,[[    2     0 30495  3760  1757     2]], +1.86,[[   0 2055  134    4 5334    2]]
31101dc9937f108e27e08a5f34be44f0090b8b6b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
3e432d71512ffbd790a482c716e7079ee78ce732,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.","[[    0 36962 41616    19  3018  1142   131   278     9  2339     6  7409
   4570     8   340  7201     6    70  1330     7  2879     4     2]]"
f85ca6135b101736f5c16c5b5d40895280016023,0.0,Entailment,[[    2     0 30495  3760  1757     2]],baseline transformer BIBREF8,[[    0 15609  7012 40878   163  8863 45935   398     2]]
660284b0a21fe3801e64dc9e0e51da5400223fe3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],GM$\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset.,"[[    0 16972  1629 37457  1215  1629   530   574 35499   357 22792    87
   2210  8369    13  1337 12758    15  4998 13691 41616     4     2]]"
6a219d7c58451842aa5d6819a7cdf51c55e9fc0f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No specific domain is covered in the corpus.,[[    0  3084  2167 11170    16  2913    11     5 42168     4     2]]
ffa7f91d6406da11ddf415ef094aaf28f3c3872d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Best proposed metric has average correlation with human judgement of 0.913 and 0.846 compared to best compared metrics result of 0.758 and 0.829 on WikiBio and WebNLG challenge.,"[[    0 19183  1850 14823    34   674 22792    19  1050 17219     9   321
      4   466  1558     8   321     4   398  3761  1118     7   275  1118
  12758   898     9   321     4 37073     8   321     4   398  2890    15
  45569 40790     8  6494 27027   534  1539     4     2]]"
955cbea7e5ead36fb89cd6229a97ccb3febcf8bc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],task of humor identification in social media texts is analyzed as a classification problem,"[[    0 45025     9 12073 10614    11   592   433 14301    16 13773    25
     10 20257   936     2]]"
da55bd769721b878dd17f07f124a37a0a165db02,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," What kind of topic (or sub-topic) a student has a problem with, At which examination mode (i.e., quiz, chapter level training or exercise, section level training or exercise, or final examination) the student is working right now,  the exact question number and exact problem formulation","[[    0   653   761     9  5674    36   368  2849    12 45260    43    10
   1294    34    10   936    19     6   497    61  9027  5745    36   118
      4   242   482 25060     6  7285   672  1058    50  3325     6  2810
    672  1058    50  3325     6    50   507  9027    43     5  1294    16
    447   235   122     6  1437     5  6089   864   346     8  6089   936
  32018     2]]"
09c86ef78e567033b725fc56b85c5d2602c1a7c3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],simply averaging the predictions from the constituent single models,[[    0 13092 26308  8343     5 12535    31     5 31350   881  3092     2]]
7f207549c75f5c4388efc15ed28822672b845663,0.0,Entailment,[[    2     0 30495  3760  1757     2]],20 minutes,[[  0 844 728   2]]
c22394a3fb0dbf2fc7d3a70ad6435803f5a16ebd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
9de2f73a3db0c695e5e0f5a3d791fdc370b1df6e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They use text transcription.,[[    0  1213   304  2788 37118     4     2]]
132f752169adf6dc5ade3e4ca773c11044985da4,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," tweet dataset created by Wang et al. , CrowdFlower dataset","[[    0  3545 41616  1412    30  9705  4400  1076     4  2156 24544 16197
   8285 41616     2]]"
bc31a3d2f7c608df8c019a64d64cb0ccc5669210,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BERTbase,[[    0 11126   565 11070     2]]
a9337636b52de375c852682a2561af2c1db5ec63,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
5a6926de13a8cc25ce687c22741ba97a6e63d4ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"US presidential primaries, Democratic and Republican National Conventions",[[    0  3048  1939 19050     6  1557     8  1172   496  2585 40357     2]]
a267d620af319b48e56c191aa4c433ea3870f6fb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the car,[[  0 627 512   2]]
3b77b4defc8a139992bd0b07b5cf718382cb1a5f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier,"[[    0  3684 38257  1421    30 10581     5 32407     9    70     5  2405
   1617     8  1058    10  7268   179 48866  7300  2088  1501   293  1380
  24072     2]]"
d604f5fb114169f75f9a38fab18c1e866c5ac28b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Precision, recall, F1, accuracy",[[    0 22763 37938     6  6001     6   274   134     6  8611     2]]
cfbec1ef032ac968560a7c76dec70faf1269b27c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Knowledge Base Question Answering ,[[    0 38912 21945 11056 15680   660  4184  2961  1437     2]]
9da1e124d28b488b0d94998d32aa2fa8a5ebec51,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Overall F1 score:
- He and Sun (2017) 58.23
- Peng and Dredze (2017) 58.99
- Xu et al. (2018) 59.11","[[    0 28965   274   134  1471    35 50118    12    91     8  2083    36
   3789    43  4893     4  1922 50118    12 34597     8   211  2050  2158
     36  3789    43  4893     4  2831 50118    12 30226  4400  1076     4
     36  2464    43  5169     4  1225     2]]"
12159f04e0427fe33fa05af6ba8c950f1a5ce5ea,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding","[[    0 30695     9 28255     6  5018   923    11 46644  2961     6  4230
      9  2136 44493     6  2931  1836     8 21026     9 33183 11303     2]]"
71e4ba4e87e6596aeca187127c0d088df6570c57,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
127d5ddfabec5c58832e5865cbd8ed0978c25a13,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The encoder is essentially the same as tweet2vec, with the input as words instead of characters.","[[    0   133  9689 15362    16  5700     5   276    25  3545   176 25369
      6    19     5  8135    25  1617  1386     9  3768     4     2]]"
b06512c17d99f9339ffdab12cedbc63501ff527e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
bc730e4d964b6a66656078e2da130310142ab641,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Logistic Regression, Multilayer Perceptron","[[    0 23345  5580  6304 21791     6 14910   718 19777  2595 16771  2839
      2]]"
ed2eb4e54b641b7670ab5a7060c7b16c628699ab,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SR,[[    0 17973     2]]
e3c9e4bc7bb93461856e1f4354f33010bc7d28d5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ","[[    0   104 20954  2156  3480  2156  8837   791  2156  3480    73 11621
    791  2744  4656     6   910    12  4135  2156 42169   250  1437     2]]"
907b3af3cfaf68fe188de9467ed1260e52ec6cf1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different","[[    0 42390 24008  2485     9  2184   268     6  7837     8 44163    32
   3625   430   227     5   278     9  6245  8200  4486   340     8   167
    786  8200   106     6    53    13   274 18633  5110     6 22150  2485
      6  2454     6  9944  1694  2580     8 21653 35103    51    32    45
   3625   430     2]]"
114934e1a1e818630ff33ac5c4cd4be6c6f75bb2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],NCEL consistently outperforms various baselines with a favorable generalization ability,"[[    0  6905  3721  6566  9980 33334  1337 11909 38630    19    10  9879
    937  1938  1460     2]]"
cb196725edc9cdb2c54b72364f3bbf7c76471490,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
f88036174b4a0dbf4fe70ddad884d16082c5748d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
2eb9280d72cde9de3aabbed993009a98a5fe0990,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"13,939",[[   0 1558    6  466 3416    2]]
e101e38efaa4b931f7dd75757caacdc945bb32b4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10","[[    0   771  3175   991     8   289 35664   163  8863 45935   245     6
  10553  4400  1076     4   163  8863 45935   466     6     8   305  3175
    991  4400  1076     4   163  8863 45935   698     2]]"
e3a2d8886f03e78ed5e138df870f48635875727e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],by crawling,[[    0  1409 33265     2]]
c88a846197b72d25e04ec55f00ee3e72f655504c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],corpus of state speeches delivered during the annual UN General Debate,"[[    0  7215   642   687     9   194 13467  2781   148     5  1013  2604
   1292 36743     2]]"
c09a92e25e6a81369fcc4ae6045491f2690ccc10,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Human evaluators were asked to evaluate on a scale from 1 to 5 the validity of the lexicon annotations made by the experts and crowd contributors.,"[[    0 33837 37131   257  3629    58   553     7 10516    15    10  3189
     31   112     7   195     5 25295     9     5 36912 17505 47234   156
     30     5  2320     8  2180 17233     4     2]]"
e35c2fa99d5c84d8cb5d83fca2b434dcd83f3851,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy","[[    0  2709     5   786    12 24905  5564  2349     6    52  5864    15
     10   889     9  8963   599  2349    31   163  8863 45935   134     6
     52   304    10   889    19   277  2107   599  2349    31   163  8863
  45935  1646    14    32  1687 32101     2]]"
b591853e938984e6069d738371500ebdec50d256,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the IMDb movie review dataset BIBREF17,[[    0   627  9206 42198  1569  1551 41616   163  8863 45935  1360     2]]
b8d7d055ddb94f5826a9aad7479b4a92a9c8a2f0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],RNN,[[    0   500 20057     2]]
45e9533586199bde19313cd43b3d0ecadcaf7a33,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
03e9ac1a2d90152cd041342a11293a1ebd33bcc3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],NLG datasets,[[    0 27027   534 42532     2]]
1dc2da5078a7e5ea82ccd1c90d81999a922bc9bf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
7705dd04acedaefee30d8b2c9978537afb2040dc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],None,[[    0 29802     2]]
8e52637026bee9061f9558178eaec08279bf7ac6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],machine translation platform Apertium BIBREF5,[[    0 37556 19850  1761    83 11497  4031   163  8863 45935   245     2]]
149da739b1c19a157880d9d4827f0b692006aa2c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SVM, MLP, FastText, CNN, BERT, DialogFlow, Rasa NLU","[[    0   104 20954     6 10725   510     6  9612 39645     6  3480     6
    163 18854     6 28985  2154 41779     6   248  8810 12817   791     2]]"
35b3ce3a7499070e9b280f52e2cb0c29b0745380,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
b5608076d91450b0d295ad14c3e3a90d7e168d0e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
9ecde59ffab3c57ec54591c3c7826a9188b2b270,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MSMARCO,  HOTPOTQA, RECORD,  MULTIRC, NEWSQA, and DROP.","[[    0  6222 36659  6335     6  1437 33466   510  3293  1864   250     6
  14895 11200     6  1437   256 25938 44160     6  7857  1864   250     6
      8 10994  5733     4     2]]"
60ce4868af45753c9e124e64e518c32376f12694,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Stanford Sentiment Analysis Dataset BIBREF36,"[[    0 36118  1891 12169  8913  5213 16673   281   594   163  8863 45935
   3367     2]]"
85590bb26fed01a802241bc537d85ba5ef1c6dc2,0.0,Entailment,[[    2     0 30495  3760  1757     2]], we use several of the MCQA baseline models first introduced in BIBREF0,"[[    0    52   304   484     9     5 11826  1864   250 18043  3092    78
   2942    11   163  8863 45935   288     2]]"
d571e0b0f402a3d36fb30d70cdcd2911df883bc7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
2439b6b92d73f660fe6af8d24b7bbecf2b3a3d72,0.0,Entailment,[[    2     0 30495  3760  1757     2]],by evaluating their model on five different benchmarks,[[    0  1409 15190    49  1421    15   292   430 22485     2]]
dc2a2c177cd5df6da5d03e6e74262bf424850ec9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries.","[[    0  1694    67  1316    13   559 31681 17886     7   430   340  1715
      6  5056     7     5  7089  1850    11   163  8863 45935   176     7
   6929   430  6639     4  7806    52   311    14    52    32   441     7
  36029 10280  1954   786    12   438 29921 44937  4836    36   463 28766
    340  7201    43    19   239  8611    36   250  2492  4571    62     7
   8940 20186   190    77  6846    13     5   559  9415     9  1715    36
    463  1058   129    15   314    12 45250    50   235    12 45250  7201
    322   166 14095    14     5 10490     9 19197  1937 26991  2459  5616
    335    13     5 20257     6  3069 12653    10   430  9453     9    42
  14749    77  3565   340 11441     7     5    80   340 30700     4   166
     67   311    14   144 40846   179  3693  1575     6    61    32  5407
      7     5 24135     8  5581     9  1154 34455  4216    11   430 13171
      6    32     5   276   420     5    80   749     4     2]]"
8a7615fc6ff1de287d36ab21bf2c6a3b2914f73d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF","[[    0 37426   574  4014   448     6  6479   574  4014   448    12 16256
      6  6479   574  4014   448    12  9822   597     6  6479   574  4014
    448    12 16256    12  9822   597     2]]"
4907096cf16d506937e592c50ae63b642da49052,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"detecting abusive language extremely laborious, it is difficult to build a large and reliable dataset","[[    0 17701  9041   154 14202  2777  2778  4178  6514     6    24    16
   1202     7  1119    10   739     8  7058 41616     2]]"
2ad4d3d222f5237ed97923640bc8e199409cbe52,0.0,Entailment,[[    2     0 30495  3760  1757     2]],completion times and accuracies ,[[    0   175 37189   498     8 49060 15668  1437     2]]
4d47bef19afd70c10bbceafd1846516546641a2f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder","[[    0  5605    12 42184   337  2777  1421     7 29699     5 13931     7
  13931  9689 15362  2156  1437   542   118    12 42184   337  1421     7
  29699     5  5044 15362     2]]"
19b7312cfdddb02c3d4eaa40301a67143a72a35a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )","[[    0 20365  2216 12535     6 22422  7728  3788  5044 15362  7397   982
     23  2808 28302 38036   306  2402   511    10 45382  8660    36  2808
  28302 38036   245  4839     8  3253    41   542 16101 25376 46644  2961
   5448    36    90    12   104  9009   163  8863 45935  2022  4839     2]]"
5bb3c27606c59d73fd6944ba7382096de4fa58d8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],MULTIPLE CHOICE QUESTION ANSWERING,"[[    0   448 25938  3808  3850 43719  9292 36374  7744  5102 11871  2076
   1862     2]]"
b4f881331b975e6e4cab1868267211ed729d782d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"33,663 distinct review keywords ",[[    0  3103     6 39505 11693  1551 32712  1437     2]]
4dcf67b5e7bd1422e7e70c657f6eacccd8de06d3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],16,[[   0 1549    2]]
cd2878c5a52542ddf080b20bec005d9a74f2d916,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"technology, religion, fashion, publishing, sports or recreation, real estate, agriculture/environment, law, security/military, tourism, construction, museums or libraries, banking/investment banking, automotive","[[    0 31007     6  6825     6  2734     6 10467     6  1612    50 14579
      6   588  2587     6  6300    73 37555     6   488     6   573    73
  29317     6  5724     6  1663     6 19773    50 18146     6  3454    73
  12406  1757  3454     6  8568     2]]"
984fc3e726848f8f13dfe72b89e3770d00c3a1af,0.0,Entailment,[[    2     0 30495  3760  1757     2]],KL-divergences of language models for the news article and the already added news references,"[[    0   530   574    12   417  8538   571  8457     9  2777  3092    13
      5   340  1566     8     5   416   355   340 13115     2]]"
9b7655d39c7a19a23eb8944568eb5618042b9026,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26","[[    0  5383   387 45935  1922     6   163  8863 45935  1360     6   163
   8863 45935  1366     6   163  8863 45935  1646     6   163  8863 45935
   1978     6   163  8863 45935  1244     6   163  8863 45935  2481     2]]"
113d791df6fcfc9cecfb7b1bebaf32cc2e4402ab,0.0,Entailment,[[    2     0 30495  3760  1757     2]],15.4 MB,[[    0   996     4   306 17025     2]]
f8c1b17d265a61502347c9a937269b38fc3fcab1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"On the datasets DE-EN, JA-EN, RO-EN, and EN-DE, the baseline achieves 29.79, 21.57, 32.70, and 26.02  BLEU score, respectively. The 1.5-entmax achieves  29.83, 22.13, 33.10, and 25.89 BLEU score, which is a difference of +0.04, +0.56, +0.40, and -0.13 BLEU score versus the baseline. The α-entmax achieves 29.90, 21.74, 32.89, and 26.93 BLEU score, which is a difference of +0.11, +0.17, +0.19, +0.91 BLEU score versus the baseline.","[[    0  4148     5 42532  5885    12  2796     6 38860    12  2796     6
  10033    12  2796     6     8 13245    12 10089     6     5 18043 35499
   1132     4  5220     6   733     4  4390     6  2107     4  3083     6
      8   973     4  4197  1437   163  3850   791  1471     6  4067     4
     20   112     4   245    12  1342 29459 35499  1437  1132     4  6361
      6   820     4  1558     6  2357     4   698     6     8   564     4
   5046   163  3850   791  1471     6    61    16    10  2249     9  2055
    288     4  3387     6  2055   288     4  4419     6  2055   288     4
   1749     6     8   111   288     4  1558   163  3850   791  1471  4411
      5 18043     4    20 47188    12  1342 29459 35499  1132     4  3248
      6   733     4  5243     6  2107     4  5046     6     8   973     4
   6478   163  3850   791  1471     6    61    16    10  2249     9  2055
    288     4  1225     6  2055   288     4  1360     6  2055   288     4
   1646     6  2055   288     4  6468   163  3850   791  1471  4411     5
  18043     4     2]]"
d087539e6a38c42f0a521ff2173ef42c0733878e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.","[[    0 17165 34775  7373  1395 16085   209     6   187    51  2703     5
   1294     8  3254  3092     7   458     5   276 32644     8  4195   980
      4   152 27301  4971    49   801     7   617  1888  1421 10070     4
      2]]"
935d6a6187e6a0c9c0da8e53a42697f853f5c248,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the aggregate of enterprises in a particular field,[[    0   627 13884     9 10445    11    10  1989   882     2]]
2a6003a74d051d0ebbe62e8883533a5f5e55078b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the CRX model,[[   0  627 4307 1000 1421    2]]
e5c8e9e54e77960c8c26e8e238168a603fcdfcc6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
203337c15bd1ee05763c748391d295a1f6415b9b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Projected Layer,[[    0 33347   196 43262     2]]
c35806cf68220b2b9bb082b62f493393b9bdff86,0.0,Entailment,[[    2     0 30495  3760  1757     2]],accuracy of 87.0%,[[    0  7904 45386     9  8176     4   288   207     2]]
dc2a2c177cd5df6da5d03e6e74262bf424850ec9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains,"[[    0  2765 36457    10   559  9415  6929     7   349   340  1566     8
   1058   129    15   314    12 45250    50   235    12 45250  6639     9
    258 30526     8  7302 30700     2]]"
c49ee6ac4dc812ff84d255886fd5aff794f53c39,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
975e60535724f4149c7488699a199ba2920a062c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
1d9aeeaa6efa1367c22be0718f5a5635a73844bd,0.0,Entailment,[[    2     0 30495  3760  1757     2]], the context and sequential nature of the text,[[    0     5  5377     8 29698  2574     9     5  2788     2]]
b5bc34e1e381dbf972d0b594fe8c66ff75305d71,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively","[[    0  2709  2370 42532   217   944   487  6006 35153     8 13302   139
  44691   245     4   288     6    84  1850  5448  9980 33334   163 18854
     12   448  5199  5383   387 45935  3170    30  2055   288     4  2890
      8  2055   288     4  5607  4067   482  1307   819 25941    15  1111
  42532     6  9499   274   134  5139    30  2055   288     4  6750     8
   2055   176     4  3367    15  6253  4396     8 13302   139 44691   306
      4   288     6  4067     2]]"
72755c2d79210857cfff60bfbcb55f83c71ada51,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"104 telephone calls, transcripts contain 168,195 Spanish word tokens,  translations contain 159,777 English word tokens","[[    0 17573  7377  1519     6 20382  5585 26372     6 23688  3453  2136
  22121     6  1437 41762  5585 28083     6 33416  2370  2136 22121     2]]"
f8da63df16c4c42093e5778c01a8e7e9b270142e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],we compare the Annodis segmentation with the automatically produced segmentation,"[[   0 1694 8933    5 3921 1630  354 2835 1258   19    5 6885 2622 2835
  1258    2]]"
f225a9f923e4cdd836dd8fe097848da06ec3e0cc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SQuAD,[[    0   104 12444  2606     2]]
06cc8fcafc0880cf69a2514bb7341642b9833041,0.0,Entailment,[[    2     0 30495  3760  1757     2]], INLINEFORM1 cases,[[    0  2808 28302 38036   134  1200     2]]
f2155dc4aeab86bf31a838c8ff388c85440fce6e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
cb6a8c642575d3577d1840ca2f4cd2cc2c3397c5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
fe2666ace293b4bfac3182db6d0c6f03ea799277,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"to acquire very large Vietnamese corpus and to use them in building a classifier,  design and development of big data warehouse and analytic framework for Vietnamese documents, to building a system, which is able to incrementally learn new corpora and interactively process feedback","[[    0   560  6860   182   739 16859 42168     8     7   304   106    11
    745    10  1380 24072     6  1437  1521     8   709     9   380   414
  12283     8 43474  7208    13 16859  2339     6     7   745    10   467
      6    61    16   441     7 30401  2368  1532    92 22997   102     8
  10754  6608   609  6456     2]]"
5b551ba47d582f2e6467b1b91a8d4d6a30c343ec,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," Distinct-1/2, UMA = User Matching Accuracy, MRR
= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)","[[    0 11281 24115    12   134    73   176     6   121  5273  5457 27913
   9018   154 42688     6 18838   500 50118  5214 30750  7382  1588 46966
  17816     6 18390  5457 34587 10715 12832    81 18043    36 28017 14133
     13 25444 10324 15029   228  1421    43     2]]"
c5abe97625b9e1c8de8208e15d59c704a597b88c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40,"[[    0   530   534    12   250   176   347    12   611  7153     8   229
    534    12   250   176   347    12 47526   258  1323     5 40750     9
     10  1471     9   843     2]]"
71b1af123fe292fd9950b8439db834212f0b0e32,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process","[[    0   134     4  4028 45068  2630   531 28467    41 48335  1471   227
    321     8   231    36   179 15871    43  9172   141  9031 38600  1122
      5    80  1617    11    10   576  1763    32     4    83  1471     9
    231  8711   182   239 37015    36   118     4   242   482  1969 17796
   6119  4783   238   150  4276  8711   117 37015   482   132     4  4028
  45068  2630   531  1471     5  1445   278     9   112     6 22410 15029
     11     5 41616   482  1437   441     7   304  6731  1715    36   242
      4   571     4 45073  5119     6     5    29 34820     6 15690 15721
     43   114  1552     6    45   441     7  8469    19   349    97   148
      5 47760   609     2]]"
6becff2967fe7c5256fe0b00231765be5b9db9f1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution","[[    0   102  1675  1938  1385  3059    19  7974  1575     6     5  4532
  47382     9  1380  3854  1675  1938  1385     6     5 26544 37178   227
   5135     8  6126  1380  3854     2]]"
32d99dcd8d46e2cda04a9a9fa0e6693d2349a7a9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],An additive term added to the cost function for any one of the words of concept word-groups,"[[    0  4688 37480  1385   355     7     5   701  5043    13   143    65
      9     5  1617     9  4286  2136    12 36378     2]]"
d3ca5f1814860a88ff30761fec3d860d35e39167,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)","[[    0 48089  9860 47145     6 17515   196  6513  1459   331  5428  6588
   7742    36 34189  4014   238  1437   323 37681  6271    36   104 20954
    238 23431  9624  5447    36  9822   597    43     2]]"
bbb77f2d6685c9257763ca38afaaef29044b4018,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Gaze Sarcasm using Multi Instance Logistic Regression.,"[[    0   534 10129   208  9636 16836   634 19268  8857  2389  9359  5580
   6304 21791     4     2]]"
5eda469a8a77f028d0c5f1acd296111085614537,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-En, En-Ru","[[    0 13365    12 16040     6  2271    12 29220     6  4967    12 16040
      6  2271    12 15286     6  3830    12 16040     6  2271    12 13365
      6  1586    12 16040     6  2271    12 36439     2]]"
bfc1de5fa4da2f0e301fd22aea19cf01e2bb5b31,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English, Spanish, Finnish",[[    0 35007     6  3453     6 21533     2]]
cb384dc5366b693f28680374d31ff45356af0461,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
ea56148a8356a1918bedcf0a99ae667c27792cfe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"FCE ,  two alternative annotations of the CoNLL 2014 Shared Task dataset","[[    0  5268   717  2156  1437    80  3626 47234     9     5   944   487
   6006   777 38559 12927 41616     2]]"
cc608df2884e1e82679f663ed9d9d67a4b6c03f3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"precision, recall, F1 and accuracy",[[    0  5234 37938     6  6001     6   274   134     8  8611     2]]
72ce05546c81ada05885026470f4c8c218805055,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
f17ca24b135f9fe6bb25dc5084b13e1637ec7744,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Best: Expansion (Exp). Worst: Comparison (Comp).,"[[    0 19183    35 32408    36 39891   322 31846    35 37070    36 24699
    322     2]]"
3e1829e96c968cbd8ad8e9ce850e3a92a76b26e4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],212 accounts,[[    0 22145  2349     2]]
e9cfbfdf30e48cffdeca58d4ac6fdd66a8b27d7a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],provide only a description of the document cluster's topic along with the propositions,"[[    0 13138  1949   129    10  8194     9     5  3780 18016    18  5674
    552    19     5 41356     2]]"
e4a315e9c190cf96493eefe04ce4ba6ae6894550,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Henderson:2017, MobileNet model",[[    0   725  1397  4277    35  3789     6  6698 15721  1421     2]]
00050f7365e317dc0487e282a4c33804b58b1fb3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
682e26262abba473412f68cbeb5f69aa3b9968d7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],no prior work has explored the target of the offensive language,"[[    0  2362  2052   173    34 16217     5  1002     9     5  2555  2777
      2]]"
68df324e5fa697baed25c761d0be4c528f7f5cf7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation","[[    0  8739    12 45963 22565     6 44489 17362     6  9944  1069 28017
   7192     6 33868 17362     2]]"
5fda8539a97828e188ba26aad5cda1b9dd642bc8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],F1 score of 97.5 on MSR and 95.7 on AS,"[[   0  597  134 1471    9 8783    4  245   15 6253  500    8 6164    4
   406   15 6015    2]]"
f03df5d99b753dc4833ef27b32bb95ba53d790ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"have a larger proportion of friends/followers (i.e. they have, on average, the same number of friends but a smaller number of followers) than those spreading viral content only","[[    0 11990    10  2514 10301     9   964    73 28481   268    36   118
      4   242     4    51    33     6    15   674     6     5   276   346
      9   964    53    10  2735   346     9  6059    43    87   167  9592
   7696  1383   129     2]]"
ceb767e33fde4b927e730f893db5ece947ffb0d8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Demographics, Diagnosis History, Medication History, Procedure History, Symptoms, Labs, Procedures, Treatments, Hospital movements, and others","[[    0 24658 43685     6 23465 13310  7298     6  5066 14086  7298     6
  40209  7298     6 32393     6 20404     6 44232     6 26129  2963     6
   2392  7467     6     8   643     2]]"
17a1eff7993c47c54eddc7344e7454fbe64191cd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Human evaluation for interpretability using the word intrusion test and automated evaluation for interpretability using a semantic category-based approach based on the method and category dataset (SEMCAT).,"[[    0 33837 10437    13 18107  4484   634     5  2136 32028  1296     8
  11554 10437    13 18107  4484   634    10 46195  4120    12   805  1548
    716    15     5  5448     8  4120 41616    36   104  5330   347  2571
    322     2]]"
0f6216b9e4e59252b0c1adfd1a848635437dfcdc,0.0,Entailment,[[    2     0 30495  3760  1757     2]], Selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment provided by organizers and  tweets translated form English to Spanish.,"[[    0 30418     9  6245    19    13   349  3545    10  6929  9072     5
  10603     9     5 11926    50  5702  1286    30  9921     8  1437  6245
  16877  1026  2370     7  3453     4     2]]"
07d15501a599bae7eb4a9ead63e9df3d55b3dc35,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
f94b53db307685d572aefad52cd55f53d23769c2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3","[[    0  2050 30623     5 37832     9    41 36019  2630     6  1437   381
  10020  5448    11   163  8863 45935   246    16   818     5   276    25
    381  1343     4    36 29612 45935   398 31311  4682     5 43141    18
  13071    16    45 33756    11   163  8863 45935   246     2]]"
b6f7fadaa1bb828530c2d6780289f12740229d84,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English-German, English-French",[[    0 35007    12 27709     6  2370    12 28586     2]]
6b53e1f46ae4ba9b75117fc6e593abded89366be,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"As the simplest baseline, a sensitive data recogniser and classifier, Conditional Random Fields (CRF), spaCy ","[[    0  1620     5 31625 18043     6    10  5685   414 15603  5999     8
   1380 24072     6 12108 24176 34638 15502    36  9822   597   238 18179
  25826  1437     2]]"
0a75a52450ed866df3a304077769e1725a995bb7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Decoder predicts the sequence of phoneme or grapheme at each time based on the previous output and context information with a beam search strategy,"[[    0 15953 15362 17876     5 13931     9 43676 17185    50 47660   700
   1794    23   349    86   716    15     5   986  4195     8  5377   335
     19    10 23480  1707  1860     2]]"
2c78993524ca62bf1f525b60f2220a374d0e3535,0.0,Entailment,[[    2     0 30495  3760  1757     2]],rupnik2016news,[[    0 15566  8256  9029  2926     2]]
6becff2967fe7c5256fe0b00231765be5b9db9f1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution","[[    0   102  1675  1938  1385  3059    19  7974  1575     6  1437     5
   4532 47382     9  1380  3854     6 26544 37178   227  5135     8  6126
   1380  3854     2]]"
2916bbdb95ef31ab26527ba67961cf5ec94d6afe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The corpus comprises 8,275 sentences and 167,739 words in total.","[[    0   133 42168 16755   290     6 21138 11305     8 28833     6   406
   3416  1617    11   746     4     2]]"
a74190189a6ced2a2d5b781e445e36f4e527e82a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],significant improvements clearly demonstrate that our approach is effective at improving model performance,"[[    0 18880  5139  2563  8085    14    84  1548    16  2375    23  3927
   1421   819     2]]"
1038542243efe5ab3e65c89385e53c4831cd9981,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"DTA18, DTA19",[[   0  495 3847 1366    6  211 3847 1646    2]]
b4f881331b975e6e4cab1868267211ed729d782d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"33,663",[[    0  3103     6 39505     2]]
352bc6de5c5068c6c19062bad1b8f644919b1145,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we considered 4 different proportions i.e., INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 of the training data to train the classifier","[[    0  1694  1687   204   430 27311   939     4   242   482  2808 28302
  38036   288  2156  2808 28302 38036   134  2156  2808 28302 38036   176
      8  2808 28302 38036   246     9     5  1058   414     7  2341     5
   1380 24072     2]]"
f9bf6bef946012dd42835bf0c547c0de9c1d229f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation.,"[[    0 42949 37118    21 16274    19   943 14105    11 48081 29617    19
    786 21993 44919     4     2]]"
b962cc817a4baf6c56150f0d97097f18ad6cd9ed,0.0,Entailment,[[    2     0 30495  3760  1757     2]],These 8 tasks require different competencies and a different level of understanding of the document to be well answered,"[[    0  4528   290  8558  2703   430 12766 14768     8    10   430   672
      9  2969     9     5  3780     7    28   157  7173     2]]"
ef3567ce7301b28e34377e7b62c4ec9b496f00bf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Groningen Meaning Bank (GMB),[[    0   534  2839 34264 35972   788    36   534  8651    43     2]]
1be54c5b3ea67d837ffba2290a40c1e720d9587f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
e1ab11885f72b4658263a60751d956ba661c1d61,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (Subscript 1: ""We did not participate in subtask 5 (E-c)"") Authors participated in EI-Reg, EI-Oc, V-Reg and V-Oc subtasks.","[[    0 33683    19  1383  1716    35    36 23055 32761   112    35    22
    170   222    45  4064    11 30757  4970   195    36   717    12   438
     43  8070 40090  7849    11   381   100    12 23007     6   381   100
     12   673   438     6   468    12 23007     8   468    12   673   438
  30757 40981     4     2]]"
761de1610e934189850e8fda707dc5239dd58092,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17","[[    0   642 35709    12   805 19850 13304    15    10  1903  2777   163
   8863 45935   698     6   295 42705     9 11054  9248    31  6154 31992
   5564   414   163  8863 45935  1570  2156  1503   337   248 20057    12
    805  1421    36 39151 11674    43   163  8863 45935   176     6  5428
  22098  1421   163  8863 45935  1366     6  4003    12 42184   337  1421
    163  8863 45935  1225     6  3228    12   560    12 42274    36   448
    176   448    43  1421   163  8863 45935   398     6   124    12 48235
    163  8863 45935  1360     2]]"
3c3807f226ba72fc41f59f0338f12a49a0c35605,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
06eb9f2320451df83e27362c22eb02f4a426a018,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"raw text, text cleaning through document logical structure detection, removal of keyphrase sparse sections of the document","[[    0  9056  2788     6  2788  8143   149  3780 16437  3184 12673     6
   7129     9   762 40726 28593  9042     9     5  3780     2]]"
dac087e1328e65ca08f66d8b5307d6624bf3943f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
55c8f7acbfd4f5cde634aaecd775b3bb32e9ffa3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"PER, WER, WER 100",[[    0 21260     6   305  2076     6   305  2076   727     2]]
1dc2da5078a7e5ea82ccd1c90d81999a922bc9bf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
1cb100182508cf55b3509283c0e2bbcd527d625e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles","[[    0 12105  1450     7   340 11208  7201    50     7  5059  4570     6
   7900  4570     6  5059  4570     6   340 11208  7201     2]]"
c21b87c97d1afac85ece2450ee76d01c946de668,0.0,Entailment,[[    2     0 30495  3760  1757     2]], pointer networks with coverage mechanism (PG-net)BIBREF0,"[[    0 41515  4836    19  1953  9562    36  8332    12  4135    43  5383
    387 45935   288     2]]"
71fe5822d9fccb1cb391c11283b223dc8aa1640c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets","[[    0 33919  2055 13379    12  1116    12 30938     6 12244   176 25369
      6 40815  2055   404 21309    36    90 21210    12  4483   238 40815
   2055   404 21309    36   611  6435    12  4483   238 19127 44445    36
     90 21210    12  4483   238  3107 10068   330  1629 21390     6  3829
      6    50   769    12    90  1694  2580     2]]"
71bd5db79635d48a0730163a9f2e8ef19a86cd66,0.0,Entailment,[[    2     0 30495  3760  1757     2]],semantics-altering grammatical modifiers,[[    0 26976 46304    12   337 14322 25187 45816 47605     2]]
ef4dba073d24042f24886580ae77add5326f2130,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ","[[    0   597   134  4391     9  5663     4  2831    15     5 13925    12
   3888   414     6  1437  3337     4   996    15     5 11270    12 11674
    414     8  6121     4  4540    15     5 11270    12   791  1864   414
   1437     2]]"
c01784b995f6594fdb23d7b62f20a35ae73eaa77,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game","[[    0 12501  3009   775    30 15582   818   457     9     5 30772   426
      6   144     9     5   685   426    32    11     5 11111   278     6
    147    10   182   251 13931     9  2163    16  1552    13  1298     5
    177     2]]"
b1cf5739467ba90059add58d11b73d075a11ec86,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
55bd59076a49b19d3283af41c5e3ccb875f3eb0c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CNN ,[[    0 16256  1437     2]]
a130306c6662ff489df13fb3f8faa7cba8c52a21,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," f-pooling, fo-pooling, and ifo-pooling ","[[    0   856    12 10416   154     6  9565    12 10416   154     6     8
    114   139    12 10416   154  1437     2]]"
777217e025132ddc173cf33747ee590628a8f62f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Calculate test log-likelihood on the three considered datasets,"[[    0 15117 13300   877  1296  7425    12  3341 45286    15     5   130
   1687 42532     2]]"
adbf33c6144b2f5c40d0c6a328a92687a476f371,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a9337636b52de375c852682a2561af2c1db5ec63,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
cd2878c5a52542ddf080b20bec005d9a74f2d916,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Technology, Religion, Fashion, Publishing, Sports coach, Real Estate, Law, Environment, Tourism, Construction, Museums, Banking, Security, Automotive.","[[    0 38976     6 34530     6  9265     6 25290     6  1847   704     6
   2822  9730     6  2589     6  9356     6 11860     6  8911     6 19345
   8014     6 12539     6  2010     6  6628  9719     4     2]]"
20e38438471266ce021817c6364f6a46d01564f2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.","[[    0  3762    64   206 44612   134    12   642 49747   118   134 49424
   1629    25    10  2408  3059    19   349  1246     6    61  1022    25
   1058  6938     4    20 39054     9  2992    68   642 49747   118   134
  24303  1629     7 44612   134    12   642 49747   118   134 49424   181
  49747   118   134 24303  1629    16     7  1920   159     5  2408     9
   1365  7721     4   286  1365  7721  1060 18102    32  8955   321    50
    112     6 44612   134    12   642 49747   118   134 49424   181 49747
    118   134 24303  1629   817     5  1421 24051  3625   540  1056     7
    106     4 17965 37365 45935  1922  2029  2029    41  8257    31     5
   4263    11 30666    35     5 30666     9 49959 48877 45152  1640   134
     12   642    43   642 48832   134 43988   134    12   642    43   642
  24303  1629    19  2098     7    68   642  1629  8369   321  1320    71
     68   642  1629  8369   321     6    61   839     5  1421 16298   540
      7  7721   683    51    32 12461  8967     4     2]]"
197b276d0610ebfacd57ab46b0b29f3033c96a40,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK, Pattern-based approach","[[    0 23345  5580  6304 21791     6 14910   179 48866  7300  2088  1501
    293     6 34638  5761     6 33202 43389     6 45085   208 20954     6
    208 20954    19  4516   771 17342     6 42762    12   805  1548     2]]"
0fc2b5bc2ead08a6fe0280fb3a47477c6df1587c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
bb2de20ee5937da7e3e6230e942bec7b6e8f61ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],daily newspaper of the year 2015-2016,[[    0 16624  2924     9     5    76   570    12  9029     2]]
c08aab979dcdc8f4fe8ec1337c3c8290ab13414e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
8a1c0ef69b6022a0642ca131a8eacb5c97016640,0.0,Entailment,[[    2     0 30495  3760  1757     2]],using tweets that one has replied or quoted to as contextual information,"[[    0 10928  6245    14    65    34  6849    50  5304     7    25 37617
    335     2]]"
9bfebf8e5bc0bacf0af96a9a951eb7b96b359faa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time","[[    0 20365 10324    12  4483  1029 40584  4391     9   112     4  5479
     12   134     4  6551     6  1050 37131   257  3629  6813 17159  1421
  39512     7 18043  5549   207     9     5    86     2]]"
352bc6de5c5068c6c19062bad1b8f644919b1145,0.0,Entailment,[[    2     0 30495  3760  1757     2]],535,[[    0 37020     2]]
e35a7f9513ff1cc0f0520f1d4ad9168a47dc18bb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"traditional phrase-based statistical machine translation (SMT), NMT system","[[    0 30172 11054    12   805 17325  3563 19850    36 15153   565   238
    234 11674   467     2]]"
2ddb51b03163d309434ee403fef42d6b9aecc458,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
58355e2a782bf145b61ee2a3e0e426119985c179,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The dataset from the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference. ,"[[    0   133 41616    31     5 14585   493   574  2454   178  2482 34145
  12791     9     5 11270 10537 25011 27140   954  2815     4  1437     2]]"
fd8e23947095fe2230ffe1a478945829b09c8c95,0.0,Entailment,[[    2     0 30495  3760  1757     2]],utilize the machinery of language modeling using deep neural networks to learn Dolores embeddings.,"[[    0 32843  2072     5 13922     9  2777 19039   634  1844 26739  4836
      7  1532 13520  4765 33183   417  1033     4     2]]"
01e2d10178347d177519f792f86f25575106ddc7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LORELEI datasets of Uzbek, Mandarin and Turkish","[[    0   574  7563  3850   100 42532     9 23502     6 33830     8  4423
      2]]"
8c852fc29bda014d28c3ee5b5a7e449ab9152d35,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) ","[[    0 43871   208 20954  5389    15  2136   542  1023 27809     6  1437
   2311 43606   337  2597  7787    12 23036    12 47184    36 37426   574
   4014   448   238  1437 30505 23794   337 44304  3658    36 16256    43
   1437     2]]"
78292bc57ee68fdb93ed45430d80acca25a9e916,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"To this end, we introduce the negated LAMA dataset. We construct it by simply inserting negation elements (e.g., “not”) in LAMA cloze statement","[[    0  3972    42   253     6    52  6581     5 15183  1070   226 19455
  41616     4   166 12558    24    30  1622 39886 15183  1258  4785    36
    242     4   571   482    44    48  3654    17    46    43    11   226
  19455 42771  2158   445     2]]"
6ca938324dc7e1742a840d0a54dc13cc207394a1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"German newscrawl distributed by WMT'18 , English newscrawl data, WMT'18 English-German (en-de) news translation task , WMT'18 English-Turkish (en-tr) news task","[[    0 27709    92  3866 33889  7664    30   305 11674   108  1366  2156
   2370    92  3866 33889   414     6   305 11674   108  1366  2370    12
  27709    36   225    12  2794    43   340 19850  3685  2156   305 11674
    108  1366  2370    12 37300    36   225    12  4328    43   340  3685
      2]]"
2f901dab6b757e12763b23ae8b37ae2e517a2271,0.0,Entailment,[[    2     0 30495  3760  1757     2]],German–English,[[    0 27709  2383 35007     2]]
f71b52e00e0be80c926f153b9fe0a06dd93af11e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"average content score across the paintings is 3.7, average creativity score is 3.9, average style score is 3.9 ","[[    0 20365  1383  1471   420     5 13013    16   155     4   406     6
    674 11140  1471    16   155     4   466     6   674  2496  1471    16
    155     4   466  1437     2]]"
beac555c4aea76c88f19db7cc901fa638765c250,0.0,Entailment,[[    2     0 30495  3760  1757     2]],it captures other information rather than only the translational equivalent in the case of verbs,"[[    0   405 19369    97   335  1195    87   129     5 37297  5033  6305
     11     5   403     9 47041     2]]"
cc354c952b5aaed2d4d1e932175e008ff2d801dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Females are given higher sentiment intensity when predicting anger, joy or valence, but males are given higher sentiment intensity when predicting  fear.
African American names are given higher score on the tasks of anger, fear, and sadness intensity prediction,  but European American names are given higher scores on joy and valence task.","[[    0   597   991  4575    32   576   723  5702 10603    77 15924  6378
      6  5823    50  7398  4086     6    53 14705    32   576   723  5702
  10603    77 15924  1437  2490     4 50118 30112   470  2523    32   576
    723  1471    15     5  8558     9  6378     6  2490     6     8 17437
  10603 16782     6  1437    53   796   470  2523    32   576   723  4391
     15  5823     8  7398  4086  3685     4     2]]"
dd6b378d89c05058e8f49e48fd48f5c458ea2ebc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT","[[    0 43597 24176 34638 15502     6  6479   574  4014   448    12  9822
    597     6 19268    12 47744 13807     6 12334 11126   565     2]]"
e8f969ffd637b82d04d3be28c51f0f3ca6b3883e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Quantitative evaluation methods using ROUGE, Recall, Precision and F1.","[[    0 44572 26709 10437  6448   634   248  5061  8800     6 35109     6
  29484     8   274   134     4     2]]"
ff338921e34c15baf1eae0074938bf79ee65fdd2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],by answering always YES (in batch 2 and 3) ,"[[    0  1409 15635   460 32463    36   179 14398   132     8   155    43
   1437     2]]"
0f7867f888109b9e000ef68965df4dde2511a55f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes, In case of a tie, we pick one of the most frequent classes randomly","[[    0  1694  3253   484  3480     8   248 20057  3092  2633    11 36576
    255  4546 45935  1092     8   255  4546 45935  1570     8  7006     5
   1380    19     5   144  2834     6    96   403     9    10  3318     6
     52  1339    65     9     5   144  7690  4050 22422     2]]"
5a65ad10ff954d0f27bb3ccd9027e3d8f7f6bb76,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Akbik et al. (2018), Link et al. (2012)","[[    0   250 41238   967  4400  1076     4    36  2464   238  4341  4400
   1076     4    36 14517    43     2]]"
127d5ddfabec5c58832e5865cbd8ed0978c25a13,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a simple word-level encoder, The encoder is essentially the same as tweet2vec, with the input as words instead of characters.","[[    0   102  2007  2136    12  4483  9689 15362     6    20  9689 15362
     16  5700     5   276    25  3545   176 25369     6    19     5  8135
     25  1617  1386     9  3768     4     2]]"
62ea141d0fb342dfb97c69b49d1c978665b93b3c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"grammatical, spelling and word order errors",[[    0 28526 45816     6 24684     8  2136   645  9126     2]]
5a22293b055f5775081d6acdc0450f7bd5f5de04,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"OpATT BIBREF6, Neural Content Planning with conditional copy (NCP+CC) BIBREF4","[[    0 13926 26107   163  8863 45935   401     6 44304 12803  9619    19
  23431  5375    36  6905   510  2744  3376    43   163  8863 45935   306
      2]]"
63c3550c6fb42f41a0c93133e9fca12ac00df9b3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
df5a4505edccc0ee11349ed6e7958cf6b84c9ed4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators,"[[    0  9119 17970    31  2491 31464   463   661     8   316   786    12
  27128  1073   463   661   340  6639     8   172 45068  1070    30  2038
  45068  3629     2]]"
f94b53db307685d572aefad52cd55f53d23769c2,0.0,Entailment,[[    2     0 30495  3760  1757     2]], Fisher Information Ratio,[[    0  6868  3522 20475     2]]
a5e49cdb91d9fd0ca625cc1ede236d3d4672403c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
9b97805a0c093df405391a85e4d3ab447671c86a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Exact Match (EM), Macro-averaged F1 scores (F1)","[[    0  9089  7257  9018    36  5330   238 32449    12  9903  4628   274
    134  4391    36   597   134    43     2]]"
45e9533586199bde19313cd43b3d0ecadcaf7a33,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
7006c66a15477b917656f435d66f63760d33a304,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Average success rate is higher by 2.6 percent points.,"[[    0 46315  1282   731    16   723    30   132     4   401   135   332
      4     2]]"
81d607fc206198162faa54a796717c2805282d9b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Individuals with legal training,[[    0 43174    29    19  1030  1058     2]]
b591853e938984e6069d738371500ebdec50d256,0.0,Entailment,[[    2     0 30495  3760  1757     2]],IMDb movie review,[[    0  3755 42198  1569  1551     2]]
bf52c01bf82612d0c7bbf2e6a5bb2570c322936f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Using Pearson corelation measure,  for example, ROUGE-1-P is 0.257 and ROUGE-3-F 0.878.","[[    0 36949 16116  2731 35019  2450     6  1437    13  1246     6   248
   5061  8800    12   134    12   510    16   321     4 31297     8   248
   5061  8800    12   246    12   597   321     4   398  5479     4     2]]"
53dfcd5d7d2a81855ec1728f0d8e6e24c5638f1e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU-1, Meteor , Rouge-L ","[[    0 30876   791    12   134     6 16582  2156 14941    12   574  1437
      2]]"
477d9d3376af4d938bb01280fe48d9ae7c9cf7f7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4), METEOR (MET), ROUGE-L (R-L)","[[    0 30876   791    12   134    36   387   134   238   163  3850   791
     12   176    36   387   176   238   163  3850   791    12   246    36
    387   246   238   163  3850   791    12   306    36   387   306   238
  30782   717  3411    36 43543   238   248  5061  8800    12   574    36
    500    12   574    43     2]]"
44497509fdf5e87cff05cdcbe254fbd288d857ad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
c2e475adeddcdc4d637ef0d4f5065b6a9b299827,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU-4, NIST-4, ROUGE-4","[[    0 30876   791    12   306     6   234 11595    12   306     6   248
   5061  8800    12   306     2]]"
badc9db40adbbf2ea7bac29f2e4e3b6b9175b1f9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"F1 score of 92.19 on homographic pun detection, 80.19 on homographic pun location, 89.76 on heterographic pun detection.","[[    0   597   134  1471     9  8403     4  1646    15  9486 25510  7434
  12673     6  1812     4  1646    15  9486 25510  7434  2259     6  8572
      4  5067    15 39872 25510  7434 12673     4     2]]"
6ca938324dc7e1742a840d0a54dc13cc207394a1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"German newscrawl, English newscrawl, WMT'18 English-German (en-de) news, WMT'18 English-Turkish (en-tr) news task, WMT'18 English-German (en-de) news translation task and we validate our findings on the WMT'18 English-Turkish (en-tr) news task","[[    0 27709    92  3866 33889     6  2370    92  3866 33889     6   305
  11674   108  1366  2370    12 27709    36   225    12  2794    43   340
      6   305 11674   108  1366  2370    12 37300    36   225    12  4328
     43   340  3685     6   305 11674   108  1366  2370    12 27709    36
    225    12  2794    43   340 19850  3685     8    52 28754    84  4139
     15     5   305 11674   108  1366  2370    12 37300    36   225    12
   4328    43   340  3685     2]]"
b6f466e0fdcb310ecd212fd90396d9d13e0c0504,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Data already contain errors,[[    0 30383   416  5585  9126     2]]
3c93894c4baf49deacc6ed2a14ef5e0f13b7d96f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"density of users, gender distribution",[[    0 37373     9  1434     6  3959  3854     2]]
f56d07f73b31a9c72ea737b40103d7004ef6a079,0.0,Entailment,[[    2     0 30495  3760  1757     2]],A homographic and heterographic benchmark datasets by BIBREF9.,"[[    0   250  9486 25510     8 39872 25510  5437 42532    30   163  8863
  45935   466     4     2]]"
e2e977d7222654ee8d983fd8ba63b930e9a5a691,0.0,Entailment,[[    2     0 30495  3760  1757     2]],uni-directional RNN,[[    0 20967    12 42184   337   248 20057     2]]
932b39fd6c47c6a880621a62e6a978491d881d60,0.0,Entailment,[[    2     0 30495  3760  1757     2]],TransE,[[    0 19163   717     2]]
6424e442b34a576f904d9649d63acf1e4fdefdfc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d6e353e0231d09fd5dcba493544d53706f3fe1ab,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"To compare the conversions between USVC and PitchNet, we employed an automatic evaluation score and a human evaluation score.","[[    0  3972  8933     5 30869   227   382 14858     8 24902 15721     6
     52  7460    41  8408 10437  1471     8    10  1050 10437  1471     4
      2]]"
df0257ab04686ddf1c6c4d9b0529a7632330b98e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"On Coin Collector, proposed model finds shorter path in fewer number of interactions with enironment.
On Cooking World, proposed model uses smallest amount of steps and on average has bigger score and number of wins by significant margin.","[[    0  4148 17203 38934     6  1850  1421  5684 10941  2718    11  4163
    346     9 11324    19  1177 11680  1757     4 50118  4148 32284   623
      6  1850  1421  2939 15654  1280     9  2402     8    15   674    34
   2671  1471     8   346     9  2693    30  1233  2759     4     2]]"
627b8d7b5b985394428c974aca5ba0c1bbbba377,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
0ad4359e3e7e5e5f261c2668fe84c12bc762b3b8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). 
Stanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018).","[[    0 35212  4086 20257 11909 38630    35   248 16966   487    36 29971
   1843  4400  1076     4  1014   238 33202  6018    12   500 16966   487
     36   495  1657  4400  1076     4   777   238 13180    12   500 16966
    487    36  1864   811  4400  1076     4   570   238   255  3573 20057
     36   448  1438  4400  1076     4   570   238 11077    12   574  4014
    448    36 41064     6 14585  1843     6     8  9967   570   238 33202
  14469    12   574  4014   448    12 18814    36   574  9060     6  1209
   9060     6     8 25918   193   238  5815    12 33731   574  4014   448
     36   574  9060     6  1209  9060     6     8 25918   193   238 13180
     12   574  4014   448    36 39087  1097     6 44069     6     8 32942
    193   238  6479  9157 33731    36   565  3314     8 16087   193   238
    272  4179   523 11077    12   574  4014   448    36 43625   118     6
    854  3036     6     8  2094   199   238 11077 15721    36 16764  2590
   4400  1076     4   199   238  3480    36 18806   777   238 33202 35212
     36  1301 42678     6  5078     6     8   221  1438  7755   570   238
    226  4014   448    12 16256    36  1301 24181  4400  1076     4   336
    238 47893    12 43619  4014   448    36 28243  1891     6  3889   329
   4550 30268     6     8   208  7046  1071  2802   193   238  9543   487
   2055  7275  2055   944 30660    36 37730  2279  4400  1076     4   193
    238  9543   487  2055  7275  2055 17678 17357    36   510 20413  4400
   1076     4   199   322  1437 50118 36118  1891  7278 22205    96 23861
  11909 38630    35  9882  1342 33221  3631 11077    12   574  4014   448
     36   975  2154   415  2583  4400  1076     4   193   238 11077    12
    805  3480    36   448  1438  4400  1076     4   336   238   272  4179
    523 11077    12   574  4014   448    36 43625   118     6   854  3036
      6     8  2094   199   238   234  3388    36   448  6435 35905   337
   1439     8 11698   193   238 17789 25258 12156    12 35798  3658    36
    104  2457  4400  1076     4   199   238  4787   808  5564 19030  9689
   1630   268    35    36   487   324     8 35596   337   193   238  6479
    574  4014   448    19 44030  3716   154    36   347  2457     6 17900
      6     8 32942   199   322     2]]"
7595260c5747aede0b32b7414e13899869209506,0.0,Entailment,[[    2     0 30495  3760  1757     2]],IMDb dataset of movie reviews,[[    0  3755 42198 41616     9  1569  6173     2]]
a1b3e2107302c5a993baafbe177684ae88d6f505,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Dataset contains 3606 total sentences and 79087 total entities.,"[[    0 43703   281   594  6308 10253   401   746 11305     8  7589  3669
    406   746  8866     4     2]]"
5daeb8d4d6f3b8543ec6309a7a35523e160437eb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English,[[    0 35007     2]]
6424e442b34a576f904d9649d63acf1e4fdefdfc,0.0,Entailment,[[    2     0 30495  3760  1757     2]], Wall Street Journal (WSJ) portion of the Penn Treebank,"[[    0  2298   852  3642    36 13691   863    43  4745     9     5  5953
  11077  5760     2]]"
50e80cfa84200717921840fddcf3b051a9216ad8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
7ab9c0b4ceca1c142ff068f85015a249b14282d0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],construct context-gloss pairs from glosses of all possible senses (in WordNet) of the target word,"[[    0 42843  5377    12  7210  5434 15029    31 27293   293     9    70
    678 24074    36   179 15690 15721    43     9     5  1002  2136     2]]"
c37f65c9f0d543a35c784263b79236ccf1c44fac,0.0,Entailment,[[    2     0 30495  3760  1757     2]],LSTM,[[   0  574 4014  448    2]]
f5e6f43454332e0521a778db0b769481e23e7682,0.0,Entailment,[[    2     0 30495  3760  1757     2]],firstly translates a source language into the pivot language which is later translated to the target language,"[[    0  9502   352 19303    10  1300  2777    88     5 27475  2777    61
     16   423 16877     7     5  1002  2777     2]]"
f8da63df16c4c42093e5778c01a8e7e9b270142e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Segmentation quality is evaluated by calculating the precision, recall, and F-score of the automatic segmentations in comparison to the segmentations made by expert annotators from the ANNODIS subcorpus.","[[    0 42775 36197  1318    16 15423    30 29770     5 15339     6  6001
      6     8   274    12 31673     9     5  8408  2835  1635    11  6676
      7     5  2835  1635   156    30  3827 45068  3629    31     5 31256
   7111  1729  2849  7215   642   687     4     2]]"
692c9c5d9ff9cd3e0ce8b5e4fa68dda9bd23dec1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"20,000",[[  0 844   6 151   2]]
71a0c4f19be4ce1b1bae58a6e8f2a586e125d074,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"quality class labels assigned by the Wikipedia community, a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI","[[    0  8634  1380 14105  5530    30     5 28274   435     6    10  2225
     16  1687     7    33    57  3903    36   118     4   242     4    16
  13541 16274    43   114    24  2856    10  2225    11     5   211  7976
    510  8503    50    16  3680  3903    30   143     9     5   511 14041
     35 21147     6 14850   487 21992     6  8438  2562   574     6   381
   2562   574     6   255  2562   574     6 17378  3888     6  8242 10537
      6    38  7454   500     6    50 17147   100     2]]"
aefa333b2cf0a4000cd40566149816f5b36135e7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Accuracy,[[    0 36984 45386     2]]
00c57e45ac6afbdfa67350a57e81b4fad0ed2885,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
deb89bca0925657e0f91ab5daca78b9e548de2bd,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.","[[    0  2621    73 10155  4086     9 46098  3277     6 43676 14414 35937
      6 31617   873  2617     6   239    12  9289 27578  2507     8   239
     12  1644 27578  2507     4     2]]"
dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"support vector machine BIBREF18 , random forest, extra tree and naive bayes classifier BIBREF19","[[    0 22930 37681  3563   163  8863 45935  1366  2156  9624  6693     6
   1823  3907     8 25672 11751   293  1380 24072   163  8863 45935  1646
      2]]"
8de9f14c7c4f37ab103bc8a639d6d80ade1bc27b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],1-hop links to 2-hops,[[    0   134    12  9547  5678     7   132    12 39294     2]]
bd6dc38a9ac8d329114172194b0820766458dacc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the ERP data: BIBREF0,[[    0   627 13895   510   414    35   163  8863 45935   288     2]]
b8d0e4e0e820753ffc107c1847fe1dfd48883989,0.0,Entailment,[[    2     0 30495  3760  1757     2]],More than that in some cases (next to adjacent) ,"[[    0  9690    87    14    11   103  1200    36 25616     7 12142    43
   1437     2]]"
a4d115220438c0ded06a91ad62337061389a6747,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Facebook status update messages,[[    0 18064  2194  2935  3731     2]]
7ae38f51243cb80b16a1df14872b72a1f8a2048f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],we plot T-distributed Stochastic Neighbor Embedding (tSNE) corresponding to INLINEFORM0 and V/C classification tasks in Fig. FIGREF8 .,"[[    0  1694  6197   255    12 17165 18926   312  4306 11599 36968  6133
  13093   154    36    90   104  9009    43 12337     7  2808 28302 38036
    288     8   468    73   347 20257  8558    11 20001     4 37365 45935
    398   479     2]]"
ee417fea65f9b1029455797671da0840c8c1abbe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
c0af8b7bf52dc15e0b33704822c4a34077e09cd1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers.","[[    0  9685   808 43606   337   226  4014   448  4836    19   132     6
    231     6   262     6   290     6     8  1437   361 13171     4     2]]"
600b097475b30480407ce1de81c28c54a0b3b2f8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
1bb7eb5c3d029d95d1abf9f2892c1ec7b6eef306,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning.","[[    0   176     4  3414   207    13  1437 10490    12 10715  1058     6
    132     4  5449   207    13  7018 34775     6   231     4  2481   207
     13  2937  2239     4     2]]"
93beae291b455e5d3ecea6ac73b83632a3ae7ec7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ","[[    0 18377     5  5883 36173     6    52    64  6925     5  2136  3585
      9   349  8135  2136     7     5  1445  4195  3645     4  1437     2]]"
9b7655d39c7a19a23eb8944568eb5618042b9026,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21","[[    0  5383   387 45935  1360     6   163  8863 45935  1366     6 29031
    118 47101   163  8863 45935  1558     6   599   487 21992   163  8863
  45935   401     6   163  8863 45935  1646     6   230  2154 24699    12
    487 21992   163  8863 45935   844     6  8607   234 21992   234  2076
    163  8863 45935  2146     2]]"
f85f2a532e7e700d9f8f9c09cd08d4e47b87bdd3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],create fair systems,[[    0 32845  2105  1743     2]]
4c18081ae3b676cc7831403d11bc070c10120f8e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CLUTO, Carrot2 Lingo",[[    0  7454  6972   673     6  1653 12179   176 17900   139     2]]
01f4a0a19467947a8f3bdd7ec9fac75b5222d710,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Unlabeled sentence-level F1, perplexity, grammatically judgment performance","[[    0  9685 33480   196  3645    12  4483   274   134     6 33708  1571
      6 25187 45236  7579   819     2]]"
8a1c0ef69b6022a0642ca131a8eacb5c97016640,0.0,Entailment,[[    2     0 30495  3760  1757     2]],text sequences of context tweets,[[    0 29015 26929     9  5377  6245     2]]
3cf1edfa6d53a236cf4258afd87c87c0a477e243,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
11e8bd4abf5f8bdabad3e8f0691e6d0ad6c326af,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"NO-MOVE: the only position considered is the starting point, RANDOM: As in BIBREF4, turn to a randomly selected heading, then execute a number of *WALK actions of an average route, JUMP: at each sentence, extract entities from the map and move between them in the order they appear","[[    0 13449    12  8756  8856    35     5   129   737  1687    16     5
   1158   477     6 43015  3765    35   287    11   163  8863 45935   306
      6  1004     7    10 22422  3919  3393     6   172 11189    10   346
      9  1009   771 23284  2163     9    41   674  3420     6   344 23825
     35    23   349  3645     6 14660  8866    31     5  5456     8   517
    227   106    11     5   645    51  2082     2]]"
cb12c19f9d14bef7b2f778892d9071eea2d6c63d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT","[[    0 37447   176 25369  2156  3480     6   211  1889     6 11077    12
    574  4014   448     6 10994 20057     6   226  4014 35306     6   230
     12   574  4014   448     6  6178   534   530     6   305 12550     6
    208    12   771 12550     6 11202 26970    12 16256     6   226  4014
    448    12 11621 20057     6   289   487    12 26107     2]]"
d51dc36fbf6518226b8e45d4c817e07e8f642003,0.0,Entailment,[[    2     0 30495  3760  1757     2]],6946,[[   0 4563 3761    2]]
bbb77f2d6685c9257763ca38afaaef29044b4018,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the MILR classifier,[[    0   627 21695   500  1380 24072     2]]
fb3d30d59ed49e87f63d3735b876d45c4c6b8939,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Precision, Recall, F-measure, accuracy","[[    0 22763 37938     6 35109     6   274    12  1794 24669     6  8611
      2]]"
63bb2040fa107c5296351c2b5f0312336dad2863,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Word clusters are extracted using k-means on word embeddings,"[[    0 44051 28255    32 27380   634   449    12  1794  1253    15  2136
  33183   417  1033     2]]"
de0b650022ad8693465242ded169313419eed7d9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
392fb87564c4f45d0d8d491a9bb217c4fce87f03,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LastStateRNN, AvgRNN, AttentionRNN ","[[    0 10285 13360   500 20057     6 43730   500 20057     6 35798   500
  20057  1437     2]]"
197b276d0610ebfacd57ab46b0b29f3033c96a40,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK and Pattern-based","[[    0 23345  5580  6304 21791     6 14910   179 48866  7300  2088  1501
    293     6 34638  5761     6 33202 43389     6 45085   208 20954     6
    208 20954    19  4516   771 17342     8 42762    12   805     2]]"
bd5bd1765362c2d972a762ca12675108754aa437,0.0,Entailment,[[    2     0 30495  3760  1757     2]],1 percent,[[  0 134 135   2]]
32a3c248b928d4066ce00bbb0053534ee62596e7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],morphosyntactic descriptions (MSD),[[    0 47716   366 45122 28201 24173    36  6222   495    43     2]]
ecc63972b2783ee39b3e522653cfb6dc5917d522,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They group the existing works in terms of the objective function they optimize - within-tweet relationships, inter-tweet relationships, autoencoder, and weak supervision.","[[    0  1213   333     5  2210  1364    11  1110     9     5  4554  5043
     51 22016   111   624    12    90 21210  4158     6  3222    12    90
  21210  4158     6  7241 18057   438 15362     6     8  3953 13702     4
      2]]"
a6d37b5975050da0b1959232ae756fc09e5f87e8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a simple word-level encoder, with the input as words instead of characters","[[    0   102  2007  2136    12  4483  9689 15362     6    19     5  8135
     25  1617  1386     9  3768     2]]"
cee8cfaf26e49d98e7d34fa1b414f8f31d6502ad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing","[[    0 28481     5  9437    11 14719  1120  2464  1397     6    53    33
    155  5044 15362 13171   101    14    11   181  1696 10626  4759  1825
    154     2]]"
761de1610e934189850e8fda707dc5239dd58092,0.0,Entailment,[[    2     0 30495  3760  1757     2]],M2M Transformer,[[    0   448   176   448  5428 22098     2]]
186b7978ee33b563a37139adff1da7d51a60f581,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"closed test limits all the data for learning should not be beyond the given training set, while open test does not take this limitation","[[    0 25315  1296  4971    70     5   414    13  2239   197    45    28
   1684     5   576  1058   278     6   150   490  1296   473    45   185
     42 22830     2]]"
a313e98994fc039a82aa2447c411dda92c65a470,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CFILT-preorder system,[[    0 25388  3063   565    12  5234 10337   467     2]]
8dc707a0daf7bff61a97d9d854283e65c0c85064,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
56b7319be68197727baa7d498fa38af0a8440fe4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BERT,[[    0 11126   565     2]]
50be4a737dc0951b35d139f51075011095d77f2a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],labeled features,[[    0 33480   196  1575     2]]
9a8b9ea3176d30da2453cac6e9347737c729a538,0.0,Entailment,[[    2     0 30495  3760  1757     2]], the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes,"[[    0     5  9284   234  2076  1421  4824    10   274   134  1471     9
     68   288     4 30739  1629    15 33689  1538 22680     8    68   288
      4   466  3818  1629    15  5154  2775   150     5   939   176   428
    176   234  2076  1421  4824    10   274   134  1471     9    68   288
      4 36011  1629    15 33689  1538 22680     8    68   288     4   466
   2518  1629    15  5154  2775     2]]"
b970f48d30775d3468952795bc72976baab3438e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"identifying the questions we wish to explore, Can text analysis provide a new perspective on a “big question” that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines","[[    0  8009  4945     5  1142    52  2813     7  5393     6  2615  2788
   1966   694    10    92  4263    15    10    44    48  8527   864    17
     46    14    34    57 13224   773    13   107 33647  1336    64    52
   3922    99    52 14095 33647  1034     7  4686     7  1533 24148     2]]"
b637d6393ef3af7462917b81861531022b291933,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
fd753ab5177d7bd27db0e0afc12411876ee607df,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.,"[[    0   133 18043   467    13     5   208  6447  3685    16    10   182
   2007  7425  5580 39974  1380 24072    19  6814 17294     4    20 18043
     13     5   274  6447  3685 17382 23645     8 38845    65     9     5
    504  7373 22422     4     2]]"
511517efc96edcd3e91e7783821c9d6d5a6562af,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BF, BA, SFU and Sherlock",[[    0 21265     6 19399     6 17575   791     8 34453     2]]
78536da059b884d6ad04680baeb894895458055c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)","[[    0 10806  2550    31 35540  9860 47145  4210 27368    36  5383   387
  45935   306    43     7  7737 40419 28413    36  5383   387 45935   245
      6  5383   387 45935   401     6  5383   387 45935   406     6  5383
    387 45935   398   238  7382 47779 44304 14641    36  5383   387 45935
    466     6  5383   387 45935   698   238 30505 23794   337 44304 14641
     36  5383   387 45935  1225    43     8   144   682  2937  2239    12
    805 41885   101 12658 43606   337 14813 15362 27893  1258    31 34379
     36 11126   565    43    36  5383   387 45935  1092    43     2]]"
67ec8ef85844e01746c13627090dc2706bb2a4f3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
50cb50657572e315fd452a89f3e0be465094b66f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
b46c0015a122ee5fb95c2a45691cb97f80de1bb6,0.0,Entailment,[[    2     0 30495  3760  1757     2]], one-layer CNN,[[    0    65    12 39165  3480     2]]
48088a842f7a433d3290eb45eb0d4c6ab1d8f13c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)","[[    0 30612 48350  1501   293    36 20485   238  9359  5580  6304 21791
     36 33919   238  7737 40419 14969    36   104 20954   238 34638  6311
   5019    36 30455   238 24257  4843 25802   196 32073    36 47872   238
   1437 30505 23794   337 44304 14641    36 16256   238  7382 41937 44304
  14641    36   500 20057    43     2]]"
cb384dc5366b693f28680374d31ff45356af0461,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
8f16dc7d7be0d284069841e456ebb2c69575b32b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],versions of LiLi,[[    0 44784     9  5991 41198     2]]
5e324846a99a5573cd2e843d1657e87f4eb22fa6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Bayesian classifier has been modified, removing the bias towards frequent labels in the training data","[[    0 20861 44871  1380 24072    34    57 10639     6  8201     5  9415
   1567  7690 14105    11     5  1058   414     2]]"
7697baf8d8d582c1f664a614f6332121061f87db,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Structural Support Vector Machine,[[    0 44523  9799  7737 40419 14969     2]]
37be0d479480211291e068d0d3823ad0c13321d3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En","[[    0 14746  3044    15  2370     6     5   274   134  1471     9     5
   1421  1058    15  1111    36  1301   298    43    16  4268     4   398
      6  1437   274   134  1471    16   129  3550     4   134    13     5
   1421  1058    15 20045    12 16040     2]]"
89497e93980ab6d8c34a6d95ebf8c1e1d98ba43f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d28d86524292506d4b24ae2d486725a6d57a3db3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],33.33,[[   0 3103    4 3103    2]]
59e58c6fc63cf5b54b632462465bfbd85b1bf3dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],our methodology does not use a seed list of offensive words,"[[    0  2126 18670   473    45   304    10  5018   889     9  2555  1617
      2]]"
3de0487276bb5961586acc6e9f82934ef8cb668c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MEDDOCAN, NUBes ",[[    0 32653 46570  1889     6   234 12027   293  1437     2]]
6aa2a1e2e3666f2b2a1f282d4cbdd1ca325eb9de,0.0,Entailment,[[    2     0 30495  3760  1757     2]],719313,[[    0   406 30995  1558     2]]
b1cf5739467ba90059add58d11b73d075a11ec86,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
051df74dc643498e95d16e58851701628fdfd43e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],crawling and pre-processing an OSG web forum,"[[    0   438 42595     8  1198    12 39221    41  8192   534  3748  7900
      2]]"
252a645af9876241fb166e5822992ce17fec6eb6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
fbaf060004f196a286fef67593d2d76826f0304e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Amazon reviews, Yelp restaurant reviews, restaurant reviews",[[    0 25146  6173     6 29730  2391  6173     6  2391  6173     2]]
aefa333b2cf0a4000cd40566149816f5b36135e7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],ratio of correct `translations',[[    0  6528  1020     9  4577 22209  9981 48111   108     2]]
1142784dc4e0e4c0b4eca1feaf1c10dc46dd5891,0.0,Entailment,[[    2     0 30495  3760  1757     2]], two salient roles called Anchors and Punctual speakers,"[[    0    80 41159  4502   373 29860   994     8 14687  3894  5564  6864
      2]]"
8748e8f64af57560d124c7b518b853bf2711c13e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
b14217978ad9c3c9b6b1ce393b1b5c6e7f49ecab,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask","[[    0 12444  4330 43803 14263 15680 34587 38091     6 31563  1142    11
  14365    18  1806  1578 12738     2]]"
a1dac888f63c9efaf159d9bdfde7c938636f07b1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],same datasets as BIBREF7,[[    0 41690 42532    25   163  8863 45935   406     2]]
7784d321ccc64db5141113b6783e4ba92fdd4b20,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
6bfba3ddca5101ed15256fca75fcdc95a53cece7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Loaded language, Name calling or labeling, Repetition, Exaggeration or minimization, Doubt, Appeal to fear/prejudice, Flag-waving, Causal oversimplification, Slogans,  Appeal to authority, Black-and-white fallacy, dictatorship, Thought-terminating cliché, Whataboutism, Reductio ad Hitlerum, Red herring, Bandwagon, Obfuscation, intentional vagueness, confusion, Straw man","[[    0 47167   196  2777     6 10704  1765    50 27963     6  2825 37258
      6  3015 17864  1258    50 15970  1938     6 11260 25933     6 16049
      7  2490    73  5234 12864  2463     6 18238    12   605 13286     6
   8316 25016 10981 38731  5000     6  4424  2154  1253     6  1437 16049
      7  3446     6  1378    12   463    12  9830 44469     6 23422     6
  30631    12 42985  1295 35341     6   653  9006  1809     6  1211 21491
   1020  2329 16423   783     6  1211    69  4506     6  6191 45199     6
   5816 48996  1258     6 18797   748 11993 14186     6  9655     6 37781
    313     2]]"
133eb4aa4394758be5f41744c60c99901b2bc01c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
38f58f13c7f23442d5952c8caf126073a477bac0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Best results authors obtain is EM 51.10 and F1 63.11,"[[    0 19183   775  7601  6925    16 14850  4074     4   698     8   274
    134  5549     4  1225     2]]"
348886b4762db063711ef8b7a10952375fbdcb57,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
a1dac888f63c9efaf159d9bdfde7c938636f07b1,0.0,Entailment,[[    2     0 30495  3760  1757     2]], the same datasets as BIBREF7,[[    0     5   276 42532    25   163  8863 45935   406     2]]
adbf33c6144b2f5c40d0c6a328a92687a476f371,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
9da1e124d28b488b0d94998d32aa2fa8a5ebec51,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For Named entity the maximum precision was 66.67%, and the average 62.58%, same values for Recall was 55.97% and 50.33%, and for F1 57.14% and 55.64%. Where for Nominal Mention had maximum recall of 74.48% and average of 73.67%, Recall had values of 54.55% and 53.7%,  and F1 had values of  62.97% and 62.12%. Finally the Overall F1 score had maximum value of 59.11% and average of 58.77%","[[    0  2709 30436 10014     5  4532 15339    21  5138     4  4111  4234
      8     5   674  5356     4  4432  4234   276  3266    13 35109    21
   3490     4  6750   207     8   654     4  3103  4234     8    13   274
    134  4981     4  1570   207     8  3490     4  4027  2153  4820    13
  11276  6204   256 19774    56  4532  6001     9  6657     4  3818   207
      8   674     9  6521     4  4111  4234 35109    56  3266     9  4431
      4  3118   207     8  4268     4   406  4234  1437     8   274   134
     56  3266     9  1437  5356     4  6750   207     8  5356     4  1092
   2153  3347     5  7806   274   134  1471    56  4532   923     9  5169
      4  1225   207     8   674     9  4893     4  4718   207     2]]"
2da4c3679111dd92a1d0869dae353ebe5989dfd2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ESTER1, ESTER2, ETAPE, REPERE","[[    0  1723  7831   134     6 12936  2076   176     6  4799 38851     6
   4979   510 23142     2]]"
8e44c02c2d9fa56fb74ace35ee70a5add50b52ae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
bf3b27a4f4be1f9ae31319877fd0c75c03126fd5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e5bc73974c79d96eee2b688e578a9de1d0eb38fd,0.0,Entailment,[[    2     0 30495  3760  1757     2]], by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly,"[[    0    30  3044  5868    15    10  9624 37105     9   654  1440 10014
      8   654  1537 44875 26567  1142    14     5 27778   338 12547   115
     45  1948 12461     2]]"
5c88d601e8fca96bffebfa9ef22331ecf31c6d75,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
7784d321ccc64db5141113b6783e4ba92fdd4b20,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
2fbb6322e485e7743ec3fb4bb02d44bf4b5ea8a6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"antonym and synonym pairs, collected from WordNet BIBREF9 and Wordnik","[[    0   927 41433     8 17796 41433 15029     6  4786    31 15690 15721
    163  8863 45935   466     8 15690  8256     2]]"
249c805ee6f2ebe4dbc972126b3d82fb09fa3556,0.0,Entailment,[[    2     0 30495  3760  1757     2]],average dissimilarity of all pairs of tags in the list of recommended tags,"[[    0 20365 14863 36692  1571     9    70 15029     9 19445    11     5
    889     9  5131 19445     2]]"
425bd2ccfd95ead91d8f2b1b1c8ab9fc3446cb82,0.0,Entailment,[[    2     0 30495  3760  1757     2]],accuracy,[[    0  7904 45386     2]]
509af1f11bd6f3db59284258e18fdfebe86cae47,0.0,Entailment,[[    2     0 30495  3760  1757     2]],diversity score as the ratio of observed number versus optimal number,"[[    0   417 31104  1471    25     5  1750     9  6373   346  4411 19329
    346     2]]"
784ce5a983c5f2cc95a2c60ce66f2a8a50f3636f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
9bb7ae50bff91571a945c1af025ed2e67714a788,0.0,Entailment,[[    2     0 30495  3760  1757     2]],hLSTM,[[   0  298  574 4014  448    2]]
18c5d366b1da8447b5404eab71f4cc658ba12e6f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Stanford NER, spaCy 2.0, recurrent model with a CRF top layer","[[    0 36118  1891   234  2076     6 18179 25826   132     4   288     6
  35583  1421    19    10  4307   597   299 10490     2]]"
11e8bd4abf5f8bdabad3e8f0691e6d0ad6c326af,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"NO-MOVE, RANDOM, JUMP",[[    0 13449    12  8756  8856     6 43015  3765     6   344 23825     2]]
c6a0b9b5dabcefda0233320dd1548518a0ae758e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CJFA encoder,[[    0   347   863  5944  9689 15362     2]]
c2eb743c9d0baf1781c3c0df9533fab588250af3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers","[[    0  5320 10074   226  4014 13123     6 13696    12 24590   312 10074
    226  4014 13123     6 12169  4086 14813  1630   268     6  3107    12
  39165  4210 27368     2]]"
440faf8d0af8291d324977ad0f68c8d661fe365e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The Reuters-8 dataset (with stop words removed),"[[    0   133  1201    12   398 41616    36  5632   912  1617  2928    43
      2]]"
88e5d37617e14d6976cc602a168332fc23644f19,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," `Conversations Gone Awry' dataset, subreddit ChangeMyView","[[    0 22209  9157  3697  1635 25242 11614  1506   108 41616     6 45757
   7229  2387 22130     2]]"
8de9f14c7c4f37ab103bc8a639d6d80ade1bc27b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],one additional hop,[[    0  1264   943 13591     2]]
3116453e35352a3a90ee5b12246dc7f2e60cfc59,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self","[[    0 22930 37681  3563  1380 24072    36   104 20954   238  7425  5580
  39974  1380 24072    36 33919   238  7300  2088  1501   293  1380 24072
     36 20485   238  9624  6693    36 30455   238  3480     6   226  4014
    448  2156   226  4014   448    12 24810     6   226  4014   448    12
  13367     2]]"
bc84c5a58c57038910f7720d7a784560054d3e1a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"French (Fr), German (De), Dutch (Nl), Russian (Ru), Spanish (Es), Italian (It), Turkish (Tr), Persian (Fa), Swedish (Sv), Mongolian (Mn) and Chinese (Zh)","[[    0 28586    36 29220   238  1859    36 13365   238  5979    36   487
    462   238  1083    36 36439   238  3453    36 15286   238  3108    36
    243   238  4423    36 12667   238 27775    36 44663   238  9004    36
    104   705   238 39982   811    36   448   282    43     8  1111    36
   1301   298    43     2]]"
3cd185b7adc835e1c4449eff81222f5fc15c8500,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Extract features from the LDA model and use them in a binary classification task,"[[    0 40884 15664  1575    31     5   226  3134  1421     8   304   106
     11    10 32771 20257  3685     2]]"
c7eb71683f53ab7acffd691a36cad6edc7f5522e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
9ee07edc371e014df686ced4fb0c3a7b9ce3d5dc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WebQSP, SimpleQuestions",[[    0 27521  1864  4186     6 21375 46865     2]]
2e37e681942e28b5b05639baaff4cd5129adb5fb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
a313e98994fc039a82aa2447c411dda92c65a470,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
ad1be65c4f0655ac5c902d17f05454c0d4c4a15d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"More than 2,100 texts were paired with 15 questions each, resulting in a total number of approx. 32,000 annotated questions. 13% of the questions are not answerable.  Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based).  The final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%).","[[    0  9690    87   132     6  1866 14301    58 11153    19   379  1142
    349     6  5203    11    10   746   346     9 36612     4  2107     6
    151 45068  1070  1142     4   508   207     9     5  1142    32    45
   1948   868     4  1437  2548     9     5  1948   868  1142     6   158
      6 13726   115    28  7173    31     5  2788  2024    36 29015    12
    805    43     8   155     6   466  1570  1142  1552     5   304     9
  39758  9401  2655    36 32761    12   805   322  1437    20   507 41616
  16755   508     6   466  3416  1142     6   155     6   398  2518     9
     61  2703 39758  9401  2655    36   118     4   242     4   974     4
    306 23528     2]]"
f2e8497aa16327aa297a7f9f7d156e485fe33945,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Experienced medical doctors used a linguistic annotation tool to annotate entities.,"[[    0 45395 33582  1131  3333   341    10 39608 47760  3944     7 45068
    877  8866     4     2]]"
c87fcc98625e82fdb494ff0f5309319620d69040,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
6236762b5631d9e395f81e1ebccc4bf3ab9b24ac,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
4944cd597b836b62616a4e37c045ce48de8c82ca,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MR, CR, SUBJ, MPQA, SST, TREC, MRPC","[[    0 12642     6  4307     6 25272   863     6  3957  1864   250     6
    208  4014     6   255 40698     6 18838  4794     2]]"
861187338c5ad445b9acddba8f2c7688785667b1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d004ca2e999940ac5c1576046e30efa3059832fa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"classic RNN model, avgRNN model, attentionRNN model and multiattention RNN model with and without a projected layer","[[    0 41755   248 20057  1421     6 44678   500 20057  1421     6  1503
    500 20057  1421     8  3228  2611 19774   248 20057  1421    19     8
    396    10  5635 10490     2]]"
dd09db5eb321083dba16c2550676e60682f9a0cd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Ekman’s six basic emotions,  neutral",[[   0  717  330  397   17   27   29  411 3280 8597    6 1437 7974    2]]
2268c9044e868ba0a16e92d2063ada87f68b5d03,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They increased F1 Score by 0.029 in Sentence Level Classification, and by 0.044 in Fragment-Level classification","[[    0  1213  1130   274   134 14702    30   321     4 41168    11 12169
   4086 12183 40509     6     8    30   321     4 40847    11 35233  1757
     12 38809 20257     2]]"
41b2355766a4260f41b477419d44c3fd37f3547d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"systematic and substantial racial biases, biases from data collection, rules of annotation","[[    0 19675  5183     8  6143  6689 31681     6 31681    31   414  2783
      6  1492     9 47760     2]]"
4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Linear SVM, RBF SVM, and Random Forest","[[    0 40253  4352   208 20954     6 11191   597   208 20954     6     8
  34638  5761     2]]"
1dac4bc5af239024566fcb0f43bb9ff1c248ecec,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
c35806cf68220b2b9bb082b62f493393b9bdff86,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"In SNLI, our best model achieves the new state-of-the-art accuracy of 87.0%,  we can see that our models outperform other models by large margin, achieving the new state of the art., Our models achieve the new state-of-the-art accuracy on SST-2 and competitive accuracy on SST-5","[[    0  1121 13687 27049     6    84   275  1421 35499     5    92   194
     12  1116    12   627    12  2013  8611     9  8176     4   288  4234
   1437    52    64   192    14    84  3092  9980  3899    97  3092    30
    739  2759     6  9499     5    92   194     9     5  1808   482  1541
   3092  3042     5    92   194    12  1116    12   627    12  2013  8611
     15   208  4014    12   176     8  2695  8611    15   208  4014    12
    245     2]]"
cc850bc8245a7ae790e1f59014371d4f35cd46d7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They use a multi-class classifier to determine the section it should be cited,"[[    0  1213   304    10  3228    12  4684  1380 24072     7  3094     5
   2810    24   197    28  4418     2]]"
2e89ebd2e4008c67bb2413699589ee55f59c4f36,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity., Classification Objective Function, Regression Objective Function, Triplet Objective Function","[[    0 45061     5 23341   215    14     5  2622  3645 33183   417  1033
     32  9031 38600  6667     8    64    28  1118    19 12793   833    12
  42116  1571   482 40509 44676 42419     6  6304 21791 44676 42419     6
   8424    90 44676 42419     2]]"
1088255980541382a2aa2c0319427702172bbf84,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"At the macro level, it is important to decide which is the appropriate field to attend to next, micro level (i.e., within a field) it is important to know which values to attend to next, fuse the attention weights at the two levels","[[    0  3750     5 12303   672     6    24    16   505     7  2845    61
     16     5  3901   882     7  2725     7   220     6  5177   672    36
    118     4   242   482   624    10   882    43    24    16   505     7
    216    61  3266     7  2725     7   220     6 38689     5  1503 23341
     23     5    80  1389     2]]"
51fe4d44887c5cc5fc98b65ca4cb5876f0a56dad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CNN, BERT",[[    0 16256     6   163 18854     2]]
cb77d6a74065cb05318faf57e7ceca05e126a80d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec","[[    0   387   424  4400  1076     4   208 20954     6  3066     8   289
  35664   885    73  7210  7067     6   226 29069  4400  1076     4   885
     73 16963 39645     6   226 29069  4400  1076     4   885    73 14742
    176 25369     2]]"
0d34c0812f1e69ea33f76ca8c24c23b0415ebc8d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"polarity scores, which are minimum, mean, and maximum polarity scores, from each review","[[    0   642 19231  1571  4391     6    61    32  3527     6  1266     6
      8  4532  8385 21528  4391     6    31   349  1551     2]]"
9c44df7503720709eac933a15569e5761b378046,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
596aede2b311deb8cb0a82d2e7de314ef6e83e4e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
06eb9f2320451df83e27362c22eb02f4a426a018,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Level 1, Level 2 and Level 3.",[[    0 38809   112     6 12183   132     8 12183   155     4     2]]
701571680724c05ca70c11bc267fb1160ea1460a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
7bd6a6ec230e1efb27d691762cc0674237dc7967,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," Penn Treebank, WikiText2",[[    0  5953 11077  5760     6 45569 39645   176     2]]
9eabb54c2408dac24f00f92cf1061258c7ea2e1a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"paragraph, lines, textspan element (paragraph segmentation, line segmentation, Information on physical page segmentation(for PDF only))","[[    0 46845     6  2301     6  2788 36407  7510    36 46845  2835  1258
      6   516  2835  1258     6  3522    15  2166  1842  2835  1258  1640
   1990 22745   129 35122     2]]"
79f9468e011670993fd162543d1a4b3dd811ac5d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively.","[[    0 44891     7     5 11909 38630     6  5495  2620   574   473    45
    109   357    11  1110     9 33708  1571    15   230  4571   673     8
  14850   487 21992   193   305 11674 42532     6    53    24   473    30
     62     7   321     4  2518 12156    12 30876   791   332    15   230
   4571   673     8   321     4  2022 12156    12 30876   791    15 14850
    487 21992   193   305 11674     4    96  1110     9 14171 45816  1571
      8  1223  9525  2389     6    24  4391   357    87     5 11909 38630
     15    62     7  3337     4   245   207     8  6521   207     9     5
   1200  4067     4     2]]"
003d6f9722ddc2ee13e879fefafc315fb8e87cb9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
58ef2442450c392bfc55c4dc35f216542f5f2dbb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
23e2971c962bb6486bc0a66ff04242170dd22a1d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],It depends on the dataset. Experimental results over two datasets reveal that textual and visual features are complementary. ,"[[    0   243  7971    15     5 41616     4 41701   775    81    80 42532
   4991    14 46478     8  7133  1575    32 25402     4  1437     2]]"
b3de9357c569fb1454be8f2ac5fcecaea295b967,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"10,000",[[  0 698   6 151   2]]
4a4ce942a7a6efd1fa1d6c91dedf7a89af64b729,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The number of redundant answers to collect from the crowd is predicted to efficiently capture the diversity of all answers from all visual questions.,"[[    0   133   346     9 31813  5274     7  5555    31     5  2180    16
   6126     7 14146  5604     5  5845     9    70  5274    31    70  7133
   1142     4     2]]"
0810b43404686ddfe4ca84783477ae300fdd2ea4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The transformer layer,[[    0   133 40878 10490     2]]
4bc2784be43d599000cb71d31928908250d4cef3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content.,"[[    0  4688  5064     9  5008 38813  2606    61 19999   543 11717    12
    805 46644  2961    19  3793 11717    12   805 46644  2961    19     5
   1606 24899  1021   856 10928 19942 28255     7   432    19 28269  1383
      4     2]]"
a6d3e57de796172c236e33a6ceb4cca793dc2315,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (Experimental Setup missing subsections)
To be selected: We compared REFRESH against a baseline which simply selects the first m leading sentences from each document (LEAD) and two neural models similar to ours (see left block in Figure 1), both trained with cross-entropy loss.
Answer: LEAD","[[    0 33683    19  1383  1716    35    36 45395 40955 45524  1716 48848
     43 50118  3972    28  3919    35   166  1118  4979  5499 34956   136
     10 18043    61  1622 38845     5    78   475   981 11305    31   349
   3780    36  3850  2606    43     8    80 26739  3092  1122     7 15157
     36  7048   314  1803    11 17965   112   238   258  5389    19  2116
     12  1342 47145   872     4 50118 33683    35 10611  2606     2]]"
56a8826cbee49560592b2d4b47b18ada236a12b9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Exposure, Characterization, Polarization",[[    0  9089 42286     6 35177  1938     6 26279  1938     2]]
2f901dab6b757e12763b23ae8b37ae2e517a2271,0.0,Entailment,[[    2     0 30495  3760  1757     2]],German–English,[[    0 27709  2383 35007     2]]
a1645d0ba50e4c29f0feb806521093e7b1459081,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Social Honeypot, which is not of high quality",[[    0 28565 13834  8024     6    61    16    45     9   239  1318     2]]
49eb52b3ec0647e165a5e41488088c80a20cc78f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],context inference,[[    0 46796 42752     2]]
ddf5e1f600b9ce2e8f63213982ef4209bab01fd8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Spoken-SQuAD,[[    0 12582 22036    12   104 12444  2606     2]]
53aa07cc4cc4e7107789ae637dbda8c9f6c1e6aa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"low coverage of audio, difficulty in cross-speaker clustering","[[    0  5481  1953     9  6086     6  9600    11  2116    12 18462  4218
  46644  2961     2]]"
b3de9357c569fb1454be8f2ac5fcecaea295b967,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"10,000 Arabic tweet dataset ",[[    0   698     6   151 19645  3545 41616  1437     2]]
b3307d5b68c57a074c483636affee41054be06d1,6.6667,Entailment,[[    2     0 30495  3760  1757     2]],"hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length, NLI models tend to predict entailment for sentence pairs with a high lexical overlap","[[    0 33027  1242 35571    12  8338 18043 14023   357    87   778   528
      7 25072    15    49 36912  3569  2031     8  3645  5933     6 12817
    100  3092  3805     7  7006 31648  1757    13  3645 15029    19    10
    239 36912  3569 27573     2]]"
876700622bd6811d903e65314ac75971bbe23dcc,0.0,Entailment,[[    2     0 30495  3760  1757     2]], high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task,"[[    0   239    12  8634 42532  1437    31 11202   717  6486    12  9029
     44    48 35212  8913  5213    11   599    17    46  3685     2]]"
fd0a3e9c210163a55d3ed791e95ae3875184b8f8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WSJ-SI84, WSJ-SI284","[[    0 13691   863    12  6850  6232     6 18483   863    12  6850 32769
      2]]"
0689904db9b00a814e3109fb1698086370a28fa2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Document to Vector (Doc2Vec),[[    0 47088     7 40419    36 42291   176   846  3204    43     2]]
2cd37743bcc7ea3bd405ce6d91e79e5339d7642e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
2a3e36c220e7b47c1b652511a4fdd7238a74a68f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],244 ,[[    0 30043  1437     2]]
2dbf6fe095cd879a9bf40f110b7b72c8bdde9475,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the group-specific embedding representations are tied through a global embedding,"[[    0   627   333    12 14175 33183 11303 30464    32  3016   149    10
    720 33183 11303     2]]"
88e62ea7a4d1d2921624b8480b5c6b50cfa5ad42,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The matrix containing co-occurrences of the words which occur with the both words of every given pair of words.,"[[    0   133 36173  8200  1029    12 23462   710 46957     9     5  1617
     61  5948    19     5   258  1617     9   358   576  1763     9  1617
      4     2]]"
26e2d4d0e482e6963a76760323b8e1c26b6eee91,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," this paper makes use of the official training and test sets, covering in total 630 speakers with 8 utterances each","[[    0    42  2225   817   304     9     5   781  1058     8  1296  3880
      6  4631    11   746 31956  6864    19   290 18672  5332   349     2]]"
0d34c0812f1e69ea33f76ca8c24c23b0415ebc8d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"three hand-crafted polarity scores, which are minimum, mean, and maximum polarity scores","[[    0  9983   865    12 29496  8385 21528  4391     6    61    32  3527
      6  1266     6     8  4532  8385 21528  4391     2]]"
f651cd144b7749e82aa1374779700812f64c8799,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU , FKGL , SARI ",[[    0 30876   791  2156   274   530 10020  2156   208 32665  1437     2]]
01209a3bead7c87bcdc628be2a5a26b41abde9d1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SNLI BIBREF22 and MultiNLI BIBREF23, Quora Question Pairs dataset BIBREF24,  Stanford Sentiment Treebank (SST) BIBREF25","[[    0 12436 27049   163  8863 45935  2036     8 19268   487 27049   163
   8863 45935  1922     6  3232  4330 15680   221 18022 41616   163  8863
  45935  1978     6  1437  8607 12169  8913 11077  5760    36   104  4014
     43   163  8863 45935  1244     2]]"
7cd22ca9e107d2b13a7cc94252aaa9007976b338,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
75df70ce7aa714ec4c6456d0c51f82a16227f2cb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam","[[    0   725  2028   118     6  2370     6   229  2279  2095     6  5477
  14801     6  6331 12336   242     6 30244  3644     8  2529   857 23590
      2]]"
88e5d37617e14d6976cc602a168332fc23644f19,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. ","[[    0  4688  4939  1732     9     5  2210   128  9157  3697  1635 25242
  11614  1506   108 41616     8     5  7229  2387 22130 41616     6    10
  45757  1060   129 47760    16   549     5  1607  1552   814    30     5
   6844 42976     4  1437     2]]"
4ac2c3c259024d7cd8e449600b499f93332dab60,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
df0257ab04686ddf1c6c4d9b0529a7632330b98e,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," On the other hand, phase 1 of Go-Explore finds an optimal trajectory with approximately half the interactions with the environment, Moreover, the trajectory length found by Go-Explore is always optimal (i.e. 30 steps) whereas both DQN++ and DRQN++ have an average length of 38 and 42 respectively., Especially interesting is that the performance of DRRN is substantially lower than that of the Go-Explore Seq2Seq model","[[    0   374     5    97   865     6  4359   112     9  2381    12 47526
   5684    41 19329 15217    19  2219   457     5 11324    19     5  1737
      6  7905     6     5 15217  5933   303    30  2381    12 47526    16
    460 19329    36   118     4   242     4   389  2402    43  9641   258
    211  1864   487 42964     8 10994  1864   487 42964    33    41   674
   5933     9  2843     8  3330  4067   482 17570  2679    16    14     5
    819     9 10994 39151    16 12246   795    87    14     9     5  2381
     12 47526  1608  1343   176 14696  1343  1421     2]]"
044f922604b4b3f42ae381419fd5cd5624fa0637,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For certain POS tags, e.g. VERB, PRON.","[[    0  2709  1402 29206 19445     6   364     4   571     4 37616   387
      6  4729  2191     4     2]]"
fabf6fdcfb4c4c7affaa1e4336658c1e6635b1bf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"This dataset was created by BIBREF8, another English dataset from BIBREF8 ,  dataset from The Sarcasm Detector","[[    0   713 41616    21  1412    30   163  8863 45935   398     6   277
   2370 41616    31   163  8863 45935   398  2156  1437 41616    31    20
    208  9636 16836 11185 26997     2]]"
3b371ea554fa6639c76a364060258454e4b931d4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],NowThisNews Facebook page,[[   0 5975  713 5532  622 1842    2]]
d3ca5f1814860a88ff30761fec3d860d35e39167,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines","[[    0 48089  8150     6 26234  1190  1417  1421  2156 35540  9860 47145
      6 12108 24176 34638 15502  2156  7737 40419 28413     2]]"
c59078efa7249acfb9043717237c96ae762c0a8c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d7644c674887ca9708eb12107acd964ae53b216d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)","[[    0 43623     9  8776   352  8791   196 39320    36   104  3376   238
  17736     9     5   226  5384   990  8776   352  8791   196 43006    36
  10463  3376   238 12270     9 30856   352  8791   196 39320    36   771
   3376   238 17736     9     5   226  5384   990 30856   352  8791   196
  43006    36   574   771  3376   238   211 46147     9     5   226  5384
    990 30856   352  8791   196 43006    36 39751  3376   238  8317  2893
   4193  2961   944 24645    36  3376   238  4326   229    12  7293 12270
     36 32428   238   211 40904    36   417    43     2]]"
603fee7314fa65261812157ddfc2c544277fcf90,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By 14 times.,[[   0 2765  501  498    4    2]]
8b9c12df9f89040f1485b3847a29f11b5c9262e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
feafcc1c4026d7f55a2c8ce7850d7e12030b5c22,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the model can be trained end-to-end,[[   0  627 1421   64   28 5389  253   12  560   12 1397    2]]
f88036174b4a0dbf4fe70ddad884d16082c5748d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
1d197cbcac7b3f4015416f0152a6692e881ada6c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],off-the-shelf toolbox of OpenIE,"[[    0  1529    12   627    12  8877 13491  3944  8304     9  2117  7720
      2]]"
d3093062aebff475b4deab90815004051e802aa6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information","[[    0   104 20954    19   542  1023  4040     6   380  4040     6 46681
   4040  1575     6    19   674  2136 33183 11303     6    19   674 11229
   2136 33183   417  1033     6  3480     8 19682 20057     6   208 20954
      6  3480     6 19682 20057    19  1129   335     2]]"
c85b6f9bafc4c64fc538108ab40a0590a2f5768e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],column Ens Test in Table TABREF19,[[    0 42351 31392  4500    11  9513   255  4546 45935  1646     2]]
2268c9044e868ba0a16e92d2063ada87f68b5d03,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The best ensemble topped the best single model by 0.029 in F1 score on dev (external).,"[[    0   133   275 12547  8319     5   275   881  1421    30   321     4
  41168    11   274   134  1471    15  8709    36 43166   322     2]]"
307e8ab37b67202fe22aedd9a98d9d06aaa169c5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
6bbbb9933aab97ce2342200447c6322527427061,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"researchers asked subjects to report their emotions after reading each article, multiplied the document-emotion matrix and word-document matrix to derive emotion-word matrix for these words, Depechemood simply creates dictionaries of words where each word has scores between 0 and 1 for all of these 8 emotion categories","[[    0   241  1090   271  7873   553  9352     7   266    49  8597    71
   2600   349  1566     6 39582     5  3780    12   991 19187 36173     8
   2136    12 43017 36173     7 34882 11926    12 14742 36173    13   209
   1617     6   926  2379 25666  5715  1622  6670 45073  5119     9  1617
    147   349  2136    34  4391   227   321     8   112    13    70     9
    209   290 11926  6363     2]]"
d9980676a83295dda37c20cfd5d58e574d0a4859,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"copy, copy-marked, copy-dummies",[[    0 44273     6  5375    12 18584     6  5375    12   417 38098     2]]
9c44df7503720709eac933a15569e5761b378046,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English,[[    0 35007     2]]
1097768b89f8bd28d6ef6443c94feb04c1a1318e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
8bf7f1f93d0a2816234d36395ab40c481be9a0e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
eb2d5edcdfe18bd708348283f92a32294bb193a5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"KG-A2C, A2C, A2C-chained, A2C-Explore","[[    0   530   534    12   250   176   347     6    83   176   347     6
     83   176   347    12   611  7153     6    83   176   347    12 47526
      2]]"
87bb3105e03ed6ac5abfde0a7ca9b8de8985663c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They do not require the availability of a backward translation engine.,"[[    0  1213   109    45  2703     5  7265     9    10 18173 19850  3819
      4     2]]"
ddb23a71113cbc092cbc158066d891cae261e2c6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The BreakingNews dataset,[[    0   133 17252  5532 41616     2]]
abe2393415e533cb06311e74ed1c5674cff8571f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency","[[    0 30876   791  2156   234 11595  2156 30782   717  3411  2156   248
   5061  8800    12   574     6   230  2688 28012  2156 10437  8543     6
   8408 10437     6  1050 10437     6  3527 17668 10437     6  2136  5849
    731    36 45117   238 21833  9126     8    49  3505     6  6626  6761
    743     6  3264  4484     9     5  4195    13   931   304    11    10
    340  1218     2]]"
7561a968470a8936d10e1ba722d2f38b5a9a4d38,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"30,000",[[  0 541   6 151   2]]
28067da818e3f61f8b5152c0d42a531bf0f987d4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4,"[[    0 33683    19  1383  1716    35    36 46102 48848    43  9637 16057
  12545    13   295    12 28526  3365   112    12   306     2]]"
9658b5ffb5c56e5a48a3fea0342ad8fc99741908,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
dd6b378d89c05058e8f49e48fd48f5c458ea2ebc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT","[[    0 43597 24176 34638 15502     6  6479   574  4014   448    12  9822
    597     6 19268    12 47744 13807     6 12334 11126   565     2]]"
99e78c390932594bd833be0f5c890af5c605d808,0.0,Entailment,[[    2     0 30495  3760  1757     2]],QA PGNet and Multi-decoder QA PGNet,"[[    0  1864   250 14499 15721     8 19268    12 11127 15362  1209   250
  14499 15721     2]]"
ab0fd94dfc291cf3e54e9b7a7f78b852ddc1a797,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BIBREF26,[[    0  5383   387 45935  2481     2]]
981fd79dd69581659cb1d4e2b29178e82681eb4d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Introduce a ""Refinement Adjustment LSTM-based component"" to the decoder","[[    0 47032  1755    10    22 31842 47706 29726  1757   226  4014   448
     12   805  7681   113     7     5  5044 15362     2]]"
2c947447d81252397839d58c75ebcc71b34379b5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CoinCollector , CookingWorld ",[[    0 17339 44252   368  2156 32284 10988  1437     2]]
dcc1115aeaf87118736e86f3e3eb85bf5541281c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Random Forest,[[    0 45134  5761     2]]
b634ff1607ce5756655e61b9a6f18bc736f84c83,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Energy,[[    0 30189     2]]
feafcc1c4026d7f55a2c8ce7850d7e12030b5c22,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the objective of our model is sum of the two processes, jointly trained using ""teacher-forcing"" algorithm, we feed the ground-truth summary to each decoder and minimize the objective, At test time, each time step we choose the predicted word by $\hat{y} = argmax_{y^{\prime }} P(y^{\prime }|x)$ , use beam search to generate the draft summaries, and use greedy search to generate the refined summaries., the model can be trained end-to-end","[[    0   627  4554     9    84  1421    16  6797     9     5    80  5588
      6 13521  5389   634    22   859  8365    12 43837   113 17194     6
     52  3993     5  1255    12 36014  4819     7   349  5044 15362     8
  15925     5  4554     6   497  1296    86     6   349    86  1149    52
   2807     5  6126  2136    30 49959  8849 45152   219 24303  5457 29480
  29459 49747   219 35227 49918 28752 49384   221  1640   219 35227 49918
  28752 35524 15483  1178    43  1629  2156   304 23480  1707     7  5368
      5  2479 32933  5119     6     8   304 34405  1707     7  5368     5
  15616 32933  5119   482     5  1421    64    28  5389   253    12   560
     12  1397     2]]"
37f8c034a14c7b4d0ab2e0ed1b827cc0eaa71ac6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"translation probabilities, Labeled Attachment Scores (LAS)","[[    0 48235 43471     6  8250 13587  7279 26284 26341    36   574  2336
     43     2]]"
a616a3f0d244368ec588f04dfbc37d77fda01b4c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese","[[    0 24727 33830     6 12093     6  2370     6 34518   811     6 21533
      6  1515     6 27428     6 11145     6  1083     6  3453     6 23879
    605   895  4715     6 43494  1111     2]]"
ffa7f91d6406da11ddf415ef094aaf28f3c3872d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Their average correlation tops the best other model by 0.155 on WikiBio.,"[[    0 16837   674 22792 13657     5   275    97  1421    30   321     4
  21403    15 45569 40790     4     2]]"
1591068b747c94f45b948e12edafe74b5e721047,0.0,Entailment,[[    2     0 30495  3760  1757     2]],10K user-generated image (snap) and textual caption pairs,"[[    0   698   530  3018    12 21842  2274    36 43189    43     8 46478
   3747 15029     2]]"
6a14379fee26a39631aebd0e14511ce3756e42ad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BERT-base, BERT-large, BERT-uncased, BERT-cased","[[    0 11126   565    12 11070     6   163 18854    12 11802     6   163
  18854    12 23735 11835     6   163 18854    12   438 11835     2]]"
5fda8539a97828e188ba26aad5cda1b9dd642bc8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MSR: 97.7 compared to 97.5 of baseline
AS: 95.7 compared to 95.6 of baseline","[[    0  6222   500    35  8783     4   406  1118     7  8783     4   245
      9 18043 50118  2336    35  6164     4   406  1118     7  6164     4
    401     9 18043     2]]"
7e38e0279a620d3df05ab9b5e2795044f18d4471,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"personal attack, racism, and sexism",[[    0 26892   908     6  8222     6     8 26032     2]]
5cc2daca2a84ddccba9cdd9449e51bb3f64b3dde,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"state-of-the-art Transformer architecture, Kaldi, speech clustergen statistical speech synthesizer","[[    0  4897    12  1116    12   627    12  2013  5428 22098  9437     6
    229 22564     6  1901 18016  4138 17325  1901 33689  6315     2]]"
efe9bad55107a6be7704ed97ecce948a8ca7b1d2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"NoKD, PKD, BERTBASE teacher model","[[    0  3084   530   495     6 25011   495     6   163 18854   387 24199
   3254  1421     2]]"
46c9e5f335b2927db995a55a18b7c7621fd3d051,0.0,Entailment,[[    2     0 30495  3760  1757     2]],15 clinical patient phenotypes,[[    0   996  5154  3186 35833 43981     2]]
cc354c952b5aaed2d4d1e932175e008ff2d801dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," the number of systems consistently giving higher scores to sentences with female noun phrases, higher scores to sentences with African American names on the tasks of anger, fear, and sadness,  joy and valence tasks, most submissions tended to assign higher scores to sentences with European American names","[[    0     5   346     9  1743  6566  1311   723  4391     7 11305    19
   2182 44875 22810     6   723  4391     7 11305    19  1704   470  2523
     15     5  8558     9  6378     6  2490     6     8 17437     6  1437
   5823     8  7398  4086  8558     6   144 18219 21131     7 28467   723
   4391     7 11305    19   796   470  2523     2]]"
43eecc576348411b0634611c81589f618cd4fddf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SeqGAN, LeakGAN, MaliGAN, DialogGAN, DPGAN","[[    0 14696  1343 38416     6  1063   677 38416     6 16193 38416     6
  28985  2154 38416     6   211  8332  1889     2]]"
76121e359dfe3f16c2a352bd35f28005f2a40da3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"text classification for themes including sentiment, web-page, science, medical and healthcare","[[    0 29015 20257    13 12049   217  5702     6  3748    12  8596     6
   2866     6  1131     8  3717     2]]"
b66c9a4021b6c8529cac1a2b54dacd8ec79afa5f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"global (the whole document), local context (e.g., the section/topic)","[[    0  9424    36   627  1086  3780   238   400  5377    36   242     4
    571   482     5  2810    73 45260    43     2]]"
1ec152119cf756b16191b236c85522afeed11f59,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They measure self-similarity, intra-sentence similarity and maximum explainable variance of the embeddings in the upper layers.","[[    0  1213  2450  1403    12 42116  1571     6 18592    12 19530  4086
  37015     8  4532  3922   868 37832     9     5 33183   417  1033    11
      5  2853 13171     4     2]]"
d509081673f5667060400eb325a8050fa5db7cc8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],2174 million tokens for English and 989 million tokens for Russian,"[[    0   176 29221   153 22121    13  2370     8   361  5046   153 22121
     13  1083     2]]"
55139fcfe04ce90aad407e2e5a0067a45f31e07e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Google translation API,[[    0 20441 19850 21013     2]]
4fa6fbb9df1a4c32583d4ef70d2b29ece4b3d802,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BIBREF11 , BIBREF26 ",[[    0  5383   387 45935  1225  2156   163  8863 45935  2481  1437     2]]
7cd22ca9e107d2b13a7cc94252aaa9007976b338,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
1e4dbfc556cf237accb8b370de2f164fa723687b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set","[[    0 20365  2216 12535     6 23168     5  2249     9 12535   227    84
   1850  3092     6    52   311    41  1246  4986    31     5 19107   844
    330 26567   278     2]]"
1d3e914d0890fc09311a70de0b20974bf7f0c9fe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800","[[    0  3573   245   347 10644    12   417  1496  3175     6  9537  5383
     12   417  1496  3175     6  9543   245   347 10644    12 25666     6
   9543   306  3764  5330   495     6  9543   176 16972     6   344   487
  21992  3813     6 39671  4444   717  3048     6 33797    12  3913     2]]"
e44a6bf67ce3fde0c6608b150030e44d87eb25e3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR)","[[    0 27275    36  4546   673   238  5100   659    36   534  2547   238
   1284    36   673  3813   238     8  3140    36 36659    43     2]]"
d004ca2e999940ac5c1576046e30efa3059832fa,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," four attention mechanisms instead of one, a projection layer for the word embeddings","[[    0   237  1503 14519  1386     9    65     6    10 18144 10490    13
      5  2136 33183   417  1033     2]]"
31d695ba855d821d3e5cdb7bea638c7dbb7c87c7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"GRU-based encoder, interaction block, and classifier consisting of stacked fully-connected layers.","[[    0 11621   791    12   805  9689 15362     6 10405  1803     6     8
   1380 24072 17402     9 19030  1950    12 21618 13171     4     2]]"
bfc2dc913e7b78f3bd45e5449d71383d0aa4a890,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Knowledge Store (KS) , Knowledge Graph ( INLINEFORM0 ),  Relation-Entity Matrix ( INLINEFORM2 ), Task Experience Store ( INLINEFORM15 ), Incomplete Feature DB ( INLINEFORM29 )","[[    0 38912 21945  7248    36 18307    43  2156 27831 20919    36  2808
  28302 38036   288 31311  1437  8136  1258    12 49448 29830    36  2808
  28302 38036   176 31311 12927 14360  7248    36  2808 28302 38036   996
  31311    96 27527 31967 18496    36  2808 28302 38036  2890  4839     2]]"
ca5a82b54cb707c9b947aa8445aac51ea218b23a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Plain dialogues with unique dialogue indexes, Plain Information Dictionary information (e.g., extracted entities) collected for the whole dialogue, Pairs of questions (i.e., user requests) and responses (i.e., bot responses), Triples in the form of (User Request, Next Action, Response)","[[    0 16213  1851 25730  3663    19  2216  6054 15271     6 19699  3522
  41243   335    36   242     4   571   482 27380  8866    43  4786    13
      5  1086  6054     6   221 18022     9  1142    36   118     4   242
    482  3018  5034    43     8  8823    36   118     4   242   482 14084
   8823   238  6892 12349    11     5  1026     9    36 44518 18593     6
   4130  5828     6 19121    43     2]]"
18c5d366b1da8447b5404eab71f4cc658ba12e6f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer","[[    0 36118  1891   234  2076     6 18179 25826   132     4   288  2156
  35583  1421    19    10  4307   597   299 10490     2]]"
de5b6c25e35b3a6c5e40e350fc5e52c160b33490,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Best proposed model result vs best previous result:
Arxiv dataset: Rouge 1 (43.62 vs 42.81), Rouge L (29.30 vs 31.80), Meteor (21.78 vs 21.35)
Pubmed dataset: Rouge 1 (44.85 vs 44.29), Rouge L (31.48 vs 35.21), Meteor (20.83 vs 20.56)","[[    0 19183  1850  1421   898  1954   275   986   898    35 50118  8138
   1178  1879 41616    35 14941   112    36  3897     4  5379  1954  3330
      4  6668   238 14941   226    36  2890     4   541  1954  1105     4
   2940   238 16582    36  2146     4  5479  1954   733     4  2022    43
  50118 44110  4567 41616    35 14941   112    36  3305     4  4531  1954
   3550     4  2890   238 14941   226    36  2983     4  3818  1954  1718
      4  2146   238 16582    36   844     4  6361  1954   291     4  4419
     43     2]]"
984fc3e726848f8f13dfe72b89e3770d00c3a1af,0.0,Entailment,[[    2     0 30495  3760  1757     2]],KL divergence between the language model of INLINEFORM5 and articles in INLINEFORM6,"[[    0   530   574 37178   227     5  2777  1421     9  2808 28302 38036
    245     8  7201    11  2808 28302 38036   401     2]]"
ee7e9a948ee6888aa5830b1a3d0d148ff656d864,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"roughly 40,000 Manhattan listings",[[    0 10344   352   843     6   151  6562 16568     2]]
4f253dfced6a749bf57a1b4984dc962ce9550184,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By conducting a survey among engineers,[[   0 2765 7909   10 2658  566 8610    2]]
ae90c5567746fe25af2fcea0cc5f355751e05c71,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"US dataset, Italian dataset",[[    0  3048 41616     6  3108 41616     2]]
891c2001d6baaaf0da4e65b647402acac621a7d2,0.0,Entailment,[[    2     0 30495  3760  1757     2]], by taking the first principal component (PC) of its contextualized representations in a given layer,"[[    0    30   602     5    78  5402  7681    36  4794    43     9    63
  37617  1538 30464    11    10   576 10490     2]]"
22ccee453e37536ddb0c1c1d17b0dbac04c6c607,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English ,[[    0 35007  1437     2]]
2b3cac7af10d358d4081083962d03ea2798cf622,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
30eacb4595014c9c0e5ee9669103d003cfdfe1e5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],relation classification dataset of the SemEval 2010 task 8,"[[    0 47114 20257 41616     9     5 11202   717  6486  1824  3685   290
      2]]"
2fbb6322e485e7743ec3fb4bb02d44bf4b5ea8a6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English Wikipedia dump from June 2016,[[    0 35007 28274 12371    31   502   336     2]]
6d4400f45bd97b812e946b8a682b018826e841f1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering","[[    0 27840  8117    30    95   546    23    10  2783     9  3156     6
   6694    70 24173    19   233    12  1116    12 40511   335     6    38
   5049  9294   705  1851 46644  2961     2]]"
d3ff2986ca8cb85a9a5cec039c266df756947b43,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"words embeddings, style, and morality features",[[    0 30938 33183   417  1033     6  2496     6     8 28241  1575     2]]
56b034c303983b2e276ed6518d6b080f7b8abe6a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"FSD BIBREF12 , Twitter, and Google datasets","[[    0   597  6243   163  8863 45935  1092  2156   599     6     8  1204
  42532     2]]"
887c6727e9f25ade61b4853a869fe712fe0b703d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The neural projector must be invertible.,[[    0   133 26739 35282   531    28    11  9942  4748     4     2]]
6bf93968110c6e3e3640360440607744007a5228,0.0,Entailment,[[    2     0 30495  3760  1757     2]],we do not know exactly,[[   0 1694  109   45  216 2230    2]]
d667731ea20605580c398a1224a0094d1155ebbb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
0a5ffe4697913a57fda1fd5a188cd5ed59bdc5c7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish","[[    0 40028   571  9063     6 27248     6  9096     6 13501     6  2370
      6  1515     6  1859     6 12852     6  3108     6 11814     6 27775
      6 11145     6 13053     6 37650   811     6  3453     8  9004     2]]"
d6e353e0231d09fd5dcba493544d53706f3fe1ab,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Automatic: Normalized cross correlation (NCC)
Manual: Mean Opinion Score (MOS)","[[    0 37434 29177    35 26411  1538  2116 22792    36   487  3376    43
  50118  6407  5564    35 30750 20253 14702    36   448  3196    43     2]]"
545e92833b0ad4ba32eac5997edecf97a366a244,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Increasing number of message passing iterations showed consistent improvement in performance - around 1 point improvement compared between 1 and 4 iterations,"[[    0 41084  7913   346     9  1579  3133 37922   969  4292  3855    11
    819   111   198   112   477  3855  1118   227   112     8   204 37922
      2]]"
f03112b868b658c954db62fc64430bebbaa7d9e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
70e9210fe64f8d71334e5107732d764332a81cb1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CNN-DNN-BLSTM-HMM,"[[    0 16256    12   495 20057    12  7976  4014   448    12   725 16261
      2]]"
2ccc26e11df4eb26fcccdd1f446dc749aff5d572,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
31b92c03d5b9be96abcc1d588d10651703aff716,0.0,Entailment,[[    2     0 30495  3760  1757     2]],0.7033,[[   0  288    4 3083 3103    2]]
16535db1d73a9373ffe9d6eedaa2369cefd91ac4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],PubMed+PMC,[[    0 44110 21243  2744  5683   347     2]]
c7486d039304ca9d50d0571236429f4f6fbcfcf7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Turkish,[[    0 37300     2]]
2c947447d81252397839d58c75ebcc71b34379b5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CoinCollector, CookingWorld",[[    0 17339 44252   368     6 32284 10988     2]]
f7ed3b9ed469ed34f46acde86b8a066c52ecf430,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The LexVec BIBREF7,[[    0   133 14786   846  3204   163  8863 45935   406     2]]
7d2f812cb345bb3ab91eb8cbbdeefd4b58f65569,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
2c6b50877133a499502feb79a682f4023ddab63e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Simple English,[[    0 45093  2370     2]]
78292bc57ee68fdb93ed45430d80acca25a9e916,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Create the negated LAMA dataset and  query the pretrained language models with both original LAMA and negated LAMA statements and compare their predictions.,"[[    0 44758     5 15183  1070   226 19455 41616     8  1437 25860     5
  11857 26492  2777  3092    19   258  1461   226 19455     8 15183  1070
    226 19455  1997     8  8933    49 12535     4     2]]"
ad1be65c4f0655ac5c902d17f05454c0d4c4a15d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Distribution of category labels, number of answerable-not answerable questions, number of text-based and script-based questions, average text, question, and answer length, number of questions per text","[[    0 42390 35719     9  4120 14105     6   346     9  1948   868    12
   3654  1948   868  1142     6   346     9  2788    12   805     8  8543
     12   805  1142     6   674  2788     6   864     6     8  1948  5933
      6   346     9  1142   228  2788     2]]"
662870a90890c620a964720b2ca122a1139410ea,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English, French, German ",[[    0 35007     6  1515     6  1859  1437     2]]
fb2b536dc8e442dffab408db992b971e86548158,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
6a31db1aca57a818f36bba9002561724655372a7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"32,595",[[    0  2881     6 35305     2]]
14b8ae5656e7d4ee02237288372d9e682b24fdb8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"obscure and hard to understand,  lack of previous work and baselines on irony generation","[[    0  2413  3866  2407     8   543     7  1346     6  1437  1762     9
    986   173     8 11909 38630    15 21490  2706     2]]"
aef607d2ac46024be17b1ddd0ed3f13378c563a6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They measured the under-translated words with low word importance score as calculated by Attribution.
method","[[    0  1213  9550     5   223    12  9981 32914  1617    19   614  2136
   3585  1471    25  9658    30 45478     4 50118 45416     2]]"
9b4dc790e4ff49562992aae4fad3a38621fadd8b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BOW-Tags, BOW-KL(Tags), BOW-All, GloVe","[[    0   387  4581    12 49181     6   163  4581    12   530   574  1640
  49181   238   163  4581    12  3684     6  4573   139 30660     2]]"
4aa9b60c0ccd379c6fb089c84a6c7b872ee9ec4f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Adaptive Multi-task Learning
, Margin Ranking (MR) Loss
, Pairwise Neural Ranking Model
","[[    0 46874  2088 19268    12 45025 13807 50118     6  7572   179 31563
     36 12642    43 19700 50118     6 34587 10715 44304 31563  7192 50118
      2]]"
d6ea7a30b0b61ae126b00b59d2a14fff2ef887bf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use","[[    0 48527  2320     8  2598  2634    64   694  6456    15  1142     8
    244    19 42972  6910  3009   106   482  4686     7  1533 24148     6
   6594   304     2]]"
5152b78f5dfee26f1b13f221c1405ffa9b9ba3a4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
55c8f7acbfd4f5cde634aaecd775b3bb32e9ffa3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Phoneme Error Rate (PER), Word Error Rate (WER), Word Error Rate 100 (WER 100)","[[    0 17297   261 17185 37943 14064    36 21260   238 15690 37943 14064
     36 45117   238 15690 37943 14064   727    36 45117   727    43     2]]"
eb2d5edcdfe18bd708348283f92a32294bb193a5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],a score of 40,[[   0  102 1471    9  843    2]]
1170e4ee76fa202cabac9f621e8fbeb4a6c5f094,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
2007bfb8f66e88a235c3a8d8c0a3b3dd88734706,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"A comparison of common words, We aggregated the score for each word and normalized each article by emotions. To better compare the result, we added a baseline of 100 random articles from a Reuters news dataset as a non-religious general resource","[[    0   250  6676     9  1537  1617     6   166 26683  1070     5  1471
     13   349  2136     8 35247   349  1566    30  8597     4   598   357
   8933     5   898     6    52   355    10 18043     9   727  9624  7201
     31    10  1201   340 41616    25    10   786    12 32300   937  5799
      2]]"
568fb7989a133564d84911e7cb58e4d8748243ef,0.0,Entailment,[[    2     0 30495  3760  1757     2]],explores the state space through keeping track of previously visited states by maintaining an archive,"[[    0 23242  4765     5   194   980   149  2396  1349     9  1433  3790
    982    30  6780    41 22397     2]]"
74b338d5352fe1a6fd592e38269a4c81fe79b866,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Readability (RED),  Number of Words (LEN), Avg. Fixation Duration (FDUR), Avg. Fixation Count (FC), Avg. Saccade Length (SL), Regression Count (REG), Skip count (SKIP), Count of regressions from second half
to first half of the sentence (RSF), Largest Regression Position (LREG),  Edge density of the saliency gaze
graph (ED),  Fixation Duration at Left/Source
(F1H, F1S),  Fixation Duration at Right/Target
(F2H, F2S),  Forward Saccade Word Count of
Source (PSH, PSS),  Forward SaccadeWord Count of Destination
(PSDH, PSDS), Regressive Saccade Word Count of
Source (RSH, RSS),  Regressive Saccade Word Count of
Destination (RSDH, RSDS)","[[    0 25439  4484    36 36015   238  1437 12270     9 27341    36   574
   2796   238 43730     4 25088  1258 44077    36 24667  2492   238 43730
      4 25088  1258 12440    36  5268   238 43730     4   208  7904  1829
  41852    36 11160   238  6304 21791 12440    36 32239   238  6783  3212
     36 17342  3808   238 12440     9 44702  2485    31   200   457 50118
    560    78   457     9     5  3645    36  8105   597   238   226  5384
    990  6304 21791 21355    36   574 32239   238  1437 12591 16522     9
      5  6641 33625 28341 50118 44143    36  1691   238  1437 25088  1258
  44077    23 10039    73  7061 50118  1640   597   134   725     6   274
    134   104   238  1437 25088  1258 44077    23  5143    73 41858 50118
   1640   597   176   725     6   274   176   104   238  1437 10038   208
   7904  1829 15690 12440     9 50118  7061    36  3888   725     6   221
   8108   238  1437 10038   208  7904  1829 44051 12440     9 37166 50118
   1640  3888 38462     6  4868  5433   238  6304 22075   208  7904  1829
  15690 12440     9 50118  7061    36  8105   725     6 20569   238  1437
   6304 22075   208  7904  1829 15690 12440     9 50118 42551  8111    36
   8105 38462     6 15388  5433    43     2]]"
afb77b11da41cd0edcaa496d3f634d18e48d7168,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BERT based fine-tuning, Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer","[[    0 11126   565   716  2051    12 24641   154     6 45707   786 43871
  13171     6 45707  6479    12   574  4014   448 10490     6 45707  3480
  10490     2]]"
0d9fcc715dee0ec85132b3f4a730d7687b6a06f4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the number of distinct word recognition outputs that an attacker can induce, not just the number of words on which the model is “fooled”","[[    0   627   346     9 11693  2136  4972 39512    14    41 12082    64
  28944     6    45    95     5   346     9  1617    15    61     5  1421
     16    44    48   506  8110   196    17    46     2]]"
ae90c5567746fe25af2fcea0cc5f355751e05c71,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"US dataset, Italian dataset",[[    0  3048 41616     6  3108 41616     2]]
551457ed34ca7fc0878c85bc664b135c21059b58,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data, selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset","[[    0 23830    15 17235   722    36  2808 28302 38036   134   727   530
  10960    43     9 27472 33273  1901   414     6 38845    10 37105     9
     10   112     6  5987    12  4509    36  2808 28302 38036   176   112
      4   134   448 10960    43 35237 14286   196 41616     2]]"
e79a5b6b6680bd2f63e9f4adbaae1d7795d81e38,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Russsian,[[    0 44867  7485   811     2]]
5c5aeee83ea3b34f5936404f5855ccb9869356c1,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," four machine translation tasks, IWSLT 2017 German $\rightarrow $ English BIBREF27, KFTT Japanese $\rightarrow $ English BIBREF28, WMT 2016 Romanian $\rightarrow $ English BIBREF29, WMT 2014 English $\rightarrow $ German BIBREF30","[[    0   237  3563 19850  8558     6    38   771 11160   565   193  1859
  49959  4070 17214    68  2370   163  8863 45935  2518     6   229 11615
    565  2898 49959  4070 17214    68  2370   163  8863 45935  2517     6
    305 11674   336 21624 49959  4070 17214    68  2370   163  8863 45935
   2890     6   305 11674   777  2370 49959  4070 17214    68  1859   163
   8863 45935   541     2]]"
f56d07f73b31a9c72ea737b40103d7004ef6a079,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The homographic dataset contains 2,250 contexts, 1,607 of which contain a pun. The heterographic dataset consists of 1,780 contexts with 1,271 containing a pun.","[[    0   133  9486 25510 41616  6308   132     6  5714 38270     6   112
      6 32774     9    61  5585    10  7434     4    20 39872 25510 41616
  10726     9   112     6 33066 38270    19   112     6 32145  8200    10
   7434     4     2]]"
8acab64ba72831633e8cc174d5469afecccf3ae9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
2df4a045a9cd7b44874340b6fdf9308d3c55327a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They did not use any platform, instead they hired undergraduate students to do the annotation.","[[    0  1213   222    45   304   143  1761     6  1386    51  4547 19555
    521     7   109     5 47760     4     2]]"
12f7fac818f0006cf33269c9eafd41bbb8979a48,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Inception V3, biLSTM",[[    0  1121 20900   468   246     6  4003   574  4014   448     2]]
41ac23e32bf208b69414f4b687c4f324c6132464,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English, German",[[    0 35007     6  1859     2]]
c32adef59efcb9d1a5b10e1d7c999a825c9e6d9a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
2ad4d3d222f5237ed97923640bc8e199409cbe52,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
218615a005f7f00606223005fef22c07057d9d77,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Answer with content missing: (Data section) Penn Treebank (PTB),"[[    0 33683    19  1383  1716    35    36 30383  2810    43  5953 11077
   5760    36 10311   387    43     2]]"
6a31db1aca57a818f36bba9002561724655372a7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"32,595 posts",[[    0  2881     6 35305  4570     2]]
74e866137b3452ec50fb6feaf5753c8637459e62,0.0,Entailment,[[    2     0 30495  3760  1757     2]], higher tiers of the pyramid,[[    0   723 34325     9     5 33344     2]]
b65a83a24fc66728451bb063cf6ec50134c8bfb0,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," Two methods: first is to simply pick initial few sentences,  second is to capture the relation between the two most important entities  (select the first sentence which contains both these entities).","[[    0  1596  6448    35    78    16     7  1622  1339  2557   367 11305
      6  1437   200    16     7  5604     5  9355   227     5    80   144
    505  8866  1437    36 38450     5    78  3645    61  6308   258   209
   8866   322     2]]"
deb89bca0925657e0f91ab5daca78b9e548de2bd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"presence/absence of consonants, presence/absence of phonemic nasal, presence/absence of bilabial, presence/absence of high-front vowels, and presence/absence of high-back vowels","[[    0 13529  4086    73 10155  4086     9 46098  3277     6  2621    73
  10155  4086     9 43676 14414 35937     6  2621    73 10155  4086     9
  31617   873  2617     6  2621    73 10155  4086     9   239    12  9289
  27578  2507     6     8  2621    73 10155  4086     9   239    12  1644
  27578  2507     2]]"
c1c44fd96c3fa6e16949ae8fa453e511c6435c68,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences.","[[    0 49414    42   609    16  1122     7     5 42771  2158  3685    11
    163 18854    18  1198    12 21714   609     6  3891    30   634     5
   1460     9     5 37617  2777  1421     5  5044 15362    64  5368    55
  34574     8  1632 26929     4     2]]"
6b9b9e5d154cb963f6d921093539490daa5ebbae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],clipped PMI; NNEGPMI,[[   0 3998 8246 2784  100  131  234  487 7170 5683  100    2]]
0f6216b9e4e59252b0c1adfd1a848635437dfcdc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Spanish tweets were scraped between November 8, 2017 and January 12, 2018, Affect in Tweets Distant Supervision Corpus (DISC)","[[    0 41226  6245    58 11573   196   227   759   290     6   193     8
    644   316     6   199     6 37089    11 18868  2580 11281   927  1582
  14675 28556    36 37056   347    43     2]]"
2ccc26e11df4eb26fcccdd1f446dc749aff5d572,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
392fb87564c4f45d0d8d491a9bb217c4fce87f03,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," LastStateRNN, AvgRNN, AttentionRNN","[[    0  1426 13360   500 20057     6 43730   500 20057     6 35798   500
  20057     2]]"
0af16b164db20d8569df4ce688d5a62c861ace0b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe","[[    0 14118    12  1116    12 30938    36   387  4581   238  1385 13135
   2383   179 15189  3780 13135    36 20249  2688   597   238 26739    12
    805  2136 33183 11303     6  9359  5580  6304 21791    36 33919   238
  34638  5761    36 30455   238 14159 16256   163  8863 45935   698    19
   2557  2136 33183 11303    25  4573   139 30660     2]]"
e54257585cc75564341eb02bdc63ff8111992f82,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (LVL1, LVL2, LVL3) 
- Stanford CoreNLP
- Optical Character Recognition (OCR) system, ParsCIT 
- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion.","[[    0 33683    19  1383  1716    35    36 19996   574   134     6 38669
    574   176     6 38669   574   246    43  1437 50118    12  8607  9025
    487 21992 50118    12 36132 35177 23288  7469    36  4571   500    43
    467     6 31443   347  2068  1437 50118    12   617  4091  8614     5
   8135  2788    31   672   132  1198 31931   196  2339     7     5   511
     35  1270     6 42211     6 20372     6  7740     6  1330   173     6
   3618     8  6427     4     2]]"
fd2c6c26fd0ab3c10aae4f2550c5391576a77491,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
6bf5620f295b5243230bc97b340fae6e92304595,0.0,Entailment,[[    2     0 30495  3760  1757     2]],same baseline as used by lang2011unsupervised,[[    0 41690 18043    25   341    30 22682 22748   879 16101 25376     2]]
f2155dc4aeab86bf31a838c8ff388c85440fce6e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
a69a59b6c0ab27bcee1a780d6867df21e30aec08,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
6844683935d0d8f588fa06530f5068bf3e1ed0c0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"$\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus","[[    0  1629 37457 40051   405 25522  5683   100 24303  1640   605     6
    438    43  1629  1411     7  2430 40186    77     5  2136    12 46796
   1763 44612   605     6   438    43  1629   473    45  2082    11     5
   1058 42168     2]]"
4625cfba3083346a96e573af5464bc26c34ec943,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.
For the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.","[[    0  2709     5 45569 39012 41616     6     5  3855    81 18043   234
  11674    16   132     4  1225   163  3850   791     6   112     4   406
    274   530 10020     8   112     4  3570   208 32665     4 50118  2709
      5 45569 34647 41616     6     5  3855    81 18043   234 11674    16
   1437   290     4  3272   163  3850   791     4     2]]"
70e9210fe64f8d71334e5107732d764332a81cb1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],HMM-based system,[[    0   725 16261    12   805   467     2]]
0a75a52450ed866df3a304077769e1725a995bb7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information","[[    0 11127 15362  3685     6    61 17876     5  1002 13931 18102    23
     86  2808 28302 38036   246   716    15   986  4195     8  5377   335
      2]]"
76ed74788e3eb3321e646c48ae8bf6cdfe46dca1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"POS, gender/number and stem POS",[[    0 42740     6  3959    73 30695     8 10114 29206     2]]
692c9c5d9ff9cd3e0ce8b5e4fa68dda9bd23dec1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"22,880 users",[[    0  2036     6 32954  1434     2]]
55139fcfe04ce90aad407e2e5a0067a45f31e07e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Using Google translation API.,[[    0 36949  1204 19850 21013     4     2]]
893ec40b678a72760b6802f6abf73b8f487ae639,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate,"[[    0   133  7601   969   367  6245   147  5063     8 33583 13453  1383
   5152    53     5  1421    21   441     7 28224     2]]"
1e185a3b8cac1da939427b55bf1ba7e768c5dae4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],VAE based phone classification,[[    0  9788   717   716  1028 20257     2]]
9176d2ba1c638cdec334971c4c7f1bb959495a8e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)","[[    0  1694   304   278   112     9     5  1300 11170    25     5   129
   1300    19  5702  6929   335   148  1058     6     8    52 10516     5
   5389  1421    15   278   112     9     5  1002 11170     6  5972    36
    387   530   238 12057    36   717   238 12625    36 13269   238     8
   3920    36   448    43     2]]"
e5c8e9e54e77960c8c26e8e238168a603fcdfcc6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],From all reported results proposed method (NB+Lex) shows best accuracy on all 3 datasets - some models are not evaluated and not available in literature.,"[[    0  7605    70   431   775  1850  5448    36 20485  2744 43551    43
    924   275  8611    15    70   155 42532   111   103  3092    32    45
  15423     8    45   577    11 13144     4     2]]"
a712718e6596ba946f29a99838d82f95b9ebb1ce,0.0,Entailment,[[    2     0 30495  3760  1757     2]],it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too,"[[    0   405    34   321     4 40044  3855    11  8611 12818     7 17678
   8756  4041     8  1437   321     4 38049  3855    11   274   134  1471
   1437 12818     7 17678  8756  4041   350     2]]"
808f0ad46ca4eb4ea5492f9e14ca043fe1e206cc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"45,821 characters",[[   0 1898    6  398 2146 3768    2]]
d571e0b0f402a3d36fb30d70cdcd2911df883bc7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
1522ccedbb1f668958f24cca070f640274bc2549,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2","[[    0 33683    19  1383  1716    35    36   717  6486  9762  4369 18715
   2810    43 29484     6 35109     6   274   134    12  3866  4765     6
    312 12127   914     6 30782   717  3411     6   248  5061  8800    12
    176     2]]"
cb77d6a74065cb05318faf57e7ceca05e126a80d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CNN modelBIBREF0, Stanford CRF modelBIBREF21","[[    0 16256  1421  5383   387 45935   288     6  8607  4307   597  1421
   5383   387 45935  2146     2]]"
52f8a3e3cd5d42126b5307adc740b71510a6bdf5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Detection of an aspect in a review, Prediction of the customer general satisfaction, Prediction of the global trend of an aspect in a given review, Prediction of whether the rating of a given aspect is above or under a given value, Prediction of the exact rating of an aspect in a review, Prediction of the list of all the positive/negative aspects mentioned in the review, Comparison between aspects, Prediction of the strengths and weaknesses in a review","[[    0 43170 20970     9    41  6659    11    10  1551     6 27851     9
      5  2111   937 11658     6 27851     9     5   720  2904     9    41
   6659    11    10   576  1551     6 27851     9   549     5   691     9
     10   576  6659    16  1065    50   223    10   576   923     6 27851
      9     5  6089   691     9    41  6659    11    10  1551     6 27851
      9     5   889     9    70     5  1313    73 33407  5894  2801    11
      5  1551     6 37070   227  5894     6 27851     9     5 13348     8
  18659    11    10  1551     2]]"
23b2901264bda91045258b5d4120879ae292e950,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP,"[[    0 36949   211  3632   872 15296     5   274   134  1471    30  2055
    288     4  4432    13 18838  4794     8  2055   288     4  5352    13
   1209  1864   510     2]]"
90d946ccc3abf494890e147dd85bd489b8f3f0e8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
8bf7f1f93d0a2816234d36395ab40c481be9a0e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
3ddff6b707767c3dd54d7104fe88b628765cae58,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Universal Dependencies v1.2 treebanks for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German,
Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish","[[    0 35011 43290 14768   748   134     4   176  3907 27045    13     5
    511   545 11991    35 29586     6 27248     6  9096     6 13501     6
   2370     6  1515     6  1859     6 50118 15248  6909   811     6  3108
      6 11814     6 27775     6 11145     6 13053     6 37650   811     6
   3453     6     8  9004     2]]"
fa30a938b58fc05131c3854f12efe376cbad887f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. ","[[    0 33683    19  1383  1716    35    36 41836   155    43  2700  2730
     18  1421   163    12   448   674  5177   856    12 31673    16   321
      4 34295     6   321     4 36022     6   321     4 35775    15 37089
   2088     6 37857 32419     8  3703 27757 42532  4067     4  1437     2]]"
b0dbe75047310fec4d4ce787be5c32935fc4e37b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By evaluating their model on adversarial sets containing misleading sentences,"[[    0  2765 15190    49  1421    15 37930 27774  3880  8200 12030 11305
      2]]"
b1bc9ae9d40e7065343c12f860a461c7c730a612,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Existential (OneShape, MultiShapes), Spacial (TwoShapes, Multishapes), Quantification (Count, Ratio) datasets are generated from ShapeWorldICE","[[    0  9089   661 12986    36  3762 45336     6 19268  3609 22179   238
   2064 27015    36  9058  3609 22179     6 14910  1173 22179   238 28256
   5000    36 23329     6 20475    43 42532    32  5129    31 29937 10988
   9292     2]]"
52f9cd05d8312ae3c7a43689804bac63f7cac34b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
ababb79dd3c301f4541beafa181f6a6726839a10,0.0,Entailment,[[    2     0 30495  3760  1757     2]],“Intelligence Squared Debates” (IQ2 for short),"[[    0    17    48 22886 45852 13840  6537 10532  1626    17    46    36
  30018   176    13   765    43     2]]"
af5730d82535464cedfa707a03415ac2e7a21295,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set, Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus), RTE5","[[    0 47681    36  1116  5773    12  4697  3768    43    31     5 24704
  23132  6003  8024  1864   250  1058   278     6  1554 13851   660  3654
   1070  4052    12 25313   687    36 32804   347    43     9     5  2117
    470   496 28556   238   248  6433   245     2]]"
bdf93053b1b9b0a21f77ed370cf4d5a10df70e3e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"accuracy and F1-score of 89.6% and 89.2%, respectively","[[    0  7904 45386     8   274   134    12 31673     9  8572     4   401
    207     8  8572     4   176  4234  4067     2]]"
58ef2442450c392bfc55c4dc35f216542f5f2dbb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
77c34f1033702278f7f044806c1eba0c6ecb8b04,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
808f0ad46ca4eb4ea5492f9e14ca043fe1e206cc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"45,821 characters",[[   0 1898    6  398 2146 3768    2]]
7697baf8d8d582c1f664a614f6332121061f87db,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SVMhmm ,[[    0   104 20954   298  5471  1437     2]]
a99fdd34422f4231442c220c97eafc26c76508dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
f875337f2ecd686cd7789e111174d0f14972638d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Affective Text, Fairy Tales, ISEAR",[[    0   250 45700  2088 14159     6 37857 32419     6  3703 27757     2]]
f8f4e4a50d2b3fbd193327e79ea32d8d057e1414,0.0,Entailment,[[    2     0 30495  3760  1757     2]],crowdsourcing,[[    0   438 34298    29 27824     2]]
e330e162ec29722f5ec9f83853d129c9e0693d65,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
9b97805a0c093df405391a85e4d3ab447671c86a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Exact Match (EM) and Macro-averaged F1 scores (F1) ,"[[    0  9089  7257  9018    36  5330    43     8 32449    12  9903  4628
    274   134  4391    36   597   134    43  1437     2]]"
0752d71a0a1f73b3482a888313622ce9e9870d6e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],system presented by deri2016grapheme,[[    0 19675  2633    30  1935   118  9029   571  8645   700  1794     2]]
d9980676a83295dda37c20cfd5d58e574d0a4859,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"copy, copy-marked, copy-dummies",[[    0 44273     6  5375    12 18584     6  5375    12   417 38098     2]]
cf874cd9023d901e10aa8664b813d32501e7e4d2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Named Entity Recognition, including entities such as proteins, genes, diseases, treatments, drugs, etc. in the biomedical domain","[[    0   487  7486 46718 23288  7469     6   217  8866   215    25 17792
      6 14819     6  6357     6  8289     6  2196     6  4753     4    11
      5 35241 11170     2]]"
8dda1ef371933811e2a25a286529c31623cca0c6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],One experienced annotator tagged all tweets,[[    0  3762  2984 45068  2630 20390    70  6245     2]]
ce2b921e4442a21555d65d8ce4ef7e3bde931dfc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)","[[    0 28586    36 12997   238  1083    36  2070   238 19645    36   271
    238  1111    36 13808   238 19840    36  3592   238     8 16859    36
   6873    43     2]]"
99e78c390932594bd833be0f5c890af5c605d808,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"QA PGNet, Multi-decoder QA PGNet with lookup table embedding","[[    0  1864   250 14499 15721     6 19268    12 11127 15362  1209   250
  14499 15721    19 47938  2103 33183 11303     2]]"
31b92c03d5b9be96abcc1d588d10651703aff716,0.0,Entailment,[[    2     0 30495  3760  1757     2]],0.7033,[[   0  288    4 3083 3103    2]]
29f2954098f055fb19d9502572f085862d75bf61,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"KNN
RF
SVM
MLP",[[    0   530 20057 50118 30455 50118   104 20954 50118 10537   510     2]]
91e326fde8b0a538bc34d419541b5990d8aae14b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WMT15 German-to-English, RWTH German-English dataset","[[    0   771 11674   996  1859    12   560    12 35007     6 34573  3732
   1859    12 35007 41616     2]]"
fabcd71644bb63559d34b38d78f6ef87c256d475,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Baseline models are:
- Chen et al., 2015a
- Chen et al., 2015b
- Liu et al., 2016
- Cai and Zhao, 2016
- Cai et al., 2017
- Zhou et al., 2017
- Ma et al., 2018
- Wang et al., 2019","[[    0 40258  7012  3092    32    35 50118    12 10136  4400  1076   482
    570   102 50118    12 10136  4400  1076   482   570   428 50118    12
  13768  4400  1076   482   336 50118    12   230  1439     8 34219     6
    336 50118    12   230  1439  4400  1076   482   193 50118    12 28279
   4400  1076   482   193 50118    12  3066  4400  1076   482   199 50118
     12  9705  4400  1076   482   954     2]]"
1b23c4535a6c10eb70bbc95313c465e4a547db5e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"In encoder they use convolutional, NIN and bidirectional LSTM layers and in decoder they use unidirectional LSTM ","[[    0  1121  9689 15362    51   304 15380 23794   337     6   234  2444
      8  2311 43606   337   226  4014   448 13171     8    11  5044 15362
     51   304   542   808 43606   337   226  4014   448  1437     2]]"
4d706ce5bde82caf40241f5b78338ea5ee5eb01e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Probes are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases.,"[[    0 10653 16768    32 26956  3092  5389    15  9214   740   605  4926
      7   146 12535    59 39608    36  8628  3999 28201     8 46195    43
   3611     9  1617     8 22810     4     2]]"
d53299fac8c94bd0179968eb868506124af407d1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Using F1 Micro measure, the KNN classifier perform 0.6762, the RF 0.6687, SVM 0.6712 and MLP 0.6778.","[[    0 36949   274   134 10719  2450     6     5   229 20057  1380 24072
   3008   321     4  4111  5379     6     5 18044   321     4  4280  5677
      6   208 20954   321     4  4111  1092     8 10725   510   321     4
   4111  5479     4     2]]"
1269c5d8f61e821ee0029080c5ba2500421d5fa6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],data augmentation,[[    0 23687 26713 36197     2]]
4c07c33dfaf4f3e6db55e377da6fa69825d0ba15,0.0,Entailment,[[    2     0 30495  3760  1757     2]],300,[[   0 2965    2]]
1d3e914d0890fc09311a70de0b20974bf7f0c9fe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800","[[    0  3573   245   347 10644    12   417  1496  3175     6  9537  5383
     12   417  1496  3175     6  9543   245   347 10644    12 25666     6
   9543   306  3764  5330   495     6  9543   176 16972     6   344   487
  21992  3813     6 39671  4444   717  3048     6 33797    12  3913     2]]"
06cc8fcafc0880cf69a2514bb7341642b9833041,0.0,Entailment,[[    2     0 30495  3760  1757     2]],100 000 documents,[[    0  1866 12096  2339     2]]
12d7055baf5bffb6e9e95e977c000ef2e77a4362,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added","[[    0 37362  1757    77     5  1202 37105    19  3827 47234    16  4281
     19     5  2405  2180 47760    16   155     4   245   274   134  1471
      6   203  2514    87    77    10  9624   278     9  3827 47234    32
    355     2]]"
e831041d50f3922265330fcbee5a980d0e2586dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"participants were instructed to read the sentences naturally, without any specific task other than comprehension","[[    0 42038  3277    58 14027     7  1166     5 11305  8366     6   396
    143  2167  3685    97    87 40494     2]]"
d78f7f84a76a07b777d4092cb58161528ca3803c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],backward greedy search over each sentence's label sequence to identify word boundaries,"[[    0  1644  7767 34405  1707    81   349  3645    18  6929 13931     7
   3058  2136 10156     2]]"
203337c15bd1ee05763c748391d295a1f6415b9b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the model with multi-attention mechanism and a projected layer,"[[    0   627  1421    19  3228    12  2611 19774  9562     8    10  5635
  10490     2]]"
d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Targeted Insult (TIN): Posts which contain an insult/threat to an individual, group, or others , Untargeted (UNT): Posts containing non-targeted profanity and swearing.","[[    0 41858   196  6326  6070    36   565  2444  3256 23227    61  5585
     41 18566    73 25836     7    41  1736     6   333     6    50   643
   2156 32330 45358   196    36  4154   565  3256 23227  8200   786    12
  23976   196  8546 30854     8 21854     4     2]]"
10d450960907091f13e0be55f40bcb96f44dd074,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
26e2d4d0e482e6963a76760323b8e1c26b6eee91,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Once split into 8 subsets (A-H), the test set used are blocks D+H and blocks F+H","[[    0 11475  3462    88   290 18621  2580    36   250    12   725   238
      5  1296   278   341    32  5491   211  2744   725     8  5491   274
   2744   725     2]]"
5c5aeee83ea3b34f5936404f5855ccb9869356c1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German","[[    0 10231  3563 19850  8558    35  1859 43839  2370     6  2898 43839
   2370     6 21624 43839  2370     6  2370 43839  1859     2]]"
84bad9a821917cb96584cf5383c6d2a035358d7c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation.","[[    0   133   414    21  4786   634   155  6411    35  6190    10   651
      9  4792  3218    14    58  2964     7  5555 39758  9401 42752  1142
      6   172  2268     5  5203   414  2783     9  1142     6 14301     8
   5274  1241  8817 27824    15  1645 35644 19683     8  2029   335    59
    103  2139   618 39221  2402     8     5 41616 26567     4     2]]"
5f60defb546f35d25a094ff34781cddd4119e400,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
8b0abc1907c2bf3e0256f8cf85e0ba66a839bd92,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
1b72aa2ec3ce02131e60626639f0cf2056ec23ca,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Level A: 14100 Tweets
Level B: 4640 Tweets
Level C: 4089 Tweets","[[    0 38809    83    35   501  1866 18868  2580 50118 38809   163    35
   4059  1749 18868  2580 50118 38809   230    35   843  5046 18868  2580
      2]]"
792f6d76d2befba2af07198584aac1b189583ae4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],established task,[[    0 22234  3685     2]]
d1ec42b2b5a3c956ff528543636e024bfde5e5ba,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN","[[    0 26170  8007    12 15887     6  6002  8007    12 15887  2744 42848
      6  6002  8007    12 15887  2744 42271    12 11615     6  6002  8007
     12 15887  2744 42848    12 11615     6  6002  8007    12 15887  2744
  17868    12   500  5061  8800     6  6002  8007    12 15887  2744 17868
     12   104  2796     2]]"
8d074aabf4f51c8455618c5bf7689d3f62c4da1d,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," ambiguous words, unknown words",[[    0 33406  1617     6  4727  1617     2]]
7b9ca0e67e394f1674f0bcf1c53dfc2d474f8613,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English , German, French",[[    0 35007  2156  1859     6  1515     2]]
2ee715c7c6289669f11a79743a6b2b696073805d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. 

For Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.","[[    0  2709  6776    12 49448 13133     6    51  1701    80 11909 38630
     35     5    78    65   634   129  6641 11465    12   805  1575     6
      8     5   200 18043  6240   114     5 10014  2092    11     5  1270
      9     5  1566     4  1437 50118 50118  2709  6776    12 43480  3037
  23423     6    51  1701    80 11909 38630    35     5    78  5916     5
   2810    19     5  1609 36912  3569 37015     7     5  1566     6     8
      5   200    65  5916     5   144  7690  2810     4     2]]"
1ff0fccf0dca95a6630380c84b0422bed854269a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"efficiency of a communication scheme $(q_{\alpha },p_{\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence","[[    0 33939     9    10  4358  3552 44612  1343 49747 37457 29135 48037
    642 49747 37457 44263 49000  1629    30     5 16580   731     9 22121
      6    61    16  9550    25     5 13484     9 22121    14    32  1682
     11     5 32712     6  8611     9    10  3552    16  9550    25     5
  13484     9 11305  5129    30 26253  5846 46133     5  1421    14  2230
   2856     5  1002  3645     2]]"
f0848e7a339da0828278f6803ed7990366c975f0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"No-Answer Baseline (NA), Word Count Baseline, Human Performance","[[    0  3084    12 33683  7093  7012    36  4444   238 15690 12440  7093
   7012     6  3861 10193     2]]"
0f7867f888109b9e000ef68965df4dde2511a55f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Among all the classes predicted by several models, for each test sentence, class with most votes are picked. In case of a tie, one of the most frequent classes are picked randomly.","[[    0 36342    70     5  4050  6126    30   484  3092     6    13   349
   1296  3645     6  1380    19   144  2834    32  2738     4    96   403
      9    10  3318     6    65     9     5   144  7690  4050    32  2738
  22422     4     2]]"
600b097475b30480407ce1de81c28c54a0b3b2f8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"GloVe vectors trained on Wikipedia Corpus with ensembling, and GloVe vectors trained on Airbnb Data without ensembling","[[    0   534  4082 30660 44493  5389    15 28274 28556    19  1177 26660
   1527     6     8  4573   139 30660 44493  5389    15 16315  5423   396
   1177 26660  1527     2]]"
9b76f428b7c8c9fc930aa88ee585a03478bff9b3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],53 documents,[[   0 4540 2339    2]]
193ee49ae0f8827a6e67388a10da59e137e7769f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],A task for seq2seq model pra-training that recovers a masked document to its original form.,"[[    0   250  3685    13 48652   176 47762  1421 21950    12 32530    14
  29283    10 24397  3780     7    63  1461  1026     4     2]]"
a1645d0ba50e4c29f0feb806521093e7b1459081,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Social Honeypot dataset (public) and Weibo dataset (self-collected); yes,"[[    0 28565 13834  8024 41616    36 15110    43     8   166 18232 41616
     36 13367    12  9119 17970  4397  4420     2]]"
2c78993524ca62bf1f525b60f2220a374d0e3535,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"rupnik2016news, Deutsche Welle's news website","[[    0 15566  8256  9029  2926     6  8043   305  6591    18   340   998
      2]]"
bdc91d1283a82226aeeb7a2f79dbbc57d3e84a1a,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," improvement on the RTE dataset is significant, i.e., 4% absolute gain over the BERTBase","[[    0  3855    15     5   248  6433 41616    16  1233     6   939     4
    242   482   204   207  7833  2364    81     5   163 18854 34164     2]]"
f0b1d8c0a44dbe8d444a5dbe2d9c3d51e048a6f6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Metric difference between Aloha and best baseline score:
Hits@1/20: +0.061 (0.3642 vs 0.3032)
MRR: +0.0572(0.5114 vs 0.4542)
F1: -0.0484 (0.3901 vs 0.4385)
BLEU: +0.0474 (0.2867 vs 0.2393)","[[    0 25869  4063  2249   227   726 15202     8   275 18043  1471    35
  50118   725  2629  1039   134    73   844    35  2055   288     4  4124
    134    36   288     4 34639   176  1954   321     4   541  2881    43
  50118 12642   500    35  2055   288     4  2546  4956  1640   288     4
    245 20695  1954   321     4  1898  3714    43 50118   597   134    35
    111   288     4   288 37810    36   288     4  3416  2663  1954   321
      4   306 30042    43 50118 30876   791    35  2055   288     4   288
  35761    36   288     4  2517  4111  1954   321     4  1922  6478    43
      2]]"
a1ac2a152710335519c9a907eec60d9f468b19db,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT.","[[    0 46064    12    90 30954    16  4873    30 26739 13931  6694  2403
    716    15   226  4014   448    12  9822   597     8 39608  1575     6
    150  3228    12 36799 42664    16  4873    30 12547     9   226  4014
    448    12  9822   597     8   163 18854     4     2]]"
1c997c268c68149ae6fb43d83ffcd53f0e7fe57e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],ELMo embeddings are then used with a residual LSTM to learn informative morphological representations from the character sequence of each token,"[[    0  3721 17357 33183   417  1033    32   172   341    19    10 30848
    226  4014   448     7  1532 29749 38675  9779 30464    31     5  2048
  13931     9   349 19233     2]]"
1e185a3b8cac1da939427b55bf1ba7e768c5dae4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],VAE,[[   0 9788  717    2]]
eda4869c67fe8bbf83db632275f053e7e0241e8c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
eda4869c67fe8bbf83db632275f053e7e0241e8c,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," Paraphrase Database (PPDB) ,  book corpus","[[    0  2884  8258 34338 37875    36   510  6153   387    43  2156  1437
   1040 42168     2]]"
0ee73909ac638903da4a0e5565c8571fc794ab96,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"adequacy, precision and ranking values",[[    0 37462  5073     6 15339     8  7141  3266     2]]
509af1f11bd6f3db59284258e18fdfebe86cae47,0.0,Entailment,[[    2     0 30495  3760  1757     2]], we look at language constructions used and compute the corresponding diversity score as the ratio of observed number versus optimal number,"[[    0    52   356    23  2777 12558  2485   341     8 37357     5 12337
   5845  1471    25     5  1750     9  6373   346  4411 19329   346     2]]"
3d7d865e905295d11f1e85af5fa89b210e3e9fdf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],100 crowdworkers ,[[    0  1866  2180 16941  1437     2]]
21548433abd21346659505296fb0576e78287a74,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The dataset from the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference.,"[[    0   133 41616    31     5 14585   493   574  2454   178  2482 34145
  12791     9     5 11270 10537 25011 27140   954  2815     4     2]]"
7e4ef0a4debc048b244b61b4f7dc2518b5b466c0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Four discourse phenomena - deixis, lexical cohesion, VP ellipsis, and ellipsis which affects NP inflection.","[[    0 22113 19771 32242   111   263  3181   354     6 36912  3569 31657
      6 13913 28041  7418   354     6     8 28041  7418   354    61  8561
  26266  4047 20576     4     2]]"
1591068b747c94f45b948e12edafe74b5e721047,0.0,Entailment,[[    2     0 30495  3760  1757     2]],10000,[[    0 47303     2]]
9ca447c8959a693a3f7bdd0a2c516f4b86f95718,0.0,Entailment,[[    2     0 30495  3760  1757     2]],tweets are annotated with only Favor or Against for two targets - Galatasaray and Fenerbahçe,"[[    0    90  1694  2580    32 45068  1070    19   129 41620    50  9174
     13    80  3247   111  4537   415 31126   857     8   274  5777 28185
   3381   242     2]]"
664b3eadc12c8dde309e8bbd59e9af961a433cde,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
9a7ba5ed1779c664d2cac92494a43517d3e87c96,0.0,Entailment,[[    2     0 30495  3760  1757     2]],WSC collection,[[   0  771 3632 2783    2]]
70148c8d0f345ea36200d5ba19d021924d98e759,0.0,Entailment,[[    2     0 30495  3760  1757     2]],When the perception of what we hear is influenced by what we see.,"[[    0  1779     5 10518     9    99    52  1798    16 11359    30    99
     52   192     4     2]]"
955de9f7412ba98a0c91998919fa048d339b1d48,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SVM,[[    0   104 20954     2]]
66c96c297c2cffdf5013bab5e95b59101cb38655,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"F1 scores are:
HUBES-PHI: Detection(0.965), Classification relaxed (0.95), Classification strict (0.937)
Medoccan: Detection(0.972), Classification (0.967)","[[    0   597   134  4391    32    35 50118   725 12027  1723    12  7561
    100    35 38091  1640   288     4   466  3506   238 40509 11956    36
    288     4  4015   238 40509  8414    36   288     4   466  3272    43
  50118 21243  1975  7424    35 38091  1640   288     4   466  4956   238
  40509    36   288     4   466  4111    43     2]]"
7b4992e2d26577246a16ac0d1efc995ab4695d24,0.0,Entailment,[[    2     0 30495  3760  1757     2]],error detection system by Rei2016,[[    0 44223 12673   467    30 43572  9029     2]]
beac555c4aea76c88f19db7cc901fa638765c250,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Alignment points of the POS tags.,[[    0  7083 15645   332     9     5 29206 19445     4     2]]
f3c204723da53c7c8ef4dc1018ffbee545e81056,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
fd8e23947095fe2230ffe1a478945829b09c8c95,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
79a44a68bb57b375d8a57a0a7f522d33476d9f33,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," Relation Generation (RG) , Content Selection (CS),  Content Ordering (CO)","[[    0  8136  1258 17362    36 41626    43  2156 12803 30418    36  6842
    238  1437 12803  9729   154    36  6335    43     2]]"
133eb4aa4394758be5f41744c60c99901b2bc01c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
79413ff5d98957c31866f22179283902650b5bb6,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," E-book annotation data: editor tags, Amazon search terms, and  Amazon review keywords.","[[    0   381    12  6298 47760   414    35  4474 19445     6  1645  1707
   1110     6     8  1437  1645  1551 32712     4     2]]"
1f07e837574519f2b696f3d6fa3230af0b931e5d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
52f9cd05d8312ae3c7a43689804bac63f7cac34b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
df95b3cb6aa0187655fd4856ae2b1f503d533583,0.0,Entailment,[[    2     0 30495  3760  1757     2]],simple n-grams (like fastText) and unsupervised morphemes,"[[    0 41918   295    12 28526    29    36  3341  1769 39645    43     8
    542 16101 25376 45293  9506   293     2]]"
56a8826cbee49560592b2d4b47b18ada236a12b9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],an expert annotator determined if the tweet fell under a specific category,"[[    0   260  3827 45068  2630  3030   114     5  3545  1064   223    10
   2167  4120     2]]"
5aa12b4063d6182a71870c98e4e1815ff3dc8a72,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
f03df5d99b753dc4833ef27b32bb95ba53d790ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Accounts that spread fake news are mostly unverified, recently created and have on average high friends/followers ratio","[[    0 43767    29    14  2504  4486   340    32  2260   542 37451     6
    682  1412     8    33    15   674   239   964    73 28481   268  1750
      2]]"
e8f969ffd637b82d04d3be28c51f0f3ca6b3883e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L","[[    0 21993   248 10207  9162 14823     6 35109     6 29484     8  2808
  28302 38036   288  4391    13   248 10207  9162    12   134     6  1437
   2808 28302 38036   176  4391    13   248 10207  9162    12   176     8
    248 10207  9162    12   574     2]]"
07d15501a599bae7eb4a9ead63e9df3d55b3dc35,0.0,Entailment,[[    2     0 30495  3760  1757     2]],using the Meaning Extraction Method,[[    0 10928     5 35972 19188 22870 16410     2]]
1462eb312944926469e7cee067dfc7f1267a2a8c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],three,[[   0 9983    2]]
29f2954098f055fb19d9502572f085862d75bf61,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)","[[    0   229  3864 18759 16853 19357    36   530 20057   238 34638  5761
     36 30455   238  7737 40419 14969    36   104 20954   238 19268    12
  39165  2595 16771  2839    36 10537   510    43     2]]"
cf171fad0bea5ab985c53d11e48e7883c23cdc44,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Those tweets are mostly much noisier and shorter compared to the reviews in the movie corpus. In total, there are 1,716 tweets. 973 of them are negative and 743 of them are positive.","[[    0 11195  6245    32  2260   203   117   354   906     8 10941  1118
      7     5  6173    11     5  1569 42168     4    96   746     6    89
     32   112     6   406  1549  6245     4   361  5352     9   106    32
   2430     8   262  3897     9   106    32  1313     4     2]]"
67b66fe67a3cb2ce043070513664203e564bdcbd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF.","[[    0  1213  8933    19     5   511  3092    35    30 10829 26655    36
   3789   238    30   221  4040   260  1758     8  9938    36  3789   238
     30   256 18439  9707  3900     8   229  1766 22576    36  3789   238
   1437    30   468  1829 20101    36  3789   238  4619   710   212   118
      8   384  6804    36  3789   238    30   468  7529 33063  3900    36
   3789   238    30    36   347  1439  4400  1076   482   199   238     8
   4307   597     4     2]]"
7793805982354947ea9fc742411bec314a6998f6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],We performed the annotation with freely available tools for the Portuguese language.,"[[    0   170  3744     5 47760    19 13215   577  3270    13     5 13053
   2777     4     2]]"
5e324846a99a5573cd2e843d1657e87f4eb22fa6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The Näive-Bayes classifier is corrected so it is not biased to most frequent classes,"[[    0   133   234  1561  2088    12 20861   293  1380 24072    16 17261
     98    24    16    45 21099     7   144  7690  4050     2]]"
06b5272774ec43ee5facfa7111033386f06cf448,0.0,Entailment,[[    2     0 30495  3760  1757     2]],sentence,[[    0 19530  4086     2]]
c70bafc35e27be9d1efae60596bc0dd390c124c0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
0ee73909ac638903da4a0e5565c8571fc794ab96,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.","[[    0  1096  1050 45068  3629  4173    10  9624  7728     9   727 41762
     30  1614  8198  5073     6 25404  6761     8  1374  7141    15    10
    195    12  2300  3189     4     2]]"
03ebb29c08375afc42a957c7b2dc1a42bed7b713,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"proposed ontology, which, in our evaluation procedure, was populated with 3121 events entries from 51 documents.","[[    0 27128  7878 25099  4383     6    61     6    11    84 10437  7089
      6    21 18269    19   155 22388  1061 11410    31  4074  2339     4
      2]]"
7ee5c45b127fb284a4a9e72bb9b980a602f7445a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
dca86fbe1d57b44986055b282a03c15ef7882e51,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Ground truth is not established in the paper,[[    0 43416  3157    16    45  2885    11     5  2225     2]]
d67c01d9b689c052045f3de1b0918bab18c3f174,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Answer with content missing: (Table 2) Accuracy of best AS reader results including ensembles are 78.4 and 83.7 when trained on BookTest compared to 71.0 and 68.9 when trained on CBT for Named endity and Common noun respectively.,"[[    0 33683    19  1383  1716    35    36 41836   132    43 42688     9
    275  6015 10746   775   217  1177 26660  1634    32  7004     4   306
      8  8101     4   406    77  5389    15  5972 34603  1118     7  6121
      4   288     8  5595     4   466    77  5389    15  6933   565    13
  30436   253  1571     8  9732 44875  4067     4     2]]"
7561a968470a8936d10e1ba722d2f38b5a9a4d38,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"collection of over 30,000 images with 5 crowdsourced descriptions each","[[    0 44443     9    81   389     6   151  3156    19   195  8817 22241
  24173   349     2]]"
22ccee453e37536ddb0c1c1d17b0dbac04c6c607,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English,[[    0 35007     2]]
9cf070d6671ee4a6353f79a165aa648309e01295,0.0,Entailment,[[    2     0 30495  3760  1757     2]],1500 sentences,[[    0 36559 11305     2]]
45f7c03a686b68179cadb1413c5f3c1d373328bd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses","[[    0 10800  5069    81  2248     6   151 40043  7201     6   217    81
   2357     6   151    19   455  2788     6    59  6247 43814    12  1646
      6   208 22210    12  8739   846    12   176     6     8  1330 34377
   1469   853  9764     2]]"
df5a4505edccc0ee11349ed6e7958cf6b84c9ed4,0.0,Entailment,[[    2     0 30495  3760  1757     2]], news articles in free-text format,[[    0   340  7201    11   481    12 29015  7390     2]]
40c0f97c3547232d6aa039fcb330f142668dea4b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
dcea88698949da4a1bd00277c06df06c33f6a5ff,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The task is set up to mimic (albeit, in an oversimplified manner) the input-output symbol alignments and local syntactic properties that models must learn in many natural language tasks, such as translation, tagging and summarization.","[[    0   133  3685    16   278    62     7 26361    36 41324     6    11
     41 10981 38731  3786  4737    43     5  8135    12 46234  7648 12432
   2963     8   400 45774 28201  3611    14  3092   531  1532    11   171
   1632  2777  8558     6   215    25 19850     6 34694     8 39186  1938
      4     2]]"
fb1227b3681c69f60eb0539e16c5a8cd784177a7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Salience features positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in.
The relative authority of entity features:   comparative relevance of the news article to the different entities occurring in it.","[[    0 18111 11465  1575 42611  1575     6 21263 13135     8     5  3425
  29206  3184     9     5 10014     8     5  3645    24 11493    11     4
  50118   133  5407  3446     9 10014  1575    35  1437  1437 30590 21623
      9     5   340  1566     7     5   430  8866 14196    11    24     4
      2]]"
74fb77a624ea9f1821f58935a52cca3086bb0981,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Dataset contains total of 14100 annotations.,[[    0 43703   281   594  6308   746     9   501  1866 47234     4     2]]
6bfba3ddca5101ed15256fca75fcdc95a53cece7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1. Loaded language, 2. Name calling or labeling, 3. Repetition, 4. Exaggeration or minimization, 5. Doubt, 6. Appeal to fear/prejudice, 7. Flag-waving, 8. Causal oversimplification, 9. Slogans, 10. Appeal to authority, 11. Black-and-white fallacy, dictatorship, 12. Thought-terminating cliché, 13. Whataboutism, 14. Reductio ad Hitlerum, 15. Red herring, 16. Bandwagon, 17. Obfuscation, intentional vagueness, confusion, 18. Straw man","[[    0   134     4 45759  2777     6   132     4 10704  1765    50 27963
      6   155     4  2825 37258     6   204     4  3015 17864  1258    50
  15970  1938     6   195     4 11260 25933     6   231     4 16049     7
   2490    73  5234 12864  2463     6   262     4 18238    12   605 13286
      6   290     4  8316 25016 10981 38731  5000     6   361     4  4424
   2154  1253     6   158     4 16049     7  3446     6   365     4  1378
     12   463    12  9830 44469     6 23422     6   316     4 30631    12
  42985  1295 35341     6   508     4   653  9006  1809     6   501     4
   1211 21491  1020  2329 16423   783     6   379     4  1211    69  4506
      6   545     4  6191 45199     6   601     4  5816 48996  1258     6
  18797   748 11993 14186     6  9655     6   504     4 37781   313     2]]"
d509081673f5667060400eb325a8050fa5db7cc8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"2174000000, 989000000",[[    0   176 29221 33413     6   361  5046 33413     2]]
e8a32460fba149003566969f92ab5dd94a8754a4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Concept Raw Context model, Concept-Concept Context model","[[    0  9157 16771  8214 43885  1421     6 32253    12  9157 16771 43885
   1421     2]]"
b2ecfd5480a2a4be98730e2d646dfb84daedab17,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
6cd8bad8a031ce6d802ded90f9754088e0c8d653,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Best proposed model achieves F1 score of 84.9 compared to best previous result of 84.1.,"[[    0 19183  1850  1421 35499   274   134  1471     9  7994     4   466
   1118     7   275   986   898     9  7994     4   134     4     2]]"
506d21501d54a12d0c9fd3dbbf19067802439a04,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Words that a user wants them to appear in the generated output.,"[[    0 38257    14    10  3018  1072   106     7  2082    11     5  5129
   4195     4     2]]"
22b740cc3c8598247ee102279f96575bdb10d53f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
662870a90890c620a964720b2ca122a1139410ea,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"French, English, Spanish, Italian, Portuguese, Hebrew, Arabic","[[    0 28586     6  2370     6  3453     6  3108     6 13053     6 27428
      6 19645     2]]"
111afb77cfbf4c98e0458606378fa63a0e965e36,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
5fa36dc8f7c4e65acb962fc484989d20b8fdaeec,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
46227b4265f1d300a5ed71bf40822829de662bc2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"AMR Bank, CNN-Dailymail",[[    0  2620   500   788     6  3480    12 26339  6380     2]]
01a41c0a4a7365cd37d28690735114f2ff5229f2,0.0,Entailment,[[    2     0 30495  3760  1757     2]], http://www.blogger.com,[[    0  2054   640  1401     4 24635  2403     4   175     2]]
567dc9bad8428ea9a2658c88203a0ed0f8da0dc3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS ,"[[    0 37426   574  4014   448  2744 16256  1640   571  8645   700  1794
     12  4483    43     8  6479   574  4014   448  2744 16256  1640   534
  49197 42740  1437     2]]"
603fee7314fa65261812157ddfc2c544277fcf90,0.0,Entailment,[[    2     0 30495  3760  1757     2]],up to 1.95 times larger,[[   0  658    7  112    4 4015  498 2514    2]]
792f6d76d2befba2af07198584aac1b189583ae4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Hashtag prediction for social media has been addressed earlier,[[    0 45620 10058 16782    13   592   433    34    57  4873   656     2]]
c571deefe93f0a41b60f9886db119947648e967c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],MIMIC-III,[[    0   448  3755  2371    12 24457     2]]
8060a773f6a136944f7b59758d08cc6f2a59693b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],1000 hours data,[[    0 20078   722   414     2]]
5a8cc8f80509ea77d8213ed28c5ead501c68c725,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," Most prior work focuses on a different aspect of offensive language such as abusive language BIBREF0 , BIBREF1 , (cyber-)aggression BIBREF2 , (cyber-)bullying BIBREF3 , BIBREF4 , toxic comments INLINEFORM0 , hate speech BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , and offensive language BIBREF11 . Prior work has focused on these aspects of offensive language in Twitter BIBREF3 , BIBREF7 , BIBREF8 , BIBREF11 , Wikipedia comments, and Facebook posts BIBREF2 .","[[    0  1993  2052   173  7235    15    10   430  6659     9  2555  2777
    215    25 14202  2777   163  8863 45935   288  2156   163  8863 45935
    134  2156    36  4469  1943 45406  7165 21791   163  8863 45935   176
   2156    36  4469  1943 45406 17559  4048   163  8863 45935   246  2156
    163  8863 45935   306  2156  8422  1450  2808 28302 38036   288  2156
   4157  1901   163  8863 45935   245  2156   163  8863 45935   401  2156
    163  8863 45935   406  2156   163  8863 45935   398  2156   163  8863
  45935   466  2156   163  8863 45935   698  2156     8  2555  2777   163
   8863 45935  1225   479  7987   173    34  2061    15   209  5894     9
   2555  2777    11   599   163  8863 45935   246  2156   163  8863 45935
    406  2156   163  8863 45935   398  2156   163  8863 45935  1225  2156
  28274  1450     6     8   622  4570   163  8863 45935   176   479     2]]"
5529f26f72ce47440c2a64248063a6d5892b9fde,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (Evaluation section) Given that in CLIR the primary goal is to get a better ranked list of documents against a translated query, we only report Mean Average Precision (MAP).","[[    0 33683    19  1383  1716    35    36   717  6486  9762  2810    43
   6211    14    11  5289  5216     5  2270   724    16     7   120    10
    357  4173   889     9  2339   136    10 16877 25860     6    52   129
    266 30750  8317 29484    36 43861   322     2]]"
fb2b536dc8e442dffab408db992b971e86548158,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
b08f88d1facefceb87e134ba2c1fa90035018e83,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
1bdc990c7e948724ab04e70867675a334fdd3051,0.0,Entailment,[[    2     0 30495  3760  1757     2]],from Food.com,[[   0 7761 3652    4  175    2]]
57f23dfc264feb62f45d9a9e24c60bd73d7fe563,0.0,Entailment,[[    2     0 30495  3760  1757     2]],609,[[    0 37295     2]]
5b551ba47d582f2e6467b1b91a8d4d6a30c343ec,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Byte-Pair Encoding perplexity  (BPE PPL),
BLEU-1,
BLEU-4,
ROUGE-L,
percentage of distinct unigram (D-1),
percentage of distinct bigrams(D-2),
user matching accuracy(UMA),
Mean Reciprocal Rank(MRR)
Pairwise preference over baseline(PP)","[[    0 47447    12   510  2456 14813 19519 33708  1571  1437    36   387
  16035   221  7205   238 50118 30876   791    12   134     6 50118 30876
    791    12   306     6 50118   500  5061  8800    12   574     6 50118
  13566  1580     9 11693   542  1023  4040    36   495    12   134   238
  50118 13566  1580     9 11693   380 27809  1640   495    12   176   238
  50118 12105  8150  8611  1640   791  5273   238 50118  5096   260  7382
   1588 46966 17816  1640 12642   500    43 50118   510  2456 10715 12832
     81 18043  1640  5756    43     2]]"
a5e5cda1f6195ab1336855f1e39a609d61326d62,0.0,Entailment,[[    2     0 30495  3760  1757     2]],dimension corresponding to the concept that the particular word belongs to,"[[    0 47987 12337     7     5  4286    14     5  1989  2136 12918     7
      2]]"
218bc82796eb8d91611996979a4a42500131a936,0.0,Entailment,[[    2     0 30495  3760  1757     2]],MLP,[[    0 10537   510     2]]
fde700d5134a9ae8f7579bea1f1b75f34d7c1c4c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The speech was collected from respondents using an android application.,"[[    0   133  1901    21  4786    31 10011   634    41 42492  2502     4
      2]]"
0fc2b5bc2ead08a6fe0280fb3a47477c6df1587c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
2858620e0498db2f2224bfbed5263432f0570832,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets.,"[[    0 20930    15  2103   775  1286  2992  3660     7  2432 38806   196
  15716    56   513   913   111 19220 15428  2249     9   321     4  3103
    332    15    70   130 42532     4     2]]"
bc16ce6e9c61ae13d46970ebe6c4728a47f8f425,0.0,Entailment,[[    2     0 30495  3760  1757     2]],4.49 turns,[[   0  306    4 3414 4072    2]]
62f27fe08ddb67f16857fab2a8a721926ecbb6fb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Irony accuracy is judged only by human ; senriment preservation and content preservation are judged  both by human and using automatic metrics (ACC and BLEU).,"[[    0 38398   219  8611    16 16247   129    30  1050 25606 22583  6103
   1342 18498     8  1383 18498    32 16247  1437   258    30  1050     8
    634  8408 12758    36 21678     8   163  3850   791   322     2]]"
23b2901264bda91045258b5d4120879ae292e950,0.0,Entailment,[[    2     0 30495  3760  1757     2]],+0.58,[[   0 2744  288    4 4432    2]]
dac087e1328e65ca08f66d8b5307d6624bf3943f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
a130306c6662ff489df13fb3f8faa7cba8c52a21,0.0,Entailment,[[    2     0 30495  3760  1757     2]],dynamic average pooling,[[    0   417 44080   674  3716   154     2]]
43f074bacabd0a355b4e0f91a1afd538c0a6244f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"workers are first asked to find those microposts where the model predictions are deemed correct,  asked to find the keyword that best indicates the class of the microposts","[[    0 16941    32    78   553     7   465   167 14926  6884  2603    29
    147     5  1421 12535    32  7661  4577     6  1437   553     7   465
      5 36585    14   275  8711     5  1380     9     5 14926  6884  2603
     29     2]]"
8d4ac4afbf5b14f412171729ceb5e822afcfa3f4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
6844683935d0d8f588fa06530f5068bf3e1ed0c0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],A finite corpora may entirely omit rare word combinations,[[    0   250 40001 22997   102   189  4378 43388  3159  2136 21092     2]]
1eef2d2c296fdd10b08bf7b4ff7792cccf177d3b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
286078813136943dfafb5155ee15d2429e7601d9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
fbaf060004f196a286fef67593d2d76826f0304e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Amazon reviews BIBREF23 , BIBREF24, Yelp restaurant reviews dataset,  restaurant reviews dataset as part of a Kaggle competition BIBREF26","[[    0 25146  6173   163  8863 45935  1922  2156   163  8863 45935  1978
      6 29730  2391  6173 41616     6  1437  2391  6173 41616    25   233
      9    10   229  7165   459  1465   163  8863 45935  2481     2]]"
7e62a53823aba08bc26b2812db016f5ce6159565,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus","[[    0   100  2068   387  2370    12   725  2028   118 12980 42168     6
     38  6447   100  2370    12   725  2028   118 12980 42168     2]]"
bfc2dc913e7b78f3bd45e5449d71383d0aa4a890,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (list)
LiLi should have the following capabilities:
1. to formulate an inference strategy for a given query that embeds processing and interactive actions.
2. to learn interaction behaviors (deciding what to ask and when to ask the user).
3. to leverage the acquired knowledge in the current and future inference process.
4. to perform 1, 2 and 3 in a lifelong manner for continuous knowledge learning.","[[    0 33683    19  1383  1716    35    36  8458    43 50118 41198 41198
    197    33     5   511  5587    35 50118   134     4     7 33461    41
  42752  1860    13    10   576 25860    14 33183    29  5774     8 10813
   2163     4 50118   176     4     7  1532 10405 17156    36 11127  8231
     99     7  1394     8    77     7  1394     5  3018   322 50118   246
      4     7  6270     5  3566  2655    11     5   595     8   499 42752
    609     4 50118   306     4     7  3008   112     6   132     8   155
     11    10 15353  4737    13 11152  2655  2239     4     2]]"
8f16dc7d7be0d284069841e456ebb2c69575b32b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"various versions of LiLi as baselines, Single, Sep, F-th, BG, w/o PTS","[[    0 10806  6514  7952     9  5991 41198    25 11909 38630     6 15773
      6  6974     6   274    12   212     6 29594     6   885    73   139
  42601     2]]"
d41e20ec716b5904a272938e5a8f5f3f15a7779e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],act paragraphs containing any word from a predetermined list of LGTBQ terms ,"[[    0  7257 36153  8200   143  2136    31    10 41679   889     9  9528
  16566  1864  1110  1437     2]]"
1d9aeeaa6efa1367c22be0718f5a5635a73844bd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],information about the context and sequential nature of the text,[[    0 31480    59     5  5377     8 29698  2574     9     5  2788     2]]
b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54,0.0,Entailment,[[    2     0 30495  3760  1757     2]],multilingual NMT (MNMT) BIBREF19,"[[    0 42527 41586   234 11674    36 35306 11674    43   163  8863 45935
   1646     2]]"
ad08b215dca538930ef1f50b4e49cd25527028ad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
f161e6d5aecf8fae3a26374dcb3e4e1b40530c95,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ELMO BIBREF11, BERT BIBREF12 and ClinicalBERT BIBREF13","[[    0  3721  8756   163  8863 45935  1225     6   163 18854   163  8863
  45935  1092     8 20868 11126   565   163  8863 45935  1558     2]]"
27cf16bc9ef71761b9df6217f00f39f21130ce15,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
81dbe9a9ddaa5d02b02e01a306d898015a56ffb6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the series of posts that trigger an intervention,[[   0  627  651    9 4570   14 7691   41 6530    2]]
a6d37b5975050da0b1959232ae756fc09e5f87e8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The encoder is essentially the same as tweet2vec, with the input as words instead of characters","[[    0   133  9689 15362    16  5700     5   276    25  3545   176 25369
      6    19     5  8135    25  1617  1386     9  3768     2]]"
79f9468e011670993fd162543d1a4b3dd811ac5d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.","[[    0  2747  2620    34  4824  3855    81    70 18043  6448   634 26911
    241  1090 33708  1571     8  1437 18388   506    12 30876   791 14823
      4  1437    20  4532  7213 33708  1571  3855   361  3367     6  1549
     16  3491    13 14850   487 21992  3789  1437   305 11674  1437 41616
      8  1437  2929     6  3305    13   230  4571   673 41616     4     2]]"
747b847d687f703cc20a87877c5b138f26ff137d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13 , and EN-DE section of the Europarl corpus BIBREF14 ","[[    0   627  2370    36  2796    43     8  1859    36 10089    43  9042
      9     5   944   487  6006  2338 42168   163  8863 45935  1558  2156
      8 13245    12 10089  2810     9     5  5122  5489   462 42168   163
   8863 45935  1570  1437     2]]"
a15bc19674d48cd9919ad1cf152bf49c88f4417d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],DSTC2,[[   0  495 4014  347  176    2]]
ffeb67a61ecd09542b1c53c3e4c3abd4da0496a8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"concept map BIBREF5 , a labeled graph showing concepts as nodes and relationships between them as edges","[[    0 38498  5456   163  8863 45935   245  2156    10 16274 20992  2018
  14198    25 32833     8  4158   227   106    25 15716     2]]"
0ab3df10f0b7203e859e9b62ffa7d6d79ffbbe50,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"average classification accuracy, execution performance",[[    0 20365 20257  8611     6  7356   819     2]]
29923a824c98b3ba85ced964a0e6a2af35758abe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets","[[    0 20320 31965 20382    58  1051     7  2038 37297  3629   482  1337
  34661  6240     7     5 41762     6  1437 34661  1649     5 31669  7527
      9  2341     6   709     8  1296  3880     2]]"
009ce6f2bea67e7df911b3f93443b23467c9f4a1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"QANet , BIBREF14,  fine-tuned a BERT model","[[    0  1864  1889   594  2156   163  8863 45935  1570     6  1437  2051
     12 24641   196    10   163 18854  1421     2]]"
92d1a6df3041667dc662376938bc65527a5a1c3c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
3116453e35352a3a90ee5b12246dc7f2e60cfc59,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self","[[    0 22930 37681  3563  1380 24072    36   104 20954   238  7425  5580
  39974  1380 24072    36 33919   238  7300  2088  1501   293  1380 24072
     36 20485   238  9624  6693    36 30455   238  3480     6   226  4014
    448  2156   226  4014   448    12 24810     6   226  4014   448    12
  13367     2]]"
73a7acf33b26f5e9475ee975ba00d14fd06f170f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer","[[    0  1640   134    43     5    86     5  3186    34    57  7242     5
  28667     6    36   176    43  1713    14  7691     5 28667    36   560
   5948    50 25698   238    36   246    43     5  5239     9 24146     6
     36   306    43     5 13135 21263     9     5 28667     6     8    36
    245    43     5  2259     9 28667     6   440 31652     2]]"
f1f1dcc67b3e4d554bfeb508226cdadb3c32d2e9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28","[[    0 37504   717  6486    12  9029 10045 12927   195   163  8863 45935
   2518  2156   163  8863 45935  2517     2]]"
005cca3c8ab6c3a166e315547a2259020f318ffb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The resulting taxonomy of the framework is shown in Figure FIGREF10,"[[    0   133  5203   629 38217     9     5  7208    16  2343    11 17965
  37365 45935   698     2]]"
ba539cab80d25c3e20f39644415ed48b9e4e4185,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.","[[    0  1121  1323    12 11672     6     5 15603  6315  3974    15     5
   3544  2649 30514  2136     6   124  1529     7  7974  2136  7314   160
      7    10  2136    19  1122  3854   420  4050     8   124  1529     7
   3618  1421  7314   160     7    10    55 14569  2136  4972  1421  5389
     19  2514     8   540 14120 42168     4     2]]"
6415f38a06c2f99e8627e8ba6251aa4b364ade2d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e2b0cd30cf56a4b13f96426489367024310c3a05,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Spearman's rank-order correlation,[[    0 29235   271   397    18  7938    12 10337 22792     2]]
a69a59b6c0ab27bcee1a780d6867df21e30aec08,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
57f23dfc264feb62f45d9a9e24c60bd73d7fe563,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
c2b8ee872b99f698b3d2082d57f9408a91e1b4c1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Babelfy, DBpedia Spotlight, Entityclassifier.eu, FOX, LingPipe MUC-7, NERD-ML, Stanford NER, TagMe 2","[[    0   387   873 24973   219     6 18496 47745 35730     6 46718  4684
  24072     4 25171     6  7481     6 17900   510 11387   256 12945    12
    406     6   234  2076   495    12 10537     6  8607   234  2076     6
  12650  5096   132     2]]"
c9bc6f53b941863e801280343afa14248521ce43,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English,[[    0 35007     2]]
cd32a38e0f33b137ab590e1677e8fb073724df7f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English ,[[    0 35007  1437     2]]
bf52c01bf82612d0c7bbf2e6a5bb2570c322936f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],we observe that many variants of Rouge scores do not have high correlations with human pyramid scores,"[[    0  1694 14095    14   171 21740     9 14941  4391   109    45    33
    239 43879    19  1050 33344  4391     2]]"
bc31a3d2f7c608df8c019a64d64cb0ccc5669210,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BERTbase,[[    0 11126   565 11070     2]]
a6a48de63c1928238b37c2a01c924b852fe752f8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Lead-3, Lead-1-AMR",[[    0 32258    12   246     6 14243    12   134    12  2620   500     2]]
fd2c6c26fd0ab3c10aae4f2550c5391576a77491,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
54e945ea4b014e11ed4e1e61abc2aa9e68fea310,0.0,Entailment,[[    2     0 30495  3760  1757     2]],average target BLEU score of 29.65,[[    0 20365  1002   163  3850   791  1471     9  1132     4  3506     2]]
90d946ccc3abf494890e147dd85bd489b8f3f0e8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"gender bias, normalized version of INLINEFORM0, ratio of occurrence of male and female words in the model generated text, Causal occupation bias conditioned on occupation, causal occupation bias conditioned on gender, INLINEFORM1","[[    0 29967  9415     6 35247  1732     9  2808 28302 38036   288     6
   1750     9 21263     9  2943     8  2182  1617    11     5  1421  5129
   2788     6  8316 25016 14349  9415 36550    15 14349     6 41214 14349
   9415 36550    15  3959     6  2808 28302 38036   134     2]]"
42084c41343e5a6ae58a22e5bfc5ce987b5173de,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d915b401bb96c9f104a0353bef9254672e6f5a47,0.0,Entailment,[[    2     0 30495  3760  1757     2]],to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions,"[[    0   560   617 10759  9946     5  1421    15     5   414  3184    11
    645     7  2097 20403     9   190 31515 24173     2]]"
f59f1f5b528a2eec5cfb1e49c87699e0c536cc45,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Dataset contains 3606 total sentences and 79087 total entities.,"[[    0 43703   281   594  6308 10253   401   746 11305     8  7589  3669
    406   746  8866     4     2]]"
149da739b1c19a157880d9d4827f0b692006aa2c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SVM, MLP, FastText, CNN, BERT, Google's DialogFlow, Rasa NLU","[[    0   104 20954     6 10725   510     6  9612 39645     6  3480     6
    163 18854     6  1204    18 28985  2154 41779     6   248  8810 12817
    791     2]]"
d7b60abb0091246e29d1a9c28467de598e090c20,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
968b7c3553a668ba88da105eff067d57f393c63f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],those that contain a high number of retweets,[[    0 27768    14  5585    10   239   346     9  5494  1694  2580     2]]
384bf1f55c34b36cb03f916f50bbefade6c86a75,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
9c33b340aefbc1f15b6eb6fb3e23ee615ce5b570,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we use a CNN BIBREF19 , in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers.","[[    0  1694   304    10  3480   163  8863 45935  1646  2156    11  1989
     10   237    12  8433  3215   132   495  3480 39650    80 15380 23794
    337     8    80  1950  3665  7397 13171     4     2]]"
ed2eb4e54b641b7670ab5a7060c7b16c628699ab,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SR,[[    0 17973     2]]
86bf75245358f17e35fc133e46a92439ac86d472,0.0,Entailment,[[    2     0 30495  3760  1757     2]], the performance differences across all tasks are small enough ,[[   0    5  819 5550  420   70 8558   32  650  615 1437    2]]
cfcdd73e712caf552ba44d0aa264d8dace65a589,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For ins scope data collection:crowd workers which provide questions and commands related to topic domains and additional data the rephrase and scenario crowdsourcing tasks proposed by BIBREF2 is used. 
For out of scope data collection:  from workers mistakes-queries written for one of the 150 intents that did not actually match any of the intents and using scoping and scenario tasks with prompts based on topic areas found on Quora, Wikipedia, and elsewhere.","[[    0  2709  7540  7401   414  2783    35   438 34298  1138    61   694
   1142     8 16388  1330     7  5674 30700     8   943   414     5   769
  40726     8  5665  8817 27824  8558  1850    30   163  8863 45935   176
     16   341     4  1437 50118  2709    66     9  7401   414  2783    35
   1437    31  1138  6160    12  2253  9709  1982    13    65     9     5
   3982  6979  4189    14   222    45   888   914   143     9     5  6979
   4189     8   634  2850 12232     8  5665  8558    19 33187   716    15
   5674   911   303    15  3232  4330     6 28274     6     8  5140     4
      2]]"
0f567251a6566f65170a1329eeeb5105932036b2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"current state-of-the-art approach BIBREF14 , BIBREF15","[[    0 28311   194    12  1116    12   627    12  2013  1548   163  8863
  45935  1570  2156   163  8863 45935   996     2]]"
e438445cf823893c841b2bc26cdce32ccc3f5cbe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
c4628d965983934d7a2a9797a2de6a411629d5bc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],It learns a representation of medical records. The learned representation (embeddings) can be used for other predictive tasks involving information from electronic health records.,"[[    0   243 25269    10  8985     9  1131  2189     4    20  2435  8985
     36 35804   417  1033    43    64    28   341    13    97 27930  8558
   3329   335    31  5175   474  2189     4     2]]"
bc84c5a58c57038910f7720d7a784560054d3e1a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese","[[    0 28586     6  1859     6  5979     6  1083     6  3453     6  3108
      6  4423     6 27775     6  9004     6 39982   811     8  1111     2]]"
d00bbeda2a45495e6261548710afa6b21ea32870,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"first a model is trained on the training set and then this model is used to predict the labels of the silver data, This silver data is then simply added to our training set, after which the model is retrained","[[    0  9502    10  1421    16  5389    15     5  1058   278     8   172
     42  1421    16   341     7  7006     5 14105     9     5  4334   414
      6   152  4334   414    16   172  1622   355     7    84  1058   278
      6    71    61     5  1421    16  5494 26492     2]]"
cd06d775f491b4a17c9d616a8729fd45aa2e79bf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],neutral sentiment,[[    0 12516  5702     2]]
88e62ea7a4d1d2921624b8480b5c6b50cfa5ad42,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"frequencies of the other words which occur with both of them (i.e., second order co–occurrences)","[[    0   506 42172 14768     9     5    97  1617    61  5948    19   258
      9   106    36   118     4   242   482   200   645  1029  2383 23462
    710 46957    43     2]]"
c32adef59efcb9d1a5b10e1d7c999a825c9e6d9a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"German, English, Spanish, Finnish, French, Russian,  Swedish.","[[    0 27709     6  2370     6  3453     6 21533     6  1515     6  1083
      6  1437  9004     4     2]]"
22744c3bc68f120669fc69490f8e539b09e34b94,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
fb3d30d59ed49e87f63d3735b876d45c4c6b8939,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Precision, Recall and F-measure",[[    0 22763 37938     6 35109     8   274    12  1794 24669     2]]
bf3b27a4f4be1f9ae31319877fd0c75c03126fd5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],about 500,[[   0 9006 1764    2]]
c00ce1e3be14610fb4e1f0614005911bb5ff0302,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"relu, selu, tanh",[[    0   241  6487     6   842  6487     6 15149   298     2]]
01e2d10178347d177519f792f86f25575106ddc7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Switchboard Telephone Speech Corpus BIBREF21, LORELEI (Low Resource Languages for Emergent Incidents) Program","[[    0 45112  4929 32181 27242 28556   163  8863 45935  2146     6   226
   7563  3850   100    36 32758 13877 44166    13 11746 13907   603 22612
     43  4928     2]]"
3e88fb3d28593309a307eb97e875575644a01463,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets","[[    0 33919  2055 13379    12  1116    12 30938     6 12244   176 25369
      6 40815  2055   404 21309    36    90 21210    12  4483   238 40815
   2055   404 21309    36   611  6435    12  4483   238 19127 44445    36
     90 21210    12  4483   238  3107 10068   330  1629 21390     6  3829
      6    50   769    12    90  1694  2580     2]]"
81a35b9572c9d574a30cc2164f47750716157fc8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10","[[    0   771  3175   991     8   289 35664   163  8863 45935   245     6
  10553  4400  1076     4   163  8863 45935   466     6     8   305  3175
    991  4400  1076     4   163  8863 45935   698     2]]"
bdf93053b1b9b0a21f77ed370cf4d5a10df70e3e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"accuracy and F1-score of 89.6% and 89.2%, respectively","[[    0  7904 45386     8   274   134    12 31673     9  8572     4   401
    207     8  8572     4   176  4234  4067     2]]"
9bd080bb2a089410fd7ace82e91711136116af6c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BiLSTM+CNN(grapheme-level) which turns out to be performing on par with BiLSTM+CNN(character-level) under the same configuration,"[[    0 37426   574  4014   448  2744 16256  1640   571  8645   700  1794
     12  4483    43    61  4072    66     7    28  4655    15  2242    19
   6479   574  4014   448  2744 16256  1640 23375    12  4483    43   223
      5   276 20393     2]]"
f9edd8f9c13b54d8b1253ed30e7decc1999602da,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set","[[    0  9983 14073 42810    19   430  1530     9  6864    11     5 10437
    278     6   130 14073 42810    19   430   346     9  5385    11     5
  10437   278    32  6533     6  1437    78    65     6 10011    19    23
    513   601  5492  5453    32  1165     7     5 10437   278     6 10011
     19   545  5453     7     5   709     8     5  1079     9 10011     7
      5  3618   278     6   200 11808     6 10011    19    23   513   290
   5453    32  1165     7     5 10437   278     6 10011    19   231    50
    262  5453     7     5   709     8     5  1079     9 10011     7     5
   3618   278     2]]"
05887a8466e0a2f0df4d6a5ffc5815acd7d9066a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Target-1,[[    0 41858    12   134     2]]
28067da818e3f61f8b5152c0d42a531bf0f987d4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
ee2ad0ab64579cffb60853db6a8c0f971d7cf0ff,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
06be47e2f50b902b05ebf1ff1c66051925f5c247,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
b06512c17d99f9339ffdab12cedbc63501ff527e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
2cd37743bcc7ea3bd405ce6d91e79e5339d7642e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
f697d00a82750b14376fe20a5a2b249e98bebe9b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Bi-LSTM-CRF,[[    0 37426    12   574  4014   448    12  9822   597     2]]
6c96e910bd98c9fd58ba2050f99b9c9bac69840a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
022c365a14fdec406c7a945a1a18e7e79df37f08,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage.,"[[    0   627  1421    16  1198    12 23830    15   230  6078    12   805
   6015   500  3685     8 14077  3685    11     5  1198    12 32530  1289
      4     2]]"
455d4ef8611f62b1361be4f6387b222858bb5e56,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CrowdFlower,[[    0   347 34298 16197  8285     2]]
73bddaaf601a4f944a3182ca0f4de85a19cdc1d2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Daily Mail news articles released by BIBREF9 ,"[[    0 26339  6313   340  7201   703    30   163  8863 45935   466  1437
      2]]"
357eb9f0c07fa45e482d998a8268bd737beb827f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the Poly-encoder from BIBREF7 humeau2019real, Feed Yourself BIBREF13 is an open-domain dialogue agent with a self-feeding model, Kvmemnn BIBREF14 is a key-value memory network with a knowledge base that uses a key-value retrieval mechanism to train over multiple domains simultaneously, We compare against four dialogue system baselines: Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline trained on the Persona-Chat dataset using the same training hyperparameters (including learning rate scheduler and length capping settings) described in Section SECREF20., a BERT bi-ranker","[[    0   627 10415    12 14210 15362    31   163  8863 45935   406 10080
    242  1180 10626  8726     6 17703 35420   163  8863 45935  1558    16
     41   490    12 46400  6054  2936    19    10  1403    12 35151  1421
      6   229   705 25683 15688   163  8863 45935  1570    16    10   762
     12 19434  3783  1546    19    10  2655  1542    14  2939    10   762
     12 19434 43372  9562     7  2341    81  1533 30700 11586     6   166
   8933   136   237  6054   467 11909 38630    35   229   705 25683 15688
      6 17703 35420     6 10415    12 14210 15362     6     8    10   163
  18854  4003    12 40081   254 18043  5389    15     5 46107    12 29665
  41616   634     5   276  1058  8944 46669 20413    36  8529  2239   731
  46825 30670     8  5933  6056  5435  9629    43  1602    11  7162  3614
  45935   844   482    10   163 18854  4003    12 40081   254     2]]"
74fb77a624ea9f1821f58935a52cca3086bb0981,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
cfbec1ef032ac968560a7c76dec70faf1269b27c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Knowledge Base Question Answering,[[    0 38912 21945 11056 15680   660  4184  2961     2]]
a3d9b101765048f4b61cbd3eaa2439582ebb5c77,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"En-Fr, En-Zh, En-Jp, En-Kr, Zh-En, Zh-Fr, Zh-Jp, Zh-Kr to English, Chinese or Korean","[[    0 16040    12 29220     6  2271    12  1301   298     6  2271    12
    863   642     6  2271    12   530   338     6 20045    12 16040     6
  20045    12 29220     6 20045    12   863   642     6 20045    12   530
    338     7  2370     6  1111    50  2238     2]]"
9bd080bb2a089410fd7ace82e91711136116af6c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"On OurNepali test dataset Grapheme-level representation model achieves average 0.16% improvement, on ILPRL test dataset it achieves maximum 1.62% improvement","[[    0  4148  1541   487  2462  3644  1296 41616   272  8645   700  1794
     12  4483  8985  1421 35499   674   321     4  1549   207  3855     6
     15 11935  4454   574  1296 41616    24 35499  4532   112     4  5379
    207  3855     2]]"
021bfb7e180d67112b74f05ecb3fa13acc036c86,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Zero Resource Toolkit (ZRTools) BIBREF7,"[[    0 38423 13877 25251 23199    36  1301 13963 22890    43   163  8863
  45935   406     2]]"
5cb610d3d5d7d447b4cd5736d6a7d8262140af58,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Multilingual training is performed by randomly alternating between languages for every new minibatch,"[[    0 45287 41586  1058    16  3744    30 22422 36830   227 11991    13
    358    92  5251  1452 11175     2]]"
ed67359889cf61fa11ee291d6c378cccf83d599d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],GloVe word vectors BIBREF16 pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC),"[[    0   534  4082 30660  2136 44493   163  8863 45935  1549  1198    12
  23830    15    80 42532    35 28274   777    19 14448  1584  3109   245
     36   771  2744   534   245    43     8  9732   230 33889    36  3376
     43     2]]"
6b9b9e5d154cb963f6d921093539490daa5ebbae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"clipped $\mathit {PMI}$, $\mathit {NNEGPMI}$","[[    0  3998  8246 49959 40051   405 25522  5683   100 24303 47110 49959
  40051   405 25522 20057  7170  5683   100 24303  1629     2]]"
af34051bf3e628c1e2a00b110bb84e5f018b419f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.

Pre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.

Multi-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\alpha _{st}=0.75$ while $\alpha _{asr}=0.25$ or $\alpha _{mt}=0.25$. For many-to-many setting, we use $\alpha _{st}=0.6, \alpha _{asr}=0.2$ and $\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.

Many-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. ","[[    0 34230  4699  4062 18043    35    20 21857  4062   163  8863 45935
    466    34   129    10  1901  9689 15362     8    10  5044 15362     4
     85    16  5389    31 12272    15     5  4062    12 43036 42168     4
  50118 50118 22763    12 32530 11909 38630    35   166  2883   130  1198
     12 32530 18043 15491    35   112    43  9689 15362  1198    12 32530
      6    11    61     5  4062  9689 15362    16 49271    31    41  6015
    500  1421   131   132    43  5044 15362  1198    12 32530     6    11
     61     5  4062  5044 15362    16 49271    31    41 14077  1421   131
      8   155    43  9689 15362    12 11127 15362  1198    12 32530     6
    147   258     5  9689 15362     8  5044 15362    32  1198    12 23830
      4    20  6015   500  1421    34     5   276  9437    19 21857  4062
   1421     6  5389    15     5 12652     9  4062    12 43036     8 32690
     12 27049  5725   176 42168     4    20 14077  1421    34    10  2788
   9689 15362     8  5044 15362    19     5   276  9437     9    61    11
  21137  2796     4    85    16    78  5389    15   305 11674   414    36
    995    12  1116    12 46400    43     8   172  2051    12 24641   196
     15    11    12 46400   414     4 50118 50118 46064    12 45025 11909
  38630    35   166    67  2883   130  3228    12 45025 18043 15491   217
     65    12   560    12 19827  2749     6   171    12   560    12  1264
   2749     6     8   171    12   560    12 19827  2749     4    96     5
     78    80  9629     6    52  2341     5  1421    19 49959 29135 18134
  45152   620 24303  5214   288     4  2545  1629   150 49959 29135 18134
  45152   281   338 24303  5214   288     4  1244  1629    50 49959 29135
  18134 45152 16100 24303  5214   288     4  1244 48292   286   171    12
    560    12 19827  2749     6    52   304 49959 29135 18134 45152   620
  24303  5214   288     4   401     6 44128 29135 18134 45152   281   338
  24303  5214   288     4   176  1629     8 49959 29135 18134 45152 16100
  24303  5214   288     4   176  1629  7586   286 14077  3685     6    52
    304   129    11    12 46400   414     4 50118 50118 10787    12   560
     12 19827  2744  5234    12 32530    35   166  2341    10   171    12
    560    12 19827  3228    12 45025  1421   147     5  9689  1630   268
      8  5044  1630   268    32 16934    31  1198    12 23830  6015   500
      8 14077  3092     4  1437     2]]"
b1ce129678e37070e69f01332f1a8587e18e06b0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"2,50,000 tweets, Stock opening and closing prices of Microsoft from August 31st, 2015 to August 25th, 2016","[[   0  176    6 1096    6  151 6245    6 3412 1273    8 3172  850    9
  3709   31  830 1105  620    6  570    7  830  564  212    6  336    2]]"
41b70699514703820435b00efbc3aac4dd67560a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],divorce,[[    0 26037 34260     2]]
b5e4866f0685299f1d7af267bbcc4afe2aab806f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],links between Wikipedia articles to generate sequences of named-entity annotated tokens,"[[    0 40822   227 28274  7201     7  5368 26929     9  1440    12 46317
  45068  1070 22121     2]]"
c2d1387e08cf25cb6b1f482178cca58030e85b70,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
869feb7f47606105005efdb6bea1c549824baea0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"13,757",[[    0  1558     6 38127     2]]
2ddb51b03163d309434ee403fef42d6b9aecc458,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LF-MMI Attention
Seq2Seq 
RNN-T 
Char E2E LF-MMI 
Phone E2E LF-MMI 
CTC + Gram-CTC","[[    0   574   597    12   448  7539 35798 50118 14696  1343   176 14696
   1343  1437 50118   500 20057    12   565  1437 50118 42379   381   176
    717 34463    12   448  7539  1437 50118 40980   381   176   717 34463
     12   448  7539  1437 50118  7164   347  2055 14171    12  7164   347
      2]]"
d201b9992809142fe59ae74508bc576f8ca538ff,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The graph representation appears to be semi-supervised. It is included in the learning pipeline for the medical recommendation, where the attention model is learned. (There is some additional evidence that is unavailable in parsed text)","[[    0   133 20992  8985  2092     7    28  4126    12 16101 25376     4
     85    16  1165    11     5  2239  4116    13     5  1131  6492     6
    147     5  1503  1421    16  2435     4    36   970    16   103   943
   1283    14    16 19216    11 47367  2788    43     2]]"
d5a8fd8bb48dd1f75927e874bdea582b4732a0cd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
3415762847ed13acc3c90de60e3ef42612bc49af,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline
Imbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000","[[    0 32758   414    35   208  4014    12   245     6   255 40698     6
   9206 10842   198   112    12   176  8611   332   357    87 18043 50118
    100  6648   337 16325 14105    35     5  3855    81     5  1542  1421
   3488    25     5   414  1516    55  4356 29683     6  6272    31   198
    231  8611   332    15   727    35 20078     7    81   291  8611   332
     15   291    35 20078     2]]"
642e8cf1d39faa1cd985d16750cdc6696c52db2f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],attentional encoder-decoder networks BIBREF0,"[[    0  2611 19774   337  9689 15362    12 11127 15362  4836   163  8863
  45935   288     2]]"
312e9cc11b9036a6324bdcb64eca6814053ffa17,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
75ff6e425ce304a35f18c0230c0d13d3913a31a9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
277a7e916e65dfefd44d2d05774f95257ac946ae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT","[[    0 43597 24176 34638 15502     6  6479   574  4014   448    12  9822
    597     6 19268    12 47744 13807     6 12334 11126   565     2]]"
93beae291b455e5d3ecea6ac73b83632a3ae7ec7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They compute the gradient of the output at each time step with respect to the input words to decide the importance.,"[[    0  1213 37357     5 43141     9     5  4195    23   349    86  1149
     19  2098     7     5  8135  1617     7  2845     5  3585     4     2]]"
8acab64ba72831633e8cc174d5469afecccf3ae9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],telephone calls,[[    0 35336 17283  1519     2]]
b5608076d91450b0d295ad14c3e3a90d7e168d0e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
612c3675b6c55b60ae6d24265ed8e20f62cb117e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
897ba53ef44f658c128125edd26abf605060fb13,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a3efe43a72b76b8f5e5111b54393d00e6a5c97ab,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
df95b3cb6aa0187655fd4856ae2b1f503d533583,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"n-gram subwords, unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords ","[[    0   282    12 28526  2849 30938     6   542 16101 25376 45293  9506
    293  2006   634  4266   506 33728   163  8863 45935  1225     7  1532
    549    55 38954 18281  7958  2849 30938  1437     2]]"
c08aab979dcdc8f4fe8ec1337c3c8290ab13414e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Fourteen ,[[    0 22113 13665  1437     2]]
82642d3111287abf736b781043d49536fe48c350,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression,"[[    0   133 47234    32   716    15  1283     9  6943     8   617 45068
   1070    30     5 40942 28667   114    89    16  1283     9  6943     2]]"
30eacb4595014c9c0e5ee9669103d003cfdfe1e5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SemEval 2010 task 8 BIBREF8,[[    0 37504   717  6486  1824  3685   290   163  8863 45935   398     2]]
d9b6c61fc6d29ad399d27b931b6cb7b1117b314a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],framework consisting of both a question answering model and a question generation model,"[[    0 48019 17402     9   258    10   864 15635  1421     8    10   864
   2706  1421     2]]"
19cf7884c0c509c189b1e74fe92c149ff59e444b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Answer with content missing: (formulas in selection): Pseudo-perplexity is perplexity where conditional joint probability is approximated.,"[[    0 33683    19  1383  1716    35    36  3899 38215    11  4230  3256
  44643 23259    12  1741 26028  1571    16 33708  1571   147 23431  2660
  18102    16 36612 24985     4     2]]"
ef7212075e80bf35b7889dc8dd52fcbae0d1400a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Linked entities may be ambiguous or too common,[[    0 17860   196  8866   189    28 33406    50   350  1537     2]]
fe2666ace293b4bfac3182db6d0c6f03ea799277,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Acquire very large Vietnamese corpus and build a classifier with it, design a develop a big data warehouse and analytic framework, build a system to incrementally learn new corpora and interactively process feedback.","[[    0 26145 17446   182   739 16859 42168     8  1119    10  1380 24072
     19    24     6  1521    10  2179    10   380   414 12283     8 43474
   7208     6  1119    10   467     7 30401  2368  1532    92 22997   102
      8 10754  6608   609  6456     4     2]]"
b0dbe75047310fec4d4ce787be5c32935fc4e37b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we also use two of its adversarial sets, namely AddSent and AddOneSent BIBREF6 , to evaluate the robustness to noise","[[    0  1694    67   304    80     9    63 37930 27774  3880     6 13953
   4287 35212     8  4287  3762 35212   163  8863 45935   401  2156     7
  10516     5  6295  1825     7  6496     2]]"
500a8ec1c56502529d6e59ba6424331f797f31f0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],700,[[   0 5987    2]]
fb1227b3681c69f60eb0539e16c5a8cd784177a7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"positional features, occurrence frequency, internal POS structure of the entity and the sentence it occurs in, relative entity frequency, centrality measures like PageRank ","[[    0 11474 24176  1575     6 21263 13135     6  3425 29206  3184     9
      5 10014     8     5  3645    24 11493    11     6  5407 10014 13135
      6  1353  1571  1797   101  7086 46052  1437     2]]"
27de1d499348e17fec324d0ef00361a490659988,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"23,700 ",[[   0 1922    6 5987 1437    2]]
61652a3da85196564401d616d251084a25ab4596,0.0,Entailment,[[    2     0 30495  3760  1757     2]],26972 sentences,[[    0 31416  4956 11305     2]]
49764eee7fb523a6a28375cc699f5e0220b81766,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
de830c534c23f103288c198eb19174c76bfd38a1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],intelligence,[[    0 29812     2]]
afb77b11da41cd0edcaa496d3f634d18e48d7168,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BERT based fine-tuning, Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer","[[    0 11126   565   716  2051    12 24641   154     6 45707   786 43871
  13171     6 45707  6479    12   574  4014   448 10490     6 45707  3480
  10490     2]]"
b9ea841b817ba23281c95c7a769873b840dee8d5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
df6d327e176740da9edcc111a06374c54c8e809c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"bag-of-words model, CNN",[[    0 14118    12  1116    12 30938  1421     6  3480     2]]
c0e341c4d2253eb42c8840381b082aae274eddad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],answer questions by obtaining information from KB tuples ,[[    0 27740  1142    30 14999   335    31 29006 13145 12349  1437     2]]
2740e3d7d33173664c1c5ab292c7ec75ff6e0802,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"diacritized corpus that was used to train the RDI BIBREF7 diacritizer and the Farasa diacritizer BIBREF31, WikiNews test set BIBREF31,  large collection of fully diacritized classical texts (2.7M tokens) from a book publisher","[[    0   417  9504  3961  1538 42168    14    21   341     7  2341     5
    248 10063   163  8863 45935   406  2269  1043  3961  6315     8     5
   4256  8810  2269  1043  3961  6315   163  8863 45935  2983     6 45569
   5532  1296   278   163  8863 45935  2983     6  1437   739  2783     9
   1950  2269  1043  3961  1538 15855 14301    36   176     4   406   448
  22121    43    31    10  1040 10710     2]]"
a3d9b101765048f4b61cbd3eaa2439582ebb5c77,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English, Chinese, Korean, we translated the English and Chinese datasets into more languages, with Google Translate","[[    0 35007     6  1111     6  2238     6    52 16877     5  2370     8
   1111 42532    88    55 11991     6    19  1204  5428 19593     2]]"
b6f7fadaa1bb828530c2d6780289f12740229d84,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English-German, English-French.",[[    0 35007    12 27709     6  2370    12 28586     4     2]]
551457ed34ca7fc0878c85bc664b135c21059b58,0.0,Entailment,[[    2     0 30495  3760  1757     2]],190 hours ( INLINEFORM1 100K instances),"[[    0 20109   722    36  2808 28302 38036   134   727   530 10960    43
      2]]"
23e16c1173b7def2c5cb56053b57047c9971e3bb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],LSTM model,[[   0  574 4014  448 1421    2]]
37edc25e39515ffc2d92115d2fcd9e6ceb18898b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SVMs, LR, BIBREF2",[[    0   104   846 13123     6 40815     6   163  8863 45935   176     2]]
0ba3ea93eef5660a79ea3c26c6a270eac32dfa4c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e35c2fa99d5c84d8cb5d83fca2b434dcd83f3851,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"public resources where suspicious Twitter accounts were annotated, list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy","[[    0 15110  1915   147  7775   599  2349    58 45068  1070     6   889
     19   277  2107   599  2349    31   163  8863 45935  1646    14    32
   1687 32101     2]]"
8b9c12df9f89040f1485b3847a29f11b5c9262e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
14b8ae5656e7d4ee02237288372d9e682b24fdb8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],ironies are often obscure and hard to understand,[[    0 11680   918    32   747 23732     8   543     7  1346     2]]
5eabfc6cc8aa8a99e6e42514ef9584569cb75dec,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
b9d07757e2d2c4be41823dd1ea3b9c7f115b5f72,0.0,Entailment,[[    2     0 30495  3760  1757     2]],ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era,"[[    0   260 29318  1111   750  2189    11   484 38481  1988   918    36
   9006 10775  3573    12  2619  3573    43     8  7201  1982    30  8844
      9    14  3567     2]]"
33554065284110859a8ea3ca7346474ab2cab100,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1,873 Twitter conversation threads, roughly 14k tweets","[[    0   134     6   398  5352   599  1607 25816     6  3667   501   330
   6245     2]]"
c81f215d457bdb913a5bade2b4283f19c4ee826c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Waseem and Hovey BIBREF5, Davidson et al. BIBREF9","[[    0   771  3175   991     8   289  7067   219   163  8863 45935   245
      6 10553  4400  1076     4   163  8863 45935   466     2]]"
375b281e7441547ba284068326dd834216e55c07,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a setup where the seeker interacts with a real conversational interface and the wizard, an intermediary, performs actions related to the seeker's message","[[    0   102 11808   147     5 38082 39999    19    10   588 28726  5033
  12332     8     5 32660     6    41 38059     6 14023  2163  1330     7
      5 38082    18  1579     2]]"
3a6e843c6c81244c14730295cfb8b865cd7ede46,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BIBREF9 , BIBREF8 ",[[    0  5383   387 45935   466  2156   163  8863 45935   398  1437     2]]
9176d2ba1c638cdec334971c4c7f1bb959495a8e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen","[[    0 24751     6  8917     6  4002     6   930     6  9206 10842     6
  29730     6  3551  1028     6  1928     6 37206     6  4647     2]]"
e86130c5b9ab28f0ec539c2bed1b1ae9efb99b7d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
ba539cab80d25c3e20f39644415ed48b9e4e4185,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.","[[    0 20340    12 11672  3974     5  3544  2649 30514  2136    25    16
      6   124  1529     7  7974  2136  7314   160     7    10  2136    19
   1122  3854   420  4050     8   124  1529     7  3618  1421  7314   160
      7    10    55 14569  2136  4972  1421  5389    19  2514     8   540
  14120 42168     4     2]]"
27de1d499348e17fec324d0ef00361a490659988,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains and 1,200 out-of-scope queries.","[[    0   883     6  5987 22680     6   217   820     6  1497    11    12
  39576 22680  4631  3982  6979  4189     6    61    64    28 38015    88
    158   937 30700     8   112     6  2619    66    12  1116    12 39576
  22680     4     2]]"
729694a9fe1e05d329b7a4078a596fe606bc5a95,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," total F-1 score on the OntoNotes dataset is 88%, total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%","[[    0   746   274    12   134  1471    15     5 13302   139 44691 41616
     16  7953  4234   746   274    12   134  2116    12 42679  1258  1471
     15     5 12730  1380 45569  1640 16472    43 41616    16  4268   207
      2]]"
b49598b05358117ab1471b8ebd0b042d2f04b2a4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"NBOW, LSTM, attentive LSTM","[[    0 20485  4581     6   226  4014   448     6 36670   226  4014   448
      2]]"
56b034c303983b2e276ed6518d6b080f7b8abe6a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"FSD dataset, Twitter dataset, Google dataset",[[    0   597  6243 41616     6   599 41616     6  1204 41616     2]]
f9f59c171531c452bd2767dc332dc74cadee5120,0.0,Entailment,[[    2     0 30495  3760  1757     2]],14,[[   0 1570    2]]
5c88d601e8fca96bffebfa9ef22331ecf31c6d75,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
584af673429c7f8621c6bf83362a37048daa0e5d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The encoder takes the images in order, one at every timestep $t$ . At time $t=5$ , we obtain the context vector through $h_e^{(t)}$ (represented by $\mathbf {Z}$ ). This vector is used to initialize each decoder's hidden state while the first input to each decoder is its corresponding image embedding $e(I_i)$ . Each decoder generates a sequence of words $\lbrace p_1,...,p_{n}\rbrace $ for each image in the sequence. ","[[    0   133  9689 15362  1239     5  3156    11   645     6    65    23
    358 15679   990  2462    68    90  1629   479   497    86    68    90
   5214   245  1629  2156    52  6925     5  5377 37681   149    68   298
   1215   242 49688  1640    90 48950  1629    36 26716    30 49959 40051
  36920 25522  1301 24303  1629 32801   152 37681    16   341     7 49161
    349  5044 15362    18  7397   194   150     5    78  8135     7   349
   5044 15362    16    63 12337  2274 33183 11303    68   242  1640   100
   1215   118    43  1629   479  4028  5044 15362 17382    10 13931     9
   1617 49959   462 37123   181  1215   134 43754     6   642 49747   282
  49712   338 37123    68    13   349  2274    11     5 13931     4  1437
      2]]"
c5b0ed5db65051eebd858beaf303809aa815e8e5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],small BERT,[[    0 23115   163 18854     2]]
b6f466e0fdcb310ecd212fd90396d9d13e0c0504,0.0,Entailment,[[    2     0 30495  3760  1757     2]], all three languages have error-corrected corpora for testing purposes,"[[    0    70   130 11991    33  5849    12 36064   196 22997   102    13
   3044  6216     2]]"
23e16c1173b7def2c5cb56053b57047c9971e3bb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BIBREF15, BIBREF19, BIBREF20 ","[[    0  5383   387 45935   996     6   163  8863 45935  1646     6   163
   8863 45935   844  1437     2]]"
6a633811019e9323dc8549ad540550d27aa6d972,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
d9412dda3279729e95fcb35cbed09e61577a896e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"precision, recall, F1 and accuracy",[[    0  5234 37938     6  6001     6   274   134     8  8611     2]]
518dae6f936882152c162058895db4eca815e649,0.0,Entailment,[[    2     0 30495  3760  1757     2]],eight layers,[[    0 19491 13171     2]]
277a7e916e65dfefd44d2d05774f95257ac946ae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT
","[[    0 43597 24176 34638 15502     6  6479   574  4014   448    12  9822
    597     6 19268    12 47744 13807     6 12334 11126   565 50118     2]]"
0737954caf66f2b4c898b356d2a3c43748b9706b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
25c1c4a91f5dedd4e06d14121af3b5921db125e9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
eac9dae3492e17bc49c842fb566f464ff18c049b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"claim, premise, backing, rebuttal, and refutation","[[    0 31628     6 18805     6  6027     6 37862   337     6     8  4885
  31320     2]]"
31d695ba855d821d3e5cdb7bea638c7dbb7c87c7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"two stacked GRU layers, attention for one model while for the another one it consists of attention and conflict combined, fully-connected layers","[[    0  7109 19030  8837   791 13171     6  1503    13    65  1421   150
     13     5   277    65    24 10726     9  1503     8  3050  2771     6
   1950    12 21618 13171     2]]"
bb3267c3f0a12d8014d51105de5d81686afe5f1b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CoNLL-YAGO, TAC2010, ACE2004, AQUAINT, WW","[[    0  8739   487  6006    12   975  3450   673     6   255  2562 24789
      6 36211 34972     6    83 15513   250 17831     6 15584     2]]"
26126068d72408555bcb52977cd669faf660bdf7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed","[[    0 18377    10 25860  2136     8  7681 13561     6     5   278     9
  14712 10689   552    19    49  7091  7681  1437  7823    32  3147     2]]"
67e9e147b2cab5ba43572ce8a17fc863690172f0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"directly solicits informative keywords from the crowd for model training, thereby providing human-understandable explanations for the improved model","[[    0 27555   352 25051  2629 29749 32712    31     5  2180    13  1421
   1058     6 12679  1976  1050    12  5087  8490   868 24962    13     5
   2782  1421     2]]"
81588e0e207303c2867c896f3911a54a1ef7c874,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Friends TV sitcom, Facebook messenger chats",[[    0 36042  1012 22195     6   622 34697 28975     2]]
132f752169adf6dc5ade3e4ca773c11044985da4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Wang et al., CrowdFlower dataset ",[[    0   771  1097  4400  1076   482 24544 16197  8285 41616  1437     2]]
37861be6aecd9242c4fdccdfcd06e48f3f1f8f81,0.0,Entailment,[[    2     0 30495  3760  1757     2]],5,[[  0 245   2]]
307e8ab37b67202fe22aedd9a98d9d06aaa169c5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
31735ec3d83c40b79d11df5c34154849aeb3fb47,0.0,Entailment,[[    2     0 30495  3760  1757     2]],20 evaluators were recruited from our institution and asked to each perform 20 annotations,"[[    0   844 37131   257  3629    58 14577    31    84  6786     8   553
      7   349  3008   291 47234     2]]"
88ab7811662157680144ed3fdd00939e36552672,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"KG-A2C-chained, KG-A2C-Explore","[[    0   530   534    12   250   176   347    12   611  7153     6   229
    534    12   250   176   347    12 47526     2]]"
1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
012b8a89aea27485797373adbcda32f16f9d7b54,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"'shallow' naive Bayes, SVM, hierarchical stacked classifiers, bidirectional recurrent neural networks","[[    0    18   298 25487   108 25672  1501   293     6   208 20954     6
  44816 19030  1380 27368     6  2311 43606   337 35583 26739  4836     2]]"
0752d71a0a1f73b3482a888313622ce9e9870d6e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],wFST,[[   0  605  597 4014    2]]
3fad42be0fb2052bb404b989cc7d58b440cd23a0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unif and Stopword,[[    0  9685  1594     8 12457 14742     2]]
8c852fc29bda014d28c3ee5b5a7e449ab9152d35,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)","[[    0 43871   208 20954     6  2311 43606   337  2597  7787    12 23036
     12 47184    36 37426   574  4014   448   238 30505 23794   337 44304
   3658    36 16256    43     2]]"
85590bb26fed01a802241bc537d85ba5ef1c6dc2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Choice-Only model, which is a variant of the well-known hypothesis-only baseline, Choice-to-choice model, tries to single out a given answer choice relative to other choices, Question-to-choice model, in contrast, uses the contextual representations for each question and individual choice and an attention model Att model to get a score","[[    0 41646    12 19933  1421     6    61    16    10 17390     9     5
    157    12  6421 31098    12  8338 18043     6 14431    12   560    12
  24335  1421     6  5741     7   881    66    10   576  1948  2031  5407
      7    97  5717     6 15680    12   560    12 24335  1421     6    11
   5709     6  2939     5 37617 30464    13   349   864     8  1736  2031
      8    41  1503  1421  7279  1421     7   120    10  1471     2]]"
58355e2a782bf145b61ee2a3e0e426119985c179,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Twitter dataset provided by organizers containing harassment and non-harassment tweets,"[[    0 22838 41616  1286    30  9921  8200  4331     8   786    12  4759
  34145  6245     2]]"
9193006f359c53eb937deff1248ee3317978e576,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Reuters,  BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2, Yelp2013","[[    0  1251     6  1437  3295 19451     6  6189 21528     6 36994  9866
      6  3957  1864   250     6  9206 10842     6   255 40698     6   208
   4014    12   134     6   208  4014    12   176     6 29730 10684     2]]"
ba539cab80d25c3e20f39644415ed48b9e4e4185,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Backoff to ""a"" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK","[[    0 19085  1529     7    22   102   113    77    41  2604   530    12
  37466 19799  2136    16 13590     6   124  1529     7    10    55 14569
   2136  4972  1421    77     5  1421 17876  2604   530     2]]"
de3b1145cb4111ea2d4e113f816b537d052d9814,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Wang et al. , maximum entropy classifier with bag of words model","[[    0   771  1097  4400  1076     4  2156  4532 47382  1380 24072    19
   3298     9  1617  1421     2]]"
f7d0fa52017a642a9f70091a252857fccca31f12,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections","[[    0  1640   118    43  3092    14   304 10798 19030   226  4014 13123
      6    36  4132    43  3092    19   430  2808 28302 38036   288     6
     36 31917    43  3092   396  2808 28302 38036   134     6    36  1879
     43  3092    14 13997   795 38270  1241  3723  2462 11616  7070     2]]"
5daeb8d4d6f3b8543ec6309a7a35523e160437eb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English,[[    0 35007     2]]
c571deefe93f0a41b60f9886db119947648e967c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],MIMIC-III,[[    0   448  3755  2371    12 24457     2]]
585626d18a20d304ae7df228c2128da542d248ff,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Coverage, Avg. MCC and avg. +ve F1 score","[[    0  8739 33943     6 43730     4   256  3376     8 44678     4  2055
    548   274   134  1471     2]]"
45be665a4504f0c7f458cf3f75a95d5a75eefd42,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d4db7df65aa4ece63e1de813e5ce98ce1b4dbe7f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values.","[[    0 31288  6487 12986   917    32    55   533     7   464    49  4392
  16763    87    49  6059   131     5   917   109    45   464    49   201
   3281 12336     6   150    49  6059   464    49   201  3281 12336    10
    319   131     5   917  1437  3805     7   146    92  1022  1330     7
    986 21643  3266     6   150     5  6059   146 31074   540  1330  1022
      7   986 21643  3266     4     2]]"
4dc268e3d482e504ca80d2ab514e68fd9b1c3af1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
48088a842f7a433d3290eb45eb0d4c6ab1d8f13c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN","[[    0 30612 48350  1501   293    36 20485   238  9359  5580  6304 21791
     36 33919   238  7737 40419 14969    36   104 20954   238 34638  6311
   5019    36 30455   238 24257  4843 25802   196 32073    36 47872   238
   3480     6   248 20057     2]]"
dd53baf26dad3d74872f2d8956c9119a27269bd5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1264 instances from simulated data, 1280 instances by adding two out-of-distribution symptoms and 944 instances manually delineated from the symptom checking portions of real-word dialogues","[[    0  1092  4027 10960    31 27361   414     6 45576 10960    30  1271
     80    66    12  1116    12 17165 35719  5298     8   361  3305 10960
  24704 39145  1070    31     5 28667  8405 14566     9   588    12 14742
  25730  3663     2]]"
ba28ce9a2f7e8524243adf288cc3f11055e667bb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
3fad42be0fb2052bb404b989cc7d58b440cd23a0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unif and Stopword,[[    0  9685  1594     8 12457 14742     2]]
3c93894c4baf49deacc6ed2a14ef5e0f13b7d96f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"occupation, industry, profile information, language use, gender ","[[    0 32283  1258     6   539     6  4392   335     6  2777   304     6
   3959  1437     2]]"
45a5961a4e1d1c22874c4918e5c98bd3c0a670b3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],7,[[  0 406   2]]
2007bfb8f66e88a235c3a8d8c0a3b3dd88734706,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By using topic modeling and unsupervised emotion detection on ISIS materials and articles from Catholic women forum,"[[    0  2765   634  5674 19039     8   542 16101 25376 11926 12673    15
   7550  3183     8  7201    31  4019   390  7900     2]]"
1b1b0c71f1a4b37c6562d444f75c92eb2c727d9b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The number of dimensions can be reduced by up to 212 times.,"[[    0   133   346     9 22735    64    28  2906    30    62     7 14544
    498     4     2]]"
51b1142c1d23420dbf6d49446730b0e82b32137c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LastStateRNN, AvgRNN, AttentionRNN","[[    0 10285 13360   500 20057     6 43730   500 20057     6 35798   500
  20057     2]]"
b0d66760829f111b8fad0bd81ca331ddd943ef41,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE","[[    0 45368  1142    59 10846 16851  6337     6    61    64    28   617
  16217     6  1437  3927     5 16851  1809 10820     6  3219     5  2975
      9  6878 18442  6448   227 28483     8   256  3850     2]]"
2c59528b6bc5b5dc28a7b69b33594b274908cca6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters","[[    0   250  4126    12 23375   716   248 20057    36 12645   500 20057
     43 12419     5    78     8    94  3768 15529     6     8    16  5951
  40997     7     5 12926     9     5  3425  3768     2]]"
21548433abd21346659505296fb0576e78287a74,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Twitter dataset provided by the organizers,[[    0 22838 41616  1286    30     5  9921     2]]
9af3142630b350c93875441e1e1767312df76d17,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
5b551ba47d582f2e6467b1b91a8d4d6a30c343ec,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence","[[    0 30876   791    12   134    73   306     8   248  5061  8800    12
    574     6 11801     9  5129 13204   634 14085  8135 17697    53 36550
     15  2724   430  3018 11729     6  3018  8150  8611    36   791  5273
    238 30750  7382  1588 46966 17816    36 12642   500   238 26739  2314
   1421    31   163  8863 45935  3103     7  2450 10324    12  4483  1029
  40584     2]]"
f1e90a553a4185a4b0299bd179f4f156df798bce,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CopyRNN (Meng et al., 2017), Multi-Task (Ye and Wang, 2018), and TG-Net (Chen et al., 2018b)","[[    0 48233   500 20057    36   448  3314  4400  1076   482   193   238
  19268    12 47744    36 43429     8  9705     6   199   238     8 40117
     12 15721    36   347  2457  4400  1076   482   199   428    43     2]]"
0810b43404686ddfe4ca84783477ae300fdd2ea4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Transformer over BERT (ToBERT),[[    0 19163 22098    81   163 18854    36  3972 11126   565    43     2]]
506d21501d54a12d0c9fd3dbbf19067802439a04,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"terms common to hosts' descriptions of popular Airbnb properties, like 'subway', 'manhattan', or 'parking'","[[    0 22091  1537     7  4452   108 24173     9  1406 16315  3611     6
    101   128 10936  1970  3934   128   397 45672  3934    50   128 15129
    154   108     2]]"
92bb41cf7bd1f7886784796a8220ed5aa07bc49b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"architecture of the classifier, sentence length,  input domain","[[    0 13161 37471  2407     9     5  1380 24072     6  3645  5933     6
   1437  8135 11170     2]]"
dd20d93166c14f1e57644cd7fa7b5e5738025cd0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],mainstream news and disinformation,[[    0 17894  8656   340     8 30526     2]]
e949b28f6d1f20e18e82742e04d68158415dc61e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For SLC task : Ituorp, ProperGander and YMJA  teams had better results.
For FLC task: newspeak and Antiganda teams had better results.","[[    0  2709   208  6447  3685  4832    85   257 10782     6 39171   534
   6072     8   854   448 27213  1437   893    56   357   775     4 50118
   2709   274  6447  3685    35   340 37271     8  3702  1023  5219   893
     56   357   775     4     2]]"
4e1a67f8dc68b55a5ce18e6cd385ae9ab90d891f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
6024039bbd1118c5dab86c41cce1175d99f10a25,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0, NTCIR PatentMT Parallel Corpus BIBREF1","[[    0 36496 17141 14479  3015 39406 28556    36  2336   510  3586    43
    163  8863 45935   288     6   234  6078  5216 35720 11674 43072 28556
    163  8863 45935   134     2]]"
96b07373756d7854bccc3c12e8d41454ab8741f5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
50e80cfa84200717921840fddcf3b051a9216ad8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
81d607fc206198162faa54a796717c2805282d9b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
cd32a38e0f33b137ab590e1677e8fb073724df7f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English ,[[    0 35007  1437     2]]
e807d347742b2799bc347c0eff19b4c270449fee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers.","[[    0   250 41616  1286    30 12334  2336  1864 17402     9  1142     6
   1637  2526  2339     6 39976     6 14198  1437     8  5631     8  5631
   5274     4     2]]"
8b0abc1907c2bf3e0256f8cf85e0ba66a839bd92,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
d93c0e78a3fe890cd534a11276e934be68583f4b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
4477bb513d56e57732fba126944073d414d1f75f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],clinical notes from the CE task in 2010 i2b2/VA,"[[    0 31903  2775    31     5 16327  3685    11  1824   939   176   428
    176    73  9788     2]]"
72ed5fed07ace5e3ffe9de6c313625705bc8f0c7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],204 tokens,[[    0 27508 22121     2]]
126ff22bfcc14a2f7e1a06a91ba7b646003e9cf0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Transformer base, two-pass CADec model",[[    0 19163 22098  1542     6    80    12 10212 26853  3204  1421     2]]
8748e8f64af57560d124c7b518b853bf2711c13e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
3d7a982c718ea6bc7e770d8c5da564fbb9d11951,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions, supervision signal provided by the discriminator will help generator to capture the event-related patterns","[[    0 34739 12203     9 26739  4836     6     5 22538    16  4453     9
   2239  6336   786 43871 26070     6 13702  6029  1286    30     5 40846
  19936    40   244 22538     7  5604     5   515    12  3368  8117     2]]"
5daeb8d4d6f3b8543ec6309a7a35523e160437eb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English ,[[    0 35007  1437     2]]
184b0082e10ce191940c1d24785b631828a9f714,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization","[[    0  7215 36275   227 14941     8     5 40811  4391    32  3953     6
     61  2019    63 12833    13  6441 39186  1938     2]]"
26327ccebc620a73ba37a95aabe968864e3392b2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],—promoting one's own points and attacking the opponents' points,"[[    0   578 12501 12653    65    18   308   332     8  6666     5  4257
    108   332     2]]"
f0b2289cb887740f9255909018f400f028b1ef26,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"indirect harassment, sexual and physical harassment",[[    0  2028 38806  4331     6  1363     8  2166  4331     2]]
c9bc6f53b941863e801280343afa14248521ce43,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English,[[    0 35007     2]]
b0a18628289146472aa42f992d0db85c200ec64b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"precision, recall , F1 score",[[    0  5234 37938     6  6001  2156   274   134  1471     2]]
a66a275a817f980c36e0b67d2e00bd823f63abf8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
71ba1b09bb03f5977d790d91702481cc406b3767,0.0,Entailment,[[    2     0 30495  3760  1757     2]],M-Bert had 76.6 F1 macro score.,"[[    0   448    12   387  2399    56  5553     4   401   274   134 12303
   1471     4     2]]"
4ac2c3c259024d7cd8e449600b499f93332dab60,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
55507f066073b29c1736b684c09c045064053ba9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related","[[    0 33038   766  1765     6  6202  1848     8 26600     6  4619 38806
   1901     6   305  6234 26780     6 10704 39752     6 14585 44584 30789
   5000     6  5902 32661  3650     6 15516 13851  1330     2]]"
4dc268e3d482e504ca80d2ab514e68fd9b1c3af1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"48,705",[[    0  3818     6 35278     2]]
f9bf6bef946012dd42835bf0c547c0de9c1d229f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
777217e025132ddc173cf33747ee590628a8f62f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"On each dataset, we compare four approaches based on sefe with two efe BIBREF10 baselines. All are fit using sgd BIBREF34 . In particular, we compare the following methods:","[[    0  4148   349 41616     6    52  8933   237  8369   716    15   842
   7068    19    80   364  7068   163  8863 45935   698 11909 38630     4
    404    32  2564   634   579 27122   163  8863 45935  3079   479    96
   1989     6    52  8933     5   511  6448    35     2]]"
9de2f73a3db0c695e5e0f5a3d791fdc370b1df6e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],both,[[    0 17143     2]]
493e971ee3f57a821ef1f67ef3cd47ade154e7c4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Bernoulli embeddings, continuous bag-of-words, Distributed Memory version of Paragraph Vector, Global Vectors, equation embeddings, equation unit embeddings","[[    0 34048  1438 10054 33183   417  1033     6 11152  3298    12  1116
     12 30938     6 11281 18926 25940  1732     9  2884 44947 40419     6
   1849   468  9041   994     6 19587 33183   417  1033     6 19587  1933
  33183   417  1033     2]]"
50cc6c5f2dcf5fb87b56007f6a825fa7c90b64ed,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Bayesian model of garg2012unsupervised as our base monolingual model,"[[    0 20861 44871  1421     9 40696 14517   879 16101 25376    25    84
   1542  6154 31992  5564  1421     2]]"
22714f6cad2d5c54c28823e7285dc85e8d6bc109,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"reduced the dataset by eliminating features, apply feature selection to select highest ranked features to train and test the model and rank the performance of incrementally adding features.","[[    0  2050 33862     5 41616    30 13209  1575     6  3253  1905  4230
      7  5163  1609  4173  1575     7  2341     8  1296     5  1421     8
   7938     5   819     9 30401  2368  1271  1575     4     2]]"
8060a773f6a136944f7b59758d08cc6f2a59693b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],23085 hours of data,[[    0 20352  4531   722     9   414     2]]
3a6e843c6c81244c14730295cfb8b865cd7ede46,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BIBREF9 , BIBREF8",[[    0  5383   387 45935   466  2156   163  8863 45935   398     2]]
d3ff2986ca8cb85a9a5cec039c266df756947b43,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"words embeddings, style, and morality features",[[    0 30938 33183   417  1033     6  2496     6     8 28241  1575     2]]
286078813136943dfafb5155ee15d2429e7601d9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"In case of Freebase knowledge base, LiLi model had better F1 score than the single model by 0.20 , 0.01, 0.159 for kwn, unk, and all test Rel type.  The values for WordNet are 0.25, 0.1, 0.2. 
","[[    0  1121   403     9  3130 11070  2655  1542     6  5991 41198  1421
     56   357   274   134  1471    87     5   881  1421    30   321     4
    844  2156   321     4  2663     6   321     4 27129    13   449 11538
      6   542   330     6     8    70  1296  8136  1907     4  1437    20
   3266    13 15690 15721    32   321     4  1244     6   321     4   134
      6   321     4   176     4  1437 50118     2]]"
71fe5822d9fccb1cb391c11283b223dc8aa1640c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Top-$k$ replies, likes, or re-tweets, FacTweet (tweet-level), LR + All Features (chunk-level), LR + All Features (tweet-level), Tweet2vec, LR + Bag-of-words","[[    0 14323 10068   330  1629 21390     6  3829     6    50   769    12
     90  1694  2580     6 19127 44445    36    90 21210    12  4483   238
  40815  2055   404 21309    36   611  6435    12  4483   238 40815  2055
    404 21309    36    90 21210    12  4483   238 12244   176 25369     6
  40815  2055 13379    12  1116    12 30938     2]]"
5a22293b055f5775081d6acdc0450f7bd5f5de04,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d859cc37799a508bbbe4270ed291ca6394afce2c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Using NMF based topic modeling and their coherence prominent topics are identified,"[[    0 36949 31736   597   716  5674 19039     8    49  1029 40584  5395
   7614    32  2006     2]]"
642e8cf1d39faa1cd985d16750cdc6696c52db2f,0.0,Entailment,[[    2     0 30495  3760  1757     2]], the dl4mt-tutorial,[[    0     5   385   462   306 16100    12    90 48960     2]]
44497509fdf5e87cff05cdcbe254fbd288d857ad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d27438b11bc70e706431dda0af2b1c0b0d209f96,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
26126068d72408555bcb52977cd669faf660bdf7,4.3478,Entailment,[[    2     0 30495  3760  1757     2]],"Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.
Evaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.","[[    0 29235   271   397 22792  3266     9  5323  1215   530   574  1421
  15423    15     5  5437  2136 37015 42532     4 50118   717  6486  9762
    775     9  5323  1215   530   574  1421    15     5 31648  1757 42532
    215    25 31648  1757 15029 41616  1412    31 15690 15721     6  8817
  22241 41616     9  7589 46195  3115 22434    25  3838 13355    50    45
      8 45068  1070  3854  2368  1122 44875    29 41616     4     2]]"
82642d3111287abf736b781043d49536fe48c350,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy","[[    0  2362  1283     9  6943     6 16658  6711     6 22938  3581     6
  16069    50   872     9  1007     2]]"
a4a1fcef760b133e9aa876ac28145ad98a609927,0.0,Entailment,[[    2     0 30495  3760  1757     2]],selection of word vectors,[[    0 44209     9  2136 44493     2]]
9a8b9ea3176d30da2453cac6e9347737c729a538,0.0,Entailment,[[    2     0 30495  3760  1757     2]],hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes,"[[    0 11108 21676   234  2076  1421  4824    10   274   134  1471     9
     68   288     4 30739  1629    15 33689  1538 22680     8    68   288
      4   466  3818  1629    15  5154  2775     2]]"
64af7f5c109ed10eda4fb1b70ecda21e6d5b96c8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"First, propaganda is a social phenomenon and takes place as an act of communication BIBREF19, and so it is more than a simple information-theoretic message of zeros and ones—it also incorporates an addresser and addressee(s), each in phatic contact (typically via broadcast media), ideally with a shared denotational code and contextual surround(s) BIBREF20.","[[    0 10993     6 10934    16    10   592 10632     8  1239   317    25
     41  1760     9  4358   163  8863 45935  1646     6     8    98    24
     16    55    87    10  2007   335    12   627 13477   636  1579     9
    992 22070     8  1980   578   405    67 24536    41  1100   254     8
   1606  1535  7048  1640    29   238   349    11  7843  5183  1511    36
  44661  1241  2308   433   238 25969    19    10  1373  3069  1242  5033
   3260     8 37617 21104  1640    29    43   163  8863 45935   844     4
      2]]"
005cca3c8ab6c3a166e315547a2259020f318ffb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],FIGREF10,[[    0 46741 45935   698     2]]
15cdd9ea4bae8891c1652da2ed34c87bbbd0edb8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"tweets from the past two years from domains like `sports', `politics', `entertainment'","[[    0    90  1694  2580    31     5   375    80   107    31 30700   101
  22209 23267  3934 22209 28373  3934 22209  1342 23801  1757   108     2]]"
48bd71477d5f89333fa7ce5c4556e4d950fb16ed,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"affixes, leading and trailing characters in words and stems, and the presence of words in large gazetteers of named entities","[[    0  3707  3181   293     6   981     8 12564  3768    11  1617     8
  17386     6     8     5  2621     9  1617    11   739   821  1222  3398
    268     9  1440  8866     2]]"
7fb27d8d5a8bb351f97236a1f6dcd8b2613b16f1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],RoBERTa,[[    0 27110 11126 38495     2]]
8e44c02c2d9fa56fb74ace35ee70a5add50b52ae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
4d28c99750095763c81bcd5544491a0ba51d9070,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Celebrities from varioius domains - Acting, Music, Politics, Business, TV, Author, Sports, Modeling. ","[[    0 31431   428 33907    31 15747  1020  6125 30700   111 13711     6
   3920     6 16226     6  2090     6  1012     6 14338     6  1847     6
   7192   154     4  1437     2]]"
e807d347742b2799bc347c0eff19b4c270449fee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BioASQ  dataset,[[    0 40790  2336  1864  1437 41616     2]]
96b07373756d7854bccc3c12e8d41454ab8741f5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
223dc2b9ea34addc0f502003c2e1c1141f6b36a7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BIBREF7,[[    0  5383   387 45935   406     2]]
4d7ff4e5d06902de85b0e9a364dc455196d06a7d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e2b0cd30cf56a4b13f96426489367024310c3a05,0.0,Entailment,[[    2     0 30495  3760  1757     2]],As the metric to assess how well the model's output fits the gold ranking Spearman's $\rho $ was used,"[[    0  1620     5 14823     7  7118   141   157     5  1421    18  4195
  10698     5  1637  7141 37828   397    18 49959   338  5410    68    21
    341     2]]"
71a0c4f19be4ce1b1bae58a6e8f2a586e125d074,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Wikipedia articles are labelled with one of six quality classes, in descending order of quality: Featured Article (“FA”), Good Article (“GA”), B-class Article (“B”), C-class Article (“C”), Start Article (“Start”), and Stub Article (“Stub”)., The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus., The arXiv dataset BIBREF2 consists of three subsets of academic articles under the arXiv repository of Computer Science (cs), from the three subject areas of: Artificial Intelligence (cs.ai), Computation and Language (cs.cl), and Machine Learning (cs.lg). In line with the original dataset formulation BIBREF2 , a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI. Failing this, it is considered to be rejected (noting that some of the papers may not have been submitted to one of these conferences). ","[[    0 47681  7201    32 22434    19    65     9   411  1318  4050     6
     11 35602   645     9  1318    35 17619  6776    36    17    48  5944
     17    46   238  2497  6776    36    17    48  4164    17    46   238
    163    12  4684  6776    36    17    48   387    17    46   238   230
     12  4684  6776    36    17    48   347    17    46   238  2776  6776
     36    17    48 33724    17    46   238     8 26009  6776    36    17
     48  5320  1792    17    46   322     6    20  1318  1380     9    10
  28274  1566    16  5530    30 28274 34910    50   143  3382  3018     6
     54    64  2268   149     5  1566    18  1067  1842     7  1338  2899
    482    20  4709  1000  1879 41616   163  8863 45935   176 10726     9
    130 18621  2580     9  5286  7201   223     5  4709  1000  1879 30076
      9 16772  4662    36 11365   238    31     5   130  2087   911     9
     35 27332  6558    36 11365     4  1439   238 34710  1258     8 22205
     36 11365     4  3998   238     8 14969 13807    36 11365     4   462
    571   322    96   516    19     5  1461 41616 32018   163  8863 45935
    176  2156    10  2225    16  1687     7    33    57  3903    36   118
      4   242     4    16 13541 16274    43   114    24  2856    10  2225
     11     5   211  7976   510  8503    50    16  3680  3903    30   143
      9     5   511 14041    35 21147     6 14850   487 21992     6  8438
   2562   574     6   381  2562   574     6   255  2562   574     6 17378
   3888     6  8242 10537     6    38  7454   500     6    50 17147   100
      4   274  8459    42     6    24    16  1687     7    28  3946    36
   3654   154    14   103     9     5  6665   189    45    33    57  4813
      7    65     9   209 14041   322  1437     2]]"
45be665a4504f0c7f458cf3f75a95d5a75eefd42,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
37861be6aecd9242c4fdccdfcd06e48f3f1f8f81,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks.","[[    0   387  3314  3644     6 44331  3648     6  1127 20206     6  2529
    857 23590     8 12634    32     5  2270  1300 11991     6     8 19850
     31   209     7 19840 14409     5   920  8558     4     2]]"
2236386729105f5cf42f73cc055ce3acdea2d452,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English,[[    0 35007     2]]
1ff0fccf0dca95a6630380c84b0422bed854269a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews,"[[    0  1409  1058    41  7241 43779 43651   467    15  1764   530 22422
  36551 11305    31 29730  6173     2]]"
da55bd769721b878dd17f07f124a37a0a165db02,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
9ee07edc371e014df686ced4fb0c3a7b9ce3d5dc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SimpleQuestions, WebQSP",[[    0 45093 46865     6  6494  1864  4186     2]]
de5b6c25e35b3a6c5e40e350fc5e52c160b33490,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"On arXiv dataset, the proposed model outperforms baselie model by (ROUGE-1,2,L)  0.67 0.72 0.77 respectively and by Meteor 0.31.
","[[    0  4148  4709  1000  1879 41616     6     5  1850  1421  9980 33334
  11909   523   324  1421    30    36   500  5061  8800    12   134     6
    176     6   574    43  1437   321     4  4111   321     4  4956   321
      4  4718  4067     8    30 16582   321     4  2983     4 50118     2]]"
eda4869c67fe8bbf83db632275f053e7e0241e8c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
b1ce129678e37070e69f01332f1a8587e18e06b0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Collected tweets and opening and closing stock prices of Microsoft.,"[[    0 18551 17970  6245     8  1273     8  3172   388   850     9  3709
      4     2]]"
9ec1f88ceec84a10dc070ba70e90a792fba8ce71,0.0,Entailment,[[    2     0 30495  3760  1757     2]],0.5115,[[    0   288     4   245 15314     2]]
17a1eff7993c47c54eddc7344e7454fbe64191cd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],semantic category-based approach,[[    0 26976 26970  4120    12   805  1548     2]]
6aaf12505add25dd133c7b0dafe8f4fe966d1f1d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM","[[    0 48269  5033   226  4014   448     6  7275 16256     6  6002  8007
  12508    12   574  4014   448     6 27188   487     6 14680 13696     6
  13579   791     6 36323 20057     6   248  1889     6   204    12 39165
  14514    12 41011   226  4014   448     6 18463   495    12   574  4014
    448     6 28256  1538   226  4014   448     2]]"
f903396d943541a8cc65edefb04ca37814ed30dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d98847340e46ffe381992f1a594e75d3fb8d385e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Logistic Regression, neural networks",[[    0 23345  5580  6304 21791     6 26739  4836     2]]
53dfcd5d7d2a81855ec1728f0d8e6e24c5638f1e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU-1, Meteor ,  Rouge-L ","[[    0 30876   791    12   134     6 16582  2156  1437 14941    12   574
   1437     2]]"
3bf0306e9bd044f723e38170c13455877b2aeec3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss,"[[    0  4684  4945 16851     8   786    12    29  1290  5033  6337   634
     10    65    12 39165  3480    19    10 32771  2116 47382   872     2]]"
8c48c726bb17a17d70ab29db4d65a93030dd5382,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"57,505 sentences",[[    0  4390     6 35255 11305     2]]
a87a009c242d57c51fc94fe312af5e02070f898b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],logistic regression models,[[    0 12376  5580 39974  3092     2]]
ef7212075e80bf35b7889dc8dd52fcbae0d1400a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"linked entities extracted from ELS's have issues because of low precision rates BIBREF11 and design challenges in training datasets BIBREF12 . These issues can be summarized into two parts: ambiguity and coarseness., the linked entities may also be too common to be considered an entity.","[[    0 16166  8866 27380    31   381 10463    18    33   743   142     9
    614 15339  1162   163  8863 45935  1225     8  1521  2019    11  1058
  42532   163  8863 45935  1092   479  1216   743    64    28 38152    88
     80  1667    35 35566     8  1029  2726 14186   482     5  3307  8866
    189    67    28   350  1537     7    28  1687    41 10014     4     2]]"
58ee0cbf1d8e3711c617b1cd3d7aca8620e26187,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for ""Starry Night"" with a low average content score","[[    0  1694   109    45    33    41   253    12   560    12  1397 41616
      6     5  5129  2370 19340   189    45   173   157    19 16538  2496
   2937    25  2343    11 17965 37365 45935  1092    13    22  5320 12795
   4172   113    19    10   614   674  1383  1471     2]]"
5eda469a8a77f028d0c5f1acd296111085614537,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation","[[    0 28586    12 35007    12 41226    36 29220    12 16040    12 15286
    238  1859    12 35007    12 28586    36 13365    12 16040    12 29220
     43     8 21624    12 35007    12 27709    36 27110    12 16040    12
  13365   238 19645    36  8138   238  3453    36 15286   238     8  1083
     36 36439   238     8  7628 19850   227  1235 21395   411  4276    12
  10393 19850     2]]"
29923a824c98b3ba85ced964a0e6a2af35758abe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations","[[    0   175 29724  3645    12  4483   163  3850   791     6   166 24704
  22585  7721   147     5  1300 12348    21 14085     7     5 19850     6
   9550     5 33708  1571     9     5 41762     6 43547     5  1750     9
   2370  3768    11     5 41762     6 15756 37015  4391   227 20382     8
  41762     2]]"
e2db361ae9ad9dbaa9a85736c5593eb3a471983d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent","[[    0   534  4082 30660     6   163 18854     6  9973 12169  4086 14813
  15362     6 35690    12  2688   597     6    96  6646 35212     2]]"
249c805ee6f2ebe4dbc972126b3d82fb09fa3556,0.0,Entailment,[[    2     0 30495  3760  1757     2]], the average dissimilarity of all pairs of tags in the list of recommended tags,"[[    0     5   674 14863 36692  1571     9    70 15029     9 19445    11
      5   889     9  5131 19445     2]]"
c0bee6539eb6956a7347daa9d2419b367bd02064,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
3fff37b9f68697d080dbd9d9008a63907137644e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
03e9ac1a2d90152cd041342a11293a1ebd33bcc3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],NLG datasets,[[    0 27027   534 42532     2]]
5913930ce597513299e4b630df5e5153f3618038,0.0,Entailment,[[    2     0 30495  3760  1757     2]],We introduce sparse attention into the Transformer architecture,[[    0   170  6581 28593  1503    88     5  5428 22098  9437     2]]
5c3e98e3cebaecd5d3e75ec2c9fc3dd267ac3c83,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences","[[    0 38398   219  4210 24072     6 12169  8913  4210 24072    13  9940
    219     6 12169  8913  4210 24072    13  6965    12   853  6119     6
   7791    31 25553 11305     7   786    12 11680   636 11305     2]]"
72e4e26d0dd79c590c28b10938952a9f9497ff1e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models","[[    0 20557  1295    10 19340    31  3156    52   304    41  2210  2701
     12 35031   636  9437     6  1337  3505     9 13931     7 13931  3092
      2]]"
49c32a2a64eb41381e5f12ccea4150cac9f3303d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
ff6c9af28f0e2bb4fb6a69f124665f8ceb966fbc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Galatasaray , Fenerbahçe ","[[    0 32302   415 31126   857  2156   274  5777 28185  3381   242  1437
      2]]"
de0b650022ad8693465242ded169313419eed7d9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
1dac4bc5af239024566fcb0f43bb9ff1c248ecec,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
5cb610d3d5d7d447b4cd5736d6a7d8262140af58,0.0,Entailment,[[    2     0 30495  3760  1757     2]],by randomly alternating between languages for every new minibatch,"[[    0  1409 22422 36830   227 11991    13   358    92  5251  1452 11175
      2]]"
75ff6e425ce304a35f18c0230c0d13d3913a31a9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
f85ca6135b101736f5c16c5b5d40895280016023,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the baseline transformer BIBREF8,[[    0   627 18043 40878   163  8863 45935   398     2]]
58c6737070ef559e9220a8d08adc481fdcd53a24,0.0,Entailment,[[    2     0 30495  3760  1757     2]],correct classification rate (CCR),[[    0 36064 20257   731    36  3376   500    43     2]]
8c48c726bb17a17d70ab29db4d65a93030dd5382,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"57,505 sentences",[[    0  4390     6 35255 11305     2]]
3f0ae9b772eeddfbfd239b7e3196dc6dfa21365f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"irony accuracy, sentiment preservation",[[    0   853  6119  8611     6  5702 18498     2]]
1c0ba6958da09411deded4a14dfea5be55687619,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
aa6d956c2860f58fc9baea74c353c9d985b05605,0.0,Entailment,[[    2     0 30495  3760  1757     2]],ROUGE,[[   0  500 5061 8800    2]]
348886b4762db063711ef8b7a10952375fbdcb57,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
e831041d50f3922265330fcbee5a980d0e2586dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],read the sentences normally without any special instructions,[[    0 12745     5 11305  6329   396   143   780  9223     2]]
61652a3da85196564401d616d251084a25ab4596,0.0,Entailment,[[    2     0 30495  3760  1757     2]],26972,[[    0 31416  4956     2]]
fc4ae12576ea3a85ea6d150b46938890d63a7d18,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
312e9cc11b9036a6324bdcb64eca6814053ffa17,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
73bddaaf601a4f944a3182ca0f4de85a19cdc1d2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Daily Mail news articles,[[    0 26339  6313   340  7201     2]]
511517efc96edcd3e91e7783821c9d6d5a6562af,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BioScope Abstracts, SFU, and BioScope Full Papers","[[    0 40790 45190 43649    29     6 17575   791     6     8 12334 45190
   6583 28735     2]]"
193ee49ae0f8827a6e67388a10da59e137e7769f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],recovers a masked document to its original form,[[    0   241   876  3697    10 24397  3780     7    63  1461  1026     2]]
ca4daafdc23f4e23d933ebabe682e1fe0d4b95ed,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
f318a2851d7061f05a5b32b94251f943480fbd15,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"By comparing scores for each word calculated using Depechemood dictionary and normalize emotional score for each article, they found Catholic and ISIS materials show similar scores","[[    0  2765 12818  4391    13   349  2136  9658   634   926  2379 25666
   5715 36451     8  2340  2072  3722  1471    13   349  1566     6    51
    303  4019     8  7550  3183   311  1122  4391     2]]"
d64383e39357bd4177b49c02eb48e12ba7ffd4fb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer","[[    0 43551 17505  6133 13093   154 43262     6 43885  6133 13093   154
  43262     6   944 21284 25940 43262     6  8526  6158 25940 43262     6
  31652 28967 27851 43262     2]]"
d67c01d9b689c052045f3de1b0918bab18c3f174,0.0,Entailment,[[    2     0 30495  3760  1757     2]],INLINEFORM2 ,[[    0  2444 28302 38036   176  1437     2]]
d78f7f84a76a07b777d4092cb58161528ca3803c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"This motivates us to carry out a backward greedy search over each sentence's label sequence to identify word boundaries. If two words segmented in a sentence are identified as nouns, and one word is immediately before the other, we assemble their boundaries, creating a new word candidate for entity recognition.","[[    0   713 21496  1626   201     7  2324    66    10 18173 34405  1707
     81   349  3645    18  6929 13931     7  3058  2136 10156     4   318
     80  1617  2835   196    11    10  3645    32  2006    25 44875    29
      6     8    65  2136    16  1320   137     5    97     6    52 24054
     49 10156     6  2351    10    92  2136  1984    13 10014  4972     4
      2]]"
9c33b340aefbc1f15b6eb6fb3e23ee615ce5b570,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They use four-layered 2D CNN and two fully connected hidden layers on the channel covariance matrix to compute the spatial aspect.,"[[    0  1213   304   237    12  8433  3215   132   495  3480     8    80
   1950  3665  7397 13171    15     5  4238 49033  2389 36173     7 37357
      5 34999  6659     4     2]]"
22b740cc3c8598247ee102279f96575bdb10d53f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
c87fcc98625e82fdb494ff0f5309319620d69040,0.0,Entailment,[[    2     0 30495  3760  1757     2]],hashtag features contain whether there is any hashtag in the tweet,"[[    0 25903 10058  1575  5585   549    89    16   143 15493    11     5
   3545     2]]"
72755c2d79210857cfff60bfbcb55f83c71ada51,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"104 telephone calls, which pair 11 hours of audio",[[    0 17573  7377  1519     6    61  1763   365   722     9  6086     2]]
8e12b5c459fa963b3e549deadb864c244879fe82,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
3f0ae9b772eeddfbfd239b7e3196dc6dfa21365f,0.0,Entailment,[[    2     0 30495  3760  1757     2]], irony accuracy and sentiment preservation,[[    0 21490  8611     8  5702 18498     2]]
ab54cd2dc83141bad3cb3628b3f0feee9169a556,0.0,Entailment,[[    2     0 30495  3760  1757     2]],A hybrid model consisting of best performing popularity-based approach with the best similarity-based approach,"[[    0   250  9284  1421 17402     9   275  4655  7347    12   805  1548
     19     5   275 37015    12   805  1548     2]]"
a778b8204a415b295f73b93623d09599f242f202,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping","[[    0 23242 17022   352 12879   414 44493     7    10   980   147 26956
  10875    16   678     6   248 18307  5448  1639    41 32161 34751  5043
   1241 16045 20410     2]]"
c2d1387e08cf25cb6b1f482178cca58030e85b70,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a15bc19674d48cd9919ad1cf152bf49c88f4417d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The manual transcriptions of the DSTC2 training set ,"[[    0   133 12769 12348  2485     9     5   211  4014   347   176  1058
    278  1437     2]]"
e35a7f9513ff1cc0f0520f1d4ad9168a47dc18bb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"traditional phrase-based statistical machine translation (SMT), NMT system","[[    0 30172 11054    12   805 17325  3563 19850    36 15153   565   238
    234 11674   467     2]]"
d859cc37799a508bbbe4270ed291ca6394afce2c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LDA, non-negative matrix factorization (NMF)","[[    0   574  3134     6   786    12 33407 36173  3724  1938    36 31156
    597    43     2]]"
00c57e45ac6afbdfa67350a57e81b4fad0ed2885,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
5da26954fbcd3cf6a7dba9f8b3c9a4b0391f67d4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model,"[[    0 14210 19160     5   335    31  6086     8  2788 26929   634  6594
    248 20057    29     8   172 15678     5   335    31   209  1715   634
     10  3993    12 16135 26739  1421     2]]"
a29c071065d26e5ee3c3bcd877e7f215c59d1d33,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels,"[[    0 29235   271   397    18  7938 22792   227     5 12793   833    12
  42116  1571     9     5  3645 33183   417  1033     8     5  1637 14105
      2]]"
a3d9b101765048f4b61cbd3eaa2439582ebb5c77,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English , Chinese",[[    0 35007  2156  1111     2]]
37a79be0148e1751ffb2daabe4c8ec6680036106,0.0,Entailment,[[    2     0 30495  3760  1757     2]],anti-nuclear-power,[[    0 10865    12 31242    12 11017     2]]
78a4ec72d76f0a736a4a01369a42b092922203b6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],EmotionLines BIBREF6,[[    0 16750 19187   574  3141   163  8863 45935   401     2]]
70148c8d0f345ea36200d5ba19d021924d98e759,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a perceptual illusion, where listening to a speech sound while watching a mouth pronounce a different sound changes how the audio is heard","[[    0   102 47777 24654     6   147  6288     7    10  1901  2369   150
   2494    10  6085 25967    10   430  2369  1022   141     5  6086    16
   1317     2]]"
b8d0e4e0e820753ffc107c1847fe1dfd48883989,0.0,Entailment,[[    2     0 30495  3760  1757     2]],NCEL considers only adjacent mentions.,[[    0  6905  3721  9857   129 12142 19197     4     2]]
d915b401bb96c9f104a0353bef9254672e6f5a47,0.0,Entailment,[[    2     0 30495  3760  1757     2]],rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions,"[[    0   338 12968 10759  9946     5  1421    15     5   414  3184    11
    645     7  2097 20403     9   190 31515 24173     2]]"
ef4dba073d24042f24886580ae77add5326f2130,0.0,Entailment,[[    2     0 30495  3760  1757     2]],F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain),"[[    0   597   134     9  5663     4  2831    15     5 13925    12  3888
  41616    36 43735  2154 11170  4397  3337     4   996    15 11270    12
  11674     8  6121     4  4540    15 11270    12   791  1864    36   242
     12  8342 11170    43     2]]"
38e4aaeabf06a63a067b272f8950116733a7895c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],A new tagging scheme that tags the words before and after the pun as well as the pun words.,"[[    0   250    92 34694  3552    14 19445     5  1617   137     8    71
      5  7434    25   157    25     5  7434  1617     4     2]]"
0ec56e15005a627d0b478a67fd627a9d85c3920e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Word vectors, usually in the context of others within the same class","[[    0 44051 44493     6  2333    11     5  5377     9   643   624     5
    276  1380     2]]"
5bb3c27606c59d73fd6944ba7382096de4fa58d8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],multiple-choice,[[    0 34654    12 24335     2]]
326588b1de9ba0fd049ab37c907e6e5413e14acd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"OpenNMT, PBMT-R, Hybrid, SBMT-SARI, Dress","[[    0 25266   487 11674     6   221 13386   565    12   500     6 25367
      6   208 13386   565    12   104 32665     6 24247     2]]"
63a1cbe66fd58ff0ead895a8bac1198c38c008aa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Show&Tell and LRCN1u,[[    0 27477   947 35438     8   226  5199   487   134   257     2]]
fff5c24dca92bc7d5435a2600e6764f039551787,0.0,Entailment,[[    2     0 30495  3760  1757     2]],they obtained computer science related topics by looking at titles and user-assigned tags,"[[    0 10010  4756  3034  2866  1330  7614    30   546    23  4867     8
   3018    12  2401  9044 19445     2]]"
26c2e1eb12143d985e4fb50543cf0d1eb4395e67,0.0,Entailment,[[    2     0 30495  3760  1757     2]],adjectives are used to create “more narrow labels [or subtypes] for individuals who do not fit with general social category expectations”,"[[    0   625 21517  3699    32   341     7  1045    44    48  4321  6787
  14105   646   368  2849 41817   742    13  2172    54   109    45  2564
     19   937   592  4120  2113    17    46     2]]"
73a7acf33b26f5e9475ee975ba00d14fd06f170f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms","[[    0   627    86     5  3186    34    57  7242     5 28667     6  1713
     14  7691     5 28667     6     5  5239     9 24146     6     5 13135
  21263     9     5 28667     6     5  2259     9 28667     6   361  5298
      2]]"
c598028815066089cc1e131b96d6966d2610467a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
4e1a67f8dc68b55a5ce18e6cd385ae9ab90d891f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
3941401a182a3d6234894a5c8a75d48c6116c45c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"cyber security (CyberAttack), death of politicians (PoliticianDeath)","[[    0  4469  1943   573    36 25826  1943 45138   238   744     9  3770
     36 39564 14932 34811    43     2]]"
bd40f33452da7711b65faaa248aca359b27fddb6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BERT had 76.6 F1 macro score on x-stance dataset.,"[[    0 11126   565    56  5553     4   401   274   134 12303  1471    15
   3023    12   620  2389 41616     4     2]]"
ac7f6497be4bcca64e75f28934b207c9e8097576,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"seven of the originally defined relation types: political_affiliation, education, founder, wife/husband, job_title, nationality, and employer","[[    0 17723     9     5  3249  6533  9355  3505    35   559  1215  3707
  26045     6  1265     6  3787     6  1141    73 21301     6   633  1215
  14691     6 26241     6     8  8850     2]]"
899ed05c460bf2aa0aa65101cad1986d4f622652,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"$3,209$ reviews ",[[    0  1629   246     6 27434  1629  6173  1437     2]]
0c234db3b380c27c4c70579a5d6948e1e3b24ff1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],LSTM,[[   0  574 4014  448    2]]
ee2c9bc24d70daa0c87e38e0558e09ab97feb4f2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
521280a87c43fcdf9f577da235e7072a23f0673e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],five annotators,[[    0  9579 45068  3629     2]]
f9f59c171531c452bd2767dc332dc74cadee5120,0.0,Entailment,[[    2     0 30495  3760  1757     2]],14 participants,[[   0 1570 3597    2]]
5cc2daca2a84ddccba9cdd9449e51bb3f64b3dde,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For speech synthesis, they build a speech clustergen statistical speech synthesizer BIBREF9. For speech recognition, they use Kaldi BIBREF11. For Machine Translation, they use a Transformer architecture from BIBREF15.","[[    0  2709  1901 37423     6    51  1119    10  1901 18016  4138 17325
   1901 33689  6315   163  8863 45935   466     4   286  1901  4972     6
     51   304   229 22564   163  8863 45935  1225     4   286 14969 41737
      6    51   304    10  5428 22098  9437    31   163  8863 45935   996
      4     2]]"
4944cd597b836b62616a4e37c045ce48de8c82ca,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification.","[[    0 37504 26970 14159  5564 17110  1571     6  5702 16782     6  2087
   9866 16782     6 11054   672  2979  8385 21528 20257     6  8607 12169
   8913 11077  5760     6  2051  4435  7153   864    12 12528 20257     4
      2]]"
3b371ea554fa6639c76a364060258454e4b931d4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],NowThisNews Facebook page,[[   0 5975  713 5532  622 1842    2]]
c0bee6539eb6956a7347daa9d2419b367bd02064,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
357eb9f0c07fa45e482d998a8268bd737beb827f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Kvmemnn,  Feed Yourself, Poly-encoder, BERT bi-ranker","[[    0   530   705 25683 15688     6  1437 17703 35420     6 10415    12
  14210 15362     6   163 18854  4003    12 40081   254     2]]"
cb6a8c642575d3577d1840ca2f4cd2cc2c3397c5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
6263b2cba18207474786b303852d2f0d7068d4b6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian","[[    0 35007     6  1859     6  3453     6 33830     6 11145     6  1083
      6  2238     8 24229     2]]"
7bd6a6ec230e1efb27d691762cc0674237dc7967,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Penn Treebank (PTB) , WikiText2 (WT-2)","[[    0 35104 11077  5760    36 10311   387    43  2156 45569 39645   176
     36 25982    12   176    43     2]]"
9cf070d6671ee4a6353f79a165aa648309e01295,0.0,Entailment,[[    2     0 30495  3760  1757     2]],1500 sentences,[[    0 36559 11305     2]]
0689904db9b00a814e3109fb1698086370a28fa2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Doc2Vec, PV-DBOW model",[[    0 42291   176   846  3204     6 23774    12 10842  4581  1421     2]]
26c2e1eb12143d985e4fb50543cf0d1eb4395e67,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Ethnic bias,[[    0 42301 12979  9415     2]]
5da26954fbcd3cf6a7dba9f8b3c9a4b0391f67d4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],combines the information from these sources using a feed-forward neural model,"[[    0 15302  3141     5   335    31   209  1715   634    10  3993    12
  16135 26739  1421     2]]"
887c6727e9f25ade61b4853a869fe712fe0b703d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists,"[[    0  1694 10759  9946    84 26739 35282    19    80  3471    35    36
    134    43  2808 28302 38036   288     8    36   176    43  2808 28302
  38036   134  8785     2]]"
ee2ad0ab64579cffb60853db6a8c0f971d7cf0ff,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
4ef11518b40cc55d86c485f14e24732123b0d907,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"FGM, FGVM, DeepFool, HotFlip, TYC","[[    0   597 16972     6 25408 20954     6  8248   597  8110     6  6003
  16197  1588     6 39223   347     2]]"
ad08b215dca538930ef1f50b4e49cd25527028ad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
559c68802ee2bb8b11e2188127418ca3a6155ba7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
009ce6f2bea67e7df911b3f93443b23467c9f4a1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],pre-trained multi-BERT,[[    0  5234    12 23830  3228    12 11126   565     2]]
b14217978ad9c3c9b6b1ce393b1b5c6e7f49ecab,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions","[[    0 47744   112    35  3232  4330 43803 14263 15680 34587 38091     6
  12927   132    35 31563  1142     2]]"
5a65ad10ff954d0f27bb3ccd9027e3d8f7f6bb76,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They compare to Akbik et al. (2018) and Link et al. (2012).,"[[    0  1213  8933     7  3773   428   967  4400  1076     4    36  2464
     43     8  4341  4400  1076     4    36 14517   322     2]]"
2e37e681942e28b5b05639baaff4cd5129adb5fb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
b9d168da5321a7d7b812c52bb102a05210fe45bd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
61a9ea36ddc37c60d1a51dabcfff9445a2225725,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e79a5b6b6680bd2f63e9f4adbaae1d7795d81e38,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Russian,[[    0 14836     2]]
3d013f15796ae7fed5272183a166c45f16e24e39,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"font type, font style, Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page","[[    0 42060  1907     6 28716  2496     6  3522    15     5 28716  1907
      8 28716  2496    36   242     4   571   482 45981  2857     6  7457
   5780    43     9    10 19233     8    63   737    15     5  2166  1842
      2]]"
63c3550c6fb42f41a0c93133e9fca12ac00df9b3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
2ceced87af4c8fdebf2dc959aa700a5c95bd518f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
c5abe97625b9e1c8de8208e15d59c704a597b88c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.","[[    0 48517     9   365     4   398    13     5    83   176   347    12
    611  7153  1421     6  3492     4   398    13     5   229   534    12
    250   176   347    12   611  7153  1421     6   843    13    83   176
    347    12 47526     8  3550    13   229   534    12   250   176   347
     12 47526     4     2]]"
6cd25c637c6b772ce29e8ee81571e8694549c5ab,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English WIKIBIO, French WIKIBIO , German WIKIBIO ","[[    0 35007   305 20458   100  5383   673     6  1515   305 20458   100
   5383   673  2156  1859   305 20458   100  5383   673  1437     2]]"
968b7c3553a668ba88da105eff067d57f393c63f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Viral tweets are the ones that are retweeted more than 1000 times,"[[    0   846 24811  6245    32     5  1980    14    32 24352   196    55
     87 10775   498     2]]"
88bf368491f9613767f696f84b4bb1f5a7d7cb48,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
30e21f5bc1d2f80f422c56d62abca9cd3f2cd4a1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap.","[[    0  1213  8933     5  8558    14     5 42532    32 10686    13     6
    674   346     9  1948  2261   228   864     6   346     9 19233  3505
      6   674  1948  1984 18915     6   674   864 18915     6   864    12
  27740  2136 27573     4     2]]"
56b7319be68197727baa7d498fa38af0a8440fe4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Linguistic,[[    0   574 35308  5580     2]]
9b76f428b7c8c9fc930aa88ee585a03478bff9b3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],53 documents,[[   0 4540 2339    2]]
01a41c0a4a7365cd37d28690735114f2ff5229f2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],http://www.blogger.com,[[    0  8166   640  1401     4 24635  2403     4   175     2]]
fd0a3e9c210163a55d3ed791e95ae3875184b8f8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],WSJ,[[    0 13691   863     2]]
ecd5770cf8cb12cb34285e26ab834301c17c53e1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"random forest, The question is encoded with a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question. The image is described with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 BIBREF25 . The system performs an element-wise multiplication of the image and question features, after linearly transforming the image descriptor to 1024 dimensions. The final layer of the architecture is a softmax layer.","[[    0 45041  6693     6    20   864    16 45320    19    10 46959    12
  23944   226  4014   448  1421    14  1239    11    10    65    12 10120
  47220     9   349  2136    11     5   864     4    20  2274    16  1602
     19     5 48711    12 23944  4195    31     5    94  1950  3665 10490
      9     5 30505 23794   337 44304  3658    36 16256   238   468 24592
   1549   163  8863 45935  1244   479    20   467 14023    41  7510    12
  10715 46485     9     5  2274     8   864  1575     6    71 24248 23099
  17828     5  2274 47220     7 46959 22735     4    20   507 10490     9
      5  9437    16    10  3793 29459 10490     4     2]]"
c74185bced810449c5f438f11ed6a578d1e359b4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: ""Don't be rude or hostile to others users.""","[[    0   133 38119  1635 25242 11614  1506 41616    16 22434    25  1169
   8200    10  1081   908    31    19  2544    36   118     4   242     4
  11928  3650    30    65  3018    11     5  1607  3660  1567   277    43
     50  2405  2366  1328     4    20  6844  7229  1308  3756 41616    16
  22434    19   549    50    45    10  4865  1258  2140    56    10  1129
   2928    30    10 36990    13  4565     9 16654   132    35    22  6766
     75    28 21820    50 11928     7   643  1434    72     2]]"
f4496316ddd35ee2f0ccc6475d73a66abf87b611,0.0,Entailment,[[    2     0 30495  3760  1757     2]],dataset created by ceccarelli2013learning from the CoNLL 2003 data,"[[    0 36146   281   594  1412    30  8635  7309  1322 10054 10684 29888
     31     5   944   487  6006  4999   414     2]]"
111afb77cfbf4c98e0458606378fa63a0e965e36,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
e97186c51d4af490dba6faaf833d269c8256426c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
6568a31241167f618ef5ede939053feaa2fb0d7e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
dc1cec824507fc85ac1ba87882fe1e422ff6cffb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],3500 questions collected from the internet and books.,[[   0 2022  612 1142 4786   31    5 2888    8 2799    4    2]]
7fa3c2c0cf7f559d43e84076a9113a390c5ba03a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
92294820ac0d9421f086139e816354970f066d8a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1","[[    0 25869 18715   311   357   775    15    70 12758  1118     7 18043
   4682 16463   257   134  1437    15 28279  3462    36   605 27209    30
    321     4  1225  1118     7 18043   322 16463   257   134  1471    15
   5620 46744    16  2248     4  4280  1118     7   275 18043  3557     4
   3706     6    97 12758    15   674    30   112     2]]"
63a1cbe66fd58ff0ead895a8bac1198c38c008aa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Show&Tell model, LRCN1u",[[    0 27477   947 35438  1421     6   226  5199   487   134   257     2]]
a32c792a0cef03218bf66322245677fc2d5e5a31,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
10fb7dc031075946153baf0a0599e126de29e3a4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"converts WSD to a sequence learning task,  leverage gloss knowledge, by extending gloss knowledge","[[    0  3865 24038   305  6243     7    10 13931  2239  3685     6  1437
   6270 27293  2655     6    30  9148 27293  2655     2]]"
b68f72aed961d5ba152e9dc50345e1e832196a76,0.0,Entailment,[[    2     0 30495  3760  1757     2]],On average 0.64 ,[[   0 4148  674  321    4 4027 1437    2]]
1b1a30e9e68a9ae76af467e60cefb180d135e285,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"353 conversations from 40 speakers (11 nurses, 16 patients, and 13 caregivers), we build templates and expression pools using linguistic analysis","[[    0 27044  5475    31   843  6864    36  1225 10633     6   545  1484
      6     8   508 21650   238    52  1119 38577     8  8151 16326   634
  39608  1966     2]]"
b8d7d055ddb94f5826a9aad7479b4a92a9c8a2f0,0.0,Entailment,[[    2     0 30495  3760  1757     2]], Recurrent Neural Network (RNN),[[    0  7382 41937 44304  3658    36   500 20057    43     2]]
c7b6e6cb997de1660fd24d31759fe6bb21c7863f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
ed6a15f0f7fa4594e51d5bde21cc0c6c1bedbfdc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks","[[    0 15526     9 18043 17678 17357    12  9981 22098     8   475 46437
    347    32  1122     6    19   475 46437   347   608  2829  3007    15
    262    66     9   361  8558     2]]"
db72a78a7102b5f0e75a4d9e1a06a3c2e7aabb21,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Farasa BIBREF31, MADAMIRA BIBREF29, RDI (Rashwan et al., 2015), MIT (Belinkow and Glass, 2015), Microsoft ATKS BIBREF28","[[    0 20680  8810   163  8863 45935  2983     6 27557 17581  4396   163
   8863 45935  2890     6   248 10063    36   500  1671  6531  4400  1076
    482   570   238 20124    36 19877  4291  1722     8 10352     6   570
    238  3709 40831   104   163  8863 45935  2517     2]]"
8dc707a0daf7bff61a97d9d854283e65c0c85064,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
8bb0011ad1d63996d5650770f3be18abdd9f7fc6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
38e4aaeabf06a63a067b272f8950116733a7895c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a new tagging scheme consisting of three tags, namely { INLINEFORM0 }","[[    0   102    92 34694  3552 17402     9   130 19445     6 13953 25522
   2808 28302 38036   288 35524     2]]"
6263b2cba18207474786b303852d2f0d7068d4b6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)","[[    0 35007    36  5404 44765   238  1859    36 26795  2614   238  3453
     36 21067 10505   238 33830    36 41064 24309   238 11145    36 41715
   1584   238  1083    36 41384   238  2238    36 14696  5156   238     8
  24229    36 19877  8425    43     2]]"
12c50dea84f9a8845795fa8b8c1679328bd66246,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus","[[    0  6842  2571 41616     6   291   340 36378     6  6868 11028   112
  42168     2]]"
e025061e199b121f2ac8f3d9637d9bf987d65cd5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],average:15.5,[[    0 20365    35   996     4   245     2]]
ed6a15f0f7fa4594e51d5bde21cc0c6c1bedbfdc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],3,[[  0 246   2]]
1062a0506c3691a93bb914171c2701d2ae9621cb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"15 emotion types, sentiment classes, positive and negative, care, harm, fairness, cheating, loyalty, betrayal, authority, subversion, sanctity, and degradation, count of question marks, exclamation marks, consecutive characters and letters, links, hashtags, users' mentions, uppercase ratio, tweet length, words embeddings","[[    0   996 11926  3505     6  5702  4050     6  1313     8  2430     6
    575     6  4798     6 16890     6 14153     6 10177     6 26760     6
   3446     6  2849 21747     6 27600  1571     6     8 30223     6  3212
      9   864  4863     6  1931 25121  4863     6  3396  3768     8  5430
      6  5678     6 32795  8299     6  1434   108 19197     6  1717 39609
   3175  1750     6  3545  5933     6  1617 33183   417  1033     2]]"
04d1b3b41fb62a7b896afe55e0e8bc5ffb8c6e39,0.0,Entailment,[[    2     0 30495  3760  1757     2]],three annotators,[[    0  9983 45068  3629     2]]
49c32a2a64eb41381e5f12ccea4150cac9f3303d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"F-score, Kappa",[[    0   597    12 31673     6 37772     2]]
ea56148a8356a1918bedcf0a99ae667c27792cfe,0.0,Entailment,[[    2     0 30495  3760  1757     2]], FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) ,"[[    0   274  8041  1296   414    36  4006   530 22121    43     8     5
     80  3626 47234     9     5   944   487  6006   777 38559 12927 41616
     36   541   530 22121    43  1437     2]]"
584af673429c7f8621c6bf83362a37048daa0e5d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],we provide the decoder with the context of the whole sequence and the content of the current image (i.e. global and local information) to generate the corresponding text that will contribute to the overall story,"[[    0  1694   694     5  5044 15362    19     5  5377     9     5  1086
  13931     8     5  1383     9     5   595  2274    36   118     4   242
      4   720     8   400   335    43     7  5368     5 12337  2788    14
     40  5042     7     5  1374   527     2]]"
9122de265577e8f6b5160cd7d28be9e22da752b2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Figures FIGREF1 and FIGREF1 contain a summary of the papers addressing speculation detection and scope resolution,"[[    0 44105  4123 37365 45935   134     8 37365 45935   134  5585    10
   4819     9     5  6665  6477  6116 12673     8  7401  3547     2]]"
46c9e5f335b2927db995a55a18b7c7621fd3d051,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Thirteen different phenotypes are present in the dataset.,"[[    0 11329 30990   430 35833 43981    32  1455    11     5 41616     4
      2]]"
20e38438471266ce021817c6364f6a46d01564f2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds","[[    0  2401 17641  1626   349  1058  1246    19    10  2408    11 10301
      7 44612   134    12   642    43 47110     8    42  2408 42972  1022
     25  1058  6938     2]]"
ae7c93646aa5f3206cd759904965b4d484d12f83,0.0,Entailment,[[    2     0 30495  3760  1757     2]],absolute improvement of 18.2% over the Pointer-Gen baseline,"[[    0 41843  3855     9   504     4   176   207    81     5  6002  8007
     12 15887 18043     2]]"
34dc0838632d643f33c8dbfe7bd4b656586582a2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)","[[    0 13650    12   805  3864 18759    12 43942 12514  1421    36 20057
    238 14813 15362    12 15953 15362 18043    19 16181  1503    36 45780
     12 15953    43     2]]"
8e52637026bee9061f9558178eaec08279bf7ac6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],using the machine translation platform Apertium ,[[    0 10928     5  3563 19850  1761    83 11497  4031  1437     2]]
f0848e7a339da0828278f6803ed7990366c975f0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance","[[    0   104 20954     6   440    12 33683  7093  7012    36  4444    43
   2156 15690 12440  7093  7012     6  3861 10193     2]]"
b249b60a8c94d0e40d65f1ffdfcac527dab57516,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
00050f7365e317dc0487e282a4c33804b58b1fb3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
bb2de20ee5937da7e3e6230e942bec7b6e8f61ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],daily newspaper of the year 2015-2016,[[    0 16624  2924     9     5    76   570    12  9029     2]]
e82fa03f1638a8c59ceb62bb9a6b41b498950e1f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Two knowledge-based systems,
two traditional word expert supervised systems, six recent neural-based systems, and one BERT feature-based system.","[[    0  9058  2655    12   805  1743     6 50118  7109  2065  2136  3827
  20589  1743     6   411   485 26739    12   805  1743     6     8    65
    163 18854  1905    12   805   467     4     2]]"
e48e750743aef36529fbea4328b8253dbe928b4d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],WASSA-2017 Shared Task on Emotion Intensity,"[[    0   771  2336  3603    12  3789 38559 12927    15  3676 19187  7299
  40904     2]]"
b634ff1607ce5756655e61b9a6f18bc736f84c83,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Energy with accuracy of 0.538,[[    0 30189    19  8611     9   321     4 38425     2]]
443d2448136364235389039cbead07e80922ec5c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LSA, TextRank, LexRank and ILP-based summary.","[[    0   574  3603     6 14159 46052     6 14786 46052     8 11935   510
     12   805  4819     4     2]]"
f71b52e00e0be80c926f153b9fe0a06dd93af11e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"average content score across the paintings is 3.7, average creativity score is 3.9, average style score is 3.9","[[    0 20365  1383  1471   420     5 13013    16   155     4   406     6
    674 11140  1471    16   155     4   466     6   674  2496  1471    16
    155     4   466     2]]"
443d2448136364235389039cbead07e80922ec5c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LSA, TextRank, LexRank",[[    0   574  3603     6 14159 46052     6 14786 46052     2]]
d3dbb5c22ef204d85707d2d24284cc77fa816b6c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SAN Baseline, BNA, DocQA, R.M-Reader, R.M-Reader+Verifier and DocQA+ELMo","[[    0  9300  7093  7012     6   163  4444     6 19761  1864   250     6
    248     4   448    12 46347     6   248     4   448    12 46347  2744
  21119 24072     8 19761  1864   250  2744  3721 17357     2]]"
54e945ea4b014e11ed4e1e61abc2aa9e68fea310,0.0,Entailment,[[    2     0 30495  3760  1757     2]],seq2seq model with global attention gives the best results with an average target BLEU score of 29.65,"[[    0 47762   176 47762  1421    19   720  1503  2029     5   275   775
     19    41   674  1002   163  3850   791  1471     9  1132     4  3506
      2]]"
7ee5c45b127fb284a4a9e72bb9b980a602f7445a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],(c) previous best model trained on S-SQuAD BIBREF5 by using Dr.QA BIBREF20 ,"[[    0  1640   438    43   986   275  1421  5389    15   208    12   104
  12444  2606   163  8863 45935   245    30   634   925     4  1864   250
    163  8863 45935   844  1437     2]]"
435570723b37ee1f5898c1a34ef86a0b2e8701bb,0.0,Entailment,[[    2     0 30495  3760  1757     2]], English-Spanish MT system ,[[    0  2370    12 41226 14077   467  1437     2]]
7ab9c0b4ceca1c142ff068f85015a249b14282d0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"construct context-gloss pairs from all possible senses of the target word in WordNet, thus treating WSD task as a sentence-pair classification problem","[[    0 42843  5377    12  7210  5434 15029    31    70   678 24074     9
      5  1002  2136    11 15690 15721     6  4634  8959   305  6243  3685
     25    10  3645    12 44170 20257   936     2]]"
c17b609b0b090d7e8f99de1445be04f8f66367d4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Highest scores for ROUGE-1, ROUGE-2 and ROUGE-L on CNN/DailyMail test set are 43.85, 20.34 and 39.90 respectively; on the XSum test set 38.81, 16.50 and 31.27 and on the NYT test set 49.02, 31.02 and 45.55","[[    0 47296 20921  4391    13   248  5061  8800    12   134     6   248
   5061  8800    12   176     8   248  5061  8800    12   574    15  3480
     73 26339 23969  1296   278    32  3557     4  4531     6   291     4
   3079     8  3191     4  3248  4067   131    15     5  1577 38182  1296
    278  2843     4  6668     6   545     4  1096     8  1105     4  2518
      8    15     5 36319  1296   278  2766     4  4197     6  1105     4
   4197     8  2248     4  3118     2]]"
d5a8fd8bb48dd1f75927e874bdea582b4732a0cd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
2ea4347f1992b0b3958c4844681ff0fe4d0dd1dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Embedding Layer, Neural Network Layers, Loss Function, Metrics","[[    0 42578 13093   154 43262     6 44304  3658   226 24950     6 19700
  42419     6  4369 18715     2]]"
7b4fb6da74e6bd1baea556788a02969134cf0800,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
0a5ffe4697913a57fda1fd5a188cd5ed59bdc5c7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish","[[    0 40028   571  9063     6 27248     6  9096     6 13501     6  2370
      6  1515     6  1859     6 12852     6  3108     6 11814     6 27775
      6 11145     6 13053     6 37650   811     6  3453  2156  9004     2]]"
e0122fc7b0143d5cbcda2120be87a012fb987627,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)","[[    0   627  1503  1421     6  8909  4629   250     6    67  9980 33334
      5   275  2210   557   775    36   771   591   321     4 33636     7
    321     4 37199    43     2]]"
db72a78a7102b5f0e75a4d9e1a06a3c2e7aabb21,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Farasa, RDI",[[    0 20680  8810     6   248 10063     2]]
8df35c24af9efc3348d3b8d746df116480dfe661,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
27275fe9f6a9004639f9ac33c3a5767fea388a98,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Dimension size, window size, architecture, algorithm, epochs, hidden dimension size, learning rate, loss function, optimizer algorithm.","[[    0 45137 17699  1836     6  2931  1836     6  9437     6 17194     6
  43660    29     6  7397 21026  1836     6  2239   731     6   872  5043
      6 17775  6315 17194     4     2]]"
59e58c6fc63cf5b54b632462465bfbd85b1bf3dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"It does not use a seed list to gather tweets so the dataset does not skew to specific topics, dialect, targets.","[[    0   243   473    45   304    10  5018   889     7  7365  6245    98
      5 41616   473    45 36292     7  2167  7614     6 37508     6  3247
      4     2]]"
3fff37b9f68697d080dbd9d9008a63907137644e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
bb3267c3f0a12d8014d51105de5d81686afe5f1b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CoNLL-YAGO, TAC2010, ACE2004, AQUAINT, WW","[[    0  8739   487  6006    12   975  3450   673     6   255  2562 24789
      6 36211 34972     6    83 15513   250 17831     6 15584     2]]"
05c49b9f84772e6df41f530d86c1f7a1da6aa489,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The current implementation of Macaw supports a command line interface as well as mobile, desktop, and web apps.","[[    0   133   595  5574     9  1775  1584  4548    10  5936   516 12332
     25   157    25  1830     6 14050     6     8  3748  3798     4     2]]"
62ea141d0fb342dfb97c69b49d1c978665b93b3c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"spelling, word order and grammatical errors",[[    0 18462  7633     6  2136   645     8 25187 45816  9126     2]]
bfbd6040cb95b179118557352e8e3899ef25c525,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
9ca447c8959a693a3f7bdd0a2c516f4b86f95718,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
97d1ac71eed13d4f51f29aac0e1a554007907df8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically","[[    0   387  2399  1421    33    10  4532  5933     9 29600   131    52
   6647    42 22830    30  1271    55   737 33183   417  1033     6    52
  27545  6731   646  3998    29   742 22121    23     5   386     9   349
   3645     6     8   349   646  3998    29   742  7648 22671  1575    13
      5  3645 25029    24     6  3780 30464    32  2435 45007  3435     2]]"
0682bf049f96fa603d50f0fdad0b79a5c55f6c97,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
7d483077ed7f2f504d59f4fc2f162741fa5ac23b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
7e62a53823aba08bc26b2812db016f5ce6159565,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"IITB English-Hindi parallel corpus BIBREF22, ILCI English-Hindi parallel corpus","[[    0   100  2068   387  2370    12   725  2028   118 12980 42168   163
   8863 45935  2036     6    38  6447   100  2370    12   725  2028   118
  12980 42168     2]]"
545e92833b0ad4ba32eac5997edecf97a366a244,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Removing the master node deteriorates performance across all datasets,"[[    0 31157 13871     5  4710 37908 26694  1626   819   420    70 42532
      2]]"
3321d8d0e190d25958e5bfe0f3438b5c2ba80fd1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],from 3rd to 9th grade science questions collected from 12 US states,"[[   0 7761  155 2586    7  361  212 4978 2866 1142 4786   31  316  382
   982    2]]"
f225a9f923e4cdd836dd8fe097848da06ec3e0cc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SQuAD,[[    0   104 12444  2606     2]]
9ec1f88ceec84a10dc070ba70e90a792fba8ce71,0.0,Entailment,[[    2     0 30495  3760  1757     2]],0.6103,[[    0   288     4   401 18159     2]]
96a4091f681872e6d98d0efee777d9e820cb8dae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters,"[[    0 30383 47760 31681   147  3545  8200 26401  1617    32 45068  1070
     25  4157    50  2555   396   143 37163    59     5   592  5377     9
   3545   268     2]]"
f7a89b9cd2792f23f2cb43d50a01b8218a6fbb24,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"PER, LOC, ORG, MISC",[[    0 21260     6 27560     6  6532   534     6   256 32953     2]]
709feae853ec0362d4e883db8af41620da0677fe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters,"[[    0   642  4113 39879  2485     7 12142  3768     8 28052    10   400
   1825  1291   227     5  3768    25    10  4460 10160 42472  2408 12483
      5  2408 12438    15     5  4472   227  3768     2]]"
bd5bd1765362c2d972a762ca12675108754aa437,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 )., full paragraph-level neural network model achieves the best macro-average F1-score of 48.82% in predicting implicit discourse relations, which outperforms previous neural tensor network models (e.g., BIBREF18 ) by more than 2 percents and outperforms the best previous system BIBREF19 by 1 percent., Then we also created ensemble models by applying majority voting to combine results of ten runs. From table 5 , each ensemble model obtains performance improvements compared with single model. The full model achieves performance boosting of (51.84 - 48.82 = 3.02) and (94.17 - 93.21 = 0.96) in macro F1-scores for predicting implicit and explicit discourse relations respectively. ","[[    0   627  3280  1421  5167   205   819    13 16257 16045 19771  3115
     25   157     6    61    16 10451    19   986   275   898    36  6617
      4  2546   207 12303   274   134    12 31673     8  8060     4  3546
    207  8611    25   431    11   163  8863 45935  1225 32801     6   455
  17818    12  4483 26739  1546  1421 35499     5   275 12303    12 20365
    274   134    12 31673     9  2929     4  6551   207    11 15924 33583
  19771  3115     6    61  9980 33334   986 26739  7281   368  1546  3092
     36   242     4   571   482   163  8863 45935  1366  4839    30    55
     87   132   228   438  4189     8  9980 33334     5   275   986   467
    163  8863 45935  1646    30   112   135   482  1892    52    67  1412
  12547  3092    30  9889  1647  3434     7  9637   775     9  2724  1237
      4  1740  2103   195  2156   349 12547  1421  6168 30759   819  5139
   1118    19   881  1421     4    20   455  1421 35499   819 11606     9
     36  4708     4  6232   111  2929     4  6551  5457   155     4  4197
     43     8    36  6405     4  1360   111  8060     4  2146  5457   321
      4  5607    43    11 12303   274   134    12  3866  4765    13 15924
  33583     8 16045 19771  3115  4067     4  1437     2]]"
54830abe73fef4e629a36866ceeeca10214bd2c8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter","[[    0   627   226  3134  8369     7  6492  1743     8   576     5  3585
      9   557     6    52    33  8069   485  3444  7201    15    42  2087
      8  2633    10   629 38217     9  6492  1743   716    15   226  3134
      9     5   485   557     6    52 15423  3703 17314     8 15584   771
  14041  7201    31   211  7976   510   998     8   341     5 21630 20424
  17194    25    41 10437 43797     2]]"
4c18081ae3b676cc7831403d11bc070c10120f8e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],simple clustering algorithm which uses the cosine similarity between word embeddings,"[[    0 41918 46644  2961 17194    61  2939     5 12793   833 37015   227
   2136 33183   417  1033     2]]"
6d1217b3d9cfb04be7fcd2238666fa02855ce9c5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21","[[    0 37426   574  4014   448  5383   387 45935  1570     6  6479   574
   4014   448  2744 16256  5383   387 45935   844     6  6479   574  4014
    448  2744  9822 34823   387 45935   134     6  6479   574  4014   448
   2744 16256  2744  9822 34823   387 45935   176     6  3480  1421  5383
    387 45935   288     8  8607  4307   597  1421  5383   387 45935  2146
      2]]"
af34051bf3e628c1e2a00b110bb84e5f018b419f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation","[[    0 34230  4699  4062 18043     6  9689 15362  1198    12 32530     6
     11    61     5  4062  9689 15362    16 49271    31    41  6015   500
   1421     6  5044 15362  1198    12 32530     6    11    61     5  4062
   5044 15362    16 49271    31    41 14077  1421     6  9689 15362    12
  11127 15362  1198    12 32530     6   147   258     5  9689 15362     8
   5044 15362    32  1198    12 23830     6   171    12   560    12 19827
   3228    12 45025  1421   147     5  9689  1630   268     8  5044  1630
    268    32 16934    31  1198    12 23830  6015   500     8 14077  3092
      6 28802  2744  5234    12 21714    35   163  8863 45935  1366   211
   7976   510    35 17075    73  2133 40054    73  4688  1988   281 15872
    347  1366  1850    10 29884  3228    12 45025  1860    13  1901 19850
      2]]"
6b367775a081f4d2423dc756c9b65b6eef350345,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
e374169ee10f835f660ab8403a5701114586f167,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"username, display name, profile image, location, description","[[    0 48852     6  2332   766     6  4392  2274     6  2259     6  8194
      2]]"
c49ee6ac4dc812ff84d255886fd5aff794f53c39,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
36ae003c7cb2a1bbfa90b89c671bc286bd3b3dfd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics","[[    0  2611 13183    32  3030    30  1050  5017     8    49 26328     9
      5  3768     6     8    32 34852    19  1050    12  3341 12720     2]]"
2e89ebd2e4008c67bb2413699589ee55f59c4f36,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
4ef11518b40cc55d86c485f14e24732123b0d907,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4","[[    0   597 16972     6 25408 20954     6  8248   597  8110   163  8863
  45935   245     6  6003 16197  1588   163  8863 45935   246    43     8
  39223   347   163  8863 45935   306     2]]"
97d0f9a1540a48e0b4d30d7084a8c524dd09a4c3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Chunks is group of tweets from single account that  is consecutive in time - idea is that this group can show secret intention of malicious accounts.,"[[    0  4771 29201    16   333     9  6245    31   881  1316    14  1437
     16  3396    11    86   111  1114    16    14    42   333    64   311
   3556  6589     9 15237  2349     4     2]]"
fb5fb11e7d01b9f9efe3db3417b8faf4f8d6931f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Logistic regression, LSTM, End-to-end memory networks, Deep projective reader","[[    0 23345  5580 39974     6   226  4014   448     6  4680    12   560
     12  1397  3783  4836     6  8248   695  2088 10746     2]]"
37edc25e39515ffc2d92115d2fcd9e6ceb18898b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SVM INLINEFORM0, SVM INLINEFORM1, LR INLINEFORM2, MaxEnt","[[    0   104 20954  2808 28302 38036   288     6   208 20954  2808 28302
  38036   134     6 40815  2808 28302 38036   176     6  4471 30495     2]]"
2916bbdb95ef31ab26527ba67961cf5ec94d6afe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"8,275 sentences and 167,739 words in total","[[    0   398     6 21138 11305     8 28833     6   406  3416  1617    11
    746     2]]"
e8a32460fba149003566969f92ab5dd94a8754a4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Our first model is the Concept Raw Context model (CRC) which utilizes concept mentions in a large scale KB to jointly learn embeddings of both words and concepts. Our second model is the Concept-Concept Context model (3C) which learns the embeddings of concepts from their conceptual contexts (i.e., contexts containing surrounding concepts only)","[[    0  2522    78  1421    16     5 32253  8214 43885  1421    36   347
   5199    43    61 33778  4286 19197    11    10   739  3189 29006     7
  13521  1532 33183   417  1033     9   258  1617     8 14198     4  1541
    200  1421    16     5 32253    12  9157 16771 43885  1421    36   246
    347    43    61 25269     5 33183   417  1033     9 14198    31    49
  28647 38270    36   118     4   242   482 38270  8200  3817 14198   129
     43     2]]"
a74190189a6ced2a2d5b781e445e36f4e527e82a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By evaluating the performance of the approach using accuracy and AUC,"[[    0  2765 15190     5   819     9     5  1548   634  8611     8    83
  12945     2]]"
ceb767e33fde4b927e730f893db5ece947ffb0d8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Demographics Age, DiagnosisHistory, MedicationHistory, ProcedureHistory, Symptoms/Signs, Vitals/Labs, Procedures/Results, Meds/Treatments, Movement, Other.","[[    0 24658 43685  8927     6 23465 13310 38261     6  5066 14086 38261
      6 40209 38261     6 32393    73 25675    29     6   468 19196    73
    574 10155     6 44232    73 41981     6  5066    29    73   565 12353
   2963     6 11753     6  1944     4     2]]"
37a79be0148e1751ffb2daabe4c8ec6680036106,0.0,Entailment,[[    2     0 30495  3760  1757     2]],anti-nuclear-power,[[    0 10865    12 31242    12 11017     2]]
a778b8204a415b295f73b93623d09599f242f202,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.,"[[    0 45134 11580   208  4291  5448  2939    10 34751  5043     7  5456
    414 44493     7    10   980   147 26956 10875    16   678     4     2]]"
c0e341c4d2253eb42c8840381b082aae274eddad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],hierarchical matching between questions and relations with residual learning,"[[    0   298   906 13161  3569  8150   227  1142     8  3115    19 30848
   2239     2]]"
53014cfb506f6fffb22577bf580ae6f4d5317ce5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22","[[    0   627  3480    73 26339 23969   340  5586 41616   163  8863 45935
   1978     6     5   188   469  1513   660  3654  1070 28556    36  9873
    565   131   163  8863 45935  1244   238  1577 38182   163  8863 45935
   2036     2]]"
d01c51155e4719bf587d114bcd403b273c77246f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018","[[    0 20556  3076 43072 28556     6    38  2068 28197 42168     6  2117
  23055    90 46913   199     2]]"
12f7fac818f0006cf33269c9eafd41bbb8979a48,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models","[[    0 42431  1421    16   716    15  2051    12 24641   154    41    96
  20900   468   246  1421   163  8863 45935   134    81  7133 19930  1033
      9  2339     6   150    84 46478  1421    16   716    15    10 44816
   4003   574  4014   448     4   166   617  9637     5    80    88    10
   2660  1421     4  2156 26739  1546  3092     2]]"
d087539e6a38c42f0a521ff2173ef42c0733878e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.","[[    0  5771    89    34    57  2210   173    15  4881   234 21992  1421
  32644 10070   163  8863 45935   996     6  7018 34775  7373  1395 16085
    209     6   187    51  2703     5  1294     8  3254  3092     7   458
      5   276 32644     8  4195   980     4   152 27301  4971    49   801
      7   617  1888  1421 10070     4     2]]"
c7b6e6cb997de1660fd24d31759fe6bb21c7863f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],1913 signals,[[   0 1646 1558 8724    2]]
a103636c8d1dbfa53341133aeb751ffec269415c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"majority baseline corresponds to a model's accuracy if it always predicts the majority class in the dataset, lexicon-based approach","[[    0 19517 18043 40189     7    10  1421    18  8611   114    24   460
  17876     5  1647  1380    11     5 41616     6 36912 17505    12   805
   1548     2]]"
7793805982354947ea9fc742411bec314a6998f6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Automatic,[[    0 37434 29177     2]]
e374169ee10f835f660ab8403a5701114586f167,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"username, display name, profile image, location and description","[[    0 48852     6  2332   766     6  4392  2274     6  2259     8  8194
      2]]"
66c96c297c2cffdf5013bab5e95b59101cb38655,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BERT remains only 0.3 F1-score points behind, and would have achieved the second position among all the MEDDOCAN shared task competitors. Taking into account that only 3% of the gold labels remain incorrectly annotated,  Table ","[[    0 11126   565  1189   129   321     4   246   274   134    12 31673
    332   639     6     8    74    33  4824     5   200   737   566    70
      5 22718 46570  1889  1373  3685  6117     4 10883    88  1316    14
    129   155   207     9     5  1637 14105  1091 27821 45068  1070     6
   1437  9513  1437     2]]"
2740e3d7d33173664c1c5ab292c7ec75ff6e0802,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the diacritized corpus that was used to train the RDI BIBREF7 diacritizer , WikiNews , a large collection of fully diacritized classical texts","[[    0   627  2269  1043  3961  1538 42168    14    21   341     7  2341
      5   248 10063   163  8863 45935   406  2269  1043  3961  6315  2156
  45569  5532  2156    10   739  2783     9  1950  2269  1043  3961  1538
  15855 14301     2]]"
b85fc420eb2f77f6f14f375cc1fcc5155eb5c0a8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
e44a6bf67ce3fde0c6608b150030e44d87eb25e3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"abortion, gay rights, Obama, marijuana",[[    0 27275     6  5100   659     6  1284     6  3140     2]]
6b7354d7d715bad83183296ce2f3ddf2357cb449,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BERT,[[    0 11126   565     2]]
f3c204723da53c7c8ef4dc1018ffbee545e81056,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
fc4ae12576ea3a85ea6d150b46938890d63a7d18,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
b21bc09193699dc9cfad523f3d5542b0b2ff1b8e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],MLP,[[    0 10537   510     2]]
55569d0a4586d20c01268a80a7e31a17a18198e2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we simply adopted the official training script of BERT, with default hyperparameters, to fine-tune each model until training loss converged","[[    0  1694  1622  5091     5   781  1058  8543     9   163 18854     6
     19  6814  8944 46669 20413     6     7  2051    12    90  4438   349
   1421   454  1058   872 25111  4462     2]]"
565189b672efee01d22f4fc6b73cd5287b2ee72c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)","[[    0 21941  5489   462 42168  2156   305 11674    92   620   990   777
      6   491    12 45331  1766    12  1225     6 28274    31   305 11674
    777     6 19268    12  4154     6  1281    12 24751 22799     6 20632
      6  9732    12   347 33889    36   771 11674   193    43     2]]"
71e4ba4e87e6596aeca187127c0d088df6570c57,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
0ad4359e3e7e5e5f261c2668fe84c12bc762b3b8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks","[[    0 46503  3907 16697 26739  4836   217 21740     9 11077    12   574
   4014   448     6 11077    12   805  3480     6   248 16966   487     6
      8   786    12 21512  3092   217 21740     9   226  4014 13123     6
   3480    29     6 30848     6     8  1403    12  2611 19774   716  4836
      2]]"
6aa2a1e2e3666f2b2a1f282d4cbdd1ca325eb9de,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Book, Electronics, Beauty and Music each have 6000, IMDB 84919, Yelp 231163, Cell Phone 194792 and Baby 160792 labeled data.","[[    0 24751     6 12057     6 12625     8  3920   349    33 36300     6
   9206 10842   290  3414  1646     6 29730   883  1225  5449     6 13696
  12091 21868  6617     8 10517   545  3570  6617 16274   414     4     2]]"
5260cb56b7d127772425583c5c28958c37cb9bea,0.0,Entailment,[[    2     0 30495  3760  1757     2]],160,[[    0 13726     2]]
2c6b50877133a499502feb79a682f4023ddab63e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English,[[    0 35007     2]]
6a633811019e9323dc8549ad540550d27aa6d972,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
0f12dc077fe8e5b95ca9163cea1dd17195c96929,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"generated with the various combinations of INLINEFORM4 person INLINEFORM5 and INLINEFORM6 emotion word INLINEFORM7 values across the eleven templates, differ only in one word corresponding to gender or race","[[    0 21842    19     5  1337 21092     9  2808 28302 38036   306   621
   2808 28302 38036   245     8  2808 28302 38036   401 11926  2136  2808
  28302 38036   406  3266   420     5 19353 38577     6 10356   129    11
     65  2136 12337     7  3959    50  1015     2]]"
0af16b164db20d8569df4ce688d5a62c861ace0b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN","[[    0   387  4581    12 33919     6   163  4581    12 30455     4 35690
   2688   597    12 30455     6 14159 16256     6   230    12 39645 16256
      2]]"
c497e8701060583d91bb64b9f9202d40047effc4,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," we look into the archived snapshots of two major news websites (CNN, NBC), and then extract the tweet blocks that are embedded in the news articles","[[    0    52   356    88     5 37240 40617     9    80   538   340  7656
     36 16256     6  3890   238     8   172 14660     5  3545  5491    14
     32 14224    11     5   340  7201     2]]"
b91671715ad4fad56c67c28ce6f29e180fe08595,0.0,Entailment,[[    2     0 30495  3760  1757     2]],None,[[    0 29802     2]]
ce6a3ca102a5ee62e86fc7def3b20b1f10d1eb25,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
88ab7811662157680144ed3fdd00939e36552672,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state, to leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-space","[[    0   102  5448    14 36959 17539  8476  3204  2258    11  2788    12
  34732   634     5  1374  7970  3491     8     5  2655 20992   194     6
      7  6270  2655 36386     7  1477  2210  6942 16964    13  4098    19
  14960 19936  2617   814    12 25414     2]]"
8b3d3953454c88bde88181897a7a2c0c8dd87e23,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Skip–gram, CBOW",[[    0 28056  2383 28526     6  6933  4581     2]]
7595260c5747aede0b32b7414e13899869209506,0.0,Entailment,[[    2     0 30495  3760  1757     2]],IMDb,[[    0  3755 42198     2]]
feb448860918ef5b905bb25d7b855ba389117c1f,0.0,Entailment,[[    2     0 30495  3760  1757     2]], $\textbf {All India Radio}$ news channel,"[[    0 49959 29015 36920 25522  3684   666  4611 24303  1629   340  4238
      2]]"
154a721ccc1d425688942e22e75af711b423e086,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Amazon Mechanical Turk,[[    0 25146 35644 19683     2]]
2df4a045a9cd7b44874340b6fdf9308d3c55327a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Classification system use n-grams, bag-of-words, common words and hashtags as features and SVM, random forest, extra tree and NB classifiers.","[[    0 21527  5000   467   304   295    12 28526    29     6  3298    12
   1116    12 30938     6  1537  1617     8 32795  8299    25  1575     8
    208 20954     6  9624  6693     6  1823  3907     8 28868  1380 27368
      4     2]]"
27dbbd63c86d6ca82f251d4f2f030ed3e88f58fa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"RNN-based NMT model, Transformer-NMT","[[    0   500 20057    12   805   234 11674  1421     6  5428 22098    12
    487 11674     2]]"
e587559f5ab6e42f7d981372ee34aebdc92b646e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"In case of read speech datasets,  their best model got the highest nov93 score of 16.1 and the highest nov92 score of 13.3.
In case of Conversational Speech, their best model got the highest SWB of 8.3 and the highest CHM of 19.3. ","[[    0  1121   403     9  1166  1901 42532     6  1437    49   275  1421
    300     5  1609   117   705  6478  1471     9   545     4   134     8
      5  1609   117   705  6617  1471     9   508     4   246     4 50118
   1121   403     9 38119  5033 27242     6    49   275  1421   300     5
   1609  8883   387     9   290     4   246     8     5  1609  3858   448
      9   753     4   246     4  1437     2]]"
b65a83a24fc66728451bb063cf6ec50134c8bfb0,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," finding the important sentences from the story, extracting the key information from those sentences using their AMR graphs","[[    0  2609     5   505 11305    31     5   527     6 37213     5   762
    335    31   167 11305   634    49  3326   500 36386     2]]"
78a5546e87d4d88e3d9638a0a8cd0b7debf1f09d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"conversational search, conversational question answering, conversational recommendation, conversational natural language interface to structured and semi-structured data","[[    0  3865  3697  5033  1707     6 28726  5033   864 15635     6 28726
   5033  6492     6 28726  5033  1632  2777 12332     7 16697     8  4126
     12 25384  4075   414     2]]"
da9c0637623885afaf023a319beee87898948fe9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
b80a3fbeb49a8968e149955bdcf199556478eeff,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
cb196725edc9cdb2c54b72364f3bbf7c76471490,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
82595ca5d11e541ed0c3353b41e8698af40a479b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Mentioning of political parties names and political twitter handles is the organic way to show political affiliation; adding Chowkidar or its variants to the profile is the inorganic way.,"[[    0   448 19774   154     9   559  1799  2523     8   559  7409 14617
     16     5  6523   169     7   311   559 23114   131  1271 19986 30704
    271    50    63 21740     7     5  4392    16     5    11 34098   169
      4     2]]"
f17ca24b135f9fe6bb25dc5084b13e1637ec7744,0.0,Entailment,[[    2     0 30495  3760  1757     2]],explicit discourse relations,[[    0 23242 17022 19771  3115     2]]
1b23c4535a6c10eb70bbc95313c465e4a547db5e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part, On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP)","[[    0  1694 12558    41  9689 15362    19   484 15380 23794   337 13171
    163  8863 45935  1570  1432    30   234  2444 13171   163  8863 45935
    996    25     5   795   233    11     5  9689 15362     8 13997   106
     19  1844  2311 43606   337   251   765    12  1279  3783    36 37426
     12   574  4014   448    43   163  8863 45935  1549    23     5   723
    233     6   374     5  5044 15362   526     6    52   304    10  2526
   1844   542   808 43606   337   226  4014   448    19   720  1503   163
   8863 45935  1558    14    16  9658    30    10  3228    12 39165 45332
   2839    36 10537   510    43     2]]"
8c852fc29bda014d28c3ee5b5a7e449ab9152d35,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)","[[    0 43871   208 20954     6  2311 43606   337  2597  7787    12 23036
     12 47184    36 37426   574  4014   448   238 30505 23794   337 44304
   3658    36 16256    43     2]]"
25c1c4a91f5dedd4e06d14121af3b5921db125e9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
4d28c99750095763c81bcd5544491a0ba51d9070,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Amitabh Bachchan, Ariana Grande, Barack Obama, Bill Gates, Donald Trump,
Ellen DeGeneres, J K Rowling, Jimmy Fallon, Justin Bieber, Kevin Durant, Kim Kardashian, Lady Gaga, LeBron James,Narendra Modi, Oprah Winfrey","[[    0   250  8974   873   298 13312 14717     6  6416  1113  9803     6
   4282  1284     6  1585 10090     6   807   140     6 50118 31870   225
    926 22648  1535     6   344   229 32290     6  5905 17501     6  3289
  16370     6  2363 11023     6  1636  7584     6  4645 16169     6  9517
    957     6   487  1322 36470  4698     6 20015  5711 16127     2]]"
7d2f812cb345bb3ab91eb8cbbdeefd4b58f65569,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (Whole Method and Results sections) Self-paced reading times widely benefit ERP prediction, while eye-tracking data seems to have more limited benefit to just the ELAN, LAN, and PNP ERP components.
Select:
- ELAN, LAN
- PNP ERP","[[    0 33683    19  1383  1716    35    36 14447  4104 16410     8 12499
   9042    43 12156    12 20764  2600   498  3924  1796 13895   510 16782
      6   150  2295    12 29978   414  1302     7    33    55  1804  1796
      7    95     5 17678  1889     6 32425     6     8   221 10949 13895
    510  6411     4 50118 45356    35 50118    12 17678  1889     6 32425
  50118    12   221 10949 13895   510     2]]"
ed67359889cf61fa11ee291d6c378cccf83d599d,0.0,Entailment,[[    2     0 30495  3760  1757     2]], pre-trained GloVe word vectors ,[[    0  1198    12 23830  4573   139 30660  2136 44493  1437     2]]
08b57deb237f15061e4029b6718f1393fa26acce,0.0,Entailment,[[    2     0 30495  3760  1757     2]],people in the US that use Amazon Mechanical Turk,[[    0 11970    11     5   382    14   304  1645 35644 19683     2]]
82595ca5d11e541ed0c3353b41e8698af40a479b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Organic: mention of political parties names in the profile attributes, specific mentions of political handles in the profile attributes.
Inorganic:  adding Chowkidar to the profile attributes, the effect of changing the profile attribute in accordance with Prime Minister's campaign, the addition of election campaign related keywords to the profile.","[[    0 37034   636    35  4521     9   559  1799  2523    11     5  4392
  16763     6  2167 19197     9   559 14617    11     5  4392 16763     4
  50118  1121 34098    35  1437  1271 19986 30704   271     7     5  4392
  16763     6     5  1683     9  2992     5  4392 21643    11 10753    19
   1489   692    18   637     6     5  1285     9   729   637  1330 32712
      7     5  4392     4     2]]"
68df324e5fa697baed25c761d0be4c528f7f5cf7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation","[[    0  8739    12 45963 22565     6 44489 17362     6  9944  1069 28017
   7192     6 33868 17362     2]]"
60ce4868af45753c9e124e64e518c32376f12694,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset","[[    0   134     6 31145 22422  3919  6245    11     5  8607 12169  8913
   5213 16673   281   594   163  8863 45935  3367     6    70   316     6
  38911  2216  2370 32795  8299     8    49  3059  6245    31     5   276
   8607 41616     2]]"
75df70ce7aa714ec4c6456d0c51f82a16227f2cb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)","[[    0   530  2279  2095     6 19840     6  5477 14801     6  2529   857
  23590     6 30244  3644     6  1437  2370     8  6331 12336   242    36
    179  2103     6  1716    11  2788    43     2]]"
c17b609b0b090d7e8f99de1445be04f8f66367d4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Best results on unigram:
CNN/Daily Mail: Rogue F1 43.85
NYT: Rogue Recall 49.02
XSum: Rogue F1 38.81","[[    0 19183   775    15   542  1023  4040    35 50118 16256    73 26339
   6313    35 29426   274   134  3557     4  4531 50118  9873   565    35
  29426 35109  2766     4  4197 50118  1000 38182    35 29426   274   134
   2843     4  6668     2]]"
03c967763e51ef2537793db7902e2c9c17e43e95,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Conditional Copy (CC) model ,[[    0 43597 24176 22279    36  3376    43  1421  1437     2]]
de3b1145cb4111ea2d4e113f816b537d052d9814,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," Wang et al. BIBREF21, paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets","[[    0  9705  4400  1076     4   163  8863 45935  2146     6  2225    30
    163  8863 45935  3103    11    61    51   341  4532 47382  1380 24072
     19  3298     9  1617  1421     7 36029  1337  3722 42532     2]]"
92dfacbbfa732ecea006e251be415a6f89fb4ec6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)","[[    0   487  8215   118 11991    36   329   922     6  3023  5410     6
    295  3662     6   579  4184   238   208  6157   139 11991    36   282
   2527     6   579  1242     6   326 22617    43     2]]"
64af7f5c109ed10eda4fb1b70ecda21e6d5b96c8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"an intentional and potentially multicast communication, “the expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends""","[[    0   260 18797     8  2905 43630  1988  4358     6    44    48   627
   8151     9    41  2979    50    41   814    30  2172    50  1134 12507
   1887     7  2712     5  5086    50     5  2163     9    97  2172    50
   1134    19  5135     7 41679  3587   113     2]]"
154a721ccc1d425688942e22e75af711b423e086,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Amazon Mechanical Turk,[[    0 25146 35644 19683     2]]
49764eee7fb523a6a28375cc699f5e0220b81766,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
dc1cec824507fc85ac1ba87882fe1e422ff6cffb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Dataset of total 3500 questions from the Internet and other sources such as books of general knowledge questions, history, etc.","[[    0 43703   281   594     9   746  1718   612  1142    31     5  3742
      8    97  1715   215    25  2799     9   937  2655  1142     6   750
      6  4753     4     2]]"
897ba53ef44f658c128125edd26abf605060fb13,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
8b3d3953454c88bde88181897a7a2c0c8dd87e23,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"integrated vector-res, vector-faith, Skip–gram, CBOW","[[    0 24894  8358 37681    12  1535     6 37681    12 32118     6  6783
   2383 28526     6  6933  4581     2]]"
8d074aabf4f51c8455618c5bf7689d3f62c4da1d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"lacks of complete review approaches, datasets and toolkits ","[[    0   462  3400     9  1498  1551  8369     6 42532     8  3944   330
   2629  1437     2]]"
9658b5ffb5c56e5a48a3fea0342ad8fc99741908,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
a7d020120a45c39bee624f65443e09b895c10533,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"newly acquired facts are retained in the KB and used in inference for future queries, and that the accumulated knowledge in addition to the updated KB including past inference performances are leveraged to guide future interaction and learning","[[    0  4651   352  3566  4905    32 12544    11     5 29006     8   341
     11 42752    13   499 22680     6     8    14     5 15323  2655    11
   1285     7     5  4752 29006   217   375 42752  4476    32 15178  4628
      7  4704   499 10405     8  2239     2]]"
1329280df5ee9e902b2742bde4a97bc3e6573ff3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
c598028815066089cc1e131b96d6966d2610467a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
709feae853ec0362d4e883db8af41620da0677fe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters","[[    0 32675 42472    12 43776   196 40535  1503    64    28  1602    25
     10  5043     7  5456 22680     8   762    12 19434 15029     7     5
   8985     9  8135     6 10160 42472    12 43776   196 40535  1503  3352
      7   582  1503     7     5 12142  3768     9   349  2452     8  2471
      5   400  1825  1291   227  3768    25    10  4190 10160 42472  2408
     13  1503     6 10160 42472  2408   129  5864    29    15     5  4472
    227  3768     2]]"
891c2001d6baaaf0da4e65b647402acac621a7d2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They use the first principal component of a word's contextualized representation in a given layer as its static embedding.,"[[    0  1213   304     5    78  5402  7681     9    10  2136    18 37617
   1538  8985    11    10   576 10490    25    63 25156 33183 11303     4
      2]]"
0ba3ea93eef5660a79ea3c26c6a270eac32dfa4c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
a253749e3b4c4f340778235f640ce694642a4555,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ESTER1, ESTER2, ETAPE, REPERE","[[    0  1723  7831   134     6 12936  2076   176     6  4799 38851     6
   4979   510 23142     2]]"
8dda1ef371933811e2a25a286529c31623cca0c6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],One,[[   0 3762    2]]
9eabb54c2408dac24f00f92cf1061258c7ea2e1a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"paragraphs, lines, Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation","[[    0 46845    29     6  2301     6  3522    15  2166  1842  2835  1258
     36  1990 22745    29   129   238 17818  2835  1258     6     8   516
   2835  1258     2]]"
fff5c24dca92bc7d5435a2600e6764f039551787,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
fd753ab5177d7bd27db0e0afc12411876ee607df,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly","[[    0   104  6447  3685    16    10   182  2007  7425  5580 39974  1380
  24072     6   274  6447  3685 17382 23645     8 38845    65     9     5
    504  7373 22422     2]]"
126ff22bfcc14a2f7e1a06a91ba7b646003e9cf0,0.0,Entailment,[[    2     0 30495  3760  1757     2]], MT system on the data released by BIBREF11,"[[    0 14077   467    15     5   414   703    30   163  8863 45935  1225
      2]]"
77c34f1033702278f7f044806c1eba0c6ecb8b04,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
9a05a5f4351db75da371f7ac12eb0b03607c4b87,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Europarl BIBREF31, MultiUN BIBREF32","[[    0 21941  5489   462   163  8863 45935  2983     6 19268  4154   163
   8863 45935  2881     2]]"
b6ae8e10c6a0d34c834f18f66ab730b670fb528c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"training data has posts from politics, business, science and other popular topics; the trained model is applied to millions of unannotated posts on all of Reddit","[[    0 32530   414    34  4570    31  2302     6   265     6  2866     8
     97  1406  7614   131     5  5389  1421    16  5049     7  2535     9
    542 37250  1070  4570    15    70     9  6844     2]]"
27275fe9f6a9004639f9ac33c3a5767fea388a98,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Hyperparameters explored were: dimension size, window size, architecture, algorithm and epochs.","[[    0 46572 46669 20413 16217    58    35 21026  1836     6  2931  1836
      6  9437     6 17194     8 43660    29     4     2]]"
d53299fac8c94bd0179968eb868506124af407d1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Table TABREF10,  The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set,  While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject","[[    0 41836   255  4546 45935   698     6  1437    20   229 20057  1380
  24072  2045     7  3008     5   275   420    70   237 12758     4   152
     16  1153   528     7     5  3228    12  4684  2574     9     5   414
    278     6  1437   616   209  1380 27368   222    45  3008  1605   157
      6    51   694    10   205  1158   477    13   499   173    15    42
   2087     2]]"
b9d168da5321a7d7b812c52bb102a05210fe45bd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
f7a89b9cd2792f23f2cb43d50a01b8218a6fbb24,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"PER, LOC, ORG, MISC",[[    0 21260     6 27560     6  6532   534     6   256 32953     2]]
c7eb71683f53ab7acffd691a36cad6edc7f5522e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
61a9ea36ddc37c60d1a51dabcfff9445a2225725,0.0,Entailment,[[    2     0 30495  3760  1757     2]], the news external references in Wikipedia,[[    0     5   340  6731 13115    11 28274     2]]
58ee0cbf1d8e3711c617b1cd3d7aca8620e26187,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer","[[    0 11321    52   109    45    33    41   253    12   560    12  1397
  41616     6     5  5129  2370 19340   189    45   173   157    19 16538
   2496  2937     2]]"
b249b60a8c94d0e40d65f1ffdfcac527dab57516,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
86bf75245358f17e35fc133e46a92439ac86d472,0.0,Entailment,[[    2     0 30495  3760  1757     2]],only modest gains on three of the four downstream tasks,[[    0  8338  6473  3077    15   130     9     5   237 18561  8558     2]]
bc67b91dd73acded2d52fd4fee732b7a9722ea8b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],It is a framework used to describe algorithms for neural networks represented as graphs. Main idea is that that representation of each vertex is updated based on messages from its neighbors.,"[[    0   243    16    10  7208   341     7  6190 16964    13 26739  4836
   4625    25 36386     4  4326  1114    16    14    14  8985     9   349
  49102    16  4752   716    15  3731    31    63  6611     4     2]]"
aa2948209cc33b071dbf294822e72bb136678345,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"AutoJudge consistently and significantly outperforms all the baselines, RC models achieve better performance than most text classification models (excluding GRU+Attention), Comparing with conventional RC models, AutoJudge achieves significant improvement","[[    0 40845 40145  6566     8  3625  9980 33334    70     5 11909 38630
      6 19682  3092  3042   357   819    87   144  2788 20257  3092    36
  34734  8837   791  2744 28062 19774   238 10081  5867    19  9164 19682
   3092     6  8229 40145 35499  1233  3855     2]]"
b66c9a4021b6c8529cac1a2b54dacd8ec79afa5f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"global (the whole document) and the local context (e.g., the section/topic) ","[[    0  9424    36   627  1086  3780    43     8     5   400  5377    36
    242     4   571   482     5  2810    73 45260    43  1437     2]]"
e587559f5ab6e42f7d981372ee34aebdc92b646e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"On WSJ datasets author's best approach achieves 9.3 and 6.9 WER compared to best results of 7.5 and 4.1 on nov93 and nov92 subsets.
On Hub5'00 datasets author's best approach achieves WER of 7.8 and 16.2 compared to best result of 7.3 and 14.2 on Switchboard (SWB) and Callhome (CHM) subsets.","[[    0  4148 18483   863 42532  2730    18   275  1548 35499   361     4
    246     8   231     4   466   305  2076  1118     7   275   775     9
    262     4   245     8   204     4   134    15   117   705  6478     8
    117   705  6617 18621  2580     4 50118  4148 12991   245   108   612
  42532  2730    18   275  1548 35499   305  2076     9   262     4   398
      8   545     4   176  1118     7   275   898     9   262     4   246
      8   501     4   176    15 11171  4929    36 11871   387    43     8
   3310  8361    36  3764   448    43 18621  2580     4     2]]"
2b3cac7af10d358d4081083962d03ea2798cf622,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a253749e3b4c4f340778235f640ce694642a4555,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ESTER1, ESTER2, ETAPE, REPERE","[[    0  1723  7831   134     6 12936  2076   176     6  4799 38851     6
   4979   510 23142     2]]"
0682bf049f96fa603d50f0fdad0b79a5c55f6c97,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d46c0ea1ba68c649cc64d2ebb6af20202a74a3c7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],It may lead to poor rare word representations and word analogies.,"[[    0   243   189   483     7  2129  3159  2136 30464     8  2136 26903
    918     4     2]]"
dd76130ec5fac477123fe8880472d03fbafddef6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ELIZA,  PARRY, A.L.I.C.E., Cleverbot","[[    0  3721 17045   250     6  1437 16083 16802     6    83     4   574
      4   100     4   347     4   717   482 40724 12749     2]]"
395b61d368e8766014aa960fde0192e4196bcb85,0.0,Entailment,[[    2     0 30495  3760  1757     2]],three datasets based on IMDB reviews and Yelp reviews,[[    0  9983 42532   716    15  9206 10842  6173     8 29730  6173     2]]
a6a48de63c1928238b37c2a01c924b852fe752f8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Lead-3 model,  Lead-1-AMR, BIBREF0 ","[[    0 32258    12   246  1421     6  1437 14243    12   134    12  2620
    500     6   163  8863 45935   288  1437     2]]"
8e44c02c2d9fa56fb74ace35ee70a5add50b52ae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a2015f02dfb376bf9b218d1c897018f4e70424d7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"45,000 scholarly articles, including over 33,000 with full text","[[    0  1898     6   151 40043  7201     6   217    81  2357     6   151
     19   455  2788     2]]"
6d4400f45bd97b812e946b8a682b018826e841f1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Looking for adjectives marking the noun ""baby"" and also looking for most-common adjectives related to certain nouns using POS-tagging","[[    0 17629    13 45272  3699 10032     5 44875    22 29126   113     8
     67   546    13   144    12 27278 45272  3699  1330     7  1402 44875
     29   634 29206    12 10058  3923     2]]"
b49598b05358117ab1471b8ebd0b042d2f04b2a4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"neural bag-of-words (NBOW) model, bidirectional long short-term memory network (LSTM), attention-based encoder","[[    0   858  9799  3298    12  1116    12 30938    36 20485  4581    43
   1421     6  2311 43606   337   251   765    12  1279  3783  1546    36
    574  4014   448   238  1503    12   805  9689 15362     2]]"
74b4779de437c697fe702e51f23e2b0538b0f631,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Spearman's INLINEFORM0 between phrasal similarities derived from our compositional functions and the human annotators,"[[    0 29235   271   397    18  2808 28302 38036   288   227 41473   281
    337 20097 16934    31    84 25206 24176  8047     8     5  1050 45068
   3629     2]]"
bc4dca3e1e83f3b4bbb53a31557fc5d8971603b2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Word Content (WC) probing task, Depth (Dep) and Top Constituent (TopC) (of the input sentence's constituent parse tree) probing tasks","[[    0 44051 12803    36 17314    43 24097  3685     6 43553    36 29774
     43     8  3107 11242 15998  1342    36 14323   347    43    36  1116
      5  8135  3645    18 31350 43756  3907    43 24097  8558     2]]"
0f12dc077fe8e5b95ca9163cea1dd17195c96929,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Sentences involving at least one race- or gender-associated word,  sentence  have to be short and grammatically simple,  sentence have to  include expressions of sentiment and emotion.","[[    0 35212  8457  3329    23   513    65  1015    12    50  3959    12
  38838  2136     6  1437  3645  1437    33     7    28   765     8 25187
  45236  2007     6  1437  3645    33     7  1437   680 17528     9  5702
      8 11926     4     2]]"
f03112b868b658c954db62fc64430bebbaa7d9e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
e3a2d8886f03e78ed5e138df870f48635875727e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They developed a classifier to find ironic sentences in twitter data,"[[    0  1213  2226    10  1380 24072     7   465 25553 11305    11  7409
    414     2]]"
b1bc9ae9d40e7065343c12f860a461c7c730a612,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ShapeWorldICE datasets: OneShape, MultiShapes, TwoShapes, MultiShapes, Count, and Ratio","[[    0 45336 10988  9292 42532    35   509 45336     6 19268  3609 22179
      6  1596  3609 22179     6 19268  3609 22179     6 12440     6     8
  20475     2]]"
e101e38efaa4b931f7dd75757caacdc945bb32b4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, Waseem et al. BIBREF10","[[    0   771  3175   991     8   289 35664   163  8863 45935   245     6
  10553  4400  1076     4   163  8863 45935   466     6   305  3175   991
   4400  1076     4   163  8863 45935   698     2]]"
c262d3d1c5a8b6fef6b594d5eee86bc2b09e3baf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
f8f4e4a50d2b3fbd193327e79ea32d8d057e1414,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Contributors record voice clips by reading from a bank of donated sentences.,"[[    0 47222   994   638  2236 16610    30  2600    31    10   827     9
   6652 11305     4     2]]"
73906462bd3415f23d6378590a5ba28709b17605,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the degree of lexical overlap between them, presence of negation words","[[    0   627  3093     9 36912  3569 27573   227   106     6  2621     9
  15183  1258  1617     2]]"
4dcf67b5e7bd1422e7e70c657f6eacccd8de06d3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
861187338c5ad445b9acddba8f2c7688785667b1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
2c59528b6bc5b5dc28a7b69b33594b274908cca6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"processes a sentence of words with misspelled characters, predicting the correct words at each step","[[    0 31931   293    10  3645     9  1617    19  2649 30514  3768     6
  15924     5  4577  1617    23   349  1149     2]]"
5eabfc6cc8aa8a99e6e42514ef9584569cb75dec,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
37f8c034a14c7b4d0ab2e0ed1b827cc0eaa71ac6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"accuracy, Labeled Attachment Scores (LAS)","[[    0  7904 45386     6  8250 13587  7279 26284 26341    36   574  2336
     43     2]]"
1038542243efe5ab3e65c89385e53c4831cd9981,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Diachronic Usage Relatedness (DURel) gold standard data set,"[[    0 29038  1488 18731 45131  3283  1825    36   495  2492   523    43
   1637  2526   414   278     2]]"
1ec152119cf756b16191b236c85522afeed11f59,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They plot the average cosine similarity between uniformly random words increases exponentially from layers 8 through 12.  
They plot the average self-similarity of uniformly randomly sampled words in each layer of BERT, ELMo, and GPT-2 and shown that the higher layer produces more context-specific embeddings.
They plot that word representations in a sentence become more context-specific in upper layers, they drift away from one another.","[[    0  1213  6197     5   674 12793   833 37015   227 41919  9624  1617
   3488 30413    31 13171   290   149   316     4  1437  1437 50118  1213
   6197     5   674  1403    12 42116  1571     9 41919 22422 36551  1617
     11   349 10490     9   163 18854     6 17678 17357     6     8   272
  10311    12   176     8  2343    14     5   723 10490  9108    55  5377
     12 14175 33183   417  1033     4 50118  1213  6197    14  2136 30464
     11    10  3645   555    55  5377    12 14175    11  2853 13171     6
     51 24144   409    31    65   277     4     2]]"
2a564b092916f2fabbfe893cf13de169945ef2e1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"there are 20,244 reviews divided into positive and negative with an average 39 words per review, each one having a star-rating score","[[    0  8585    32   291     6 30043  6173  6408    88  1313     8  2430
     19    41   674  3191  1617   228  1551     6   349    65   519    10
    999    12 21172  1471     2]]"
395b61d368e8766014aa960fde0192e4196bcb85,0.0,Entailment,[[    2     0 30495  3760  1757     2]],1 IMDB dataset and 2 Yelp datasets,[[    0   134  9206 10842 41616     8   132 29730 42532     2]]
5fa36dc8f7c4e65acb962fc484989d20b8fdaeec,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
0602a974a879e6eae223cdf048410b5a0111665e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"K-means, LEM BIBREF13, DPEMM BIBREF14","[[    0   530    12  1794  1253     6   226  5330   163  8863 45935  1558
      6 24931  5330   448   163  8863 45935  1570     2]]"
8910ee2236a497c92324bbbc77c596dba39efe46,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Sentiment analysis and paraphrase detection under adversarial attacks,"[[    0 35212  8913  1966     8 40127 34338 12673   223 37930 27774  1912
      2]]"
9225b651e0fed28d4b6261a9f6b443b52597e401,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"when using BT, cases where the source is shorter than the target are rarer; cases when they have the same length are more frequent, automatic word alignments between artificial sources tend to be more monotonic than when using natural sources","[[    0 14746   634 12482     6  1200   147     5  1300    16 10941    87
      5  1002    32  3159   338   131  1200    77    51    33     5   276
   5933    32    55  7690     6  8408  2136 12432  2963   227  7350  1715
   3805     7    28    55  6154  1242 10003    87    77   634  1632  1715
      2]]"
ce2b921e4442a21555d65d8ce4ef7e3bde931dfc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)","[[    0 28586    36 12997   238  1083    36  2070   238 19645    36   271
    238  1111    36 13808   238 19840    36  3592   238     8 16859    36
   6873    43     2]]"
5a81732d52f64e81f1f83e8fd3514251227efbc7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13","[[    0   260  2210     6 45068  1070   599 41616    14    21 11236   716
     15    10 44816  1421     9  6943    12  3368  5298   163  8863 45935
   1092  2156   163  8863 45935  1558     2]]"
a29c071065d26e5ee3c3bcd877e7f215c59d1d33,0.0,Entailment,[[    2     0 30495  3760  1757     2]], Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels,"[[    0 37828   397    18  7938 22792   227     5 12793   833    12 42116
   1571     9     5  3645 33183   417  1033     8     5  1637 14105     2]]"
7b4fb6da74e6bd1baea556788a02969134cf0800,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
71ba1b09bb03f5977d790d91702481cc406b3767,0.0,Entailment,[[    2     0 30495  3760  1757     2]],75.1% and 75.6% accuracy,[[   0 2545    4  134  207    8 3337    4  401  207 8611    2]]
3de0487276bb5961586acc6e9f82934ef8cb668c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MEDDOCAN, NUBes-PHI",[[    0 32653 46570  1889     6   234 12027   293    12  7561   100     2]]
c9305e5794b65b33399c22ac8e4e024f6b757a30,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For SLC task, the ""ltuorp"" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the ""newspeak"" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively).","[[    0  2709   208  6447  3685     6     5    22  7984   257 10782   113
    165  1437    34     5   275  4655  1437  1421    36   288     4  5449
   1922    73   288     4  2466  2517    73   288     4   401 33787    13
    274   134    73   510    73   500  1437  4067    43     8    13   274
   6447  3685     5    22  4651 39397   113   165  1437    34     5   275
   4655  1437  1421    36   288     4  1978  4652    73   288     4  2517
   5449    73   288     4   176 16295    13   274   134    73   510    73
    500  4067   322     2]]"
2c85865a65acd429508f50b5e4db9674813d67f2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"A sample from nurse-initiated telephone conversations for congestive heart failure patients undergoing telepmonitoring, post-discharge from the Health Management Unit at Changi General Hospital","[[    0   250  7728    31  9008    12   179  4933  1070  7377  5475    13
  29367  2088  1144  2988  1484 11793  8327   642 38575   154     6   618
     12  7779 15040    31     5  1309  1753  7545    23 13325   118  1292
   2392     2]]"
ce6a3ca102a5ee62e86fc7def3b20b1f10d1eb25,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
902b3123aec0f3a39319ffa9d05ab8e08a2eb567,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
1c997c268c68149ae6fb43d83ffcd53f0e7fe57e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Entities from a deep learning model are linked to the related entities from a knowledge base by a lookup.,"[[    0 30495  2192    31    10  1844  2239  1421    32  3307     7     5
   1330  8866    31    10  2655  1542    30    10 47938     4     2]]"
8ce11515634236165cdb06ba80b9a36a8b9099a2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d650101712e36594bd77b45930a990402a455222,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"build a new one, collect INLINEFORM0 cases from China Judgments Online","[[    0 23411    10    92    65     6  5555  2808 28302 38036   288  1200
     31   436 19691 30237  5855     2]]"
92d1a6df3041667dc662376938bc65527a5a1c3c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
e6583c60b13b87fc37af75ffc975e7e316d4f4e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)","[[    0   530 19142 19551   163  8863 45935  1360  2156 14092     9 27250
   1630   337   414    13  8931    12   805     6 14838     8 33508  1901
    194 12337     7   262 43676 14414    73  8628   890   873   636    36
   1589  4911 46253  1589   642  4911 46253  1589    90  4911 46253  1589
    417  4911 46253  1589   257   605 46253  1589   119 46253  1589   282
     73  4839    25   157    25   204  1617  1640 11632     6  4728     6
   1467     8 31021  1584    43     2]]"
14b74ad5a6f5b0506511c9b454e9c464371ef8c4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"De-En, Ja-En, Ro-En","[[    0 13365    12 16040     6  9982    12 16040     6  3830    12 16040
      2]]"
ee417fea65f9b1029455797671da0840c8c1abbe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
2275b0e195cd9cb25f50c5c570da97a4cce5dca8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Build a bilingual language model,   learn the target language specific parameters starting from a pretrained English LM , fine-tune both English and target model to obtain the bilingual LM.","[[    0 36590    10 33900  2777  1421     6  1437  1437  1532     5  1002
   2777  2167 17294  1158    31    10 11857 26492  2370 31160  2156  2051
     12    90  4438   258  2370     8  1002  1421     7  6925     5 33900
  31160     4     2]]"
0cfe0e33fbb100751fc0916001a5a19498ae8cb5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation.","[[    0  1213   304    80  2222 15380 23794   337     8 19220    12 10416
    154 13171    15    36   134    43    10  4069     9     5   314  5377
      6     5   314 10014     8     5  1692  5377   131     8    36   176
     43    10  4069     9     5  1692  5377     6     5   235 10014     8
      5   235  5377     4   252 10146 26511  1070     5    80   775    71
   3716   154     7   120     5    92  5377  8985     4     2]]"
4eaf9787f51cd7cdc45eb85cf223d752328c6ee4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],multilingual pronunciation corpus collected by deri2016grapheme,"[[    0 42527 41586 44919 42168  4786    30  1935   118  9029   571  8645
    700  1794     2]]"
52f8a3e3cd5d42126b5307adc740b71510a6bdf5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],ReviewQA's test set,[[    0 32773  1864   250    18  1296   278     2]]
7705dd04acedaefee30d8b2c9978537afb2040dc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Word-level Memory Neural Networks (MemNNs) proposed in Bordes et al. (2015),"[[    0 44051    12  4483 25940 44304 14641    36 35038 20057    29    43
   1850    11   163 47359  4400  1076     4    36 14420    43     2]]"
b08f88d1facefceb87e134ba2c1fa90035018e83,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
ef396a34436072cb3c40b0c9bc9179fee4a168ae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],text classification and text semantic matching,[[    0 29015 20257     8  2788 46195  8150     2]]
6cd25c637c6b772ce29e8ee81571e8694549c5ab,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WikiBio dataset,  introduce two new biography datasets, one in French and one in German","[[    0 46929 40790 41616     6  1437  6581    80    92 24613 42532     6
     65    11  1515     8    65    11  1859     2]]"
8eefa116e3c3d3db751423cc4095d1c4153d3a5f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"GENIA Corpus BIBREF3, CoNLL2003 BIBREF14, KORE50 BIBREF21 , ACE2004 BIBREF22 and MSNBC","[[    0 30965  2889 28556   163  8863 45935   246     6   944   487  6006
  35153   163  8863 45935  1570     6   229  7563  1096   163  8863 45935
   2146  2156 36211 34972   163  8863 45935  2036     8 20054     2]]"
bd6dc38a9ac8d329114172194b0820766458dacc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.
Select:
- ERP data collected and computed by Frank et al. (2015)
- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)","[[    0 33683    19  1383  1716    35    36 14447  4104 16410     8 12499
   9042    43    20  2270 41616    52   304    16     5 13895   510   414
   4786     8 43547    30  3848  4400  1076     4    36 14420   238     8
     52    67   304 19184   414    36 20668    12 29978   414     8  1403
     12 20764  2600   498    43    31  3848  4400  1076     4    36 10684
     43    61    58  4786    15     5   276   278     9 21000 11305     4
  50118 45356    35 50118    12 13895   510   414  4786     8 43547    30
   3848  4400  1076     4    36 14420    43 50118    12 19184   414    36
  20668    12 29978   414     8  1403    12 20764  2600   498    43    31
   3848  4400  1076     4    36 10684    43     2]]"
07b70b2b799b9efa630e8737df8b1dd1284f032c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a sample of  29,794 wikipedia articles and 2,794 arXiv papers ","[[    0   102  7728     9  1437  1132     6 40528 47764 47410  7201     8
    132     6 40528  4709  1000  1879  6665  1437     2]]"
a0543b4afda15ea47c1e623c7f00d4aaca045be0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
6024039bbd1118c5dab86c41cce1175d99f10a25,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Asian Scientific Paper Excerpt Corpus, NTCIR PatentMT Parallel Corpus ","[[    0 36496 17141 14479  3015 39406 28556     6   234  6078  5216 35720
  11674 43072 28556  1437     2]]"
186b7978ee33b563a37139adff1da7d51a60f581,0.0,Entailment,[[    2     0 30495  3760  1757     2]],closed test limits all the data for learning should not be beyond the given training set,"[[    0 25315  1296  4971    70     5   414    13  2239   197    45    28
   1684     5   576  1058   278     2]]"
4c07c33dfaf4f3e6db55e377da6fa69825d0ba15,0.0,Entailment,[[    2     0 30495  3760  1757     2]],300,[[   0 2965    2]]
aa7d327ef98f9f9847b447d4def04889b4508d7a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],INLINEFORM2 is queried for the “most informative” instance(s) INLINEFORM3,"[[    0  2444 28302 38036   176    16 31212  2550    13     5    44    48
   7877 29749    17    46  4327  1640    29    43  2808 28302 38036   246
      2]]"
1e775cf30784e6b1c2b573294a82e145a3f959bb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],english,[[    0 47120     2]]
f161e6d5aecf8fae3a26374dcb3e4e1b40530c95,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," simple lookup table embeddings learned from scratch, using high-performance contextual embeddings, which are ELMo BIBREF11, BERT BIBREF16 and ClinicalBERT BIBREF13","[[    0  2007 47938  2103 33183   417  1033  2435    31 12272     6   634
    239    12 15526 37617 33183   417  1033     6    61    32 17678 17357
    163  8863 45935  1225     6   163 18854   163  8863 45935  1549     8
  20868 11126   565   163  8863 45935  1558     2]]"
18942ab8c365955da3fd8fc901dfb1a3b65c1be1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],TripAdvisor,[[    0   565 10706  9167 27880     2]]
9a9d225f9ac35ed35ea02f554f6056af3b42471d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],patterns for generating all types of errors,[[    0 43106    29    13 10846    70  3505     9  9126     2]]
b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BIBREF19, BIBREF20",[[    0  5383   387 45935  1646     6   163  8863 45935   844     2]]
89497e93980ab6d8c34a6d95ebf8c1e1d98ba43f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d6ea7a30b0b61ae126b00b59d2a14fff2ef887bf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis.","[[    0 30597 10244 19199    35  1437     5 25083    36 17143  7006   994
      8  7762    43  1437    32  7154  1622 32771    50 42881 30340   131
   1437   634    10  1989 20257  3552   839  8997    61 18746    32  7097
      6   131  1582 25376     8   542 16101 25376  2239    32     5   144
   1537  8369     7  2239    31   414   131  1437     5  1933     9  2788
     14    52    32 27963    36   368 45068  1295     6    50 25776   238
   1169  8408    50 12769     6    64  2128    28   430    87    65    18
    507  1933     9  1966     4     2]]"
b3d01ac226ee979e188a4141877a6d2a5482de98,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"state-of-the-art models learn to exploit spurious statistical patterns in datasets, human annotators—be they seasoned NLP researchers or non-experts—might easily be able to construct examples that expose model brittleness","[[    0  4897    12  1116    12   627    12  2013  3092  1532     7 14958
  43310 17325  8117    11 42532     6  1050 45068  3629   578  1610    51
  19066   234 21992  2634    50   786    12 26786  1872   578 35457  2773
     28   441     7 12558  7721    14 13595  1421  5378  2582 44373     2]]"
7fb27d8d5a8bb351f97236a1f6dcd8b2613b16f1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],RoBERTa,[[    0 27110 11126 38495     2]]
32d99dcd8d46e2cda04a9a9fa0e6693d2349a7a9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,","[[    0   133   701  5043    13   143    65     9     5  1617     9  4286
   2136    12 36378    16 10639    30     5  7740     9    41 37480  1385
      7     5   701  5043     4   479  4028 33183 11303 37681 21026    16
     78  3059    19    10  4286     4   286    10  2136 11441     7   143
     65     9     5  2136    12 36378  4561   209 14198     6     5 10639
    701  1385 20145    41   712    13     5   923     9    42  2136    18
  33183 11303 37681 21026 12337     7     5  4286    14     5  1989  2136
  12918     7     6     2]]"
c22394a3fb0dbf2fc7d3a70ad6435803f5a16ebd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],ASR,[[   0 2336  500    2]]
ffde866b1203a01580eb33237a0bb9da71c75ecf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
876700622bd6811d903e65314ac75971bbe23dcc,0.0,Entailment,[[    2     0 30495  3760  1757     2]], SemEval-2016 “Sentiment Analysis in Twitter”,"[[    0 11202   717  6486    12  9029    44    48 35212  8913  5213    11
    599    17    46     2]]"
899ed05c460bf2aa0aa65101cad1986d4f622652,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"$3,209$ reviews about 553 different cars from 49 different car manufacturers","[[    0  1629   246     6 27434  1629  6173    59   195  4540   430  1677
     31  2766   430   512  4738     2]]"
0101ebfbaba75fd47868ad0c796ac44ebc19c566,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
6b6d498546f856ac20958f666fc3fd55811347e2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They use the embedding layer with a size 35 and embedding dimension of 300. They use a dense layer with 70 units and a dropout layer with a rate of 50%.,"[[    0  1213   304     5 33183 11303 10490    19    10  1836  1718     8
  33183 11303 21026     9  2993     4   252   304    10 19790 10490    19
   1510  2833     8    10  1874   995 10490    19    10   731     9   654
   2153     2]]"
3611a72f754de1e256fbd25b012197e1c24e8470,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
da9c0637623885afaf023a319beee87898948fe9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
50be4a737dc0951b35d139f51075011095d77f2a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"labelled features, which are words whose presence strongly indicates a specific class or topic","[[    0 31479 16349  1575     6    61    32  1617  1060  2621  5025  8711
     10  2167  1380    50  5674     2]]"
01209a3bead7c87bcdc628be2a5a26b41abde9d1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SNLI BIBREF22 and MultiNLI BIBREF23 datasets, Quora Question Pairs dataset BIBREF24, Stanford Sentiment Treebank (SST) BIBREF25","[[    0 12436 27049   163  8863 45935  2036     8 19268   487 27049   163
   8863 45935  1922 42532     6  3232  4330 15680   221 18022 41616   163
   8863 45935  1978     6  8607 12169  8913 11077  5760    36   104  4014
     43   163  8863 45935  1244     2]]"
c2cb6c4500d9e02fc9a1bdffd22c3df69655189f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
78a4ec72d76f0a736a4a01369a42b092922203b6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Friends, EmotionPush",[[    0 36042     6  3676 19187 46971     2]]
4d706ce5bde82caf40241f5b78338ea5ee5eb01e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CCG Supertagging CCGBank , PTB part-of-speech tagging, EWT part-of-speech tagging,
Chunking, Named Entity Recognition, Semantic Tagging, Grammar Error Detection, Preposition Supersense Role, Preposition Supersense Function, Event Factuality Detection","[[    0  3376   534  1582 10058  3923  9841  4377  3153  2156  7008   387
    233    12  1116    12 40511 34694     6   381 25982   233    12  1116
     12 40511 34694     6 50118  4771  6435   154     6 30436 46718 23288
   7469     6 11202 26970   255 12771     6 14171  3916 37943 38091     6
   5048 14413 17901   268  9401 21888     6  5048 14413 17901   268  9401
  42419     6 11373 18454 39416 38091     2]]"
6bbbb9933aab97ce2342200447c6322527427061,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By multiplying crowd-annotated document-emotion matrix with emotion-word matrix. ,"[[    0  2765 44013  2180    12 37250  1070  3780    12   991 19187 36173
     19 11926    12 14742 36173     4  1437     2]]"
3321d8d0e190d25958e5bfe0f3438b5c2ba80fd1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Used from  science exam questions of the Aristo Reasoning Challenge (ARC) corpus.,"[[    0 47640    31  1437  2866 10743  1142     9     5 37812   139 31613
    154 10045    36 11969    43 42168     4     2]]"
f0b2289cb887740f9255909018f400f028b1ef26,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"indirect, physical, sexual",[[    0  2028 38806     6  2166     6  1363     2]]
6cd8bad8a031ce6d802ded90f9754088e0c8d653,0.0,Entailment,[[    2     0 30495  3760  1757     2]],0.8% F1 better than the best state-of-the-art,"[[   0  288    4  398  207  274  134  357   87    5  275  194   12 1116
    12  627   12 2013    2]]"
784ce5a983c5f2cc95a2c60ce66f2a8a50f3636f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
d8627ba08b7342e473b8a2b560baa8cdbae3c7fd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
81d193672090295e687bc4f4ac1b7a9c76ea35df,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach","[[    0  3698     5  2136   176 25369 17194     6  1045   484   542 16101
  25376   865    12 29496  1575     6  5368  3780 44493     8  3993   106
     25  8135    88     5   323 37681  6271    36   104 20954    43  1548
      2]]"
b5e4866f0685299f1d7af267bbcc4afe2aab806f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],ilur.am,[[  0 718 710   4 424   2]]
78a5546e87d4d88e3d9638a0a8cd0b7debf1f09d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation","[[    0  8739    12 45963 22565     6 44489 17362     6  9944  1069 28017
   7192     6 33868 17362     2]]"
09a1173e971e0fcdbf2fbecb1b077158ab08f497,0.0,Entailment,[[    2     0 30495  3760  1757     2]],5 percent points.,[[  0 245 135 332   4   2]]
22815878083ebd2f9e08bc33a5e733063dac7a0f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Russian,[[    0 14836     2]]
dd53baf26dad3d74872f2d8956c9119a27269bd5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],held out from the simulated data,[[    0 11706    66    31     5 27361   414     2]]
1f07e837574519f2b696f3d6fa3230af0b931e5d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
c2e475adeddcdc4d637ef0d4f5065b6a9b299827,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU-4, NIST-4, ROUGE-4","[[    0 30876   791    12   306     6   234 11595    12   306     6   248
   5061  8800    12   306     2]]"
b7c3f3942a07c118e57130bc4c3ec4adc431d725,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
8db6f8714bda7f3781b4fbde5ebb3794f2a60cfe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
15e481e668114e4afe0c78eefb716ffe1646b494,0.0,Entailment,[[    2     0 30495  3760  1757     2]],generator network to capture the event-related patterns,[[    0 20557  2630  1546     7  5604     5   515    12  3368  8117     2]]
bdc91d1283a82226aeeb7a2f79dbbc57d3e84a1a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The average score improved by 1.4 points over the previous best result.,"[[   0  133  674 1471 2782   30  112    4  306  332   81    5  986  275
   898    4    2]]"
aa7d327ef98f9f9847b447d4def04889b4508d7a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset","[[    0   134     6  5987    12  4509    36  2808 28302 38036   176   112
      4   134   448 10960    43 35237 14286   196 41616     2]]"
b36f867fcda5ad62c46d23513369337352aa01d2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2","[[    0 44051 15721   163  8863 45935   288     6  3130 11070   163  8863
  45935   134     6   305   487  1366    36   102 37105     9 15690 15721
     43   163  8863 45935  1978  2156 13042   996   530    36   102 37105
      9  3130 11070    43   163  8863 45935   176     2]]"
955de9f7412ba98a0c91998919fa048d339b1d48,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SVM with linear kernel using bag-of-words features,"[[    0   104 20954    19 26956 34751   634  3298    12  1116    12 30938
   1575     2]]"
bc16ce6e9c61ae13d46970ebe6c4728a47f8f425,0.0,Entailment,[[    2     0 30495  3760  1757     2]],4.5 turns per dialog (8533 turns / 1900 dialogs),"[[    0   306     4   245  4072   228 25730    36  4531  3103  4072  1589
  23137 25730    29    43     2]]"
e3c9e4bc7bb93461856e1f4354f33010bc7d28d5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard","[[    0   104 20954    19 36912  3569  1575    11 10753    19   986  1364
    163  8863 45935  1549  2156   163  8863 45935  1360  2156   163  8863
  45935   134  2156   163  8863 45935   996  2156   163  8863 45935   306
      6  1503    12   805  5448   163  8863 45935   246     8    97  6448
     52 27907   505     6   103   160    12   627    12  8877 13491 19682
   3092     6   217   910    12  4135   163  8863 45935   245     8 42169
    250   163  8863 45935   401  2156    61    32     5   981  3092    15
    208 12444  2606   884  4929     2]]"
b1a068c1050e2bed12d5c9550c73e59cd5b1f78d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
35b3ce3a7499070e9b280f52e2cb0c29b0745380,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
a0543b4afda15ea47c1e623c7f00d4aaca045be0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
e838275bb0673fba0d67ac00e4307944a2c17be3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ascertain the distribution of: types of offensive language, genres where it is used, the dialects used, and the gender of users using such language","[[    0  8631 23801     5  3854     9    35  3505     9  2555  2777     6
  23409   147    24    16   341     6     5 37508    29   341     6     8
      5  3959     9  1434   634   215  2777     2]]"
787c4d4628eac00dbceb1c96020bff0090edca46,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"answer each question with either `yes', `rather yes', `rather no', or `no'., can supplement each answer with a comment of at most 500 characters","[[    0 27740   349   864    19  1169 22209 10932  3934 22209 37615  4420
   3934 22209 37615   117  3934    50 22209  2362   108   482    64 15981
    349  1948    19    10  1129     9    23   144  1764  3768     2]]"
ebf0d9f9260ed61cbfd79b962df3899d05f9ebfb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. ,"[[    0 46929 34647  1437  8572 23703  3645  1763     8  1437 45569 39012
  37353   262  5606  3645 15029     4  1437     2]]"
7b4992e2d26577246a16ac0d1efc995ab4695d24,0.0,Entailment,[[    2     0 30495  3760  1757     2]],error detection system by Rei2016,[[    0 44223 12673   467    30 43572  9029     2]]
2c7e94a65f5f532aa31d3e538dcab0468a43b264,0.0,Entailment,[[    2     0 30495  3760  1757     2]],manually ,[[    0   397 13851  1437     2]]
c34e80fbbfda0f1786d3b00e06cef5ada78a3f3c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
9193006f359c53eb937deff1248ee3317978e576,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," Reuters, BBCSport BIBREF30, Polarity BIBREF31, Subjectivity BIBREF32, MPQA BIBREF33, IMDB BIBREF34, TREC BIBREF35, SST-1 BIBREF36, SST-2 BIBREF36, Yelp2013 BIBREF26","[[    0  1201     6  3295 19451   163  8863 45935   541     6  6189 21528
    163  8863 45935  2983     6 36994  9866   163  8863 45935  2881     6
   3957  1864   250   163  8863 45935  3103     6  9206 10842   163  8863
  45935  3079     6   255 40698   163  8863 45935  2022     6   208  4014
     12   134   163  8863 45935  3367     6   208  4014    12   176   163
   8863 45935  3367     6 29730 10684   163  8863 45935  2481     2]]"
5c6fa86757410aee6f5a0762328637de03a569e9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],best model achieves 0.94 F1 score for Wikipedia and Twitter datasets and 0.95 F1 on Formspring dataset,"[[    0  7885  1421 35499   321     4  6405   274   134  1471    13 28274
      8   599 42532     8   321     4  4015   274   134    15  8575 36741
  41616     2]]"
e025061e199b121f2ac8f3d9637d9bf987d65cd5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],15.5,[[  0 996   4 245   2]]
feb448860918ef5b905bb25d7b855ba389117c1f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Through the All India Radio new channel where actors read news.,"[[    0 23803     5   404   666  4611    92  4238   147  5552  1166   340
      4     2]]"
5f60defb546f35d25a094ff34781cddd4119e400,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Using INLINEFORM0 and INLINEFORM1,[[    0 36949  2808 28302 38036   288     8  2808 28302 38036   134     2]]
a1ac2a152710335519c9a907eec60d9f468b19db,0.0,Entailment,[[    2     0 30495  3760  1757     2]],An output layer for each task,[[    0  4688  4195 10490    13   349  3685     2]]
565189b672efee01d22f4fc6b73cd5287b2ee72c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Europarl tests from 2006, 2007, 2008; WMT newstest 2014.","[[    0 21941  5489   462  3457    31  3503     6  3010     6  2266   131
    305 11674    92   620   990   777     4     2]]"
ab0fd94dfc291cf3e54e9b7a7f78b852ddc1a797,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BIBREF26 ,[[    0  5383   387 45935  2481  1437     2]]
06be47e2f50b902b05ebf1ff1c66051925f5c247,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
3e432d71512ffbd790a482c716e7079ee78ce732,0.0,Entailment,[[    2     0 30495  3760  1757     2]],a self-collected financial intents dataset in Portuguese,"[[    0   102  1403    12  9119 17970   613  6979  4189 41616    11 13053
      2]]"
a3bb9a936f61bafb509fa12ac0a61f91abcc5106,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ARC, TREC, GARD, MLBioMedLAT","[[    0 11969     6   255 40698     6   272 11250     6  9536  1020 21243
    574  2571     2]]"
2c85865a65acd429508f50b5e4db9674813d67f2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],recordings of nurse-initiated telephone conversations for congestive heart failure patients,"[[    0 14760  1033     9  9008    12   179  4933  1070  7377  5475    13
  29367  2088  1144  2988  1484     2]]"
c37f65c9f0d543a35c784263b79236ccf1c44fac,0.0,Entailment,[[    2     0 30495  3760  1757     2]],a Convolutional Neural Network (CNN),[[    0   102 30505 23794   337 44304  3658    36 16256    43     2]]
05c49b9f84772e6df41f530d86c1f7a1da6aa489,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"File IO, Standard IO, Telegram",[[    0  9966 38266     6  5787 38266     6 20681     2]]
5913930ce597513299e4b630df5e5153f3618038,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence,"[[    0   627  1503  3885    11     5  1850  9037  6608 28593  5428 22098
     64 34937    55     8    19   723  2123     2]]"
f5e6f43454332e0521a778db0b769481e23e7682,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"pivoting, pivoting$_{\rm m}$","[[    0   642  1879 12653     6 35071 12653  1629 49747 37457 22900   475
  24303  1629     2]]"
f428618ca9c017e0c9c2a23515dab30a7660f65f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Multi-Layer Perceptron, Naive Bayes Classifier, Support Vector Machine, Gradient Boosting Classifier, Stochastic Gradient Descent, K Nearest Neighbour, Random Forest","[[    0 46064    12 48159  2595 16771  2839     6  7300  2088  1501   293
   4210 24072     6  7737 40419 14969     6 24257  4843 25802   154  4210
  24072     6   312  4306 11599 24257  4843  4762  6342     6   229  3864
  18759 16853 22482     6 34638  5761     2]]"
3f856097be2246bde8244add838e83a2c793bd17,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The content relevance between the candidate summary and the human summary is evaluated using information retrieval - using the summaries as search queries and compare the overlaps of the retrieved results. ,"[[    0   133  1383 21623   227     5  1984  4819     8     5  1050  4819
     16 15423   634   335 43372   111   634     5 32933  5119    25  1707
  22680     8  8933     5 31669  7527     9     5 29164   775     4  1437
      2]]"
e0e379e546f1da9da874a2e90c79b41c60feb817,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"During training, the model is trained alternately with one mini-batch of labeled data and INLINEFORM0 mini-batches of unlabeled data.","[[    0 14229  1058     6     5  1421    16  5389 25935  7223    19    65
   7983    12 35001     9 16274   414     8  2808 28302 38036   288  7983
     12 12161  5559     9 35237 14286   196   414     4     2]]"
70a1b0f9f26f1b82c14783f1b76dfb5400444aa4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Their accuracy in word segmentation is about 94%-97%.,"[[    0 16837  8611    11  2136  2835  1258    16    59  8940 27868  6750
   2153     2]]"
bd419f4094186a5ce74ba6ac1622b24e29e553f4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],accuracy of 30.3% on single sentences and 0.3 on complete paragraphs,"[[    0  7904 45386     9   389     4   246   207    15   881 11305     8
    321     4   246    15  1498 36153     2]]"
8d4ac4afbf5b14f412171729ceb5e822afcfa3f4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
97d0f9a1540a48e0b4d30d7084a8c524dd09a4c3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],sequence of $s$ tweets,[[    0 46665     9    68    29  1629  6245     2]]
53aa07cc4cc4e7107789ae637dbda8c9f6c1e6aa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Assigning wrong words to a cluster, Splitting words across different clusters, sparse, giving low coverage","[[    0 26039 35780  1593  1617     7    10 18016     6 25873  9451  1617
    420   430 28255     6 28593     6  1311   614  1953     2]]"
0602a974a879e6eae223cdf048410b5a0111665e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"K-means, LEM, DPEMM","[[    0   530    12  1794  1253     6   226  5330     6 24931  5330   448
      2]]"
d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"offensive (OFF) and non-offensive (NOT), targeted (TIN) and untargeted (INT) insults, targets of insults and threats as individual (IND), group (GRP), and other (OTH)","[[    0 34361    36 32794    43     8   786    12 34361    36 37049   238
   3656    36   565  2444    43     8  7587 45358   196    36 17831    43
  22536     6  3247     9 22536     8  3455    25  1736    36 13796   238
    333    36 11621   510   238     8    97    36 34072    43     2]]"
67e9e147b2cab5ba43572ce8a17fc863690172f0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By involving humans for post-hoc evaluation of model's interpretability,"[[    0  2765  3329  5868    13   618    12   298  1975 10437     9  1421
     18 18107  4484     2]]"
a99fdd34422f4231442c220c97eafc26c76508dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
63488da6c7aff9e374561a24ba224e9ce7f65e40,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
493e971ee3f57a821ef1f67ef3cd47ade154e7c4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Bernoulli embeddings (b-emb) BIBREF1 , continuous bag-of-words (CBOW) BIBREF5 , Distributed Memory version of Paragraph Vector (PV-DM) BIBREF11 and the Global Vectors (GloVe) BIBREF6 model","[[    0 34048  1438 10054 33183   417  1033    36   428    12 20506    43
    163  8863 45935   134  2156 11152  3298    12  1116    12 30938    36
  25392  4581    43   163  8863 45935   245  2156 11281 18926 25940  1732
      9  2884 44947 40419    36   510   846    12 25652    43   163  8863
  45935  1225     8     5  1849   468  9041   994    36   534  4082 30660
     43   163  8863 45935   401  1421     2]]"
455d4ef8611f62b1361be4f6387b222858bb5e56,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The crowdsourcing platform CrowdFlower was used to obtain natural dialog data that prompted the user to paraphrase, explain, and/or answer a question from a Simple questions BIBREF7 dataset. The CrowdFlower users were restricted to English-speaking countries to avoid dialogs  with poor English.","[[    0   133  8817 27824  1761 24544 16197  8285    21   341     7  6925
   1632 25730   414    14  5256     5  3018     7 40127 34338     6  3922
      6     8    73   368  1948    10   864    31    10 21375  1142   163
   8863 45935   406 41616     4    20 24544 16197  8285  1434    58  9393
      7  2370    12 20369   749     7  1877 25730    29  1437    19  2129
   2370     4     2]]"
6b53e1f46ae4ba9b75117fc6e593abded89366be,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"NER model, CRF classifier trained with sklearn-crfsuite, classifier has been developed that consists of regular-expressions and dictionary look-up","[[    0 20196  1421     6  4307   597  1380 24072  5389    19  2972 38229
     12  8344   506  9228  1459     6  1380 24072    34    57  2226    14
  10726     9  1675    12 37752  2485     8 36451   356    12   658     2]]"
4f253dfced6a749bf57a1b4984dc962ce9550184,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
a1b3e2107302c5a993baafbe177684ae88d6f505,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ILPRL contains 548 sentences, OurNepali contains 3606 sentences","[[    0  3063  4454   574  6308   195  3818 11305     6  1541   487  2462
   3644  6308 10253   401 11305     2]]"
747b847d687f703cc20a87877c5b138f26ff137d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13, EN-DE section of the Europarl corpus BIBREF14","[[    0 35007    36  2796    43     8  1859    36 10089    43  9042     9
      5   944   487  6006  2338 42168   163  8863 45935  1558     6 13245
     12 10089  2810     9     5  5122  5489   462 42168   163  8863 45935
   1570     2]]"
ef3567ce7301b28e34377e7b62c4ec9b496f00bf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Groningen Meaning Bank,[[    0   534  2839 34264 35972   788     2]]
8756b7b9ff5e87e4efdf6c2f73a0512f05b5ae3f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16","[[    0   534  4082 30660     6  9652 33183   417  1033   163  8863 45935
   1570     6  3676 16873 33183   417  1033   163  8863 45935  1549     2]]"
d27438b11bc70e706431dda0af2b1c0b0d209f96,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
ef396a34436072cb3c40b0c9bc9179fee4a168ae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],text classification and text semantic matching,[[    0 29015 20257     8  2788 46195  8150     2]]
0d9fcc715dee0ec85132b3f4a730d7687b6a06f4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The expected number of unique outputs a word recognition system assigns to a set of adversarial perturbations ,"[[    0   133   421   346     9  2216 39512    10  2136  4972   467 42091
      7    10   278     9 37930 27774 32819 13157  1635  1437     2]]"
5260cb56b7d127772425583c5c28958c37cb9bea,0.0,Entailment,[[    2     0 30495  3760  1757     2]],two previous turns,[[   0 7109  986 4072    2]]
1269c5d8f61e821ee0029080c5ba2500421d5fa6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Back Translation, Mix-Source Approach",[[    0 19085 41737     6 16038    12  7061 38370     2]]
f7ed3b9ed469ed34f46acde86b8a066c52ecf430,0.0,Entailment,[[    2     0 30495  3760  1757     2]],weighted factorization of a word-context co-occurrence matrix ,"[[    0  4301   196  3724  1938     9    10  2136    12 46796  1029    12
  23462 30904 36173  1437     2]]"
c9b8d3858c112859eabee54248b874331c48f71b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
02428a8fec9788f6dc3a86b5d5f3aa679935678d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Low sensitivity to bias in prior knowledge,[[    0 32758 15608     7  9415    11  2052  2655     2]]
bc01853512eb3c11528e33003ceb233d7c1d7038,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Adversarial misspellings are a real-world problem,"[[    0  9167  3697 27774  2649 26827  1033    32    10   588    12  8331
    936     2]]"
4986f420884f917d1f60d3cea04dc8e64d3b5bf1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],crosslingual latent variables,[[    0 15329  1527  5564 42715 25083     2]]
425bd2ccfd95ead91d8f2b1b1c8ab9fc3446cb82,0.0,Entailment,[[    2     0 30495  3760  1757     2]],standard accuracy metric,[[    0 21993  8611 14823     2]]
c4628d965983934d7a2a9797a2de6a411629d5bc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],There is nothing specific about the approach that depends on medical recommendations. The approach combines graph data and text data into a single embedding.,"[[    0   970    16  1085  2167    59     5  1548    14  7971    15  1131
   4664     4    20  1548 15678 20992   414     8  2788   414    88    10
    881 33183 11303     4     2]]"
9a05a5f4351db75da371f7ac12eb0b03607c4b87,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Europarl, MultiUN",[[    0 21941  5489   462     6 19268  4154     2]]
ee2c9bc24d70daa0c87e38e0558e09ab97feb4f2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
14b74ad5a6f5b0506511c9b454e9c464371ef8c4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"De-En, Ja-En, Ro-En","[[    0 13365    12 16040     6  9982    12 16040     6  3830    12 16040
      2]]"
74e866137b3452ec50fb6feaf5753c8637459e62,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"following the pyramid framework, we design an annotation scheme","[[    0 28481   154     5 33344  7208     6    52  1521    41 47760  3552
      2]]"
c00ce1e3be14610fb4e1f0614005911bb5ff0302,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Activation function is hyperparameter. Possible values: relu, selu, tanh.","[[    0 43034  1258  5043    16  8944 46669  5906     4 32104  3266    35
   6258   257     6   842  6487     6 15149   298     4     2]]"
02428a8fec9788f6dc3a86b5d5f3aa679935678d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced","[[    0  4484     7 12775 36029 14301   190    77     5  1280     9  2052
   2655    13   430  4050    16   542 29683     6     8    77     5  1380
   3854     9     5 41616    16   542 29683     2]]"
30af1926559079f59b0df055da76a3a34df8336f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Android application,[[    0 42375  2502     2]]
6c96e910bd98c9fd58ba2050f99b9c9bac69840a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
c88a846197b72d25e04ec55f00ee3e72f655504c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],corpus of state speeches delivered during the annual UN General Debate,"[[    0  7215   642   687     9   194 13467  2781   148     5  1013  2604
   1292 36743     2]]"
f318a2851d7061f05a5b32b94251f943480fbd15,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"both corpuses used words that aim to inspire readers while avoiding fear, actual words that lead to these effects are very different in the two contexts, our findings indicate that, using proper methods, automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda","[[    0 17143 44086  9764   341  1617    14  4374     7  9769  5360   150
  11473  2490     6  3031  1617    14   483     7   209  3038    32   182
    430    11     5    80 38270     6    84  4139  6364    14     6   634
   4692  6448     6 11554  1966     9   739  3738     9 46478   414    64
    694  5808  8339  8339    88 14463 10934     2]]"
1170e4ee76fa202cabac9f621e8fbeb4a6c5f094,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
72f7ef55e150e16dcf97fe443aff9971a32414ef,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"+1.86 in terms of F1 score on CTB5, +1.80 on CTB6, +2.19 on UD1.4","[[    0  2744   134     4  5334    11  1110     9   274   134  1471    15
  12464   387   245     6  2055   134     4  2940    15 12464   387   401
      6  2055   176     4  1646    15 33846   134     4   306     2]]"
45a5961a4e1d1c22874c4918e5c98bd3c0a670b3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],seven ,[[    0 17723  1437     2]]
e6583c60b13b87fc37af75ffc975e7e316d4f4e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)","[[    0   406 43676 14414    73  8628   890   873   636    36  1589  4911
  46253  1589   642  4911 46253  1589    90  4911 46253  1589   417  4911
  46253  1589   257   605 46253  1589   119 46253  1589   282    73  4839
     25   157    25   204  1617  1640 11632     6  4728     6  1467     8
  31021  1584    43     2]]"
435570723b37ee1f5898c1a34ef86a0b2e8701bb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"hierarchical phrase-based system BIBREF29, appropriate additional baseline would be to mark translation rules with these indicator functions but without the scores, akin to identifying rules with phrases in them (Baseline + SegOn)","[[    0   298   906 13161  3569 11054    12   805   467   163  8863 45935
   2890     6  3901   943 18043    74    28     7  2458 19850  1492    19
    209  9189  8047    53   396     5  4391     6 21468     7  9397  1492
     19 22810    11   106    36 40258  7012  2055 17324  4148    43     2]]"
2a3e36c220e7b47c1b652511a4fdd7238a74a68f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],244,[[    0 30043     2]]
e5bc73974c79d96eee2b688e578a9de1d0eb38fd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],majority of questions that our system could not answer so far are in fact answerable,"[[    0 19517     9  1142    14    84   467   115    45  1948    98   444
     32    11   754  1948   868     2]]"
375b281e7441547ba284068326dd834216e55c07,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"seeker interacts with a real conversational interface, intermediary (or the wizard) receives the seeker's message and performs different information seeking actions","[[    0  1090 22193 39999    19    10   588 28726  5033 12332     6 38059
     36   368     5 32660    43  9524     5 38082    18  1579     8 14023
    430   335  1818  2163     2]]"
cfcdd73e712caf552ba44d0aa264d8dace65a589,0.0,Entailment,[[    2     0 30495  3760  1757     2]],crowsourcing platform,[[    0   438 13415 27824  1761     2]]
12159f04e0427fe33fa05af6ba8c950f1a5ce5ea,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"different number of clusters, different embeddings",[[    0 37251   346     9 28255     6   430 33183   417  1033     2]]
22744c3bc68f120669fc69490f8e539b09e34b94,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d4e5e3f37679ff68914b55334e822ea18e60a6cf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],gendered word pairs like he and she,[[    0   571 36318  2136 15029   101    37     8    79     2]]
8dd8e5599fc56562f2acbc16dd8544689cddd938,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Similar words were ranked by computing Cosine distance between the embedding vector ( INLINEFORM0 ) representation of the query equation and the context vector representation of the words ( INLINEFORM1 ). Similar equations were discovered using Euclidean distance computed between the context vector representations of the equations ( INLINEFORM2 ). We give additional example results in Appendix B.,"[[    0 46444  1617    58  4173    30 11730 11013   833  4472   227     5
  33183 11303 37681    36  2808 28302 38036   288  4839  8985     9     5
  25860 19587     8     5  5377 37681  8985     9     5  1617    36  2808
  28302 38036   134 32801 17110 43123    58  2967   634 36873  1949   260
   4472 43547   227     5  5377 37681 30464     9     5 43123    36  2808
  28302 38036   176 32801   166   492   943  1246   775    11 40643   163
      4     2]]"
45f7c03a686b68179cadb1413c5f3c1d373328bd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses","[[    0  5488  6308    81  2248     6   151 40043  7201     6   217    81
   2357     6   151    19   455  2788     6    59  6247 43814    12  1646
      6   208 22210    12  8739   846    12   176     6     8  1330 34377
   1469   853  9764     2]]"
a4d115220438c0ded06a91ad62337061389a6747,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Facebook status update messages,[[    0 18064  2194  2935  3731     2]]
d8627ba08b7342e473b8a2b560baa8cdbae3c7fd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
0767ca8ff1424f7a811222ca108a33b6411aaa8a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
220d11a03897d85af91ec88a9b502815c7d2b6f3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Advanced neural architectures and contextualized embedding models learn how to handle spelling and morphology variations.,"[[    0 44694 26739 41885     8 37617  1538 33183 11303  3092  1532   141
      7  3679 24684     8 46930 18746     4     2]]"
5b7a4994bfdbf8882f391adf1cd2218dbc2255a0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized","[[    0 13424    12 46400    12 43199  2088 18043    19  3298    12  1116
     12 30938 30464     8   208 20954  1380 24072     6   475   104  3134
      6   786    12 46400    12 43199  2088  3480  5389    15  1300 11170
      6 26739  1421    14 25924 34590  8558     6 37930 27774  1058     7
   1888  8985  2249   227 30700     6 21740     9  1844  3480    29    32
    341    13 45278  3156     8     5   256 12550    29     9  1533 13171
     32 13521 41774     2]]"
4d47bef19afd70c10bbceafd1846516546641a2f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],uni-directional model to augment the decoder,[[    0 20967    12 42184   337  1421     7 29699     5  5044 15362     2]]
498c0229f831c82a5eb494cdb3547452112a66a0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Annotations from experts are used if they have already been collected.,"[[    0  4688 43920    31  2320    32   341   114    51    33   416    57
   4786     4     2]]"
de53af4eddbc30c808d90b8a11a29217d377569e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney","[[    0 19360  5532     6  3480     6  4944     6   188   469  1513     6
   3421  4320     6 27436  1869 39804   491     6    20  8137     6 41453
   3658     6 32284  5737     6  2193 32284 22080     6  3289 16370     6
  24558  4636   261     6 41809   428  2413     6  4672     2]]"
ebf0d9f9260ed61cbfd79b962df3899d05f9ebfb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing","[[    0 32530   278    34  8572     6  3387   176  3645 15029     6     8
      5  1296   278    34   727 15029     6  1058   278  6308 40625     6
  34324     6   132     6   151    13   709     8 40598    13  3044     2]]"
62f27fe08ddb67f16857fab2a8a721926ecbb6fb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],four annotators who are proficient in English,[[    0 10231 45068  3629    54    32 36216    11  2370     2]]
7438b6b146e41c08cf8f4c5e1d130c3b4cfc6d93,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
b21bc09193699dc9cfad523f3d5542b0b2ff1b8e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],MLP,[[    0 10537   510     2]]
2ee715c7c6289669f11a79743a6b2b696073805d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .

, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0","[[    0   387   134     4    20    78 18043  2939   129     5  6641 11465
     12   805  1575    30  6367  5810   329     8  6452  1758   163  8863
  45935  1225   479     6   163   176     4    20   200 18043 42091     5
    923  4249     7    10  1763  2808 28302 38036   288  2156   114     8
    129   114  2808 28302 38036   134  2092    11     5  1270     9  2808
  28302 38036   176   479 50118 50118     6   208   134    35  8184     5
   2810    31 27663  2808 28302 38036   288    19     5  1609 36912  3569
  37015     7  2808 28302 38036   134  4832   208   134  2808 28302 38036
    176     6   208   176    35  6067     5   340    88     5   144  7690
   2810    11  2808 28302 38036   288     2]]"
53014cfb506f6fffb22577bf580ae6f4d5317ce5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum","[[    0 16256    73 26339 23969   340  5586     6   188   469  1513   660
   3654  1070 28556     6  1577 38182     2]]"
a7d020120a45c39bee624f65443e09b895c10533,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Whenever we encounter an unknown concept or relation while answering a query, we perform inference using our existing knowledge. If our knowledge does not allow us to draw a conclusion, we typically ask questions to others to acquire related knowledge and use it in inference. ","[[    0 32395    52  6376    41  4727  4286    50  9355   150 15635    10
  25860     6    52  3008 42752   634    84  2210  2655     4   318    84
   2655   473    45  1157   201     7  2451    10  6427     6    52  3700
   1394  1142     7   643     7  6860  1330  2655     8   304    24    11
  42752     4  1437     2]]"
a8f51b4e334a917702422782329d97304a2fe139,0.0,Entailment,[[    2     0 30495  3760  1757     2]],1000,[[    0 20078     2]]
a87a009c242d57c51fc94fe312af5e02070f898b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features.","[[    0 12376  5580 39974  3092   716    15   542  1023  4040  3298    12
   1116    12 30938  1575    36   387  4581   238  5702  8724    36   104
   5382   238     5 39608  1575    31    84   656 20070    36   574  1862
    238     8 21092     9   209  1575     4     2]]"
43eecc576348411b0634611c81589f618cd4fddf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MLE, SeqGAN, LeakGAN, MaliGAN, IRL, RAML, DialogGAN, DPGAN","[[    0   448  3850     6  1608  1343 38416     6  1063   677 38416     6
  16193 38416     6    38 17868     6 10646   574     6 28985  2154 38416
      6   211  8332  1889     2]]"
c8541ff10c4e0c8e9eb37d9d7ea408d1914019a9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"DSL 2015, DSL 2017, JW300 parallel corpus , NCHLT text corpora","[[    0  5433   574   570     6 35409   193     6   344   771  2965 12980
  42168  2156   234  3764 30395  2788 22997   102     2]]"
f1e90a553a4185a4b0299bd179f4f156df798bce,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CopyRNN BIBREF0, KEA BIBREF4 and Maui BIBREF8, CopyRNN*","[[    0 48233   500 20057   163  8863 45935   288     6   229 14684   163
   8863 45935   306     8 14147   118   163  8863 45935   398     6 22279
    500 20057  3226     2]]"
1e775cf30784e6b1c2b573294a82e145a3f959bb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],english,[[    0 47120     2]]
04d1b3b41fb62a7b896afe55e0e8bc5ffb8c6e39,0.0,Entailment,[[    2     0 30495  3760  1757     2]],three ,[[   0 9983 1437    2]]
3cf1edfa6d53a236cf4258afd87c87c0a477e243,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English,[[    0 35007     2]]
935d6a6187e6a0c9c0da8e53a42697f853f5c248,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the aggregate of enterprises in a particular field,[[    0   627 13884     9 10445    11    10  1989   882     2]]
2317ca8d475b01f6632537b95895608dc40c4415,0.0,Entailment,[[    2     0 30495  3760  1757     2]],sequence of $s$ tweets,[[    0 46665     9    68    29  1629  6245     2]]
3611a72f754de1e256fbd25b012197e1c24e8470,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
f1f1dcc67b3e4d554bfeb508226cdadb3c32d2e9,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)","[[    0  2370  6173  2156  1437  2391  6173    31   237   430 11991    36
  41226     6  4423     6  5979     6  1083    43     2]]"
eac9dae3492e17bc49c842fb566f464ff18c049b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"claim, premise, backing, rebuttal, refutation","[[    0 31628     6 18805     6  6027     6 37862   337     6  4885 31320
      2]]"
ecd5770cf8cb12cb34285e26ab834301c17c53e1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LSTM to encode the question, VGG16 to extract visual features. The outputs of LSTM and VGG16 are multiplied element-wise and sent to a softmax layer.","[[    0   574  4014   448     7 46855     5   864     6   468 24592  1549
      7 14660  7133  1575     4    20 39512     9   226  4014   448     8
    468 24592  1549    32 39582  7510    12 10715     8  1051     7    10
   3793 29459 10490     4     2]]"
8eefa116e3c3d3db751423cc4095d1c4153d3a5f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CoNLL2003-testA, GENIA",[[    0  8739   487  6006 35153    12 21959   250     6 32520  2889     2]]
3ddff6b707767c3dd54d7104fe88b628765cae58,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Universal Dependencies v1.2 treebanks BIBREF21 , hereafter UD1.2","[[    0 35011 43290 14768   748   134     4   176  3907 27045   163  8863
  45935  2146  2156 46566 33846   134     4   176     2]]"
8eefa116e3c3d3db751423cc4095d1c4153d3a5f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The GENIA Corpus , CoNLL2003",[[    0   133 32520  2889 28556  2156   944   487  6006 35153     2]]
79a44a68bb57b375d8a57a0a7f522d33476d9f33,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Relation Generation (RG), Content Selection (CS), Content Ordering (CO)","[[    0 29806  1258 17362    36 41626   238 12803 30418    36  6842   238
  12803  9729   154    36  6335    43     2]]"
219af68afeaecabdfd279f439f10ba7c231736e4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],WIT3's corpus,[[    0   771  2068   246    18 42168     2]]
8dd8e5599fc56562f2acbc16dd8544689cddd938,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By using Euclidean distance computed between the context vector representations of the equations,"[[    0  2765   634 36873  1949   260  4472 43547   227     5  5377 37681
  30464     9     5 43123     2]]"
0ab3df10f0b7203e859e9b62ffa7d6d79ffbbe50,0.0,Entailment,[[    2     0 30495  3760  1757     2]],average classification accuracy,[[    0 20365 20257  8611     2]]
ddf5e1f600b9ce2e8f63213982ef4209bab01fd8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Spoken-SQuAD testing set,[[    0 12582 22036    12   104 12444  2606  3044   278     2]]
16535db1d73a9373ffe9d6eedaa2369cefd91ac4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],PubMed+PMC (the data used for BioBERTv1.0) and/or CORD-19 (Covid-19 Open Research Dataset),"[[    0 44110 21243  2744  5683   347    36   627   414   341    13 12334
  11126   565   705   134     4   288    43     8    73   368   230 11200
     12  1646    36   347  1417   808    12  1646  2117  1624 16673   281
    594    43     2]]"
869feb7f47606105005efdb6bea1c549824baea0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs","[[    0   698     6   398  5208  7201     6   601     6 40528  6245     6
      8   508     6 38127  8817 22241   864    12 27740 15029     2]]"
9ecde59ffab3c57ec54591c3c7826a9188b2b270,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\ year) \times 20$ citations","[[    0 14307    84   936  8515     8    58  1027    11     5   107   336
      7   954     6    33    23   513 44612 10626   111  5362 37457    76
     43 44128 14616   291  1629 31173     2]]"
9a9d225f9ac35ed35ea02f554f6056af3b42471d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"(VVD shop_VV0 II, VVD shopping_VVG II)","[[    0  1640   846 28732  2792  1215   846   846   288  3082     6   468
  28732  3482  1215   846 39997  3082    43     2]]"
8a7615fc6ff1de287d36ab21bf2c6a3b2914f73d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21","[[    0 37426   574  4014   448  5383   387 45935  1570     6  6479   574
   4014   448  2744 16256  5383   387 45935   844     6  6479   574  4014
    448  2744  9822 34823   387 45935   134     6  6479   574  4014   448
   2744 16256  2744  9822 34823   387 45935   176     6  3480  1421  5383
    387 45935   288     8  8607  4307   597  1421  5383   387 45935  2146
      2]]"
fa527becb8e2551f4fd2ae840dbd4a68971349e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],LSTM,[[   0  574 4014  448    2]]
3e1829e96c968cbd8ad8e9ce850e3a92a76b26e4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Total dataset size: 171 account (522967 tweets),"[[    0 37591 41616  1836    35 29810  1316    36   245 26957  4111  6245
     43     2]]"
72ed5fed07ace5e3ffe9de6c313625705bc8f0c7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Most texts, however, range roughly from 150 to 250 tokens.","[[    0  2895 14301     6   959     6  1186  3667    31  3982     7  5773
  22121     4     2]]"
3d7d865e905295d11f1e85af5fa89b210e3e9fdf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],100 ,[[   0 1866 1437    2]]
1c0ba6958da09411deded4a14dfea5be55687619,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
fa527becb8e2551f4fd2ae840dbd4a68971349e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],LSTM,[[   0  574 4014  448    2]]
701571680724c05ca70c11bc267fb1160ea1460a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
3f856097be2246bde8244add838e83a2c793bd17,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval.","[[    0  4148   239   672     6    52 20826 10516     5  1383 21623   227
      5  1984  4819     8     5  1050  4819   634   335 43372     4     2]]"
ababb79dd3c301f4541beafa181f6a6726839a10,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Intelligence Squared Debates,[[    0 22886 45852 13840  6537 10532  1626     2]]
55507f066073b29c1736b684c09c045064053ba9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.","[[    0   597 40985   304     9  2228  3477   766  1765     6   634 16207
   1848     8 43721     6   149 18677  1901   101 38522 16836     6 17802
   9247     7   643     6   766 39752     6 24032 30789  5000     6 33231
   3650     8  5912  1330  2939     4     2]]"
81d193672090295e687bc4f4ac1b7a9c76ea35df,0.0,Entailment,[[    2     0 30495  3760  1757     2]],using word2vec to create features that are used as input to the SVM,"[[    0 10928  2136   176 25369     7  1045  1575    14    32   341    25
   8135     7     5   208 20954     2]]"
5aa12b4063d6182a71870c98e4e1815ff3dc8a72,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
1097768b89f8bd28d6ef6443c94feb04c1a1318e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
932b39fd6c47c6a880621a62e6a978491d881d60,0.0,Entailment,[[    2     0 30495  3760  1757     2]],TransE,[[    0 19163   717     2]]
badc9db40adbbf2ea7bac29f2e4e3b6b9175b1f9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],for the homographic dataset F1 score of 92.19 and 80.19 on detection and location and for the heterographic dataset F1 score of 89.76 on detection,"[[    0  1990     5  9486 25510 41616   274   134  1471     9  8403     4
   1646     8  1812     4  1646    15 12673     8  2259     8    13     5
  39872 25510 41616   274   134  1471     9  8572     4  5067    15 12673
      2]]"
1be54c5b3ea67d837ffba2290a40c1e720d9587f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
f875337f2ecd686cd7789e111174d0f14972638d,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," Affective Text dataset, Fairy Tales dataset, ISEAR dataset","[[    0 37089  2088 14159 41616     6 37857 32419 41616     6  3703 27757
  41616     2]]"
4986f420884f917d1f60d3cea04dc8e64d3b5bf1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CLV as a parent of the two corresponding role variables,"[[    0  7454   846    25    10  4095     9     5    80 12337   774 25083
      2]]"
35b3ce3a7499070e9b280f52e2cb0c29b0745380,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
8756b7b9ff5e87e4efdf6c2f73a0512f05b5ae3f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Pretrained word embeddings  were not used,"[[    0   510  4903 26492  2136 33183   417  1033  1437    58    45   341
      2]]"
2c7494d47b2a69f182e83455fe4c75ae3b2893e9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
0f567251a6566f65170a1329eeeb5105932036b2,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," BIBREF14, BIBREF15 ",[[    0   163  8863 45935  1570     6   163  8863 45935   996  1437     2]]
50cb50657572e315fd452a89f3e0be465094b66f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
03c967763e51ef2537793db7902e2c9c17e43e95,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)","[[    0 10273 12198  5375  1421    36   495  3721   238  1437 27663   467
     36   565  5330   238 23431  5375    36  3376   238   234  7496  2744
   3376    36  6905   510    43     2]]"
c81f215d457bdb913a5bade2b4283f19c4ee826c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Waseem-dataset, Davidson-dataset,","[[    0   771  3175   991    12 36146   281   594     6 10553    12 36146
    281   594     6     2]]"
51fe4d44887c5cc5fc98b65ca4cb5876f0a56dad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Bert + Unanswerable,[[    0   387  2399  2055  1890 27740   868     2]]
2439b6b92d73f660fe6af8d24b7bbecf2b3a3d72,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By calculating Macro F1 metric at the document level.,"[[    0  2765 29770 32449   274   134 14823    23     5  3780   672     4
      2]]"
777bb3dcdbc32e925df0f7ec3adb96f15dd3dc47,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Women represent 33.16% of the speakers,[[    0 19814  3594  2357     4  1549   207     9     5  6864     2]]
2a564b092916f2fabbfe893cf13de169945ef2e1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The number of reviews in this movie corpus is 20,244 and the average number of words in reviews is 39. Each of these reviews has a star-rating score which is indicative of sentiment.","[[    0   133   346     9  6173    11    42  1569 42168    16   291     6
  30043     8     5   674   346     9  1617    11  6173    16  3191     4
   4028     9   209  6173    34    10   999    12 21172  1471    61    16
  22206     9  5702     4     2]]"
04bde1d2b445f971e97bb46ade2d0290981c7a32,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d604f5fb114169f75f9a38fab18c1e866c5ac28b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"F1, precision, recall, accuracy",[[    0   597   134     6 15339     6  6001     6  8611     2]]
dfca00be3284cc555a6a4eac4831471fb1f5875b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"30 terms, each term-sanse pair has around 15 samples for testing","[[    0   541  1110     6   349  1385    12 14832  1090  1763    34   198
    379  7931    13  3044     2]]"
87bb3105e03ed6ac5abfde0a7ca9b8de8985663c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They use a slightly modified copy of the target to create the pseudo-text instead of full BT to make their technique cheaper,"[[    0  1213   304    10  2829 10639  5375     9     5  1002     7  1045
      5 38283    12 29015  1386     9   455 12482     7   146    49  9205
   7246     2]]"
a712718e6596ba946f29a99838d82f95b9ebb1ce,0.0,Entailment,[[    2     0 30495  3760  1757     2]],7.36% on accuracy and 9.69% on F1 score,"[[   0  406    4 3367  207   15 8611    8  361    4 4563  207   15  274
   134 1471    2]]"
223dc2b9ea34addc0f502003c2e1c1141f6b36a7,0.0,Entailment,[[    2     0 30495  3760  1757     2]], reward learning algorithm BIBREF7,[[    0  7970  2239 17194   163  8863 45935   406     2]]
a616a3f0d244368ec588f04dfbc37d77fda01b4c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese","[[    0 24727 33830     6 12093     6  2370     6 34518   811     6 21533
      6  1515     6 27428     6 11145     6  1083     6  3453     6 23879
    605   895  4715     6 43494  1111     2]]"
3cf1edfa6d53a236cf4258afd87c87c0a477e243,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English,[[    0 35007     2]]
54830abe73fef4e629a36866ceeeca10214bd2c8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\alpha =0.01$, $\beta = 0.01$ and using Gibbs sampling as a parameter estimation","[[    0   417 33951     5  3926     9     5  7614     8   465  1291   227
    226  3134  7614     8  2225  1575     8  5368  2416 19445     6  1437
   1532    10   226  3134  1421    19   727  7614   131 49959 29135  5457
    288     4  2663 47110 49959 44263  5457   321     4  2663  1629     8
    634 21630 20424    25    10 43797 32809     2]]"
5b7a4994bfdbf8882f391adf1cd2218dbc2255a0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD","[[    0  1640   134    43  7300  2088     6    36   176    43   475   104
   3134   163  8863 45935   406     6    36   246    43  7300  2088 20057
      6    36   306    43 32119 20057   163  8863 45935   306     6    36
    245    43  4516  1889   163  8863 45935  1549     6    36   401    43
    256 12550     2]]"
0c234db3b380c27c4c70579a5d6948e1e3b24ff1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],LSTM,[[   0  574 4014  448    2]]
d3aa0449708cc861a51551b128d73e11d62207d2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"break the relation names into word sequences for question-relation matching, build both relation-level and word-level relation representations, use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations, residual learning method for sequence matching, a simple KBQA implementation composed of two-step relation detection","[[    0 10339     5  9355  2523    88  2136 26929    13   864    12 47114
   8150     6  1119   258  9355    12  4483     8  2136    12  4483  9355
  30464     6   304  1844  2311 43606   337   226  4014 13123    36 37426
    574  4014 13123    43     7  1532   430  1389     9   864 30464     6
  30848  2239  5448    13 13931  8150     6    10  2007 29006  1864   250
   5574 14092     9    80    12 13975  9355 12673     2]]"
3bf0306e9bd044f723e38170c13455877b2aeec3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],by classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss $L_{\text{sen}}$,"[[    0  1409  1380  4945 16851     8   786    12    29  1290  5033  6337
    634    10    65    12 39165  3480    19    10 32771  2116 47382   872
     68   574 49747 37457 29015 45152  7305 46961  1629     2]]"
d51dc36fbf6518226b8e45d4c817e07e8f642003,0.0,Entailment,[[    2     0 30495  3760  1757     2]],3606,[[    0 14586   401     2]]
3941401a182a3d6234894a5c8a75d48c6116c45c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Tweets related to CyberAttack and tweets related to PoliticianDeath,"[[    0 45659  2580  1330     7 12324 45138     8  6245  1330     7 16373
  14932 34811     2]]"
e86130c5b9ab28f0ec539c2bed1b1ae9efb99b7d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
15cdd9ea4bae8891c1652da2ed34c87bbbd0edb8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],twitter,[[   0 1556    2]]
2ea4347f1992b0b3958c4844681ff0fe4d0dd1dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Embedding Layer, Neural Network Layers, Loss Function, Metrics","[[    0 42578 13093   154 43262     6 44304  3658   226 24950     6 19700
  42419     6  4369 18715     2]]"
f4496316ddd35ee2f0ccc6475d73a66abf87b611,0.0,Entailment,[[    2     0 30495  3760  1757     2]],a benchmark dataset created by ceccarelli2013learning from the CoNLL 2003 data,"[[    0   102  5437 41616  1412    30  8635  7309  1322 10054 10684 29888
     31     5   944   487  6006  4999   414     2]]"
40c0f97c3547232d6aa039fcb330f142668dea4b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
c9b8d3858c112859eabee54248b874331c48f71b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
1062a0506c3691a93bb914171c2701d2ae9621cb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Sentiment, Morality, Style, Words embeddings","[[    0 35212  8913     6  4266  6948     6 18149     6 27341 33183   417
   1033     2]]"
a6d3e57de796172c236e33a6ceb4cca793dc2315,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
f697d00a82750b14376fe20a5a2b249e98bebe9b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Bi-LSTM-CRF,[[    0 37426    12   574  4014   448    12  9822   597     2]]
d3aa0449708cc861a51551b128d73e11d62207d2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"break the relation names into word sequences,  relation-level and word-level relation representations, bidirectional LSTMs (BiLSTMs),  residual learning method","[[    0 10339     5  9355  2523    88  2136 26929     6  1437  9355    12
   4483     8  2136    12  4483  9355 30464     6  2311 43606   337   226
   4014 13123    36 37426   574  4014 13123   238  1437 30848  2239  5448
      2]]"
ddb23a71113cbc092cbc158066d891cae261e2c6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"main news channels, such as Yahoo News, The Guardian or The Washington Post","[[    0 17894   340  6237     6   215    25 10354   491     6    20  8137
     50    20   663  1869     2]]"
b46c0015a122ee5fb95c2a45691cb97f80de1bb6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"one-layer CNN structure from previous works BIBREF22 , BIBREF4","[[    0  1264    12 39165  3480  3184    31   986  1364   163  8863 45935
   2036  2156   163  8863 45935   306     2]]"
c77359fb9d3ef96965a9af0396b101f82a0a9de6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],from Food.com,[[   0 7761 3652    4  175    2]]
aa6d956c2860f58fc9baea74c353c9d985b05605,0.0,Entailment,[[    2     0 30495  3760  1757     2]],ROUGE BIBREF22 unigram score,"[[    0   500  5061  8800   163  8863 45935  2036   542  1023  4040  1471
      2]]"
73e83c54251f6a07744413ac8b8bed6480b2294f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"generate word embeddings specific to a domain, TDK (Türk Dil Kurumu - “Turkish Language Institution”) dictionary to obtain word polarities","[[    0 20557   877  2136 33183   417  1033  2167     7    10 11170     6
   7463   530    36   565  5172   330 14205 13830   783   257   111    44
     48 37300 22205 22082    17    46    43 36451     7  6925  2136 13744
   2192     2]]"
477d9d3376af4d938bb01280fe48d9ae7c9cf7f7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19","[[    0 30876   791    12   134    36   387   134   238   163  3850   791
     12   176    36   387   176   238   163  3850   791    12   246    36
    387   246   238   163  3850   791    12   306    36   387   306    43
    163  8863 45935  1360     6 30782   717  3411    36 43543    43   163
   8863 45935  1366     8   248  5061  8800    12   574    36   500    12
    574    43   163  8863 45935  1646     2]]"
e97186c51d4af490dba6faaf833d269c8256426c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
a5e49cdb91d9fd0ca625cc1ede236d3d4672403c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],adopt a multi-turn answer module for the span detector BIBREF1,"[[    0   625 19693    10  3228    12 15922  1948 20686    13     5  8968
  30956   163  8863 45935   134     2]]"
8bb0011ad1d63996d5650770f3be18abdd9f7fc6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
efe9bad55107a6be7704ed97ecce948a8ca7b1d2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"baseline without knowledge distillation (termed NoKD), Patient Knowledge Distillation (PKD)","[[    0 15609  7012   396  2655  7018 34775    36  1334  4567   440   530
    495   238 27690 27831 11281 34775    36 35697   495    43     2]]"
012b8a89aea27485797373adbcda32f16f9d7b54,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BIBREF11 that uses a character level n-gram language model, 'shallow' naive Bayes classifiers BIBREF12, BIBREF8, BIBREF13, BIBREF14, SVMs BIBREF15, BIBREF16 used an SVM with character n-gram, parts of speech tag features and some other engineered features, The winning approach for DSL 2015 used an ensemble naive Bayes classifier, The fasttext classifier BIBREF17, hierarchical stacked classifiers (including lexicons), bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24","[[    0  5383   387 45935  1225    14  2939    10  2048   672   295    12
  28526  2777  1421     6   128  1193 25487   108 25672  1501   293  1380
  27368   163  8863 45935  1092     6   163  8863 45935   398     6   163
   8863 45935  1558     6   163  8863 45935  1570     6 22753 13123   163
   8863 45935   996     6   163  8863 45935  1549   341    41   208 20954
     19  2048   295    12 28526     6  1667     9  1901  6694  1575     8
    103    97 19606  1575     6    20  1298  1548    13 35409   570   341
     41 12547 25672  1501   293  1380 24072     6    20  1769 29015  1380
  24072   163  8863 45935  1360     6 44816 19030  1380 27368    36  8529
  36912 46043   238  2311 43606   337 35583 26739  4836   163  8863 45935
   1922    50  1177 26660  1634     9 35583 26739  4836   163  8863 45935
   1978     2]]"
79413ff5d98957c31866f22179283902650b5bb6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"48,705 e-books from 13 publishers, search query logs of 21,243 e-books for 12 months","[[    0  3818     6 35278   364    12 17788    31   508 14419     6  1707
  25860 24113     9   733     6 30320   364    12 17788    13   316   377
      2]]"
ab9b0bde6113ffef8eb1c39919d21e5913a05081,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. ","[[    0 42606  6074  1437  6184   716     8 14969 19850  8369   851     5
    275  1374   274   288     4   245  4391     4    85    21  2766     4
   1225    13   274  8041 41616  1437  2156   733     4  5677    13     5
     78 47760     9  1437   944   487  6006    12  1570     6     8   389
      4  1558    13     5   200 47760     9   944   487  6006    12  1570
      4  1437     2]]"
d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"non-targeted profanity and swearing, targeted insults such as cyberbullying, offensive content related to ethnicity, gender or sexual orientation, political affiliation, religious belief, and anything belonging to hate speech","[[    0 13424    12 23976   196  8546 30854     8 21854     6  3656 22536
    215    25  5381 17559  4048     6  2555  1383  1330     7 23848     6
   3959    50  1363 14497     6   559 23114     6  3458  6563     6     8
    932 11441     7  4157  1901     2]]"
e431661f17347607c3d3d9764928385a8f3d9650,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They decrease MAE in 0.34,[[   0 1213 7280 8981  717   11  321    4 3079    2]]
d6401cece55a14d2a35ba797a0878dfe2deabedc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],linguistic variability,[[    0  1527   257  5580 36049     2]]
6236762b5631d9e395f81e1ebccc4bf3ab9b24ac,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
f651cd144b7749e82aa1374779700812f64c8799,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU, FKGL, SARI, Simplicity","[[    0 30876   791     6   274   530 10020     6   208 32665     6 39753
  24414     2]]"
b9d07757e2d2c4be41823dd1ea3b9c7f115b5f72,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet ,"[[    0 48011  1111   750  2189    11   484 38481  1988   918     8  7201
   1982    30  8844   148 10775  3573    12  2619  3573  4786    31     5
   2888  1437     2]]"
aef607d2ac46024be17b1ddd0ed3f13378c563a6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair","[[    0  1694  1394  2724  1050 45068  3629     7 24704  6929     5   223
     12  9981 32914  8135  1617     6     8    23   513    80 45068  3629
   6929   349  8135    12 33027  1242 35571  1763     2]]"
e9cfbfdf30e48cffdeca58d4ac6fdd66a8b27d7a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They break down the task of importance annotation to the level of single propositions and obtain a score for each proposition indicating its importance in a document cluster, such that a ranking according to the score would reveal what is most important and should be included in a summary.","[[    0  1213  1108   159     5  3685     9  3585 47760     7     5   672
      9   881 41356     8  6925    10  1471    13   349 16104  9172    63
   3585    11    10  3780 18016     6   215    14    10  7141   309     7
      5  1471    74  4991    99    16   144   505     8   197    28  1165
     11    10  4819     4     2]]"
c59078efa7249acfb9043717237c96ae762c0a8c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CDA, REG",[[    0   347  3134     6 26747     2]]
3d7a982c718ea6bc7e770d8c5da564fbb9d11951,0.0,Entailment,[[    2     0 30495  3760  1757     2]],by learning a projection function between the document-event distribution and four event related word distributions ,"[[    0  1409  2239    10 18144  5043   227     5  3780    12 21680  3854
      8   237   515  1330  2136 26070  1437     2]]"
e330e162ec29722f5ec9f83853d129c9e0693d65,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
fb5fb11e7d01b9f9efe3db3417b8faf4f8d6931f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Logistic regression, LSTM, End-to-end memory networks, Deep projective reader","[[    0 23345  5580 39974     6   226  4014   448     6  4680    12   560
     12  1397  3783  4836     6  8248   695  2088 10746     2]]"
f9aa055bf73185ba939dfb03454384810eb17ad1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No data. Pretrained model is used.,[[    0  3084   414     4 18203 26492  1421    16   341     4     2]]
f85f2a532e7e700d9f8f9c09cd08d4e47b87bdd3,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools","[[    0  2308 15308    32    67    10  5130  1300     9   414    13     5
   1901  5774   435     6   485  1364 20489   154  3959  9415    11   484
   1632  2777  5774    36   487 21992    43  3270     2]]"
f2e8497aa16327aa297a7f9f7d156e485fe33945,0.0,Entailment,[[    2     0 30495  3760  1757     2]],WebAnno,[[    0 27521  4688  2362     2]]
cf874cd9023d901e10aa8664b813d32501e7e4d2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Named Entity Recognition,[[    0   487  7486 46718 23288  7469     2]]
63b92dcc701ec77fdb3355ede5d37d2fbf057bcc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
1462eb312944926469e7cee067dfc7f1267a2a8c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities","[[    0  2522   487  2462  3644  6308   155   430  3505     9  8866     6
  11935  4454   574  6308   204   430  3505     9  8866     2]]"
218bc82796eb8d91611996979a4a42500131a936,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Eusboost, MWMOTE",[[    0   717   687 33934     6   256 29395 28002     2]]
af5730d82535464cedfa707a03415ac2e7a21295,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set, formal spoken text (excerpted from court and presidential debate transcripts in the Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus), causal or procedural text, which describes sequences of events or actions, extracted from WikiHow, annotations using the longer contexts present in the GLUE RTE training data, which came from the RTE5 dataset","[[    0 47681    36  1116  5773    12  4697  3768    43    31     5 24704
  23132  6003  8024  1864   250  1058   278     6  4828  5826  2788    36
   3463 39406   196    31   461     8  1939  2625 20382    11     5  1554
  13851   660  3654  1070  4052    12 25313   687    36 32804   347    43
      9     5  2117   470   496 28556   238 41214    50 24126  2788     6
     61  7448 26929     9  1061    50  2163     6 27380    31 45569  6179
      6 47234   634     5  1181 38270  1455    11     5 12209  9162   248
   6433  1058   414     6    61   376    31     5   248  6433   245 41616
      2]]"
38f58f13c7f23442d5952c8caf126073a477bac0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],EM Score of 51.10,[[    0  5330 14702     9  4074     4   698     2]]
9132d56e26844dc13b3355448d0f14b95bd2178a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"token-level chunk label embeddings,  chunk boundary information is passed into the task model via BIOUL encoding of the labels","[[    0 46657    12  4483 15836  6929 33183   417  1033     6  1437 15836
  16358   335    16  1595    88     5  3685  1421  1241 29468  5061   574
  45278     9     5 14105     2]]"
ca4daafdc23f4e23d933ebabe682e1fe0d4b95ed,0.0,Entailment,[[    2     0 30495  3760  1757     2]],built over all the data and therefore includes the vocabulary from both the training and testing sets,"[[    0 16353    81    70     5   414     8  3891  1171     5 32644    31
    258     5  1058     8  3044  3880     2]]"
5bc1dc6ebcb88fd0310b21d2a74939e35a4c1a11,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English, Spanish, Finnish",[[    0 35007     6  3453     6 21533     2]]
22815878083ebd2f9e08bc33a5e733063dac7a0f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Russian,[[    0 14836     2]]
3e88fb3d28593309a307eb97e875575644a01463,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets","[[    0 33919  2055 13379    12  1116    12 30938     6 12244   176 25369
      6 40815  2055   404 21309    36    90 21210    12  4483   238 40815
   2055   404 21309    36   611  6435    12  4483   238 19127 44445    36
     90 21210    12  4483   238  3107 10068   330  1629 21390     6  3829
      6    50   769    12    90  1694  2580     2]]"
d93c0e78a3fe890cd534a11276e934be68583f4b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
41ac23e32bf208b69414f4b687c4f324c6132464,0.0,Entailment,[[    2     0 30495  3760  1757     2]],small portion of the large parallel corpus for English-German is used as a simulation,"[[    0 23115  4745     9     5   739 12980 42168    13  2370    12 27709
     16   341    25    10 23805     2]]"
1ed49a8c07ef0ac15cfa6b7decbde6604decbd5b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the English-German dataset,[[    0   627  2370    12 27709 41616     2]]
08b57deb237f15061e4029b6718f1393fa26acce,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"located in the US, hired on the BIBREF22 platform","[[    0   462 23283    11     5   382     6  4547    15     5   163  8863
  45935  2036  1761     2]]"
585626d18a20d304ae7df228c2128da542d248ff,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score","[[    0  6031 41220 32018  1460     6    52  6581    10  2450   373 16514
   1640  2808 28302 38036   288 31311   598 10516     5 27930   819     6
     52   304 43730     4   256  3376     8 44678     4  2055   548   274
    134  1471     2]]"
2a6003a74d051d0ebbe62e8883533a5f5e55078b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],3C model,[[   0  246  347 1421    2]]
a5505e25ee9ae84090e1442034ddbb3cedabcf04,0.0,Entailment,[[    2     0 30495  3760  1757     2]],F1 of 0.8099,[[   0  597  134    9  321    4 2940 2831    2]]
f59f1f5b528a2eec5cfb1e49c87699e0c536cc45,0.0,Entailment,[[    2     0 30495  3760  1757     2]],3606 sentences,[[    0 14586   401 11305     2]]
09a1173e971e0fcdbf2fbecb1b077158ab08f497,0.0,Entailment,[[    2     0 30495  3760  1757     2]],0.05 F1,[[   0  288    4 2546  274  134    2]]
12c50dea84f9a8845795fa8b8c1679328bd66246,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus","[[    0  6842  2571 41616  2156   291   340 36378     6  6868 11028   112
  42168     2]]"
73906462bd3415f23d6378590a5ba28709b17605,6.6667,Entailment,[[    2     0 30495  3760  1757     2]],"hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length, NLI models tend to predict entailment for sentence pairs with a high lexical overlap","[[    0 33027  1242 35571    12  8338 18043 14023   357    87   778   528
      7 25072    15    49 36912  3569  2031     8  3645  5933     6 12817
    100  3092  3805     7  7006 31648  1757    13  3645 15029    19    10
    239 36912  3569 27573     2]]"
ca5a82b54cb707c9b947aa8445aac51ea218b23a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],It defined a sequence labeling task to extract custom entities from user input and label the next action (out of 13  custom actions defined).,"[[    0   243  6533    10 13931 27963  3685     7 14660  6777  8866    31
   3018  8135     8  6929     5   220   814    36   995     9   508  1437
   6777  2163  6533   322     2]]"
d46c0ea1ba68c649cc64d2ebb6af20202a74a3c7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
dd20d93166c14f1e57644cd7fa7b5e5738025cd0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],mainstream and disinformation news,[[    0 17894  8656     8 30526   340     2]]
01f4a0a19467947a8f3bdd7ec9fac75b5222d710,0.0,Entailment,[[    2     0 30495  3760  1757     2]],INLINEFORM0 scores,[[    0  2444 28302 38036   288  4391     2]]
92dfacbbfa732ecea006e251be415a6f89fb4ec6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The Nguni languages are similar to each other, The same is true of the Sotho languages","[[    0   133   234  8215   118 11991    32  1122     7   349    97     6
     20   276    16  1528     9     5   208  6157   139 11991     2]]"
fc1679c714eab822431bbe96f0e9cf4079cd8b8d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers","[[    0  4156     4   306   207    15 47764 47410 41616     6  8060     4
    306   207    15 10995    12 33111 22397  4687  6665     6  6791     4
    134   207  1437    15 10995    12 33111 22397 34710  1258     8 22205
   6665     6     8  7589     4   466   207    15 10995    12 33111 22397
  14969 13807  6665     2]]"
22732cb9476e521452bf0538f3fdb94cf3867651,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
fde700d5134a9ae8f7579bea1f1b75f34d7c1c4c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Android application,[[    0 42375  2502     2]]
d3093062aebff475b4deab90815004051e802aa6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information","[[    0   104 20954    19   542  1023  4040     6   380  4040     6     8
  46681  4040  1575     6   208 20954    19   674  2136 33183 11303     6
    208 20954    19   674 11229  2136 33183   417  1033     6  3480     6
  20508 41937 30505 23794   337 44304 14641     6   208 20954     8  1844
   2239  3092    19  1129   335     2]]"
72ce05546c81ada05885026470f4c8c218805055,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English,[[    0 35007     2]]
1cb100182508cf55b3509283c0e2bbcd527d625e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"refer to each article, blog post, comment, or forum posts as a document","[[   0  241 6646    7  349 1566    6 5059  618    6 1129    6   50 7900
  4570   25   10 3780    2]]"
4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94,0.0,Entailment,[[    2     0 30495  3760  1757     2]],state-of-the-art PDTB taggers,"[[    0  4897    12  1116    12   627    12  2013 11707 16566  6694  7188
      2]]"
b6ae8e10c6a0d34c834f18f66ab730b670fb528c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. ","[[    0 28373     6   265     6  2866     6     8 12738 47758     6     8
  10775   943  4570    31     5  6844   760  8596     4  1437     2]]"
051df74dc643498e95d16e58851701628fdfd43e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],data has been developed by crawling and pre-processing an OSG web forum,"[[    0 23687    34    57  2226    30 33265     8  1198    12 39221    41
   8192   534  3748  7900     2]]"
c21b87c97d1afac85ece2450ee76d01c946de668,0.0,Entailment,[[    2     0 30495  3760  1757     2]],pointer networks with coverage mechanism (PG-net),[[    0 10475  4836    19  1953  9562    36  8332    12  4135    43     2]]
ff6c9af28f0e2bb4fb6a69f124665f8ceb966fbc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Galatasaray, Fenerbahçe",[[    0 32302   415 31126   857     6   274  5777 28185  3381   242     2]]
a9a532399237b514c1227f2d6be8601474e669be,0.0,Entailment,[[    2     0 30495  3760  1757     2]], UM Inventory ,[[    0 22759 40829  1437     2]]
5bc1dc6ebcb88fd0310b21d2a74939e35a4c1a11,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English
French
Spanish
German
Greek
Bulgarian
Russian
Turkish
Arabic
Vietnamese
Thai
Chinese
Hindi
Swahili
Urdu
Finnish","[[    0 35007 50118 28586 50118 41226 50118 27709 50118 43394 50118 40028
    571  9063 50118 14836 50118 37300 50118 33054   636 50118   846  5810
  47650 50118 11329  1439 50118 24727 50118   725  2028   118 50118 15417
    895  4715 50118 38046  6588 50118   597  5246  1173     2]]"
2236386729105f5cf42f73cc055ce3acdea2d452,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
ba28ce9a2f7e8524243adf288cc3f11055e667bb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
bc4dca3e1e83f3b4bbb53a31557fc5d8971603b2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SimLex, Rare Word, Google Semantic, Semantic Textual Similarity, Word Content (WC) probing, Google Syntactic analogies, Depth, Top Constituent, part-of-speech (POS) tagging","[[    0 29656 43551     6 28751 15690     6  1204 11202 26970     6 11202
  26970 14159  5564 17110  1571     6 15690 12803    36 17314    43 24097
      6  1204 33221 28201 26903   918     6 43553     6  3107 11242 15998
   1342     6   233    12  1116    12 40511    36 42740    43 34694     2]]"
31735ec3d83c40b79d11df5c34154849aeb3fb47,0.0,Entailment,[[    2     0 30495  3760  1757     2]],20 annotatos from author's institution,[[    0   844 45068 29501    31  2730    18  6786     2]]
2c7494d47b2a69f182e83455fe4c75ae3b2893e9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
8ce11515634236165cdb06ba80b9a36a8b9099a2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
2ceced87af4c8fdebf2dc959aa700a5c95bd518f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
6d1217b3d9cfb04be7fcd2238666fa02855ce9c5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BiLSTM, BiLSTM+CNN, BiLSTM+CRF, BiLSTM+CNN+CRF, CNN, Stanford CRF","[[    0 37426   574  4014   448     6  6479   574  4014   448  2744 16256
      6  6479   574  4014   448  2744  9822   597     6  6479   574  4014
    448  2744 16256  2744  9822   597     6  3480     6  8607  4307   597
      2]]"
46227b4265f1d300a5ed71bf40822829de662bc2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"AMR Bank BIBREF10, CNN-Dailymail ( BIBREF11 BIBREF12 )","[[    0  2620   500   788   163  8863 45935   698     6  3480    12 26339
   6380    36   163  8863 45935  1225   163  8863 45935  1092  4839     2]]"
664db503509b8236bc4d3dc39cebb74498365750,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Hierarchical-k,[[    0   725   906 13161  3569    12   330     2]]
3d013f15796ae7fed5272183a166c45f16e24e39,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer, A separate fonts layer was introduced to preserve detailed information on the font configurations referenced in the tokens layer","[[    0 42060  1907     8 28716  2496    36   242     4   571   482 45981
   2857     6  7457  5780    43     9    10 19233     8    63   737    15
      5  2166  1842    36  1990 22745    29   129    43    21 17966    25
  16763     7     5 19233  4785     9     5 22121 10490     6    83  2559
  42998 10490    21  2942     7  8415  4271   335    15     5 28716 33843
  19800    11     5 22121 10490     2]]"
4e748cb2b5e74d905d9b24b53be6cfdf326e8054,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
ac7f6497be4bcca64e75f28934b207c9e8097576,0.0,Entailment,[[    2     0 30495  3760  1757     2]],sentences that were selected from the Wikipedia corpus provided by culotta2006integrating,"[[    0 19530  8457    14    58  3919    31     5 28274 42168  1286    30
  25069 16037 32701 24894 21172     2]]"
a103636c8d1dbfa53341133aeb751ffec269415c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"majority baseline, lexicon-based approach",[[    0 19517 18043     6 36912 17505    12   805  1548     2]]
ffde866b1203a01580eb33237a0bb9da71c75ecf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
206739417251064b910ae9e5ff096e867ee10fb8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
627b8d7b5b985394428c974aca5ba0c1bbbba377,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
4625cfba3083346a96e573af5464bc26c34ec943,0.0,Entailment,[[    2     0 30495  3760  1757     2]],6.37 BLEU,[[   0  401    4 3272  163 3850  791    2]]
d7b60abb0091246e29d1a9c28467de598e090c20,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"stochastic gradient descent, naive bayes, decision tree","[[    0   620  4306 11599 43141 19276     6 25672 11751   293     6   568
   3907     2]]"
290ee79b5e3872e0496a6a0fc9b103ab7d8f6c30,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Level A: Offensive language Detection
, Level B: Categorization of Offensive Language
, Level C: Offensive Language Target Identification
","[[    0 38809    83    35 23909  2777 38091 50118     6 12183   163    35
    230 40918  1938     9 23909 22205 50118     6 12183   230    35 23909
  22205  8506 36309 50118     2]]"
0d9fcc715dee0ec85132b3f4a730d7687b6a06f4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the number of distinct word recognition outputs that an attacker can induce,"[[    0   627   346     9 11693  2136  4972 39512    14    41 12082    64
  28944     2]]"
cc608df2884e1e82679f663ed9d9d67a4b6c03f3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy.","[[    0 47806    86     6  5799  4850    36 44290     6 21032     6  1546
  27113   238 15339     6  6001     6   274   134     6  8611     4     2]]"
5a81732d52f64e81f1f83e8fd3514251227efbc7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BIBREF12 , BIBREF13",[[    0  5383   387 45935  1092  2156   163  8863 45935  1558     2]]
37be0d479480211291e068d0d3823ad0c13321d3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Table TABREF6, Table TABREF8","[[    0 41836   255  4546 45935   401     6  9513   255  4546 45935   398
      2]]"
3355918bbdccac644afe441f085d0ffbbad565d7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"(+1 or -1), words of opposite polarities (e.g. “happy"" and “unhappy"") get far away from each other","[[    0  1640  2744   134    50   111   134   238  1617     9  5483 13744
   2192    36   242     4   571     4    44    48 27333   113     8    44
     48   879 27333  8070   120   444   409    31   349    97     2]]"
87bc6f83f7f90df3c6c37659139b92657c3f7a38,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we can expect a dependency edge between i and i' in the English parse tree to correspond to an edge between j and j' in the Hindi parse tree, dual decomposition inference algorithm tries to bring the parse trees in the two languages through its constraints","[[    0  1694    64  1057    10 31492  3543   227   939     8   939   108
     11     5  2370 43756  3907     7 20719     7    41  3543   227  1236
      8  1236   108    11     5 19840 43756  3907     6  6594 24165 14413
  42752 17194  5741     7   836     5 43756  3980    11     5    80 11991
    149    63 16311     2]]"
4bc2784be43d599000cb71d31928908250d4cef3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters","[[    0 16435 17699     9     5  5008 38813  2606     6  3639 19942 28255
    552    19     5  5008 38813  2606 28255     2]]"
91e326fde8b0a538bc34d419541b5990d8aae14b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],RWTH German-English dataset,[[    0 43424  3732  1859    12 35007 41616     2]]
