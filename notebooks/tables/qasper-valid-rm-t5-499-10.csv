ids,f1_seg_0,preds_seg_0,preds_seg_0_tokens,target_text,labels
63b92dcc701ec77fdb3355ede5d37d2fbf057bcc,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
777bb3dcdbc32e925df0f7ec3adb96f15dd3dc47,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Women represent 33.16% of the speakers,[[ 4047  4221   220 18495  6370    13     8  7215     1]]
ba539cab80d25c3e20f39644415ed48b9e4e4185,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.","[[   86  1903    18 11258     6     8  4206    52  9016    30     8  3673
   3041 14528  1448     6   223  1647    12  7163  1448   223     7   326
     12     3     9  1448    28  1126  3438   640  2287    11   223  1647
     12  2458   825   223     7   326    12     3     9    72  8165  1448
   5786   825  4252    28  2186    11   705     3  8689 11736   302     5
      1]]"
7705dd04acedaefee30d8b2c9978537afb2040dc,0.0,"LSTM, character-level encoder, LSTM decoder, and ","[[    0     3  7600  2305     6  1848    18  4563 23734    52     6     3
   7600  2305    20  4978    52     6    11     3]]",None,[[14794     1]]
c22394a3fb0dbf2fc7d3a70ad6435803f5a16ebd,0.0,statistical analysis of gender representation in a data set composed of four state-of-the-,"[[    0 11775  1693    13  7285  6497    16     3     9   331   356 10431
     13   662   538    18   858    18   532    18]]",ASR,[[  71 6857    1]]
e838275bb0673fba0d67ac00e4307944a2c17be3,33.3333,Users' gender is quantified using the gender score of the tweets,"[[    0 13504    31  7285    19 13500  3676   338     8  7285  2604    13
      8 10657     7     1]]","ascertain the distribution of: types of offensive language, genres where it is used, the dialects used, and the gender of users using such language","[[   38 19766     8  3438    13    10  1308    13 12130  1612     6  5349
      7   213    34    19   261     6     8 28461     7   261     6    11
      8  7285    13  1105   338   224  1612     1]]"
7705dd04acedaefee30d8b2c9978537afb2040dc,0.0,"LSTM, character-level encoder, LSTM decoder, and ","[[    0     3  7600  2305     6  1848    18  4563 23734    52     6     3
   7600  2305    20  4978    52     6    11     3]]",Word-level Memory Neural Networks (MemNNs) proposed in Bordes et al. (2015),"[[ 4467    18  4563 19159  1484  9709  3426     7    41   329    15    51
  17235     7    61  4382    16  7254  1395     3    15    17   491     5
  25695     1]]"
7fb27d8d5a8bb351f97236a1f6dcd8b2613b16f1,0.0,BIBREF2 is the current state of the art,"[[    0     3  5972 25582   371   357    19     8   750   538    13     8
    768     1]]",RoBERTa,[[ 2158 12920   382     9     1]]
73bddaaf601a4f944a3182ca0f4de85a19cdc1d2,0.0,WN15k,[[    0     3 21170  1808   157     1]],Daily Mail news articles released by BIBREF9 ,[[ 8496  9614  1506  2984  1883    57     3  5972 25582   371  1298     1]]
04d1b3b41fb62a7b896afe55e0e8bc5ffb8c6e39,0.0,3,[[  0 220   1]],three ,[[386   1]]
62f27fe08ddb67f16857fab2a8a721926ecbb6fb,0.0,the accuracy,[[   0    8 7452    1]],four annotators who are proficient in English,[[  662    46  2264  6230   113    33 26314    16  1566     1]]
e101e38efaa4b931f7dd75757caacdc945bb32b4,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10","[[ 2751    15    15    51    11  1546   208    63     3  5972 25582   371
  11116     3 23268     3    15    17   491     5     3  5972 25582   371
   1298     6    11  2751    15    15    51     3    15    17   491     5
      3  5972 25582   371  1714     1]]"
53aa07cc4cc4e7107789ae637dbda8c9f6c1e6aa,20.0,"high-quality speech translation requires hundreds of hours of transcribed audio, while high-quality","[[    0   306    18  4497  5023  7314  2311  3986    13   716    13     3
  11665 22573  2931     6   298   306    18  4497]]","low coverage of audio, difficulty in cross-speaker clustering","[[  731  2591    13  2931     6  8565    16  2269    18     7 14661    49
   9068    53     1]]"
3e88fb3d28593309a307eb97e875575644a01463,0.0,Random Forest classifier,[[    0 25942  6944   853  7903     1]],"LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets","[[    3 12564  1768  8055    18   858    18  6051     7     6 25335   357
    162    75     6     3 12564  1768   432 13977    41    17  1123    15
     17    18  4563   201     3 12564  1768   432 13977    41   524  6513
     18  4563   201  1699    75   382  1123    15    17    41    17  1123
     15    17    18  4563   201  2224    18  3229   157  3229 26719     6
    114     7     6    42     3    60    18    17  1123    15    17     7
      1]]"
03c967763e51ef2537793db7902e2c9c17e43e95,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)","[[16124  2405   825    41   308  3577   201  3847   358    41 20050   201
   1706   138  2405    41  2823   201   445  4184  1220  2823    41   567
   4184    61     1]]"
1f07e837574519f2b696f3d6fa3230af0b931e5d,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
79413ff5d98957c31866f22179283902650b5bb6,0.0,WSJ228,[[   0    3 8439  683  357 2577    1]],"48,705 e-books from 13 publishers, search query logs of 21,243 e-books for 12 months","[[ 4678     6  2518   755     3    15    18 10467    45  1179 18902     6
    960 11417  4303     7    13 12026 27730     3    15    18 10467    21
    586   767     1]]"
a2015f02dfb376bf9b218d1c897018f4e70424d7,33.3333,"3,300 scholarly articles",[[    0  6180  5426     3 26892  2984     1]],"45,000 scholarly articles, including over 33,000 with full text","[[  314  5898     3 26892  2984     6   379   147   220 11212    28   423
   1499     1]]"
74fb77a624ea9f1821f58935a52cca3086bb0981,0.0,"3,200 messages",[[   0 6180 3632 4175    1]],Dataset contains total of 14100 annotations.,[[ 2747  2244  2579   792    13   968  2915 30729     7     5     1]]
c2cb6c4500d9e02fc9a1bdffd22c3df69655189f,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
d604f5fb114169f75f9a38fab18c1e866c5ac28b,40.0,accuracy,[[   0 7452    1]],"Precision, recall, F1, accuracy",[[28464     6  7881     6   377  4347  7452     1]]
fc4ae12576ea3a85ea6d150b46938890d63a7d18,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
3941401a182a3d6234894a5c8a75d48c6116c45c,0.0,Twitter,[[   0 3046    1]],"cyber security (CyberAttack), death of politicians (PoliticianDeath)","[[ 9738  1034    41   254    63  1152   188    17    17  4365   201  1687
     13 13446    41  8931    23 29562  2962     9   189    61     1]]"
3415762847ed13acc3c90de60e3ef42612bc49af,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline
Imbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000","[[ 5586   331    10   180  4209  4525     6   332 20921     6     3  5166
   9213   300     3  9596  7452   979   394   145 20726  1318  3849   663
     26 11241    10     8  4179   147     8  1247   825  5386    38     8
    331  2347    72 24809    26     6     3  6836    45   300   431  7452
    979    30   910    10 16824    12   147   460  7452   979    30   460
     10 16824     1]]"
6568a31241167f618ef5ede939053feaa2fb0d7e,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
3c93894c4baf49deacc6ed2a14ef5e0f13b7d96f,0.0,"demographic, grammatical, and social",[[    0 14798     6     3  5096  4992   138     6    11   569     1]],"density of users, gender distribution",[[11048    13  1105     6  7285  3438     1]]
2fbb6322e485e7743ec3fb4bb02d44bf4b5ea8a6,0.0,WN16SEM dataset,[[    0     3 21170  2938   134  6037 17953     1]],"antonym and synonym pairs, collected from WordNet BIBREF9 and Wordnik","[[    3   288 19140    11 29443 14152     6  4759    45  4467  9688     3
   5972 25582   371  1298    11  4467  4953     1]]"
81d193672090295e687bc4f4ac1b7a9c76ea35df,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],using word2vec to create features that are used as input to the SVM,"[[  338  1448   357   162    75    12   482   753    24    33   261    38
   3785    12     8   180 12623     1]]"
4fa6fbb9df1a4c32583d4ef70d2b29ece4b3d802,0.0,BERT-based model,[[    0   272 24203    18   390   825     1]],"BIBREF11 , BIBREF26 ","[[    3  5972 25582   371  2596     3     6     3  5972 25582   371  2688
      1]]"
7ee5c45b127fb284a4a9e72bb9b980a602f7445a,0.0,LSTM with a corresponding LSTM feature set,"[[   0    3 7600 2305   28    3    9    3 9921    3 7600 2305 1451  356
     1]]",(c) previous best model trained on S-SQuAD BIBREF5 by using Dr.QA BIBREF20 ,"[[   41    75    61  1767   200   825  4252    30   180    18   134  5991
   6762     3  5972 25582   371   755    57   338   707     5 23008     3
   5972 25582   371  1755     1]]"
88e62ea7a4d1d2921624b8480b5c6b50cfa5ad42,10.0,second order coâ€“occurrence matrix,[[    0   511   455   576   104 16526 16826     1]],The matrix containing co-occurrences of the words which occur with the both words of every given pair of words.,"[[   37 16826     3  6443   576    18 16526     7    13     8  1234    84
   4093    28     8   321  1234    13   334   787  3116    13  1234     5
      1]]"
bb2de20ee5937da7e3e6230e942bec7b6e8f61ee,33.3333,The dataset is collected from various daily news sources from Nepal around the year 2015-2016,"[[    0    37 17953    19  4759    45   796  1444  1506  2836    45 17029
    300     8   215  1230    18 11505     1]]",daily newspaper of the year 2015-2016,[[ 1444  8468    13     8   215  1230    18 11505     1]]
fa30a938b58fc05131c3854f12efe376cbad887f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. ","[[11801    28   738  3586    10    41 20354     3  5268  1648  2291    31
      7   825   272    18   329  1348  2179     3    89    18     7  9022
     19     3 22776  4198     6     3 22776  3390     6     3 22776  2596
     30    71    89  4075   757     6  4506    63 19098     7    11  6827
  19356 17953     7  6898     5     1]]"
cee8cfaf26e49d98e7d34fa1b414f8f31d6502ad,0.0,LSTM,[[   0    3 7600 2305    1]],"follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing","[[1130    8 4648   16    3 1152  986 9457  989    6   68   43  220   20
  4978   52 7500  114   24   16 4522   32 8584 3272  655   53    1]]"
26126068d72408555bcb52977cd669faf660bdf7,0.0,"BIBREF21, BIBREF22, BIBREF23, ","[[    0     3  5972 25582   371  2658     6     3  5972 25582   371  2884
      6     3  5972 25582   371  2773     6     3]]","Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.
Evaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.","[[ 8974   291   348 18712  2620    13     3  7381   834   439   434   825
  14434    30     8 15705  1448  1126   485 17953     7     5 22714   772
     13     3  7381   834   439   434   825    30     8     3    35  5756
    297 17953     7   224    38     3    35  5756   297 14152 17953   990
     45  4467  9688     6  4374 15551 17953    13     3  4440 27632  5836
      3 29506    38     3   295 10990    42    59    11    46  2264   920
   3438  1427  1126   150   202     7 17953     5     1]]"
bc84c5a58c57038910f7720d7a784560054d3e1a,15.3846,"German, English, French",[[   0 2968    6 1566    6 2379    1]],"French (Fr), German (De), Dutch (Nl), Russian (Ru), Spanish (Es), Italian (It), Turkish (Tr), Persian (Fa), Swedish (Sv), Mongolian (Mn) and Chinese (Zh)","[[ 2379    41   371    52   201  2968    41  2962   201 10098    41   567
     40   201  4263    41 17137   201  5093    41   427     7   201  4338
     41   196    17   201 15423    41   382    52   201 25518    41   371
      9   201 16531    41   134   208   201 29841    29    41   329    29
     61    11  2830    41   956   107    61     1]]"
5a6926de13a8cc25ce687c22741ba97a6e63d4ee,0.0,swearing during the 2016 election,[[    0 23782    53   383     8  1421  4356     1]],"US presidential primaries, Democratic and Republican National Conventions",[[  837 13074  3778  2593     6 10021    11  8994   868 11347     7     1]]
f697d00a82750b14376fe20a5a2b249e98bebe9b,100.0,BiLSTM-CRF,[[   0 2106 7600 2305   18 4545  371    1]],Bi-LSTM-CRF,[[2106   18 7600 2305   18 4545  371    1]]
af34051bf3e628c1e2a00b110bb84e5f018b419f,5.4054,"MT encoder, MT decoder, End-to-End MT","[[    0     3  7323 23734    52     6     3  7323    20  4978    52     6
   3720    18   235    18  8532    26     3  7323]]","Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.

Pre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.

Multi-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\alpha _{st}=0.75$ while $\alpha _{asr}=0.25$ or $\alpha _{mt}=0.25$. For many-to-many setting, we use $\alpha _{st}=0.6, \alpha _{asr}=0.2$ and $\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.

Many-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. ","[[31664  5097 20726    10    37 13478  5097     3  5972 25582   371  1298
     65   163     3     9  5023 23734    52    11     3     9    20  4978
     52     5    94    19  4252    45  8629    30     8  5097    18 11430
  11736   302     5  1266    18 13023 20726     7    10   101  3498   386
    554    18 13023 20726 12341    10  8925 23734    52   554    18 13023
      6    16    84     8  5097 23734    52    19  2332  1601    45    46
     71  6857   825   117  9266    20  4978    52   554    18 13023     6
     16    84     8  5097    20  4978    52    19  2332  1601    45    46
      3  7323   825   117    11     3  5268 23734    52    18   221  4978
     52   554    18 13023     6   213   321     8 23734    52    11    20
   4978    52    33   554    18    17 10761     5    37    71  6857   825
     65     8   337  4648    28 13478  5097   825     6  4252    30     8
   4989    13  5097    18 11430    11     3 11430    18  8159  6122   357
  11736   302     5    37     3  7323   825    65     3     9  1499 23734
     52    11    20  4978    52    28     8   337  4648    13    84    16
      3  3838  5332     5    94    19   166  4252    30   549  7323   331
     41   670    18   858    18 22999    61    11   258  1399    18    17
    444    26    30    16    18 22999   331     5  4908    18 23615 20726
      7    10   101    92  3498   386  1249    18 23615 20726 12341   379
     80    18   235    18   348    63  1898     6   186    18   235    18
    782  1898     6    11   186    18   235    18   348    63  1898     5
     86     8   166   192  3803     6    62  2412     8   825    28  1514
      2   138  6977     3   834     2     7    17     2  2423 22426   755
   3229   298  1514     2   138  6977     3   834     2     9     7    52
      2  2423 18189   755  3229    42  1514     2   138  6977     3   834
      2    51    17     2  2423 18189   755  3229     5   242   186    18
    235    18   348    63  1898     6    62   169  1514     2   138  6977
      3   834     2     7    17     2  2423 22787     6     3     2   138
   6977     3   834     2     9     7    52     2  2423 18189  3229    11
   1514     2   138  6977     3   834     2    51    17     2  2423 18189
   3229     5     5   242     3  7323  2491     6    62   169   163    16
     18 22999   331     5  1404    18   235    18   348    63  1220  2026
     18 13023    10   101  2412     3     9   186    18   235    18   348
     63  1249    18 23615   825   213     8 23734    52     7    11    20
   4978    52     7    33     3  9942    45   554    18    17 10761    71
   6857    11     3  7323  2250     5     1]]"
6d4400f45bd97b812e946b8a682b018826e841f1,0.0,"LSTM, NVDM",[[    0     3  7600  2305     6     3 17058  7407     1]],"Looking for adjectives marking the noun ""baby"" and also looking for most-common adjectives related to certain nouns using POS-tagging","[[ 4878    21 31268     7 15285     8   150   202    96 12534    63   121
     11    92   479    21   167    18   287  2157 31268     7  1341    12
    824   150   202     7   338     3 16034    18    17 15242     1]]"
012b8a89aea27485797373adbcda32f16f9d7b54,0.0,lexical model with a n-gram model,"[[    0     3 30949   138   825    28     3     9     3    29    18  5096
    825     1]]","'shallow' naive Bayes, SVM, hierarchical stacked classifiers, bidirectional recurrent neural networks","[[    3    31     7   107 18912    31     3    29     9   757  2474    15
      7     6   180 12623     6  1382  7064  1950     3 24052   853  7903
      7     6  2647 26352     3    60 14907 24228  5275     1]]"
b1ce129678e37070e69f01332f1a8587e18e06b0,0.0,BIBREF14,[[    0     3  5972 25582   371  2534     1]],"2,50,000 tweets, Stock opening and closing prices of Microsoft from August 31st, 2015 to August 25th, 2016","[[ 3547  9286 10657     7     6  6394  2101    11  6733  1596    13  2803
     45  1660  2664     7    17     6  1230    12  1660   944   189     6
   1421     1]]"
c5abe97625b9e1c8de8208e15d59c704a597b88c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40,"[[    3 18256    18   188   357   254    18 19836    15    26    11     3
  18256    18   188   357   254    18 12882   322    15   321  1903     8
   4782 14694    13     3     9  2604    13  1283     1]]"
1c997c268c68149ae6fb43d83ffcd53f0e7fe57e,18.1818,"a LSTM with a corresponding knowledge base, a LSTM with","[[   0    3    9    3 7600 2305   28    3    9    3 9921 1103 1247    6
     3    9    3 7600 2305   28]]",Entities from a deep learning model are linked to the related entities from a knowledge base by a lookup.,"[[ 4443  2197    45     3     9  1659  1036   825    33  5229    12     8
   1341 12311    45     3     9  1103  1247    57     3     9   320   413
      5     1]]"
c37f65c9f0d543a35c784263b79236ccf1c44fac,0.0,sequence-to-sequence model,[[   0 5932   18  235   18    7   15  835 3772  825    1]],LSTM,[[   3 7600 2305    1]]
75ff6e425ce304a35f18c0230c0d13d3913a31a9,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
0ec56e15005a627d0b478a67fd627a9d85c3920e,9.0909,"each document in a collection as a vector in a vector space, each dimension represents","[[    0   284  1708    16     3     9  1232    38     3     9 12938    16
      3     9 12938   628     6   284  9340  5475]]","Word vectors, usually in the context of others within the same class","[[ 4467 12938     7     6  1086    16     8  2625    13   717   441     8
    337   853     1]]"
b80a3fbeb49a8968e149955bdcf199556478eeff,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
deb89bca0925657e0f91ab5daca78b9e548de2bd,0.0,five different binary classification tasks,[[    0   874   315 14865 13774  4145     1]],"presence/absence of consonants, presence/absence of phonemic nasal, presence/absence of bilabial, presence/absence of high-front vowels, and presence/absence of high-back vowels","[[ 3053    87 23767    13  6900   106  2366     6  3053    87 23767    13
    951  3113 26612     6  3053    87 23767    13     3  3727 15932    40
      6  3053    87 23767    13   306    18  6849 22121  3573     6    11
   3053    87 23767    13   306    18  1549 22121  3573     1]]"
ae90c5567746fe25af2fcea0cc5f355751e05c71,0.0,WikiText-TL-08 and WikiText-TL-09,"[[    0  2142  2168 13598    17    18 12733    18  4018    11  2142  2168
  13598    17    18 12733    18  4198     1]]","US dataset, Italian dataset",[[  837 17953     6  4338 17953     1]]
223dc2b9ea34addc0f502003c2e1c1141f6b36a7,100.0,BIBREF7,[[    0     3  5972 25582   371   940     1]],BIBREF7,[[    3  5972 25582   371   940     1]]
bc67b91dd73acded2d52fd4fee732b7a9722ea8b,5.7143,Message Passing Neural Network (GNN),"[[    0     3 16042  3424    53  1484  9709  3426    41 13738   567    61
      1]]",It is a framework used to describe algorithms for neural networks represented as graphs. Main idea is that that representation of each vertex is updated based on messages from its neighbors.,"[[   94    19     3     9  4732   261    12  5530 16783    21 24228  5275
   7283    38  8373     7     5  5140   800    19    24    24  6497    13
    284   548 10354    19  3250     3   390    30  4175    45   165 11195
      5     1]]"
bb3267c3f0a12d8014d51105de5d81686afe5f1b,0.0,"WN18, FB15k",[[    0     3 21170  2606     6     3 15586  1808   157     1]],"CoNLL-YAGO, TAC2010, ACE2004, AQUAINT, WW","[[  638   567 10376    18 17419  5577     6   332  5173 14926     6     3
  11539 21653     6    71 16892   188 13777     6 18548     1]]"
73a7acf33b26f5e9475ee975ba00d14fd06f170f,0.0,"labeled as ""Dialog"" or ""Question""","[[    0  3783    15    26    38    96 23770  2152   121    42    96  5991
   3340   106   121     1]]","the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms","[[    8    97     8  1868    65   118  8154     8     3 18018     6  1087
     24  7294     8     3 18018     6     8  5996    13  2261   655     6
      8  7321     3 16526    13     8     3 18018     6     8  1128    13
      3 18018     6   668  3976     1]]"
1f07e837574519f2b696f3d6fa3230af0b931e5d,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
5b7a4994bfdbf8882f391adf1cd2218dbc2255a0,0.0,"Using the baseline model, we can learn a baseline model to evaluate the performance of the","[[    0     3  3626     8 20726   825     6    62    54   669     3     9
  20726   825    12  6825     8   821    13     8]]","(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD","[[ 5637  1823   757     6  6499     3    51   134  4296     3  5972 25582
    371   940     6 10153  1823   757 17235     6     3 10820    71  3090
  17235     3  5972 25582   371  8525     3 15757  8502  5033     3  5972
  25582   371  2938     6     3 18669   283 11731     1]]"
18942ab8c365955da3fd8fc901dfb1a3b65c1be1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],TripAdvisor,[[16993   188    26 24680     1]]
dd6b378d89c05058e8f49e48fd48f5c458ea2ebc,0.0,Contextual Contextual Contextual Contextual Contextual Contextual Con,"[[   0 1193 6327 3471 1193 6327 3471 1193 6327 3471 1193 6327 3471 1193
  6327 3471 1193 6327 3471 1193]]","Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT","[[24239   138 25942  7257     7     6  2106  7600  2305    18  4545   371
      6  4908    18   382     9     7   157  6630     6  3318 12920   382
      1]]"
8756b7b9ff5e87e4efdf6c2f73a0512f05b5ae3f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Pretrained word embeddings  were not used,[[ 1266    17 10761  1448 25078    26    53     7   130    59   261     1]]
ed6a15f0f7fa4594e51d5bde21cc0c6c1bedbfdc,0.0,four,[[  0 662   1]],3,[[220   1]]
41b70699514703820435b00efbc3aac4dd67560a,66.6667,"divorce, custody",[[    0  7759     6 16701     1]],divorce,[[7759    1]]
6bf5620f295b5243230bc97b340fae6e92304595,0.0,"Bayesian Model for Translation (MBM), Bayesian Model for Translation (BMI","[[    0  2474    15 10488  5154    21 24527    41  4633   329   201  2474
     15 10488  5154    21 24527    41   279  7075]]","We use the same baseline as used by lang2011unsupervised which has been shown to be difficult to outperform. This baseline assigns a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head.","[[  101   169     8   337 20726    38   261    57 12142 13907   202 23313
     84    65   118  2008    12    36  1256    12    91   883  2032     5
    100 20726 12317     7     3     9 27632  1075    12     3     9 17429
      3   390    30   165  8953    17  2708   447  1681     6     3    23
      5    15     5     8 27804  4689    12   165   819     5     1]]"
984fc3e726848f8f13dfe72b89e3770d00c3a1af,11.7647,"feature number, id of an entity, a syllable, ","[[    0  1451   381     6     3    23    26    13    46 10409     6     3
      9     3     7    63   195   179     6     3]]",KL divergence between the language model of INLINEFORM5 and articles in INLINEFORM6,"[[  480   434 12355   122  1433   344     8  1612   825    13  3388 20006
  24030   755    11  2984    16  3388 20006 24030   948     1]]"
c32adef59efcb9d1a5b10e1d7c999a825c9e6d9a,36.3636,"English, Spanish and Italian",[[   0 1566    6 5093   11 4338    1]],"German, English, Spanish, Finnish, French, Russian,  Swedish.","[[ 2968     6  1566     6  5093     6 28124     6  2379     6  4263     6
  16531     5     1]]"
8dda1ef371933811e2a25a286529c31623cca0c6,0.0,"10,000",[[    0 13923     1]],One,[[555   1]]
4f253dfced6a749bf57a1b4984dc962ce9550184,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],By conducting a survey among engineers,[[  938 13646     3     9  3719   859  8702     1]]
f9bf6bef946012dd42835bf0c547c0de9c1d229f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation.,"[[ 8465 20267    47  3783    15    26    28  1151 11241    16   784   908
  13561     7    28   529 16020 30637     5     1]]"
d9412dda3279729e95fcb35cbed09e61577a896e,33.3333,accuracy,[[   0 7452    1]],"precision, recall, F1 and accuracy",[[11723     6  7881     6   377   536    11  7452     1]]
e4a315e9c190cf96493eefe04ce4ba6ae6894550,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Henderson:2017, MobileNet model",[[25253    10  9887     6  4873  9688   825     1]]
ababb79dd3c301f4541beafa181f6a6726839a10,0.0,IQ2,[[    0     3 20835   357     1]],Intelligence Squared Debates,[[5869 2825 1433 7120   26 9794 6203    1]]
435570723b37ee1f5898c1a34ef86a0b2e8701bb,0.0,LSTMs,[[   0    3 7600 2305    7    1]],"hierarchical phrase-based system BIBREF29, appropriate additional baseline would be to mark translation rules with these indicator functions but without the scores, akin to identifying rules with phrases in them (Baseline + SegOn)","[[ 1382  7064  1950  9261    18   390   358     3  5972 25582   371  3166
      6  2016  1151 20726   133    36    12  3946  7314  2219    28   175
  11169  3621    68   406     8  7586     6     3     9  2917    12     3
   9690  2219    28 15101    16   135    41 14885    15   747  1768 15696
   7638    61     1]]"
5bb3c27606c59d73fd6944ba7382096de4fa58d8,0.0,Yes,[[   0 2163    1]],MULTIPLE CHOICE QUESTION ANSWERING,"[[  283  4254  5494 27872     3 25683  8906     3 15367   134  9562     3
  16897 15713 21034     1]]"
8bb0011ad1d63996d5650770f3be18abdd9f7fc6,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
33554065284110859a8ea3ca7346474ab2cab100,0.0,3500 conversations and 8500 messages from 61 groups,[[   0  220 2560 9029   11  505 2560 4175   45    3 4241 1637    1]],"1,873 Twitter conversation threads, roughly 14k tweets","[[ 1914  4225   519  3046  3634  4546     7     6 10209   968   157 10657
      7     1]]"
67e9e147b2cab5ba43572ce8a17fc863690172f0,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],By involving humans for post-hoc evaluation of model's interpretability,"[[  938     3  6475  6917    21   442    18 24344  5002    13   825    31
      7  7280  2020     1]]"
867290103f762e1ddfa6f2ea30dd0a327f595182,0.0,Xinhua and Guangzhou,[[    0     3     4    77   107    76     9    11  2846  1468 25303     1]],Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB),"[[11801    28   738  3586    10    41 20367  1375    61  2830    28   988
      3 20519    13     8  2830 11358  7552  4739    41  6227   279    61
      1]]"
127d5ddfabec5c58832e5865cbd8ed0978c25a13,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The encoder is essentially the same as tweet2vec, with the input as words instead of characters.","[[   37 23734    52    19     3  8317     8   337    38 10657   357   162
     75     6    28     8  3785    38  1234  1446    13  2850     5     1]]"
701571680724c05ca70c11bc267fb1160ea1460a,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
7d2f812cb345bb3ab91eb8cbbdeefd4b58f65569,0.0,"N400, EPNP, PNP, and N400 responses","[[    0   445  5548     6 10177  9082     6   276  9082     6    11   445
   5548  7216     1]]",Unanswerable,[[ 597 3247 3321  179    1]]
e8a32460fba149003566969f92ab5dd94a8754a4,0.0,"BOC, INLINEFORM1",[[    0   272  5618     6  3388 20006 24030   536     1]],"Our first model is the Concept Raw Context model (CRC) which utilizes concept mentions in a large scale KB to jointly learn embeddings of both words and concepts. Our second model is the Concept-Concept Context model (3C) which learns the embeddings of concepts from their conceptual contexts (i.e., contexts containing surrounding concepts only)","[[  421   166   825    19     8 16688 19401  1193  6327   825    41  4545
    254    61    84  5849     7  2077  2652     7    16     3     9   508
   2643     3 17827    12 22801   669 25078    26    53     7    13   321
   1234    11  6085     5   421   511   825    19     8 16688    18  4302
   6873  1193  6327   825  6918   254    61    84   669     7     8 25078
     26    53     7    13  6085    45    70 17428  2625     7    41    23
      5    15     5     6  2625     7     3  6443  3825  6085   163    61
      1]]"
9ee07edc371e014df686ced4fb0c3a7b9ce3d5dc,0.0,"BIBREF21, BIBREF22, BIBREF23","[[    0     3  5972 25582   371  2658     6     3  5972 25582   371  2884
      6     3  5972 25582   371  2773     1]]","SimpleQuestions, WebQSP",[[9415 5991  222 2865    6 1620 2247 4274    1]]
cb12c19f9d14bef7b2f778892d9071eea2d6c63d,0.0,Message Passing Neural Network (MPN),"[[    0     3 16042  3424    53  1484  9709  3426    41  5244   567    61
      1]]","doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT","[[    3  7171   357   162    75     3     6 19602     6   309  5033     6
   7552    18  7600  2305     6     3  3913 17235     6     3  7600  2305
    567     6   205    18  7600  2305     6   180  7861   439     6   549
  11731     6   180    18 16727   308     6   679   348  1225    18   254
  17235     6     3  7600  2305    18  8727 17235     6   454   567    18
  24642     1]]"
f59f1f5b528a2eec5cfb1e49c87699e0c536cc45,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Dataset contains 3606 total sentences and 79087 total entities.,"[[ 2747  2244  2579  9181   948   792 16513    11   489  2394  4225   792
  12311     5     1]]"
26126068d72408555bcb52977cd669faf660bdf7,0.0,"BIBREF21, BIBREF22, BIBREF23, ","[[    0     3  5972 25582   371  2658     6     3  5972 25582   371  2884
      6     3  5972 25582   371  2773     6     3]]","Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed","[[ 9246     3     9 11417  1448    11  3876     3    23    26     6     8
    356    13 13012 14245     7   590    28    70  6477  3876     3    23
     26     7    33  2616     1]]"
d93c0e78a3fe890cd534a11276e934be68583f4b,0.0,58-79,[[   0    3 3449   18 4440    1]],Unanswerable,[[ 597 3247 3321  179    1]]
a1645d0ba50e4c29f0feb806521093e7b1459081,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Social Honeypot, which is not of high quality",[[ 2730 16354  3013     6    84    19    59    13   306   463     1]]
b5608076d91450b0d295ad14c3e3a90d7e168d0e,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
ba539cab80d25c3e20f39644415ed48b9e4e4185,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Backoff to ""a"" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK","[[ 3195  1647    12    96     9   121   116    46  4417   439    18  2026
   4370  1054  1448    19 15110     6   223  1647    12     3     9    72
   8165  1448  5786   825   116     8   825  9689     7  4417   439     1]]"
35b3ce3a7499070e9b280f52e2cb0c29b0745380,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
f161e6d5aecf8fae3a26374dcb3e4e1b40530c95,0.0,LSTMs,[[   0    3 7600 2305    7    1]],"ELMO BIBREF11, BERT BIBREF12 and ClinicalBERT BIBREF13","[[    3  3577  5365     3  5972 25582   371  2596     6   272 24203     3
   5972 25582   371  2122    11 14067 12920   382     3  5972 25582   371
   2368     1]]"
f1f1dcc67b3e4d554bfeb508226cdadb3c32d2e9,0.0,"WSJ2V, WSJ3V, WSJ4V","[[   0    3 8439  683  357  553    3    6    3 8439  683  519  553    3
     6    3 8439  683  591  553]]"," English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)","[[ 1566  2456     3     6  2062  2456    45   662   315  8024    41 19675
   1273     6 15423     6 10098     6  4263    61     1]]"
747b847d687f703cc20a87877c5b138f26ff137d,0.0,"WSJ2K, WSJ3K",[[   0    3 8439  683  357  439    6    3 8439  683  519  439    1]],"English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13, EN-DE section of the Europarl corpus BIBREF14","[[ 1566    41  5332    61    11  2968    41  5596    61  6795    13     8
    638   567 10376  2464 11736   302     3  5972 25582   371  2368     6
  13209    18  5596  1375    13     8  5578    52    40 11736   302     3
   5972 25582   371  2534     1]]"
4c07c33dfaf4f3e6db55e377da6fa69825d0ba15,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],300,[[3147    1]]
78536da059b884d6ad04680baeb894895458055c,0.0,"CoNLL-2010 Shared Task (BIBREF0), CoNLL-2010","[[    0   638   567 10376    18 14926  7105    26 16107    41  5972 25582
    371   632   201   638   567 10376    18 14926]]","varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)","[[10535    45 24210   695 12395    63  4501  7903     7    41  5972 25582
    371  7256    12  4224 29011  5879     7    41  5972 25582   371 11116
   5972 25582   371 11071  5972 25582   371   940     6  5972 25582   371
  13520     6   419 15983   757  1484  9709  3426     7    41  5972 25582
    371  1298     6  5972 25582   371 16968     6  1193 24817   138  1484
   9709  3426     7    41  5972 25582   371  2596    61    11   167  1310
   2025  1036    18   390  4648     7   114  2106 26352   695  4978    52
    419 12640   257    45 31220     7    41 12920   382    61    41  5972
  25582   371  2122    61     1]]"
c9b8d3858c112859eabee54248b874331c48f71b,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
a5e5cda1f6195ab1336855f1e39a609d61326d62,0.0,along the threaded line between the sextile and the vector,"[[    0   590     8     3   189    60 15624   689   344     8     3     7
  10398   699    11     8 12938     1]]",dimension corresponding to the concept that the particular word belongs to,"[[ 9340     3  9921    12     8  2077    24     8  1090  1448 16952    12
      1]]"
8748e8f64af57560d124c7b518b853bf2711c13e,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
4bc2784be43d599000cb71d31928908250d4cef3,0.0,TDNN-TDNN,[[    0     3 10494 17235    18 10494 17235     1]],"extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters","[[ 4924    13     8  6540   553  4569   308     6   617     7 17644  9068
      7   590    28     8  6540   553  4569   308  9068     7     1]]"
bbb77f2d6685c9257763ca38afaaef29044b4018,0.0,Arcane Detector with a Distinction Detector with Dist,"[[    0  1533  1608    15     3 31636   127    28     3     9  2678    17
     77  4985     3 31636   127    28  2678    17]]",the MILR classifier,[[    8  8161 12564   853  7903     1]]
a1ac2a152710335519c9a907eec60d9f468b19db,7.1429,"linguistic (sentiment, readability emotion part-of-speech","[[    0     3 24703    41  5277    23   297     6   608  2020 32128 13868
  32128   294    18   858    18     7   855 10217]]","Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT.","[[ 4908    18 23615    53    19  8705    57 24228  5932  7860  1304     3
    390    30     3  7600  2305    18  4545   371    11     3 24703   753
      6   298  1249    18  7662  4885   485    19  8705    57  8784    13
      3  7600  2305    18  4545   371    11   272 24203     5     1]]"
12159f04e0427fe33fa05af6ba8c950f1a5ce5ea,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"different number of clusters, different embeddings",[[  315   381    13  9068     7     6   315 25078    26    53     7     1]]
f85f2a532e7e700d9f8f9c09cd08d4e47b87bdd3,0.0,Unanswerable,[[   0  597 3247 3321  179    1]]," broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools","[[ 6878 18338    33    92     3     9  3435  1391    13   331    21     8
   5023  3026   573     6  1100   930 19019    53  7285 14387    16   633
    793  1612  3026    41   567  6892    61  1339     1]]"
5c3e98e3cebaecd5d3e75ec2c9fc3dd267ac3c83,34.7826,"generating ironic sentences, exploring non-ironic sentences","[[    0     3 11600  3575   447 16513     6  6990   529    18 17773   447
  16513     1]]","Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences","[[ 9046    63  4501  7903     6  4892  2998   295  4501  7903    21  9046
     63     6  4892  2998   295  4501  7903    21  5388    18 17773    63
      6  6586    45  3575   447 16513    12   529    18 17773   447 16513
      1]]"
00050f7365e317dc0487e282a4c33804b58b1fb3,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
dd20d93166c14f1e57644cd7fa7b5e5738025cd0,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],mainstream news and disinformation,[[12946  1506    11  1028  6391     1]]
6bf5620f295b5243230bc97b340fae6e92304595,0.0,"Bayesian Model for Translation (MBM), Bayesian Model for Translation (BMI","[[    0  2474    15 10488  5154    21 24527    41  4633   329   201  2474
     15 10488  5154    21 24527    41   279  7075]]",same baseline as used by lang2011unsupervised,[[  337 20726    38   261    57 12142 13907   202 23313     1]]
b49598b05358117ab1471b8ebd0b042d2f04b2a4,0.0,LSTMs,[[   0    3 7600 2305    7    1]],"NBOW, LSTM, attentive LSTM","[[    3 14972 15251     6     3  7600  2305     6 21883     3  7600  2305
      1]]"
ee2c9bc24d70daa0c87e38e0558e09ab97feb4f2,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
eac9dae3492e17bc49c842fb566f464ff18c049b,0.0,"gist, grammatical components",[[   0    3  122  343    6    3 5096 4992  138 3379    1]],"claim, premise, backing, rebuttal, refutation","[[ 1988     6     3 17398     6 16057     6     3    60  2780  1947     6
   6273    76  6821     1]]"
955cbea7e5ead36fb89cd6229a97ccb3febcf8bc,0.0,"joke detection, dehumor detection",[[    0 10802 10664     6    20  4884   127 10664     1]],task of humor identification in social media texts is analyzed as a classification problem,"[[ 2491    13 13934 10356    16   569   783 14877    19     3 16466    38
      3     9 13774   682     1]]"
729694a9fe1e05d329b7a4078a596fe606bc5a95,9.0909,"Using the above methods, we achieve a 98.99% accuracy and 98","[[   0    3 3626    8  756 2254    6   62 1984    3    9    3 3916    5
  1298 7561 7452   11    3 3916]]","F-1 score on the OntoNotes is 88%, and it is 53% on Wiki (gold).","[[  377  2292  2604    30     8   461   235 10358    15     7    19   505
   5953     6    11    34    19   305  5170    30  2142  2168    41 14910
    137     1]]"
c22394a3fb0dbf2fc7d3a70ad6435803f5a16ebd,0.0,statistical analysis of gender representation in a data set composed of four state-of-the-,"[[    0 11775  1693    13  7285  6497    16     3     9   331   356 10431
     13   662   538    18   858    18   532    18]]",Unanswerable,[[ 597 3247 3321  179    1]]
0d34c0812f1e69ea33f76ca8c24c23b0415ebc8d,8.6957,"feature generation on a review basis, hand-crafted feature generation on a review basis,","[[   0 1451 3381   30    3    9 1132 1873    6  609   18 8810 1451 3381
    30    3    9 1132 1873    6]]","three hand-crafted polarity scores, which are minimum, mean, and maximum polarity scores","[[ 386  609   18 8810    3 9618  485 7586    6   84   33 2559    6 1243
     6   11 2411    3 9618  485 7586    1]]"
aefa333b2cf0a4000cd40566149816f5b36135e7,100.0,accuracy,[[   0 7452    1]],Accuracy,[[4292 3663 4710    1]]
df5a4505edccc0ee11349ed6e7958cf6b84c9ed4,7.6923,"corpus of news articles labelled as propaganda, trusted, hoax, or sati","[[    0 11736   302    13  1506  2984     3 29506    38 25071     6  7731
      6  3534     9   226     6    42     3  9275]]",collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators,"[[ 4759    45  4475 17554   232   343    11   586   529    18  1409 11057
    232   343  1506 14290    11   258    46  2264   920    57   771    46
   2264  6230     1]]"
fb5fb11e7d01b9f9efe3db3417b8faf4f8d6931f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Logistic regression, LSTM, End-to-end memory networks, Deep projective reader","[[ 7736  3040 26625     6     3  7600  2305     6  3720    18   235    18
    989  2594  5275     6  9509   516   757  5471     1]]"
6024039bbd1118c5dab86c41cce1175d99f10a25,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Asian Scientific Paper Excerpt Corpus, NTCIR PatentMT Parallel Corpus ","[[ 6578 19268  6564 20335    49   102    17 10052   302     6   445  3838
   5705 20565  7323 27535 10052   302     1]]"
a1b3e2107302c5a993baafbe177684ae88d6f505,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Dataset contains 3606 total sentences and 79087 total entities.,"[[ 2747  2244  2579  9181   948   792 16513    11   489  2394  4225   792
  12311     5     1]]"
71fe5822d9fccb1cb391c11283b223dc8aa1640c,0.0,"LSTM-Base, LSTM-Base, LSTM-Bas","[[    0     3  7600  2305    18 14885    15     6     3  7600  2305    18
  14885    15     6     3  7600  2305    18 14885]]","Top-$k$ replies, likes, or re-tweets, FacTweet (tweet-level), LR + All Features (chunk-level), LR + All Features (tweet-level), Tweet2vec, LR + Bag-of-words","[[ 2224    18  3229   157  3229 26719     6   114     7     6    42     3
     60    18    17  1123    15    17     7     6  1699    75   382  1123
     15    17    41    17  1123    15    17    18  4563   201     3 12564
   1768   432 13977    41   524  6513    18  4563   201     3 12564  1768
    432 13977    41    17  1123    15    17    18  4563   201 25335   357
    162    75     6     3 12564  1768  8055    18   858    18  6051     7
      1]]"
7b9ca0e67e394f1674f0bcf1c53dfc2d474f8613,50.0,English,[[   0 1566    1]],"English , German, French",[[1566    3    6 2968    6 2379    1]]
c0bee6539eb6956a7347daa9d2419b367bd02064,100.0,No,[[  0 465   1]],No,[[465   1]]
9176d2ba1c638cdec334971c4c7f1bb959495a8e,0.0,"restaurants, movie reviews, and food critics",[[   0 3661    6 1974 2456    6   11  542 6800    7    1]],"Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen","[[ 3086     6 12800     6  2790     6   723     6     3  5166  9213     6
   7271    40   102     6  2358   951     6  1871     6  5677     7     6
   1228     1]]"
a778b8204a415b295f73b93623d09599f242f202,0.0,Random Kitchen Sink approach,[[    0 25942  5797 26560  1295     1]],"explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping","[[21119  8111   331 12938     7    12     3     9   628   213 13080 13608
     19   487     6   391 13383  1573   795    46 24672 20563  1681  1009
  17623 14670     1]]"
b249b60a8c94d0e40d65f1ffdfcac527dab57516,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
f03112b868b658c954db62fc64430bebbaa7d9e0,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
d41e20ec716b5904a272938e5a8f5f3f15a7779e,8.6957,"Using lexical variations, we identify a set of linguistic patterns that are not","[[    0     3  3626     3 30949   138 10914     6    62  2862     3     9
    356    13     3 24703  4264    24    33    59]]",act paragraphs containing any word from a predetermined list of LGTBQ terms ,"[[ 1810  8986     7     3  6443   136  1448    45     3     9   554 22755
    570    13 13444  9041  2247  1353     1]]"
a130306c6662ff489df13fb3f8faa7cba8c52a21,0.0,pooling function,[[   0 2201   53 1681    1]]," f-pooling, fo-pooling, and ifo-pooling ","[[    3    89    18 13194    53     6  5575    18 13194    53     6    11
      3    99    32    18 13194    53     1]]"
01a41c0a4a7365cd37d28690735114f2ff5229f2,0.0,Google Analytics,[[    0  1163 13926     1]], http://www.blogger.com,[[ 2649  1303  1986     5   115 27282     5   287     1]]
09a1173e971e0fcdbf2fbecb1b077158ab08f497,0.0,0.8 accuracy points,[[    0     3 22384  7452   979     1]],0.05 F1,[[    3 25079   377   536     1]]
91e326fde8b0a538bc34d419541b5990d8aae14b,0.0,"WSJ2016, WSJ2016, WSJ2016, WSJ2016","[[    0     3  8439   683 11505     6     3  8439   683 11505     6     3
   8439   683 11505     6     3  8439   683 11505]]","WMT15 German-to-English, RWTH German-English dataset","[[  549  7323  1808  2968    18   235    18 26749     6   391   518  4611
   2968    18 26749 17953     1]]"
b9d168da5321a7d7b812c52bb102a05210fe45bd,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
bf3b27a4f4be1f9ae31319877fd0c75c03126fd5,0.0,62,[[   0    3 4056    1]],about 500,[[  81 2899    1]]
348886b4762db063711ef8b7a10952375fbdcb57,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
133eb4aa4394758be5f41744c60c99901b2bc01c,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
7784d321ccc64db5141113b6783e4ba92fdd4b20,0.0,LSTM-based LSTM with a corresponding syllable structure,"[[   0    3 7600 2305   18  390    3 7600 2305   28    3    9    3 9921
     3    7   63  195  179 1809]]",Unanswerable,[[ 597 3247 3321  179    1]]
4d28c99750095763c81bcd5544491a0ba51d9070,15.3846,celebrities,[[    0 20076     1]],"Celebrities from varioius domains - Acting, Music, Politics, Business, TV, Author, Sports, Modeling. ","[[26071  2197    45  7516    32    23   302  3303     7     3    18  1983
     53     6  3057     6 14984     7     6  1769     6  1424     6 10236
      6  5716     6  5154    53     5     1]]"
4477bb513d56e57732fba126944073d414d1f75f,0.0,Medical search engines,[[   0 3721  960 7277    1]],clinical notes from the CE task in 2010 i2b2/VA,"[[ 3739  3358    45     8  9265  2491    16  2735     3    23   357   115
  15896  8230     1]]"
7d2f812cb345bb3ab91eb8cbbdeefd4b58f65569,9.0909,"N400, EPNP, PNP, and N400 responses","[[    0   445  5548     6 10177  9082     6   276  9082     6    11   445
   5548  7216     1]]","Answer with content missing: (Whole Method and Results sections) Self-paced reading times widely benefit ERP prediction, while eye-tracking data seems to have more limited benefit to just the ELAN, LAN, and PNP ERP components.
Select:
- ELAN, LAN
- PNP ERP","[[11801    28   738  3586    10    41 20754   109  7717    11 12772  6795
     61  8662    18 18789  1183   648  5456  1656 22568 21332     6   298
   1580    18    17 22499   331  1330    12    43    72  1643  1656    12
    131     8   262 12303     6     3 12303     6    11   276  9082 22568
   3379     5  6185    10     3    18   262 12303     6     3 12303     3
     18   276  9082 22568     1]]"
fb3d30d59ed49e87f63d3735b876d45c4c6b8939,0.0,"accuracy, reliability, FAST, METEOR",[[    0  7452     6 10581     6   377 12510     6  7934  3463  2990     1]],"Precision, Recall and F-measure",[[28464     6   419 16482    11   377    18 31038     1]]
61a9ea36ddc37c60d1a51dabcfff9445a2225725,16.6667,WSJ news articles from major news outlets,[[    0     3  8439   683  1506  2984    45   779  1506 14290     1]], the news external references in Wikipedia,[[    8  1506  3866  9811    16 16885     1]]
abe2393415e533cb06311e74ed1c5674cff8571f,0.0,accuracy,[[   0 7452    1]],"BLEU, NIST, METEOR, ROUGE-L, CIDEr","[[    3  8775 12062     6   445 13582     6  7934  3463  2990     6   391
  26260   427    18   434     6   205 13162    52     1]]"
6cd25c637c6b772ce29e8ee81571e8694549c5ab,0.0,Wikidata,[[   0 2142 2168 6757    1]],"English WIKIBIO, French WIKIBIO , German WIKIBIO ","[[ 1566 12116 14108   279  7550     6  2379 12116 14108   279  7550     3
      6  2968 12116 14108   279  7550     1]]"
7697baf8d8d582c1f664a614f6332121061f87db,0.0,AMT,[[   0   71 7323    1]],Structural Support Vector Machine,[[  472 11783    17  9709  4224 29011  5879     1]]
26327ccebc620a73ba37a95aabe968864e3392b2,66.6667,"promoting one's own points and attacking the opponents' points, tracking their relative usage throughout","[[    0     3  7312    80    31     7   293   979    11 20550     8 16383
     31   979     6  6418    70  5237  4742  1019]]",â€”promoting one's own points and attacking the opponents' points,"[[    3   318  7312    80    31     7   293   979    11 20550     8 16383
     31   979     1]]"
5a22293b055f5775081d6acdc0450f7bd5f5de04,0.0,BiLSTM with max dependency density of 82.61,"[[    0  2106  7600  2305    28  9858 27804 11048    13     3  4613     5
   4241     1]]",Unanswerable,[[ 597 3247 3321  179    1]]
92d1a6df3041667dc662376938bc65527a5a1c3c,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
627b8d7b5b985394428c974aca5ba0c1bbbba377,0.0,82,[[   0    3 4613    1]],Unanswerable,[[ 597 3247 3321  179    1]]
15cdd9ea4bae8891c1652da2ed34c87bbbd0edb8,0.0,"jokes, quotes, self deprecation etc","[[    0 10802     7     6  7599     6  1044    20  2026    75   257   672
      1]]",twitter,[[19010     1]]
4f253dfced6a749bf57a1b4984dc962ce9550184,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
cb77d6a74065cb05318faf57e7ceca05e126a80d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"CNN modelBIBREF0, Stanford CRF modelBIBREF21","[[19602   825  5972 25582   371   632     6 19796   205  8556   825  5972
  25582   371  2658     1]]"
18c5d366b1da8447b5404eab71f4cc658ba12e6f,0.0,LSTMs,[[   0    3 7600 2305    7    1]],"Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer","[[19796     3 18206     6  4174   254    63  6864     3     6     3    60
  14907   825    28     3     9   205  8556   420  3760     1]]"
b14217978ad9c3c9b6b1ce393b1b5c6e7f49ecab,0.0,"On a variety of tasks including semantic similarity, passage retrieval, and cross-en","[[    0   461     3     9  1196    13  4145   379 27632  1126   485     6
   5454 24515   138     6    11  2269    18    35]]","Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask","[[ 2415   127     9   970 10435   342 11860 25072     3 31636    23   106
      6 29153   746    16   272    53    31     7  2449  1203  8366     1]]"
5eabfc6cc8aa8a99e6e42514ef9584569cb75dec,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
78a5546e87d4d88e3d9638a0a8cd0b7debf1f09d,36.3636,"conversational recommendation, question answering, movie recommendation",[[    0  3634   138 10919     6   822 18243     6  1974 10919     1]],"conversational search, conversational question answering, conversational recommendation, conversational natural language interface to structured and semi-structured data","[[ 3634   138   960     6  3634   138   822 18243     6  3634   138 10919
      6  3634   138   793  1612  3459    12 14039    11  4772    18 16180
     26   331     1]]"
74fb77a624ea9f1821f58935a52cca3086bb0981,0.0,"3,200 messages",[[   0 6180 3632 4175    1]],"14,100 tweets",[[11363  2915 10657     7     1]]
26327ccebc620a73ba37a95aabe968864e3392b2,16.6667,"promoting one's own points and attacking the opponents' points, tracking their relative usage throughout","[[    0     3  7312    80    31     7   293   979    11 20550     8 16383
     31   979     6  6418    70  5237  4742  1019]]","The time devoted to self-coverage, opponent-coverage, and the number of adopted discussion points.","[[   37    97     3 12895    12  1044    18  9817   545     6 15264    18
   9817   545     6    11     8   381    13  7546  3071   979     5     1]]"
2da4c3679111dd92a1d0869dae353ebe5989dfd2,0.0,"CSA BIBREF1, CSA BIBREF2, CSA BIBRE","[[    0     3 24135     3  5972 25582   371  4347     3 24135     3  5972
  25582   371  4482     3 24135     3  5972 25582]]","ESTER1, ESTER2, ETAPE, REPERE","[[  262 20727  4347   262 20727  4482     3 25747  5668     6  4083  8742
    427     1]]"
71bd5db79635d48a0730163a9f2e8ef19a86cd66,0.0,"lexical overlap, word order, tenseness","[[    0     3 30949   138 21655     6  1448   455     6     3    17  5167
    655     1]]","Restrictivity , Factivity , Coreference ",[[  419 20066 10696     3     6   377 21661     3     6  9020 11788     1]]
6263b2cba18207474786b303852d2f0d7068d4b6,10.5263,"English, Chinese",[[   0 1566    6 2830    1]],"English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)","[[ 1566    41   427  2644  4824   107   201  2968    41   279    49    40
     77   201  5093    41   329     9    26  4055   201 31057    41   382
      9    23   855    23   201 16073    41 21032 13125   201  4263    41
    329    32  3523   210   201  9677    41   134    15  7115   201    11
  22831    29    41  2703    40  6801    61     1]]"
ca5a82b54cb707c9b947aa8445aac51ea218b23a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Plain dialogues with unique dialogue indexes, Plain Information Dictionary information (e.g., extracted entities) collected for the whole dialogue, Pairs of questions (i.e., user requests) and responses (i.e., bot responses), Triples in the form of (User Request, Next Action, Response)","[[16323  7478     7    28   775  7478  5538    15     7     6 16323  2784
  28767   251    41    15     5   122     5     6 21527 12311    61  4759
     21     8   829  7478     6 25072     7    13   746    41    23     5
     15     5     6  1139  6166    61    11  7216    41    23     5    15
      5     6 14761  7216   201 22709     7    16     8   607    13    41
   1265     7    49 15374     6  3021  6776     6 16361    61     1]]"
1d3e914d0890fc09311a70de0b20974bf7f0c9fe,0.0,QA,[[    0     3 23008     1]],"BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800","[[ 9580   755   254  3913    18    26   159 14608     6  9187  5972    18
     26   159 14608     6  9580   755   254  3913    18  6482     6  9580
    591 13717 11731     6  9580   357  7381     6   446   567  6892  4882
      6     3 20931  5999   427  3063     6     3  7727   725    18  6192
      1]]"
c9bc6f53b941863e801280343afa14248521ce43,100.0,English,[[   0 1566    1]],English,[[1566    1]]
286078813136943dfafb5155ee15d2429e7601d9,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
22815878083ebd2f9e08bc33a5e733063dac7a0f,66.6667,"English, Russian",[[   0 1566    6 4263    1]],Russian,[[4263    1]]
3b77b4defc8a139992bd0b07b5cf718382cb1a5f,0.0,GPT-LSTM,[[   0  350 6383   18 7600 2305    1]],AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier,"[[  432   518   127    26     7   825    57 15899     8 23446    13    66
      8  4080  1234    11   761     3     9  1249  3114    23   138  1823
    757  2474    15     7   853  7903     1]]"
0f6216b9e4e59252b0c1adfd1a848635437dfcdc,0.0,BIBREF14,[[    0     3  5972 25582   371  2534     1]],"Spanish tweets were scraped between November 8, 2017 and January 12, 2018, Affect in Tweets Distant Supervision Corpus (DISC)","[[ 5093 10657     7   130  7346    15    26   344  1671  9478  1233    11
   1762 10440  4323    71    89  4075    16 25335     7  2678  3672  2011
   6610 10052   302    41 15438   254    61     1]]"
4c18081ae3b676cc7831403d11bc070c10120f8e,8.3333,Clustering algorithms are defined as clusters of a set number of words that are grouped,"[[    0 28552    53 16783    33  4802    38  9068     7    13     3     9
    356   381    13  1234    24    33     3 31801]]",simple clustering algorithm which uses the cosine similarity between word embeddings,"[[  650  9068    53 12628    84  2284     8   576     7   630  1126   485
    344  1448 25078    26    53     7     1]]"
d78f7f84a76a07b777d4092cb58161528ca3803c,0.0,Bi-LSTM with a corresponding equivalence of a pair of,"[[   0 2106   18 7600 2305   28    3    9    3 9921    3   15 1169 2165
  1433   13    3    9 3116   13]]","This motivates us to carry out a backward greedy search over each sentence's label sequence to identify word boundaries. If two words segmented in a sentence are identified as nouns, and one word is immediately before the other, we assemble their boundaries, creating a new word candidate for entity recognition.","[[  100 18198     7   178    12  2331    91     3     9   223  2239 30337
     63   960   147   284  7142    31     7  3783  5932    12  2862  1448
  11814     5   156   192  1234  5508    15    26    16     3     9  7142
     33  4313    38   150   202     7     6    11    80  1448    19  2017
    274     8   119     6    62     3 15222    70 11814     6  1577     3
      9   126  1448  4775    21 10409  5786     5     1]]"
b249b60a8c94d0e40d65f1ffdfcac527dab57516,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
1cb100182508cf55b3509283c0e2bbcd527d625e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"refer to each article, blog post, comment, or forum posts as a document","[[2401   12  284 1108    6  875  442    6 1670    6   42 5130 3489   38
     3    9 1708    1]]"
1d197cbcac7b3f4015416f0152a6692e881ada6c,0.0,they encode the relative distance between sentence words and the answer by position-aware attention and extract,"[[    0    79 23734     8  5237  2357   344  7142  1234    11     8  1525
     57  1102    18     9  3404  1388    11  5819]]",off-the-shelf toolbox of OpenIE,"[[  326    18   532    18     7   107 10386  1464  2689    13  2384  5091
      1]]"
907b3af3cfaf68fe188de9467ed1260e52ec6cf1,0.0,Average Likert rank,[[    0 23836  2792    52    17 11003     1]],"Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different","[[21968     7    13  7847   277     6  9779    11  8889     7    33  4019
    315   344     8   356    13 10657     7     3  6443  9901  1506    11
    273   529     3  6443   135     6    68    21   377 16195  7006     6
   3137 10872     6  3159     6   419    17  1123    15    17     7    11
   4498   107 11659    79    33    59  4019   315     1]]"
2c85865a65acd429508f50b5e4db9674813d67f2,0.0,"dataset on the HCI web site, a dataset on the IMDB web site,","[[    0 17953    30     8   454  3597   765   353     6     3     9 17953
     30     8     3  5166  9213   765   353     6]]",recordings of nurse-initiated telephone conversations for congestive heart failure patients,"[[18338    13 10444    18    77   155    23   920  6596  9029    21   975
   2897  3268   842  3338  1221     1]]"
55507f066073b29c1736b684c09c045064053ba9,0.0,"vulgarity, gender, proximity to target, proximity to target","[[    0 31648   485     6  7285     6 16595    12  2387     6 16595    12
   2387     1]]","Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related","[[ 7143   564  3874     6  6619   699    11 21253     6    86 21241  5023
      6 16675    53 26567     6  5570  8310   257     6 16874    15  1947
  10133  2420     6  1318 21511  3889     6 30908   120  1341     1]]"
de0b650022ad8693465242ded169313419eed7d9,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
71fe5822d9fccb1cb391c11283b223dc8aa1640c,0.0,"LSTM-Base, LSTM-Base, LSTM-Bas","[[    0     3  7600  2305    18 14885    15     6     3  7600  2305    18
  14885    15     6     3  7600  2305    18 14885]]","LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets","[[    3 12564  1768  8055    18   858    18  6051     7     6 25335   357
    162    75     6     3 12564  1768   432 13977    41    17  1123    15
     17    18  4563   201     3 12564  1768   432 13977    41   524  6513
     18  4563   201  1699    75   382  1123    15    17    41    17  1123
     15    17    18  4563   201  2224    18  3229   157  3229 26719     6
    114     7     6    42     3    60    18    17  1123    15    17     7
      1]]"
a6d3e57de796172c236e33a6ceb4cca793dc2315,0.0,"LSTM+attention, LSTM+attention, LSTM+attention","[[    0     3  7600  2305  1220 25615     6     3  7600  2305  1220 25615
      6     3  7600  2305  1220 25615     1]]",Unanswerable,[[ 597 3247 3321  179    1]]
d27438b11bc70e706431dda0af2b1c0b0d209f96,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
b06512c17d99f9339ffdab12cedbc63501ff527e,100.0,No,[[  0 465   1]],No,[[465   1]]
b65a83a24fc66728451bb063cf6ec50134c8bfb0,0.0,selected by BIBREF0,[[    0  2639    57     3  5972 25582   371   632     1]]," Two methods: first is to simply pick initial few sentences,  second is to capture the relation between the two most important entities  (select the first sentence which contains both these entities).","[[ 2759  2254    10   166    19    12   914  1432  2332   360 16513     6
    511    19    12  4105     8  4689   344     8   192   167   359 12311
     41     7    15  3437     8   166  7142    84  2579   321   175 12311
    137     1]]"
ed67359889cf61fa11ee291d6c378cccf83d599d,0.0,a bidirectional recurrent neural network called BiLSTM,"[[    0     3     9  2647 26352     3    60 14907 24228  1229   718  2106
   7600  2305     1]]",GloVe word vectors BIBREF16 pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC),"[[ 9840   553    15  1448 12938     7     3  5972 25582   371  2938   554
     18    17 10761    30   192 17953     7    10 16885  1412    28     3
  20640     9  6051   755    41   518  1220   517  9120    11  7155   205
  10936    40    41  2823    61     1]]"
b49598b05358117ab1471b8ebd0b042d2f04b2a4,0.0,LSTMs,[[   0    3 7600 2305    7    1]],"neural bag-of-words (NBOW) model, bidirectional long short-term memory network (LSTM), attention-based encoder","[[24228  2182    18   858    18  6051     7    41 14972 15251    61   825
      6  2647 26352   307   710    18  1987  2594  1229    41  7600  2305
    201  1388    18   390 23734    52     1]]"
ad08b215dca538930ef1f50b4e49cd25527028ad,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
6ca938324dc7e1742a840d0a54dc13cc207394a1,0.0,"SST-1, SST-2, SST-3, SST-4","[[   0  180 4209 2292    6  180 4209 4949    6  180 4209 3486    6  180
  4209 4278    1]]","German newscrawl distributed by WMT'18 , English newscrawl data, WMT'18 English-German (en-de) news translation task , WMT'18 English-Turkish (en-tr) news task","[[ 2968  1506  2935   210    40  8308    57   549  7323    31  2606     3
      6  1566  1506  2935   210    40   331     6   549  7323    31  2606
   1566    18 24518    41    35    18   221    61  1506  7314  2491     3
      6   549  7323    31  2606  1566    18   382   450   157  1273    41
     35    18    17    52    61  1506  2491     1]]"
b21bc09193699dc9cfad523f3d5542b0b2ff1b8e,0.0,LSTM,[[   0    3 7600 2305    1]],MLP,[[ 283 6892    1]]
b46c0015a122ee5fb95c2a45691cb97f80de1bb6,0.0,"LSTM with a corresponding source and target domains, LSTM with ","[[   0    3 7600 2305   28    3    9    3 9921 1391   11 2387 3303    7
     6    3 7600 2305   28    3]]", one-layer CNN,[[   80    18 18270 19602     1]]
ef3567ce7301b28e34377e7b62c4ec9b496f00bf,0.0,IMDb Movie Reviews,[[    0    27 11731   115 10743 16305     1]],Groningen Meaning Bank (GMB),[[ 8554    29    53    35 25148  1925    41  7381   279    61     1]]
7bd6a6ec230e1efb27d691762cc0674237dc7967,0.0,WN18 and FB15k,[[    0     3 21170  2606    11     3 15586  1808   157     1]]," Penn Treebank, WikiText2",[[11358  7552  4739     6  2142  2168 13598    17   357     1]]
ee7e9a948ee6888aa5830b1a3d0d148ff656d864,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"roughly 40,000 Manhattan listings",[[10209     3 20431 15630 11070     1]]
ce6a3ca102a5ee62e86fc7def3b20b1f10d1eb25,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
55139fcfe04ce90aad407e2e5a0067a45f31e07e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Using Google translation API.,[[   3 3626 1163 7314 6429    5    1]]
1be54c5b3ea67d837ffba2290a40c1e720d9587f,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
c1c44fd96c3fa6e16949ae8fa453e511c6435c68,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences.","[[    3    15   658  1074    48   433    19  1126    12     8     3  3903
    776  2491    16   272 24203    31     7   554    18  9719   433     6
   2459    57   338     8  1418    13     8 28131  1612   825     8    20
   4978    52    54  3806    72  6720   295    11   793  5932     7     5
      1]]"
7561a968470a8936d10e1ba722d2f38b5a9a4d38,0.0,30K images with 5 crowdsourced descriptions each,[[    0   604   439  1383    28   305  4374 15551 15293   284     1]],"30,000",[[    3 17093     1]]
d01c51155e4719bf587d114bcd403b273c77246f,0.0,"mBERT mBERT0, cross-lingual language model mBERT","[[    0     3    51 12920   382     3    51 12920   382   632     6  2269
     18 25207  1612   825     3    51 12920   382]]","United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018","[[  907  9638 27535 10052   302     6    27  3177 16327     9    63 11736
    302     6  2384 25252 21869     7   846     1]]"
d53299fac8c94bd0179968eb868506124af407d1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Table TABREF10,  The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set,  While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject","[[ 4398     3  3221 25582   371  1714     6    37   480 17235   853  7903
   1727    12  1912     8   200   640    66   662 15905     5   100    19
   1077   788    12     8  1249    18  4057  1405    13     8   331   356
      6   818   175   853  7903     7   410    59  1912  1989   168     6
     79   370     3     9   207  1684   500    21   647   161    30    48
   1426     1]]"
ddb23a71113cbc092cbc158066d891cae261e2c6,0.0,YouTube videos,[[   0 5343 3075    1]],The BreakingNews dataset,[[   37 11429    53  6861     7 17953     1]]
968b7c3553a668ba88da105eff067d57f393c63f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],those that contain a high number of retweets,"[[ 273   24 3480    3    9  306  381   13    3   60   17 1123   15   17
     7    1]]"
f88036174b4a0dbf4fe70ddad884d16082c5748d,100.0,No,[[  0 465   1]],No,[[465   1]]
ecc63972b2783ee39b3e522653cfb6dc5917d522,26.6667,they encourage understanding of the literature as part of their objective function,[[   0   79 2454 1705   13    8 6678   38  294   13   70 5997 1681    1]],"They group the existing works in terms of the objective function they optimize - within-tweet relationships, inter-tweet relationships, autoencoder, and weak supervision.","[[  328   563     8  1895   930    16  1353    13     8  5997  1681    79
  13436     3    18   441    18    17  1123    15    17  3079     6  1413
     18    17  1123    15    17  3079     6  1510    35  4978    52     6
     11  5676 15520     5     1]]"
0af16b164db20d8569df4ce688d5a62c861ace0b,0.0,"BIBREF5, FriendsBERT, and CARERBERT","[[    0     3  5972 25582   371 11116  9779 12920   382     6    11     3
  22443   448 12920   382     1]]","BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN","[[  272 15251    18 12564     6   272 15251    18  8556     5   332  4936
  10665    18  8556     6  5027   254 17235     6   205    18 13598    17
    254 17235     1]]"
f56d07f73b31a9c72ea737b40103d7004ef6a079,15.3846,JP Morgan BIBREF22 and XML BIBREF23,"[[    0   446   345 11147     3  5972 25582   371  2884    11     3     4
   6858     3  5972 25582   371  2773     1]]",A homographic and heterographic benchmark datasets by BIBREF9.,"[[   71 13503 14797    11 26481 14797 15705 17953     7    57     3  5972
  25582   371  8797     1]]"
fbaf060004f196a286fef67593d2d76826f0304e,11.1111,"a training set that contains reviews from different domains in English (e.g.,","[[   0    3    9  761  356   24 2579 2456   45  315 3303    7   16 1566
    41   15    5  122    5    6]]","Amazon reviews, Yelp restaurant reviews, restaurant reviews",[[2536 2456    6 7271   40  102 2062 2456    6 2062 2456    1]]
67e9e147b2cab5ba43572ce8a17fc863690172f0,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"directly solicits informative keywords from the crowd for model training, thereby providing human-understandable explanations for the improved model","[[ 1461  8081     7 11152 12545    45     8  4374    21   825   761     6
      3 12550  1260   936    18  7248  2976   179  7295     7    21     8
   3798   825     1]]"
59e58c6fc63cf5b54b632462465bfbd85b1bf3dd,0.0,No,[[  0 465   1]],our methodology does not use a seed list of offensive words,"[[   69 15663   405    59   169     3     9  6677   570    13 12130  1234
      1]]"
1cb100182508cf55b3509283c0e2bbcd527d625e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles","[[ 1139  2622    12  1506 12931  2984    42    12   875  3489     6  5130
   3489     6   875  3489     6  1506 12931  2984     1]]"
15e481e668114e4afe0c78eefb716ffe1646b494,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],generator network to capture the event-related patterns,[[9877 1229   12 4105    8  605   18 3897 4264    1]]
d6ea7a30b0b61ae126b00b59d2a14fff2ef887bf,0.0,"LSTM, LSTM, LSTM+CNN, LSTM+","[[    0     3  7600  2305     6     3  7600  2305     6     3  7600  2305
   1220   254 17235     6     3  7600  2305  1220]]","Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use","[[13979  2273    11  4999  4768    54   370  3160    30   746    11   199
     28  4896  1427  5109  4890   135     5     6  1979    12  1317 15015
      6  7013   169     1]]"
184b0082e10ce191940c1d24785b631828a9f714,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization","[[18712     7   344 23777    11     8 30237  7586    33  5676     6    84
   2428   165  9570    21  4290  4505  1635  1707     1]]"
1591068b747c94f45b948e12edafe74b5e721047,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],10000,[[ 335 2313    1]]
c2eb743c9d0baf1781c3c0df9533fab588250af3,0.0,"long-term memory (LSTM), gated recurrent unit (GRU","[[    0   307    18  1987  2594    41  7600  2305    61     3     6 10530
     26     3    60 14907  1745    41   517  8503]]","Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers","[[  472 13365     3  7600  2305     7     6  7845    18     9  3404   472
  13365     3  7600  2305     7     6  4892    17  1433   695  4978    52
      7     6  2224    18 18270  4501  7903     7     1]]"
a15bc19674d48cd9919ad1cf152bf49c88f4417d,28.5714,DSTC2,[[   0    3 3592 3838  357    1]],The manual transcriptions of the DSTC2 training set ,"[[   37  3354 20267     7    13     8     3  3592  3838   357   761   356
      1]]"
c7b6e6cb997de1660fd24d31759fe6bb21c7863f,0.0,10 electrodes,[[    0   335 22173     7     1]],Unanswerable,[[ 597 3247 3321  179    1]]
f17ca24b135f9fe6bb25dc5084b13e1637ec7744,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Best: Expansion (Exp). Worst: Comparison (Comp).,"[[ 1648    10  1881  2837  1938    41 12882   137 11287     7    17    10
  30760    41  5890   102   137     1]]"
26c2e1eb12143d985e4fb50543cf0d1eb4395e67,11.7647,"stereotypes may be pervasive enough for the data to be consistently biased, stereotypes may","[[    0 26524     7   164    36   399  9856   757   631    21     8   331
     12    36  8182 30026     6 26524     7   164]]",adjectives are used to create â€œmore narrow labels [or subtypes] for individuals who do not fit with general social category expectationsâ€,"[[31268     7    33   261    12   482   105  3706  5658 11241   784   127
    769  6137     7   908    21  1742   113   103    59  1400    28   879
    569  3295  4454   153     1]]"
d667731ea20605580c398a1224a0094d1155ebbb,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
a3d9b101765048f4b61cbd3eaa2439582ebb5c77,100.0,"English, Chinese",[[   0 1566    6 2830    1]],"English , Chinese",[[1566    3    6 2830    1]]
4986f420884f917d1f60d3cea04dc8e64d3b5bf1,18.1818,crosslingual latent variables,[[    0  2269 25207    50  4669 11445     1]],CLV as a parent of the two corresponding role variables,"[[  205 15086    38     3     9  4208    13     8   192     3  9921  1075
  11445     1]]"
b637d6393ef3af7462917b81861531022b291933,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
71a0c4f19be4ce1b1bae58a6e8f2a586e125d074,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"quality class labels assigned by the Wikipedia community, a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI","[[  463   853 11241  7604    57     8 16885   573     6     3     9  1040
     19  1702    12    43   118  4307    41    23     5    15     5    19
  18294  3783    15    26    61     3    99    34  6407     3     9  1040
     16     8     3  9213  6892  3501    42    19  2904  4307    57   136
     13     8   826 13653    10    71  8440     6     3  6037   567  6892
      6 10144   188  8440     6   262   188  8440     6     3  3221  8440
      6   445 21563     6     3  4666  6858     6     3  4666 12564     6
     42 22656   196     1]]"
e330e162ec29722f5ec9f83853d129c9e0693d65,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
e949b28f6d1f20e18e82742e04d68158415dc61e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"For SLC task : Ituorp, ProperGander and YMJA  teams had better results.
For FLC task: newspeak and Antiganda teams had better results.","[[  242   180  6480  2491     3    10    94    76   127   102     6 13543
     49   517 11849    11     3   476   329 13853  2323   141   394   772
      5   242  7212   254  2491    10  1506 14661    11  4066   122   232
      9  2323   141   394   772     5     1]]"
ad1be65c4f0655ac5c902d17f05454c0d4c4a15d,8.4507,MCScript provides a dataset for assessing the contribution of script knowledge to machine comprehension,"[[    0     3  3698 18255   795     3     9 17953    21     3 20861     8
   6275    13  4943  1103    12  1437 27160     1]]","More than 2,100 texts were paired with 15 questions each, resulting in a total number of approx. 32,000 annotated questions. 13% of the questions are not answerable.  Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based).  The final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%).","[[ 1537   145  3547  2915 14877   130     3 13804    28   627   746   284
      6     3  5490    16     3     9   792   381    13  1120 12907     5
    220  8630    46  2264   920   746     5   209  5170    13     8   746
     33    59  1525   179     5  3387    13     8  1525   179   746     6
  10372 19129   228    36  9986    45     8  1499  1461    41  6327    18
    390    61    11  6180  4729   591   746   831     8   169    13  1017
      7  5167  1103    41 11815    18   390   137    37   804 17953 13009
  10670  1298  3288   746     6  6180   927  2555    13    84  1457  1017
      7  5167  1103    41    23     5    15     5  2307     5  5988   137
      1]]"
6b53e1f46ae4ba9b75117fc6e593abded89366be,0.0,"classification, classification, classification based on sequences, classification based on sequences","[[    0 13774     6 13774     6 13774     3   390    30  5932     7     6
  13774     3   390    30  5932     7     1]]","As the simplest baseline, a sensitive data recogniser and classifier, Conditional Random Fields (CRF), spaCy ","[[  282     8     3 21120 20726     6     3     9  6280   331 16236    52
     11   853  7903     6 24239   138 25942  7257     7    41  4545   371
    201  4174   254    63     1]]"
22b740cc3c8598247ee102279f96575bdb10d53f,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
d859cc37799a508bbbe4270ed291ca6394afce2c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"LDA, non-negative matrix factorization (NMF)","[[  301  4296     6   529    18 31600 16826  2945  1707    41 16568   371
     61     1]]"
8acab64ba72831633e8cc174d5469afecccf3ae9,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
67b66fe67a3cb2ce043070513664203e564bdcbd,0.0,CAS-based baseline,[[    0     3 18678    18   390 20726     1]],"They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF.","[[  328  4048    28     8   826  2250    10    57  1276   588     7    35
      3 26224     6    57  6110   348  3142    11   644     3 26224     6
     57 21475  3828  9789     9    11  4556 20856    77     3 26224     6
     57  2964    26  7392     9     3 26224     6    86 13629  7436    11
    411    32    17     9     3 26224     6    57   584 12041    32  7168
      9     3 26224     6    57    41   254     9    23     3    15    17
    491     5     6   846   201    11   205  8556     5     1]]"
4aa9b60c0ccd379c6fb089c84a6c7b872ee9ec4f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Adaptive Multi-task Learning
, Margin Ranking (MR) Loss
, Pairwise Neural Ranking Model
","[[    3 14808   757  4908    18 23615  6630     3     6 16409    77 29153
     41  9320    61  3144     7     3     6 25072 10684  1484  9709 29153
   5154     1]]"
af34051bf3e628c1e2a00b110bb84e5f018b419f,12.3077,"MT encoder, MT decoder, End-to-End MT","[[    0     3  7323 23734    52     6     3  7323    20  4978    52     6
   3720    18   235    18  8532    26     3  7323]]","Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation","[[31664  5097 20726     6 23734    52   554    18 13023     6    16    84
      8  5097 23734    52    19  2332  1601    45    46    71  6857   825
      6    20  4978    52   554    18 13023     6    16    84     8  5097
     20  4978    52    19  2332  1601    45    46     3  7323   825     6
  23734    52    18   221  4978    52   554    18 13023     6   213   321
      8 23734    52    11    20  4978    52    33   554    18    17 10761
      6   186    18   235    18   348    63  1249    18 23615   825   213
      8 23734    52     7    11    20  4978    52     7    33     3  9942
     45   554    18    17 10761    71  6857    11     3  7323  2250     6
  29260  1220  2026    18  9719    10     3  5972 25582   371  2606     3
   9213  6892    10  9707    87    29     9     9    75    40    87   188
     29 12518     7    32   102  1063  2298   254  2606  4382     3     9
  19938  1249    18 23615  1998    21  5023  7314     1]]"
53dfcd5d7d2a81855ec1728f0d8e6e24c5638f1e,0.0,F1 score,[[   0  377  536 2604    1]],"BLEU-1, Meteor ,  Rouge-L ","[[    3  8775 12062  2292     6  8146    15   127     3     6 23777    18
    434     1]]"
e97186c51d4af490dba6faaf833d269c8256426c,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
a616a3f0d244368ec588f04dfbc37d77fda01b4c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese","[[ 2830 31057     6 22982     6  1566     6 26047    29     6 28124     6
   2379     6 20428     6 16073     6  4263     6  5093     6  4320     7
  17771   173    23     6  6214    15  2830     1]]"
897ba53ef44f658c128125edd26abf605060fb13,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
e374169ee10f835f660ab8403a5701114586f167,52.6316,"user's screen name, profile image, location, description, followers count, and friend count","[[    0  1139    31     7  1641   564     6  3278  1023     6  1128     6
   4210     6 10076  3476     6    11  1565  3476]]","username, display name, profile image, location, description",[[20304     6  1831   564     6  3278  1023     6  1128     6  4210     1]]
1142784dc4e0e4c0b4eca1feaf1c10dc46dd5891,0.0,4 categories,[[   0  314 5897    1]], two salient roles called Anchors and Punctual speakers,"[[  192  5394  4741  6270   718 25874     7    11 18266    75    17  3471
   7215     1]]"
70e9210fe64f8d71334e5107732d764332a81cb1,0.0,fully convolutional front-end model,[[    0  1540   975 24817   138   851    18   989   825     1]],CNN-DNN-BLSTM-HMM,[[19602    18   308 17235    18  8775   134  2305    18   566  8257     1]]
52f8a3e3cd5d42126b5307adc740b71510a6bdf5,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],ReviewQA's test set,[[ 4543 23008    31     7   794   356     1]]
67ec8ef85844e01746c13627090dc2706bb2a4f3,0.0,They use RNNs instead of transformers,[[    0   328   169   391 17235     7  1446    13 19903     7     1]],Unanswerable,[[ 597 3247 3321  179    1]]
aa7d327ef98f9f9847b447d4def04889b4508d7a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],INLINEFORM2 is queried for the â€œmost informativeâ€ instance(s) INLINEFORM3,"[[ 3388 20006 24030   357    19   238  9889    21     8   105  5463 11152
    153  3421   599     7    61  3388 20006 24030   519     1]]"
e330e162ec29722f5ec9f83853d129c9e0693d65,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
8de9f14c7c4f37ab103bc8a639d6d80ade1bc27b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],1-hop links to 2-hops,[[ 8218 10776  2416    12  8401 10776     7     1]]
31735ec3d83c40b79d11df5c34154849aeb3fb47,0.0,"BIBREF5, BIBREF6, BIBREF8, BIBREF","[[    0     3  5972 25582   371 11116     3  5972 25582   371 11071     3
   5972 25582   371 11864     3  5972 25582   371]]",20 annotatos from author's institution,[[ 460   46 2264    9  235    7   45 2291   31    7 6568    1]]
bd40f33452da7711b65faaa248aca359b27fddb6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],BERT had 76.6 F1 macro score on x-stance dataset.,"[[  272 24203   141   489 28833   377   536 11663  2604    30     3   226
     18  8389 17953     5     1]]"
0f6216b9e4e59252b0c1adfd1a848635437dfcdc,0.0,BIBREF14,[[    0     3  5972 25582   371  2534     1]], Selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment provided by organizers and  tweets translated form English to Spanish.,"[[22246    13 10657     7    28    21   284 10657     3     9  3783     3
  16012     8 13182    13     8 13868    42  6493   937    57 14250     7
     11 10657     7 15459   607  1566    12  5093     5     1]]"
d5a8fd8bb48dd1f75927e874bdea582b4732a0cd,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
e86130c5b9ab28f0ec539c2bed1b1ae9efb99b7d,0.0,"238,432 sentences",[[    0   204  3747     6   591  2668 16513     1]],Unanswerable,[[ 597 3247 3321  179    1]]
0af16b164db20d8569df4ce688d5a62c861ace0b,0.0,"BIBREF5, FriendsBERT, and CARERBERT","[[    0     3  5972 25582   371 11116  9779 12920   382     6    11     3
  22443   448 12920   382     1]]","bag-of-words (BOW), term frequencyâ€“inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe","[[ 2182    18   858    18  6051     7    41   279 15251   201  1657  7321
    104 23536  1708  7321    41  9164  4309   371   201 24228    18   390
   1448 25078    26    53     6  7736  3040   419 22430    41 12564   201
  25942  6944    41  8556   201  5027   254 17235     3  5972 25582   371
   1714    28  2332  1448 25078    26    53    38  9840   553    15     1]]"
4ac2c3c259024d7cd8e449600b499f93332dab60,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
9a9d225f9ac35ed35ea02f554f6056af3b42471d,0.0,"collocations, tense errors, idioms","[[    0  7632 14836     7     6     3    17  5167  6854     6     3 19916
     51     7     1]]","(VVD shop_VV0 II, VVD shopping_VVG II)","[[   41   553   553   308  1814   834   553   553   632  2466     6   584
    553   308  2309   834   553 17217  2466    61     1]]"
c6a0b9b5dabcefda0233320dd1548518a0ae758e,0.0,Word2Vec,[[   0 4467  357  553   15   75    1]],CJFA encoder ,[[  205   683  4795 23734    52     1]]
312e9cc11b9036a6324bdcb64eca6814053ffa17,0.0,agreement of 86.0% between the report and the author and the annotations of the patient,"[[    0  2791    13     3  3840     5  6932   344     8   934    11     8
   2291    11     8 30729     7    13     8  1868]]",Unanswerable,[[ 597 3247 3321  179    1]]
6236762b5631d9e395f81e1ebccc4bf3ab9b24ac,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
54830abe73fef4e629a36866ceeeca10214bd2c8,14.8148,LDA and Gibbs sampling methods,[[    0   301  4296    11 17223   115     7 17222  2254     1]],"the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter","[[    8   301  4296  6315    12 10919  1002    11   787     8  3172    13
    585     6    62    43  7463  1100  4423  2984    30    48  1426    11
   2569     3     9  1104    32  3114    63    13 10919  1002     3   390
     30   301  4296    13     8  1100   585     6    62 14434  6827 10038
     11 18548   518 13653  2984    45     3  9213  6892   475    11   261
      8 17223   115     7 17222 12628    38    46  5002 15577     1]]"
a99fdd34422f4231442c220c97eafc26c76508dd,100.0,No,[[  0 465   1]],No,[[465   1]]
f8da63df16c4c42093e5778c01a8e7e9b270142e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],we compare the Annodis segmentation with the automatically produced segmentation,"[[  62 4048    8 6206   32   26  159 5508  257   28    8 3269 2546 5508
   257    1]]"
06cc8fcafc0880cf69a2514bb7341642b9833041,0.0,"7,007",[[   0 7973 1206  940    1]],100 000 documents,[[ 910 6078 2691    1]]
c7eb71683f53ab7acffd691a36cad6edc7f5522e,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
d3ff2986ca8cb85a9a5cec039c266df756947b43,0.0,feature sets that are more predictive than the baselines,[[    0  1451  3369    24    33    72 27875   145     8 20726     7     1]],"words embeddings, style, and morality features","[[ 1234 25078    26    53     7     6   869     6    11  4854   485   753
      1]]"
935d6a6187e6a0c9c0da8e53a42697f853f5c248,0.0,industry,[[  0 681   1]],the aggregate of enterprises in a particular field,[[    8 12955    13 14876    16     3     9  1090  1057     1]]
eb2d5edcdfe18bd708348283f92a32294bb193a5,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"KG-A2C, A2C, A2C-chained, A2C-Explore","[[    3 18256    18   188   357   254     6    71   357   254     6    71
    357   254    18 19836    15    26     6    71   357   254    18 12882
    322    15     1]]"
32d99dcd8d46e2cda04a9a9fa0e6693d2349a7a9,16.6667,a modification to the objective function that takes the vector representations into account,"[[    0     3     9 12767    12     8  5997  1681    24  1217     8 12938
   6497     7   139   905     1]]",An additive term added to the cost function for any one of the words of concept word-groups,"[[  389 20541  1657   974    12     8   583  1681    21   136    80    13
      8  1234    13  2077  1448    18 10739     7     1]]"
76ed74788e3eb3321e646c48ae8bf6cdfe46dca1,12.5,"long vowels, which are explicitly written, and short vowels, aka diacri","[[    0   307 22121  3573     6    84    33 21119  1545     6    11   710
  22121  3573     6     3  5667  1227     9  2685]]","POS, gender/number and stem POS",[[    3 16034     6  7285    87  5525  1152    11  6269     3 16034     1]]
565189b672efee01d22f4fc6b73cd5287b2ee72c,0.0,WN15K,[[    0     3 21170  1808   439     1]],"Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)","[[ 5578    52    40 11736   302     3     6   549  7323  1506  4377  1412
      6  3529    18  5890   297  1208  9169     6 16885    45   549  7323
   1412     6  4908    18  7443     6  3371    18 13355  6921     6 13836
      6  7155    18   254 10936    40    41   518  7323  1233    61     1]]"
74b338d5352fe1a6fd592e38269a4c81fe79b866,0.0,"sarcastic features, sexting features, sexting features","[[    0     3     7  4667 10057   753     6     3     7 10398    53   753
      6     3     7 10398    53   753     1]]","Readability (RED),  Number of Words (LEN), Avg. Fixation Duration (FDUR), Avg. Fixation Count (FC), Avg. Saccade Length (SL), Regression Count (REG), Skip count (SKIP), Count of regressions from second half
to first half of the sentence (RSF), Largest Regression Position (LREG),  Edge density of the saliency gaze
graph (ED),  Fixation Duration at Left/Source
(F1H, F1S),  Fixation Duration at Right/Target
(F2H, F2S),  Forward Saccade Word Count of
Source (PSH, PSS),  Forward SaccadeWord Count of Destination
(PSDH, PSDS), Regressive Saccade Word Count of
Source (RSH, RSS),  Regressive Saccade Word Count of
Destination (RSDH, RSDS)","[[ 3403  2020    41 13729   201  7720    13  4467     7    41  3765   567
    201    71   208   122     5 14269   257 20610    41 13916  5905   201
     71   208   122     5 14269   257     3 10628    41  5390   201    71
    208   122     5 22050  6615   312  1725   189    41  5629   201   419
  22430     3 10628    41  4386   517   201 25378  3476    41 10047  4629
    201     3 10628    13 26625     7    45   511   985    12   166   985
     13     8  7142    41  5249   371   201  7199     7    17   419 22430
  14258    41 12564  8579   201 11690 11048    13     8  5394    23  4392
  15618  8373    41  2326   201 14269   257 20610    44 14298    87 23799
     41   371   536   566     6   377   536   134   201 14269   257 20610
     44  5068    87   382   291  2782    41   371   357   566     6   377
    357   134   201 25633 22050  6615  4467     3 10628    13  9149    41
   4176   566     6  5610   134   201 25633 22050  6615   518   127    26
      3 10628    13 19344   257    41  4176 15538     6 11090   134   201
    419 10292   757 22050  6615  4467     3 10628    13  9149    41  5249
    566     6 14348   201   419 10292   757 22050  6615  4467     3 10628
     13 19344   257    41  5249 15538     6     3  5249  3592    61     1]]"
7793805982354947ea9fc742411bec314a6998f6,0.0,manually generated,[[    0 12616  6126     1]],Automatic,[[19148     1]]
32a3c248b928d4066ce00bbb0053534ee62596e7,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],morphosyntactic descriptions (MSD),"[[    3  8886    32     7    63    29    17  2708   447 15293    41  4211
    308    61     1]]"
eac9dae3492e17bc49c842fb566f464ff18c049b,0.0,"gist, grammatical components",[[   0    3  122  343    6    3 5096 4992  138 3379    1]],"claim, premise, backing, rebuttal, and refutation","[[ 1988     6     3 17398     6 16057     6     3    60  2780  1947     6
     11  6273    76  6821     1]]"
777217e025132ddc173cf33747ee590628a8f62f,0.0,a random task and a random task to learn distributed representations of the vocabulary,"[[    0     3     9  6504  2491    11     3     9  6504  2491    12   669
   8308  6497     7    13     8 19067     1]]","On each dataset, we compare four approaches based on sefe with two efe BIBREF10 baselines. All are fit using sgd BIBREF34 . In particular, we compare the following methods:","[[  461   284 17953     6    62  4048   662  6315     3   390    30   142
     89    15    28   192     3    15    89    15     3  5972 25582   371
   1714 20726     7     5   432    33  1400   338     3     7   122    26
      3  5972 25582   371  3710     3     5    86  1090     6    62  4048
      8   826  2254    10     1]]"
54e945ea4b014e11ed4e1e61abc2aa9e68fea310,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],average target BLEU score of 29.65,[[ 1348  2387     3  8775 12062  2604    13  2838     5  4122     1]]
a3bb9a936f61bafb509fa12ac0a61f91abcc5106,22.2222,"Open-domain TREC question corpus, biomedical","[[    0  2384    18 22999   332 20921   822 11736   302     6  2392  2726
   1950     1]]","ARC, TREC, GARD, MLBioMedLAT","[[    3 18971     6   332 20921     6   350 10327     6 26497    23    32
  20123  4569   382     1]]"
41b2355766a4260f41b477419d44c3fd37f3547d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"systematic and substantial racial biases, biases from data collection, rules of annotation","[[20036    11  7354     3    52     9  4703 14387    15     7     6 14387
     15     7    45   331  1232     6  2219    13 30729     1]]"
c37f65c9f0d543a35c784263b79236ccf1c44fac,0.0,sequence-to-sequence model,[[   0 5932   18  235   18    7   15  835 3772  825    1]],a Convolutional Neural Network (CNN),"[[    3     9  1193 24817   138  1484  9709  3426    41   254 17235    61
      1]]"
8c48c726bb17a17d70ab29db4d65a93030dd5382,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"57,505 sentences",[[    3  3436     6  1752   755 16513     1]]
2439b6b92d73f660fe6af8d24b7bbecf2b3a3d72,23.5294,by comparing the candidate and the candidate mentions on the same document,"[[    0    57     3 14622     8  4775    11     8  4775  2652     7    30
      8   337  1708     1]]",by evaluating their model on five different benchmarks,[[   57     3 17768    70   825    30   874   315 15705     7     1]]
b66c9a4021b6c8529cac1a2b54dacd8ec79afa5f,30.0,local context is the state of the union of all entities in a given global context,"[[    0   415  2625    19     8   538    13     8  7021    13    66 12311
     16     3     9   787  1252  2625     1]]","global (the whole document), local context (e.g., the section/topic)","[[ 1252    41   532   829  1708   201   415  2625    41    15     5   122
      5     6     8  1375    87 19710    61     1]]"
6415f38a06c2f99e8627e8ba6251aa4b364ade2d,0.0,"Nguni, English, Xitsonga, ven, tshiV","[[   0  445 8765   23    6 1566    6    3    4 7085 2444    9    6    3
  1926    6    3   17 5605  553]]",Unanswerable,[[ 597 3247 3321  179    1]]
d4db7df65aa4ece63e1de813e5ce98ce1b4dbe7f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values.","[[23636 10646  2440    33    72   952    12   483    70  3278 12978   145
     70 10076   117     8  2440   103    59   483    70 20304     7     6
    298    70 10076   483    70 20304     7     3     9   418   117     8
   2440  2134    12   143   126  1112  1341    12  1767 15816  2620     6
    298     8 10076   143     3 31685   705  1341  1112    12  1767 15816
   2620     5     1]]"
7bd6a6ec230e1efb27d691762cc0674237dc7967,0.0,WN18 and FB15k,[[    0     3 21170  2606    11     3 15586  1808   157     1]],"Penn Treebank (PTB) , WikiText2 (WT-2)","[[11358  7552  4739    41  6383   279    61     3     6  2142  2168 13598
     17   357    41   518   382    18  7318     1]]"
9a05a5f4351db75da371f7ac12eb0b03607c4b87,0.0,ACL NLP 20K,[[   0   71 8440  445 6892  460  439    1]],"Europarl BIBREF31, MultiUN BIBREF32","[[ 5578    52    40     3  5972 25582   371  3341     6  4908  7443     3
   5972 25582   371  2668     1]]"
37a79be0148e1751ffb2daabe4c8ec6680036106,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],anti-nuclear-power,[[1181   18   29   76 2482  291   18 6740    1]]
2c6b50877133a499502feb79a682f4023ddab63e,0.0,Chinese,[[   0 2830    1]],Simple English,[[9415 1566    1]]
e9cfbfdf30e48cffdeca58d4ac6fdd66a8b27d7a,10.9091,"By annotators who have access to a large corpus of documents, they can identify","[[    0   938    46  2264  6230   113    43   592    12     3     9   508
  11736   302    13  2691     6    79    54  2862]]","They break down the task of importance annotation to the level of single propositions and obtain a score for each proposition indicating its importance in a document cluster, such that a ranking according to the score would reveal what is most important and should be included in a summary.","[[  328  1733   323     8  2491    13  3172 30729    12     8   593    13
    712 12491     7    11  3442     3     9  2604    21   284 12491     3
  15716   165  3172    16     3     9  1708  9068     6   224    24     3
      9 11592  1315    12     8  2604   133  6731   125    19   167   359
     11   225    36  1285    16     3     9  9251     5     1]]"
0737954caf66f2b4c898b356d2a3c43748b9706b,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
f7a89b9cd2792f23f2cb43d50a01b8218a6fbb24,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"PER, LOC, ORG, MISC",[[   3 8742    6  301 5618    6 4674  517    6 8161 4112    1]]
adbf33c6144b2f5c40d0c6a328a92687a476f371,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
eb2d5edcdfe18bd708348283f92a32294bb193a5,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],a score of 40,[[   3    9 2604   13 1283    1]]
500a8ec1c56502529d6e59ba6424331f797f31f0,0.0,"70,000",[[    0     3 28891     1]],700,[[12283     1]]
d1ec42b2b5a3c956ff528543636e024bfde5e5ba,0.0,"BIBREF13, BIBREF16","[[    0     3  5972 25582   371  2368     6     3  5972 25582   371  2938
      1]]","Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN","[[ 4564    49    18 13714     6  4564    49    18 13714  1220   345    32
      7     6  4564    49    18 13714  1220   134   265    15    18  6245
      6  4564    49    18 13714  1220   345    32     7    18  6245     6
   4564    49    18 13714  1220 12831    18   448 26260   427     6  4564
     49    18 13714  1220 12831    18 25842     1]]"
17a1eff7993c47c54eddc7344e7454fbe64191cd,0.0,BLEU scores,[[    0     3  8775 12062  7586     1]],Human evaluation for interpretability using the word intrusion test and automated evaluation for interpretability using a semantic category-based approach based on the method and category dataset (SEMCAT).,"[[ 3892  5002    21  7280  2020   338     8  1448  2155 11733   794    11
  10069  5002    21  7280  2020   338     3     9 27632  3295    18   390
   1295     3   390    30     8  1573    11  3295 17953    41   134  6037
  18911   137     1]]"
1038542243efe5ab3e65c89385e53c4831cd9981,0.0,Ca and Cb,[[   0 1336   11  205  115    1]],"DTA18, DTA19",[[ 309 3221 2606    6  309 3221 2294    1]]
de0b650022ad8693465242ded169313419eed7d9,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
8a1c0ef69b6022a0642ca131a8eacb5c97016640,0.0,"vulgarity, profanity, slang, slang, slang,","[[    0 31648   485     6  7108   152   485     6     3     7  4612     6
      3     7  4612     6     3     7  4612     6]]",text sequences of context tweets,[[ 1499  5932     7    13  2625 10657     7     1]]
d3ca5f1814860a88ff30761fec3d860d35e39167,0.0,"LSTM, LSTM-based methods",[[   0    3 7600 2305    6    3 7600 2305   18  390 2254    1]],"Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)","[[24210   695 12395    63     6 14230    15    26  3188  7980  1015  4946
   4817    49    41   518   371  4209   201   380 12938  4096    41   134
  12623   201  1706   138  6504  4120    41  4545   371    61     1]]"
1e185a3b8cac1da939427b55bf1ba7e768c5dae4,0.0,"BERT-Base, Word2Vec, Glove","[[    0   272 24203    18 14885    15     6  4467   357   553    15    75
      6  9840   162     1]]",VAE based phone classification,[[ 9039   427     3   390   951 13774     1]]
9de2f73a3db0c695e5e0f5a3d791fdc370b1df6e,0.0,No,[[  0 465   1]],both,[[321   1]]
97d0f9a1540a48e0b4d30d7084a8c524dd09a4c3,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Chunks is group of tweets from single account that  is consecutive in time - idea is that this group can show secret intention of malicious accounts.,"[[ 4004  6513     7    19   563    13 10657     7    45   712   905    24
     19 12096    16    97     3    18   800    19    24    48   563    54
    504  2829  8762    13 22326  3744     5     1]]"
0f12dc077fe8e5b95ca9163cea1dd17195c96929,6.6667,Gender classification criteria,[[    0   350  3868 13774  6683     1]],"generated with the various combinations of INLINEFORM4 person INLINEFORM5 and INLINEFORM6 emotion word INLINEFORM7 values across the eleven templates, differ only in one word corresponding to gender or race","[[ 6126    28     8   796 15617    13  3388 20006 24030   591   568  3388
  20006 24030   755    11  3388 20006 24030   948 13868  1448  3388 20006
  24030   940  2620   640     8 20394  7405     6  7641   163    16    80
   1448     3  9921    12  7285    42  1964     1]]"
3d7d865e905295d11f1e85af5fa89b210e3e9fdf,0.0,10,[[  0 335   1]],100 ,[[910   1]]
0ab3df10f0b7203e859e9b62ffa7d6d79ffbbe50,0.0,"precision, recall, F-measure",[[    0 11723     6  7881     6   377    18 31038     1]],average classification accuracy,[[ 1348 13774  7452     1]]
d51dc36fbf6518226b8e45d4c817e07e8f642003,0.0,"89,258 sentences",[[    0     3  3914     6   357  3449 16513     1]],3606,[[9181  948    1]]
06be47e2f50b902b05ebf1ff1c66051925f5c247,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
b6ae8e10c6a0d34c834f18f66ab730b670fb528c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"training data has posts from politics, business, science and other popular topics; the trained model is applied to millions of unannotated posts on all of Reddit","[[ 761  331   65 3489   45 6525    6  268    6 2056   11  119 1012 4064
   117    8 4252  825   19 2930   12 4040   13   73  152 2264  920 3489
    30   66   13 1624   26  155    1]]"
de5b6c25e35b3a6c5e40e350fc5e52c160b33490,3.9216,their model outperforms by 0.86 on fluency and 0.98 on grammar,"[[    0    70   825    91   883  2032     7    57  4097  3840    30  6720
   4392    11  4097  3916    30 19519     1]]","Best proposed model result vs best previous result:
Arxiv dataset: Rouge 1 (43.62 vs 42.81), Rouge L (29.30 vs 31.80), Meteor (21.78 vs 21.35)
Pubmed dataset: Rouge 1 (44.85 vs 44.29), Rouge L (31.48 vs 35.21), Meteor (20.83 vs 20.56)","[[ 1648  4382   825   741     3   208     7   200  1767   741    10  1533
    226    23   208 17953    10 23777   209    41  4906     5  4056     3
    208     7  6426     5  4959   201 23777   301    41  3166     5  1458
      3   208     7  2664     5  2079   201  8146    15   127    41  2658
      5  3940     3   208     7  1401     5  2469    61 22057  2726 17953
     10 23777   209    41  3628     5  4433     3   208     7  8537     5
   3166   201 23777   301    41  3341     5  3707     3   208     7  3097
      5  2658   201  8146    15   127 17543     5  4591     3   208     7
    460     5  4834    61     1]]"
a616a3f0d244368ec588f04dfbc37d77fda01b4c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese","[[ 2830 31057     6 22982     6  1566     6 26047    29     6 28124     6
   2379     6 20428     6 16073     6  4263     6  5093     6  4320     7
  17771   173    23     6  6214    15  2830     1]]"
5eabfc6cc8aa8a99e6e42514ef9584569cb75dec,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
1be54c5b3ea67d837ffba2290a40c1e720d9587f,100.0,No,[[  0 465   1]],No,[[465   1]]
f88036174b4a0dbf4fe70ddad884d16082c5748d,0.0,No,[[  0 465   1]],Unanswerable,[[ 597 3247 3321  179    1]]
e8f969ffd637b82d04d3be28c51f0f3ca6b3883e,0.0,"AMR, Meaning Representation, Recurrent Networks, Recurrent Networks, Random","[[    0    71  9320     6 25148   419 12640   257     6   419 14907  3426
      7     6   419 14907  3426     7     6 25942]]","standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L","[[ 1068 10264   517  5078     3  7959     6   419 16482     6 28464    11
   3388 20006 24030   632  7586    21 10264   517  5078  2292     6  3388
  20006 24030   357  7586    21 10264   517  5078  4949    11 10264   517
   5078    18   434     1]]"
5a65ad10ff954d0f27bb3ccd9027e3d8f7f6bb76,0.0,"LSTM, LSTM+attention, LSTM+attention, LSTM","[[    0     3  7600  2305     6     3  7600  2305  1220 25615     6     3
   7600  2305  1220 25615     6     3  7600  2305]]",They compare to Akbik et al. (2018) and Link et al. (2012).,"[[  328  4048    12  4823   115    23   157     3    15    17   491     5
  28068    11  7505     3    15    17   491     5 24705     5     1]]"
cb384dc5366b693f28680374d31ff45356af0461,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
82642d3111287abf736b781043d49536fe48c350,8.3333,"9,473 annotations for 9,300 tweets.","[[    0  9902  4177   519 30729     7    21  9902  5426 10657     7     5
      1]]",The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression,"[[   37 30729     7    33     3   390    30  2084    13  7562    11   856
     46  2264   920    57     8    20 25152     3 18018     3    99   132
     19  2084    13  7562     1]]"
45be665a4504f0c7f458cf3f75a95d5a75eefd42,0.0,220,[[   0  204 1755    1]],Unanswerable,[[ 597 3247 3321  179    1]]
c08aab979dcdc8f4fe8ec1337c3c8290ab13414e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Fourteen ,[[5933 6808    1]]
5e324846a99a5573cd2e843d1657e87f4eb22fa6,0.0,"human labour is eliminated at expense of accuracy, as the context of each relative only approximates","[[    0   936 12568    19 17809    44 32128  8225    13  7452     6    38
      8  2625    13   284  5237   163 24672     7]]","Bayesian classifier has been modified, removing the bias towards frequent labels in the training data","[[ 2474    15 10488   853  7903    65   118  8473     6     3  8499     8
  14387  1587  8325 11241    16     8   761   331     1]]"
feb448860918ef5b905bb25d7b855ba389117c1f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Through the All India Radio new channel where actors read news.,"[[ 4582     8   432  1547  5061   126  4245   213 10485   608  1506     5
      1]]"
50be4a737dc0951b35d139f51075011095d77f2a,0.0,a wealth of prior knowledge about many natural language processing tasks,[[   0    3    9 5987   13 1884 1103   81  186  793 1612 3026 4145    1]],"labelled features, which are words whose presence strongly indicates a specific class or topic","[[    3 29506   753     6    84    33  1234     3  2544  3053  7157  9379
      3     9   806   853    42  2859     1]]"
747b847d687f703cc20a87877c5b138f26ff137d,0.0,"WSJ2K, WSJ3K",[[   0    3 8439  683  357  439    6    3 8439  683  519  439    1]],"the English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13 , and EN-DE section of the Europarl corpus BIBREF14 ","[[    8  1566    41  5332    61    11  2968    41  5596    61  6795    13
      8   638   567 10376  2464 11736   302     3  5972 25582   371  2368
      3     6    11 13209    18  5596  1375    13     8  5578    52    40
  11736   302     3  5972 25582   371  2534     1]]"
c0e341c4d2253eb42c8840381b082aae274eddad,0.0,relation detection,[[    0  4689 10664     1]],hierarchical matching between questions and relations with residual learning,[[ 1382  7064  1950  8150   344   746    11  5836    28 27687  1036     1]]
6a219d7c58451842aa5d6819a7cdf51c55e9fc0f,0.0,"tourism, Spanish, German",[[   0 8676    6 5093    6 2968    1]],No specific domain is covered in the corpus.,[[  465   806  3303    19  2303    16     8 11736   302     5     1]]
8b0abc1907c2bf3e0256f8cf85e0ba66a839bd92,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
1462eb312944926469e7cee067dfc7f1267a2a8c,0.0,82,[[   0    3 4613    1]],three,[[386   1]]
bc4dca3e1e83f3b4bbb53a31557fc5d8971603b2,7.6923,"semantic analysis, syntactic analysis, and machine translation","[[    0 27632  1693     6  8953    17  2708   447  1693     6    11  1437
   7314     1]]","Word Content (WC) probing task, Depth (Dep) and Top Constituent (TopC) (of the input sentence's constituent parse tree) probing tasks","[[ 4467  7185    41 10038    61 12361    53  2491     6 25734   107    41
   2962   102    61    11  2224 22636   295    41 22481   254    61    41
    858     8  3785  7142    31     7 17429   260     7    15  2195    61
  12361    53  4145     1]]"
a3bb9a936f61bafb509fa12ac0a61f91abcc5106,22.2222,"Open-domain TREC question corpus, biomedical","[[    0  2384    18 22999   332 20921   822 11736   302     6  2392  2726
   1950     1]]","ARC , TREC, GARD , MLBioMedLAT ","[[    3 18971     3     6   332 20921     6   350 10327     3     6 26497
     23    32 20123  4569   382     1]]"
1e775cf30784e6b1c2b573294a82e145a3f959bb,100.0,English,[[   0 1566    1]],english,[[22269     1]]
fd753ab5177d7bd27db0e0afc12411876ee607df,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly","[[  180  6480  2491    19     3     9   182   650 28820 26625   853  7903
      6  7212   254  2491  3806     7  8438     7    11  1738     7    80
     13     8   507  2097 21306     1]]"
2916bbdb95ef31ab26527ba67961cf5ec94d6afe,0.0,"82,432",[[   0    3 4613    6  591 2668    1]],"8,275 sentences and 167,739 words in total","[[ 9478 25988 16513    11     3 27650     6   940  3288  1234    16   792
      1]]"
d53299fac8c94bd0179968eb868506124af407d1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Using F1 Micro measure, the KNN classifier perform 0.6762, the RF 0.6687, SVM 0.6712 and MLP 0.6778.","[[    3  3626   377   536  5893  3613     6     8   480 17235   853  7903
   1912  4097  3708  4056     6     8     3  8556  4097  3539  4225     6
    180 12623  4097  3708  2122    11   283  6892  4097  3708  3940     5
      1]]"
a103636c8d1dbfa53341133aeb751ffec269415c,0.0,"BIBREF13, BIBREF18","[[    0     3  5972 25582   371  2368     6     3  5972 25582   371  2606
      1]]","majority baseline corresponds to a model's accuracy if it always predicts the majority class in the dataset, lexicon-based approach","[[ 2942 20726 10423     7    12     3     9   825    31     7  7452     3
     99    34   373  9689     7     8  2942   853    16     8 17953     6
      3 30949   106    18   390  1295     1]]"
425bd2ccfd95ead91d8f2b1b1c8ab9fc3446cb82,50.0,accuracy,[[   0 7452    1]],standard accuracy metric,[[1068 7452    3 7959    1]]
d51dc36fbf6518226b8e45d4c817e07e8f642003,0.0,"89,258 sentences",[[    0     3  3914     6   357  3449 16513     1]],6946,[[   3 3951 4448    1]]
a313e98994fc039a82aa2447c411dda92c65a470,0.0,they are matched by the same pair of words,[[    0    79    33     3 10304    57     8   337  3116    13  1234     1]],CFILT-preorder system,[[ 205 4936 9012   18 2026 9397  358    1]]
87bc6f83f7f90df3c6c37659139b92657c3f7a38,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"we can expect a dependency edge between i and i' in the English parse tree to correspond to an edge between j and j' in the Hindi parse tree, dual decomposition inference algorithm tries to bring the parse trees in the two languages through its constraints","[[   62    54  1672     3     9 27804  3023   344     3    23    11     3
     23    31    16     8  1566   260     7    15  2195    12 10423    12
     46  3023   344     3   354    11     3   354    31    16     8 25763
    260     7    15  2195     6  7013    20   287  4718    16 11788 12628
      3  9000    12   830     8   260     7    15  3124    16     8   192
   8024   190   165 17765     1]]"
c09a92e25e6a81369fcc4ae6045491f2690ccc10,0.0,polarity vs. positive emotion lexicons,"[[    0     3  9618   485     3   208     7     5  1465 13868     3 30949
    106     7     1]]",Human evaluators were asked to evaluate on a scale from 1 to 5 the validity of the lexicon annotations made by the experts and crowd contributors.,"[[ 3892     3    15  7480  6230   130  1380    12  6825    30     3     9
   2643    45   209    12   305     8 21264    13     8     3 30949   106
  30729     7   263    57     8  2273    11  4374 13932     7     5     1]]"
cb6a8c642575d3577d1840ca2f4cd2cc2c3397c5,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
559c68802ee2bb8b11e2188127418ca3a6155ba7,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
2268c9044e868ba0a16e92d2063ada87f68b5d03,0.0,Yes,[[   0 2163    1]],The best ensemble topped the best single model by 0.029 in F1 score on dev (external).,"[[   37   200  8784     3 12403     8   200   712   825    57     3 11739
   3166    16   377   536  2604    30    20   208    41   994  2947   138
    137     1]]"
86bf75245358f17e35fc133e46a92439ac86d472,11.1111,cwrs provide a more detailed representation of the syntactic structure of,"[[   0    3   75  210   52    7  370    3    9   72 3117 6497   13    8
  8953   17 2708  447 1809   13]]",only modest gains on three of the four downstream tasks,[[  163 11306 11391    30   386    13     8   662 26804  4145     1]]
ffde866b1203a01580eb33237a0bb9da71c75ecf,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
68df324e5fa697baed25c761d0be4c528f7f5cf7,0.0,"actions, natural language interface, and conversational search",[[   0 2874    6  793 1612 3459    6   11 3634  138  960    1]],"Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation","[[  638    18  1649 11788 19957     6     3 27569 11946     6   419  1788
     15  2165  5154     6     3 20119 11946     1]]"
56b7319be68197727baa7d498fa38af0a8440fe4,33.3333,"linguistic (sentiment, readability emotion part-of-speech","[[    0     3 24703    41  5277    23   297     6   608  2020 32128 13868
  32128   294    18   858    18     7   855 10217]]",Linguistic,[[6741 1744 3040    1]]
22815878083ebd2f9e08bc33a5e733063dac7a0f,66.6667,"English, Russian",[[   0 1566    6 4263    1]],Russian,[[4263    1]]
252a645af9876241fb166e5822992ce17fec6eb6,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
61652a3da85196564401d616d251084a25ab4596,0.0,"70,000",[[    0     3 28891     1]],26972 sentences,[[ 2208  4327   357 16513     1]]
fde700d5134a9ae8f7579bea1f1b75f34d7c1c4c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Android application,[[3054  917    1]]
9a8b9ea3176d30da2453cac6e9347737c729a538,0.0,82.0%,[[   0    3 4613    5 6932    1]], the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes,"[[    8  9279     3 18206   825  5153     3     9   377   536  2604    13
  20324  3264   755  3229    30 13353    15  5120 13154    11 20324  4240
    927  3229    30  3739  3358   298     8     3    23   357   115   357
      3 18206   825  5153     3     9   377   536  2604    13 20324  3628
    536  3229    30 13353    15  5120 13154    11 20324  1298  2555  3229
     30  3739  3358     1]]"
71e4ba4e87e6596aeca187127c0d088df6570c57,0.0,"LSTM, MATREF2, LSTM, MATREF3","[[    0     3  7600  2305     6     3 18169  4386   371  4482     3  7600
   2305     6     3 18169  4386   371   519     1]]",Unanswerable,[[ 597 3247 3321  179    1]]
6bbbb9933aab97ce2342200447c6322527427061,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"researchers asked subjects to report their emotions after reading each article, multiplied the document-emotion matrix and word-document matrix to derive emotion-word matrix for these words, Depechemood simply creates dictionaries of words where each word has scores between 0 and 1 for all of these 8 emotion categories","[[ 4768  1380  7404    12   934    70  7848   227  1183   284  1108     6
  15574    15    26     8  1708    18    15  7259 16826    11  1448    18
  28244 16826    12    74   757 13868    18  6051 16826    21   175  1234
      6   374   855  6482    32    32    26   914   482     7     3 12472
   5414    13  1234   213   284  1448    65  7586   344     3   632    11
    209    21    66    13   175   505 13868  5897     1]]"
0ad4359e3e7e5e5f261c2668fe84c12bc762b3b8,0.0,"BiLSTM BIBREF22, BERT BIBREF23, LS","[[    0  2106  7600  2305     3  5972 25582   371  2884     6   272 24203
      3  5972 25582   371  2773     6     3  7600]]","Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks","[[    3 19090  2195 14039 24228  5275   379  6826     7    13  7552    18
   7600  2305     6  7552    18   390 19602     6     3 14151 11053     6
     11   529    18   929    15  2250   379  6826     7    13     3  7600
   2305     7     6 19602     7     6 27687     6    11  1044    18 25615
      3   390  5275     1]]"
29f2954098f055fb19d9502572f085862d75bf61,0.0,a classifier that can translate natural language into real life language,"[[    0     3     9   853  7903    24    54 13959   793  1612   139   490
    280  1612     1]]"," K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)","[[  480 10455   222  1484  9031  6693     7    41   439 17235   201 25942
   6944    41  8556   201  4224 29011  5879    41   134 12623   201  4908
     18 18270  1915  6873    52   106    41   329  6892    61     1]]"
e48e750743aef36529fbea4328b8253dbe928b4d,25.0,WASSA-2017 BIBREF0,[[    0  7896 26787    18  9887     3  5972 25582   371   632     1]],WASSA-2017 Shared Task on Emotion Intensity,"[[ 7896 26787    18  9887  7105    26 16107    30   262  7259    86   324
      7   485     1]]"
e3c9e4bc7bb93461856e1f4354f33010bc7d28d5,0.0,"Bi-LSTM, BERT",[[    0  2106    18  7600  2305     6   272 24203     1]],"SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard","[[  180 12623    28     3 30949   138   753    16  4408   663    28  1767
    930     3  5972 25582   371  2938     3     6     3  5972 25582   371
   2517     3     6     3  5972 25582   371   536     3     6     3  5972
  25582   371  1808     3     6     3  5972 25582   371  8525  1388    18
    390  1573     3  5972 25582   371   519    11   119  2254    62     3
  26102   359     6   128   326    18   532    18     7   107 10386     3
   4902  2250     6   379     3    52    18  1582     3  5972 25582   371
    755    11    71    32   188     3  5972 25582   371   948     3     6
     84    33     8  1374  2250    30   180  5991  6762  2488  1976     1]]"
d3ca5f1814860a88ff30761fec3d860d35e39167,0.0,"LSTM, LSTM-based methods",[[   0    3 7600 2305    6    3 7600 2305   18  390 2254    1]],"Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines","[[24210  8150     6 27194  1571  9789   825     3     6 24210   695 12395
     63     6 24239   138 25942  7257     7     3     6  4224 29011  5879
      7     1]]"
0a5ffe4697913a57fda1fd5a188cd5ed59bdc5c7,20.0,"German, English, Chinese",[[   0 2968    6 1566    6 2830    1]],"Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish","[[15536    29     6 19789    29     6 16870     6 23124     6  1566     6
   2379     6  2968     6  9995    29     6  4338     6 21894     6 25518
      6 16073     6 21076     6 27425    29     6  5093    11 16531     1]]"
30e21f5bc1d2f80f422c56d62abca9cd3f2cd4a1,18.6047,"they use the same answer contexts for different questions, they use the same answer contexts for","[[   0   79  169    8  337 1525 2625    7   21  315  746    6   79  169
     8  337 1525 2625    7   21]]","They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap.","[[  328  4048     8  4145    24     8 17953     7    33  3255    21     6
   1348   381    13  1525  4341   399   822     6   381    13 14145  1308
      6  1348  1525  4775  2475     7     6  1348   822  2475     7     6
    822    18  3247  3321  1448 21655     5     1]]"
5b7a4994bfdbf8882f391adf1cd2218dbc2255a0,13.7931,"Using the baseline model, we can learn a baseline model to evaluate the performance of the","[[    0     3  3626     8 20726   825     6    62    54   669     3     9
  20726   825    12  6825     8   821    13     8]]","non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized","[[  529    18 22999    18 13376   757 20726    28  2182    18   858    18
   6051     7  6497     7    11   180 12623   853  7903     6     3    51
    134  4296     6   529    18 22999    18 13376   757 19602  4252    30
   1391  3303     6 24228   825    24  9248     7     3 31086  4145     6
  23210    23   138   761    12  1428  6497  1750   344  3303     7     6
   6826     7    13  1659 19602     7    33   261    21     3    35  9886
   1383    11     8   283 11731     7    13  1317  7500    33 22801 10558
     26     1]]"
4e1a67f8dc68b55a5ce18e6cd385ae9ab90d891f,100.0,No,[[  0 465   1]],No,[[465   1]]
509af1f11bd6f3db59284258e18fdfebe86cae47,21.4286,diversity is measured as the mean of all derived captions.,"[[    0  7322    19  8413    38     8  1243    13    66     3  9942 25012
      7     5     1]]", we look at language constructions used and compute the corresponding diversity score as the ratio of observed number versus optimal number,"[[   62   320    44  1612  1449     7   261    11 29216     8     3  9921
   7322  2604    38     8  5688    13  6970   381     3  8911  6624   381
      1]]"
cc608df2884e1e82679f663ed9d9d67a4b6c03f3,33.3333,accuracy,[[   0 7452    1]],"precision, recall, F1 and accuracy",[[11723     6  7881     6   377   536    11  7452     1]]
600b097475b30480407ce1de81c28c54a0b3b2f8,0.0,general adversarial network,[[    0   879 23210    23   138  1229     1]],"GloVe vectors trained on Wikipedia Corpus with ensembling, and GloVe vectors trained on Airbnb Data without ensembling","[[ 9840   553    15 12938     7  4252    30 16885 10052   302    28     3
     35     7  8312    53     6    11  9840   553    15 12938     7  4252
     30 30247  2747   406     3    35     7  8312    53     1]]"
dd6b378d89c05058e8f49e48fd48f5c458ea2ebc,0.0,Contextual Contextual Contextual Contextual Contextual Contextual Con,"[[   0 1193 6327 3471 1193 6327 3471 1193 6327 3471 1193 6327 3471 1193
  6327 3471 1193 6327 3471 1193]]","Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT","[[24239   138 25942  7257     7     6  2106  7600  2305    18  4545   371
      6  4908    18   382     9     7   157  6630     6  3318 12920   382
      1]]"
35b3ce3a7499070e9b280f52e2cb0c29b0745380,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
8b3d3953454c88bde88181897a7a2c0c8dd87e23,0.0,secondâ€“order coâ€“occurrence methods,[[    0   511   104  9397   576   104 16526  2254     1]],"integrated vector-res, vector-faith, Skipâ€“gram, CBOW","[[ 4580 12938    18    60     7     6 12938    18 10699   107     6 25378
    104  5096     6   205   279 15251     1]]"
ee417fea65f9b1029455797671da0840c8c1abbe,100.0,No,[[  0 465   1]],No,[[465   1]]
493e971ee3f57a821ef1f67ef3cd47ade154e7c4,0.0,word2vec BIBREF16,[[    0  1448   357   162    75     3  5972 25582   371  2938     1]],"Bernoulli embeddings (b-emb) BIBREF1 , continuous bag-of-words (CBOW) BIBREF5 , Distributed Memory version of Paragraph Vector (PV-DM) BIBREF11 and the Global Vectors (GloVe) BIBREF6 model","[[ 8942  1063   195    23 25078    26    53     7    41   115    18    15
     51   115    61     3  5972 25582   371   536     3     6  7558  2182
     18   858    18  6051     7    41   254   279 15251    61     3  5972
  25582   371   755     3     6 21796  1054 19159   988    13  4734  9413
  29011    41   345   553    18  7407    61     3  5972 25582   371  2596
     11     8  3699 29011     7    41   517    40    32   553    15    61
      3  5972 25582   371   948   825     1]]"
1b23c4535a6c10eb70bbc95313c465e4a547db5e,23.0769,"LVCSR architectures are composed of encoder, decoder, and decoder","[[    0   301  7431  6857  4648     7    33 10431    13 23734    52     6
     20  4978    52     6    11    20  4978    52]]","In encoder they use convolutional, NIN and bidirectional LSTM layers and in decoder they use unidirectional LSTM ","[[   86 23734    52    79   169   975 24817   138     6   445  3162    11
   2647 26352     3  7600  2305  7500    11    16    20  4978    52    79
    169    73    23 26352     3  7600  2305     1]]"
2b3cac7af10d358d4081083962d03ea2798cf622,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
49eb52b3ec0647e165a5e41488088c80a20cc78f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],context inference,[[ 2625    16 11788     1]]
97466a37525536086ed5d6e5ed143df085682318,0.0,concept-map-based MDS,[[    0  2077    18 11576    18   390   283  3592     1]],Answer with content missing: (Baseline Method section) We implemented a simple approach inspired by previous work on concept map generation and keyphrase extraction.,"[[11801    28   738  3586    10    41 14885    15   747  7717  1375    61
    101  6960     3     9   650  1295  3555    57  1767   161    30  2077
   2828  3381    11   843 27111 16629     5     1]]"
d67c01d9b689c052045f3de1b0918bab18c3f174,0.0,"Compared to the previous best model, the attention-sum reader improved by almost 2% and","[[    0     3 25236    12     8  1767   200   825     6     8  1388    18
   4078  5471  3798    57   966     3  5406    11]]",INLINEFORM2 ,[[ 3388 20006 24030   357     1]]
0767ca8ff1424f7a811222ca108a33b6411aaa8a,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5260cb56b7d127772425583c5c28958c37cb9bea,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],160,[[11321     1]]
627b8d7b5b985394428c974aca5ba0c1bbbba377,0.0,82,[[   0    3 4613    1]],Unanswerable,[[ 597 3247 3321  179    1]]
1038542243efe5ab3e65c89385e53c4831cd9981,0.0,Ca and Cb,[[   0 1336   11  205  115    1]],Diachronic Usage Relatedness (DURel) gold standard data set,"[[ 5267 10363  2532  6952   545 16559   655    41  7874  1649    40    61
   2045  1068   331   356     1]]"
c7eb71683f53ab7acffd691a36cad6edc7f5522e,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
0752d71a0a1f73b3482a888313622ce9e9870d6e,0.0,wFST-based model,[[   0    3  210  371 4209   18  390  825    1]],wFST,[[   3  210  371 4209    1]]
f71b52e00e0be80c926f153b9fe0a06dd93af11e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"average content score across the paintings is 3.7, average creativity score is 3.9, average style score is 3.9 ","[[ 1348   738  2604   640     8  9843    19     3 25168     6  1348  6933
   2604    19     3 28640     6  1348   869  2604    19     3 28640     1]]"
113d791df6fcfc9cecfb7b1bebaf32cc2e4402ab,0.0,BLEU,[[    0     3  8775 12062     1]],15.4 MB,[[9996  591    3 4633    1]]
307e8ab37b67202fe22aedd9a98d9d06aaa169c5,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
9c33b340aefbc1f15b6eb6fb3e23ee615ce5b570,15.3846,the spatial aspect of the EEG signal is computed.,"[[    0     8 15208  2663    13     8   262  8579  3240    19 29216    26
      5     1]]",They use four-layered 2D CNN and two fully connected hidden layers on the channel covariance matrix to compute the spatial aspect.,"[[  328   169   662    18 21281   204   308 19602    11   192  1540  2895
   5697  7500    30     8  4245   576  9504   663 16826    12 29216     8
  15208  2663     5     1]]"
07d15501a599bae7eb4a9ead63e9df3d55b3dc35,0.0,using the psychological dimensions of people,[[    0   338     8 11041  8393    13   151     1]],Unanswerable,[[ 597 3247 3321  179    1]]
603fee7314fa65261812157ddfc2c544277fcf90,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],up to 1.95 times larger,[[  95   12 1300 3301  648 2186    1]]
d9b6c61fc6d29ad399d27b931b6cb7b1117b314a,42.1053,a question generation model is used to generate the question.,[[   0    3    9  822 3381  825   19  261   12 3806    8  822    5    1]],framework consisting of both a question answering model and a question generation model,"[[ 4732  5608    53    13   321     3     9   822 18243   825    11     3
      9   822  3381   825     1]]"
48088a842f7a433d3290eb45eb0d4c6ab1d8f13c,0.0,"LSTMs, LSTMs with cross-validation","[[    0     3  7600  2305     7     6     3  7600  2305     7    28  2269
     18 27769   257     1]]","NaÃ¯ve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN","[[ 1823     2   162  2474    15     7    41 14972   201  7736  3040   419
  22430    41 12564   201  4224 29011  5879    41   134 12623   201 25942
   6944     7    41  8556   201 10771  4741     3 16481    15    26  7552
      7    41  3443   382   201 19602     6   391 17235     1]]"
79f9468e011670993fd162543d1a4b3dd811ac5d,0.0,2.3 BLEU gains,[[    0     3 18561     3  8775 12062 11391     1]],"Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively.","[[    3 25236    12     8 20726     7     6     3 21159  6858   405    59
    103   394    16  1353    13   399  9247   485    30  2847  5911    11
      3  6037   567  6892  1233   549  7323 17953     7     6    68    34
    405    57    95    12  4097  2555  8662    18  8775 12062   979    30
   2847  5911    11  4097  2469  8662    18  8775 12062    30     3  6037
    567  6892  1233   549  7323     5    86  1353    13 20278  4992 10355
     11 31484   565     6    34  7586   394   145     8 20726     7    30
     95    12  6374     5  2712    11   489  5170    13     8  1488  6898
      5     1]]"
12d7055baf5bffb6e9e95e977c000ef2e77a4362,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added","[[ 4179   116     8  1256   769  2244    28  2205 30729     7    19  4838
     28     8  4080  4374 30729    19     3  9285   377   536  2604     6
    231  2186   145   116     3     9  6504   356    13  2205 30729     7
     33   974     1]]"
72f7ef55e150e16dcf97fe443aff9971a32414ef,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"+1.86 in terms of F1 score on CTB5, +1.80 on CTB6, +2.19 on UD1.4","[[    3 18446     5  3840    16  1353    13   377   536  2604    30   205
   9041 11116     3 18446     5  2079    30   205  9041 11071  1768  4416
   2294    30     3 10161 14912     1]]"
41ac23e32bf208b69414f4b687c4f324c6132464,50.0,"English, Chinese",[[   0 1566    6 2830    1]],"English, German",[[1566    6 2968    1]]
5daeb8d4d6f3b8543ec6309a7a35523e160437eb,0.0,German,[[   0 2968    1]],English,[[1566    1]]
db72a78a7102b5f0e75a4d9e1a06a3c2e7aabb21,0.0,"long vowels, which are explicitly written, and short vowels, aka diacri","[[    0   307 22121  3573     6    84    33 21119  1545     6    11   710
  22121  3573     6     3  5667  1227     9  2685]]","Farasa, RDI",[[ 5186     9     7     9     6     3 10255   196     1]]
d004ca2e999940ac5c1576046e30efa3059832fa,12.5,multi-attention and single attention models,[[    0  1249    18 25615    11   712  1388  2250     1]]," four attention mechanisms instead of one, a projection layer for the word embeddings","[[  662  1388 12009  1446    13    80     6     3     9 13440  3760    21
      8  1448 25078    26    53     7     1]]"
e374169ee10f835f660ab8403a5701114586f167,60.0,"user's screen name, profile image, location, description, followers count, and friend count","[[    0  1139    31     7  1641   564     6  3278  1023     6  1128     6
   4210     6 10076  3476     6    11  1565  3476]]","username, display name, profile image, location and description",[[20304     6  1831   564     6  3278  1023     6  1128    11  4210     1]]
27de1d499348e17fec324d0ef00361a490659988,66.6667,"23,700 queries",[[    0 12992  9295 13154     1]],"23,700 ",[[12992  9295     1]]
a6a48de63c1928238b37c2a01c924b852fe752f8,0.0,"AMR, Meaning Representation, Recurrent Networks, Deep Learning","[[    0    71  9320     6 25148   419 12640   257     6   419 14907  3426
      7     6  9509  6630     1]]","Lead-3, Lead-1-AMR",[[12208  3486     6 12208  2292    18   188  9320     1]]
c2d1387e08cf25cb6b1f482178cca58030e85b70,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
c0e341c4d2253eb42c8840381b082aae274eddad,0.0,relation detection,[[    0  4689 10664     1]],answer questions by obtaining information from KB tuples ,"[[ 1525   746    57     3 13134   251    45     3 17827     3    17   413
    965     1]]"
ff6c9af28f0e2bb4fb6a69f124665f8ceb966fbc,0.0,sports clubs,[[   0 2100 8122    1]],"Galatasaray , FenerbahÃ§e ","[[15210    17     9     7     9  2866     3     6  4163   687 17670  8970
     15     1]]"
1b72aa2ec3ce02131e60626639f0cf2056ec23ca,0.0,8000,[[    0     3 25129     1]],"Level A: 14100 Tweets
Level B: 4640 Tweets
Level C: 4089 Tweets","[[ 7166    71    10   968  2915 25335     7  7166   272    10   314 23714
  25335     7  7166   205    10  1283  3914 25335     7     1]]"
b6f466e0fdcb310ecd212fd90396d9d13e0c0504,0.0,Yes,[[   0 2163    1]], all three languages have error-corrected corpora for testing purposes,"[[   66   386  8024    43  3505    18 28832    15    26 11736   127     9
     21  2505  3659     1]]"
b65a83a24fc66728451bb063cf6ec50134c8bfb0,0.0,selected by BIBREF0,[[    0  2639    57     3  5972 25582   371   632     1]]," finding the important sentences from the story, extracting the key information from those sentences using their AMR graphs","[[ 2342     8   359 16513    45     8   733     6  5819    53     8   843
    251    45   273 16513   338    70    71  9320  8373     7     1]]"
00c57e45ac6afbdfa67350a57e81b4fad0ed2885,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
e831041d50f3922265330fcbee5a980d0e2586dd,0.0,a normal reading paradigm,[[    0     3     9  1389  1183 20491     1]],"participants were instructed to read the sentences naturally, without any specific task other than comprehension","[[ 3008   130 24088    12   608     8 16513  6212     6   406   136   806
   2491   119   145 27160     1]]"
277a7e916e65dfefd44d2d05774f95257ac946ae,0.0,"ConciergeQA, ACE, CAS, CASB, CASB,","[[    0  1193  9186   397 23008     6     3 11539     6     3 18678     6
      3 18678   279     6     3 18678   279     6]]","Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT
","[[24239   138 25942  7257     7     6  2106  7600  2305    18  4545   371
      6  4908    18   382     9     7   157  6630     6  3318 12920   382
      1]]"
ffa7f91d6406da11ddf415ef094aaf28f3c3872d,10.0,BLEU correlates by 0.81 compared to HEALTH by 0.81 ,"[[    0     3  8775 12062 30575     7    57  4097  4959     3  2172    12
      3  6021  4090  4611    57  4097  4959     3]]",Their average correlation tops the best other model by 0.155 on WikiBio.,"[[ 2940  1348 18712   420     7     8   200   119   825    57  4097 20896
     30  2142  2168 26475     5     1]]"
a0543b4afda15ea47c1e623c7f00d4aaca045be0,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],No,[[465   1]]
186b7978ee33b563a37139adff1da7d51a60f581,20.0,closed test setting is a set of closed test sets which are set according to the parameters of,"[[   0 3168  794 1898   19    3    9  356   13 3168  794 3369   84   33
   356 1315   12    8 8755   13]]",closed test limits all the data for learning should not be beyond the given training set,"[[3168  794 6790   66    8  331   21 1036  225   59   36 1909    8  787
   761  356    1]]"
73906462bd3415f23d6378590a5ba28709b17605,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"the degree of lexical overlap between them, presence of negation words","[[    8  1952    13     3 30949   138 21655   344   135     6  3053    13
  14261   257  1234     1]]"
19cf7884c0c509c189b1e74fe92c149ff59e444b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Answer with content missing: (formulas in selection): Pseudo-perplexity is perplexity where conditional joint probability is approximated.,"[[11801    28   738  3586    10    41  2032    83     9     7    16  1801
     61    10   276     7    15    76    26    32    18   883  9247   485
     19   399  9247   485   213  1706   138  4494 15834    19 24672    26
      5     1]]"
8bb0011ad1d63996d5650770f3be18abdd9f7fc6,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
81d607fc206198162faa54a796717c2805282d9b,0.0,Privacy experts from Google,[[    0 17865  2273    45  1163     1]],Yes,[[2163    1]]
01209a3bead7c87bcdc628be2a5a26b41abde9d1,0.0,WSJ15 English-Japanese,[[   0    3 8439  683 1808 1566   18  683 9750 1496   15    1]],"SNLI BIBREF22 and MultiNLI BIBREF23, Quora Question Pairs dataset BIBREF24,  Stanford Sentiment Treebank (SST) BIBREF25","[[    3  8544  8159     3  5972 25582   371  2884    11  4908 18207   196
      3  5972 25582   371  2773     6  2415   127     9 11860 25072     7
  17953     3  5972 25582   371  2266     6 19796  4892  2998   295  7552
   4739    41   134  4209    61     3  5972 25582   371  1828     1]]"
1062a0506c3691a93bb914171c2701d2ae9621cb,4.1667,"feature sets for news annotations, news annotations, and captions","[[    0  1451  3369    21  1506 30729     7     6  1506 30729     7     6
     11 25012     7     1]]","15 emotion types, sentiment classes, positive and negative, care, harm, fairness, cheating, loyalty, betrayal, authority, subversion, sanctity, and degradation, count of question marks, exclamation marks, consecutive characters and letters, links, hashtags, users' mentions, uppercase ratio, tweet length, words embeddings","[[  627 13868  1308     6  6493  2287     6  1465    11  2841     6   124
      6  6263     6  2725   655     6 15009    53     6 13260     6    36
     17  2866   138     6  5015     6   769  8674     6     3 21879   485
      6    11 26644     6  3476    13   822  6784     6  1215 12818   257
   6784     6 12096  2850    11  5487     6  2416     6 25354     7     6
   1105    31  2652     7     6  4548  6701  5688     6 10657  2475     6
   1234 25078    26    53     7     1]]"
33554065284110859a8ea3ca7346474ab2cab100,0.0,3500 conversations and 8500 messages from 61 groups,[[   0  220 2560 9029   11  505 2560 4175   45    3 4241 1637    1]],"1,873 Twitter conversation threads, roughly 14k tweets","[[ 1914  4225   519  3046  3634  4546     7     6 10209   968   157 10657
      7     1]]"
c9305e5794b65b33399c22ac8e4e024f6b757a30,15.7895,Best model among all the models was LSTM with a 62.0% accuracy and,"[[   0 1648  825  859   66    8 2250   47    3 7600 2305   28    3    9
     3 4056    5 6932 7452   11]]","For SLC task, the ""ltuorp"" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the ""newspeak"" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively).","[[  242   180  6480  2491     6     8    96    40    17    76   127   102
    121   372    65     8   200  5505   825    41 22787  2668 15020 22787
    632  2577    87 22787   948  3647    21   377 12989   345    87   448
   6898    61    11    21  7212   254  2491     8    96 15808 14661   121
    372    65     8   200  5505   825 17482     5  2266  4060    87 18189
   3840 15020 18189 22772    21   377 12989   345    87   448  6898   137
      1]]"
25c1c4a91f5dedd4e06d14121af3b5921db125e9,100.0,No,[[  0 465   1]],No,[[465   1]]
9193006f359c53eb937deff1248ee3317978e576,0.0,10 classification datasets from the Wikipedea corpus,"[[    0   335 13774 17953     7    45     8  2142  2168  3138    15     9
  11736   302     1]]"," Reuters, BBCSport BIBREF30, Polarity BIBREF31, Subjectivity BIBREF32, MPQA BIBREF33, IMDB BIBREF34, TREC BIBREF35, SST-1 BIBREF36, SST-2 BIBREF36, Yelp2013 BIBREF26","[[    3 18844     6  9938 17682     3  5972 25582   371  1458     6 19052
    485     3  5972 25582   371  3341     6 19237 10696     3  5972 25582
    371  2668     6  5220 23008     3  5972 25582   371  4201     6     3
   5166  9213     3  5972 25582   371  3710     6   332 20921     3  5972
  25582   371  2469     6   180  4209  2292     3  5972 25582   371  3420
      6   180  4209  4949     3  5972 25582   371  3420     6  7271    40
    102 11138     3  5972 25582   371  2688     1]]"
6c91d44d5334a4ac80100eead4e105d34e99a284,14.2857,"two representative model architectures: English-French, Chinese-English","[[    0   192  6978   825  4648     7    10  1566    18   371    60  5457
      3     6  2830    18 26749     1]]", Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0,"[[31220     3  5972 25582   371   536   825    11     8  7450   391 17235
     18 25001   825     3  5972 25582   371   632     1]]"
1e775cf30784e6b1c2b573294a82e145a3f959bb,100.0,English,[[   0 1566    1]],english,[[22269     1]]
ef396a34436072cb3c40b0c9bc9179fee4a168ae,20.0,"Noun-Noun Matching, Adverb-Noun Matching","[[    0   465   202    18  4168   202 12296    53     6  1980 11868    18
   4168   202 12296    53     1]]",text classification and text semantic matching,[[ 1499 13774    11  1499 27632  8150     1]]
aa2948209cc33b071dbf294822e72bb136678345,0.0,Result of a 5% CIBERT test: 0-9 CIBER,"[[    0     3 20119    13     3     9     3  2712     3  3597 12920   382
    794    10     3   632  7141     3  3597 12920]]","AutoJudge consistently and significantly outperforms all the baselines, RC models achieve better performance than most text classification models (excluding GRU+Attention), Comparing with conventional RC models, AutoJudge achieves significant improvement","[[ 2040   683 13164  8182    11  4019    91   883  2032     7    66     8
  20726     7     6     3  4902  2250  1984   394   821   145   167  1499
  13774  2250    41 21763   350  8503  1220   188    17  9174   201 20959
     53    28  7450     3  4902  2250     6  2040   683 13164  1984     7
   1516  4179     1]]"
fd753ab5177d7bd27db0e0afc12411876ee607df,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.,"[[   37 20726   358    21     8   180  6480  2491    19     3     9   182
    650 28820 26625   853  7903    28  4647  8755     5    37 20726    21
      8  7212   254  2491  3806     7  8438     7    11  1738     7    80
     13     8   507  2097 21306     5     1]]"
78292bc57ee68fdb93ed45430d80acca25a9e916,6.6667,They extend LAMA evaluation framework to focus on negation,"[[    0   328  4285   301 21250  5002  4732    12   992    30 14261   257
      1]]",Create the negated LAMA dataset and  query the pretrained language models with both original LAMA and negated LAMA statements and compare their predictions.,"[[ 6357     8 14261   920   301 21250 17953    11 11417     8  7140 10761
   1612  2250    28   321   926   301 21250    11 14261   920   301 21250
   6643    11  4048    70 20099     5     1]]"
899ed05c460bf2aa0aa65101cad1986d4f622652,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"$3,209$ reviews ",[[5583    6  357 4198 3229 2456    1]]
d3093062aebff475b4deab90815004051e802aa6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information","[[  180 12623    28    73    23  5096     6   600  2375     6  6467  5096
    753     6    28  1348  1448 25078    26    53     6    28  1348 13421
   1448 25078    26    53     7     6 19602    11     3  4902 17235     6
    180 12623     6 19602     6     3  4902 17235    28  1670   251     1]]"
bc84c5a58c57038910f7720d7a784560054d3e1a,26.6667,"German, English, French",[[   0 2968    6 1566    6 2379    1]],"French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese","[[ 2379     6  2968     6 10098     6  4263     6  5093     6  4338     6
  15423     6 25518     6 16531     6 29841    29    11  2830     1]]"
c7b6e6cb997de1660fd24d31759fe6bb21c7863f,0.0,10 electrodes,[[    0   335 22173     7     1]],1913 signals,[[ 957 2368 9650    1]]
7b4992e2d26577246a16ac0d1efc995ab4695d24,0.0,CoNLL 2014,[[    0   638   567 10376  1412     1]],error detection system by Rei2016,[[ 3505 10664   358    57   419    23 11505     1]]
71ba1b09bb03f5977d790d91702481cc406b3767,0.0,accuracy of 82.0%,[[   0 7452   13    3 4613    5 6932    1]],M-Bert had 76.6 F1 macro score.,"[[  283    18   279    49    17   141   489 28833   377   536 11663  2604
      5     1]]"
5bb3c27606c59d73fd6944ba7382096de4fa58d8,0.0,Yes,[[   0 2163    1]],multiple-choice,[[1317   18 3995  867    1]]
e2db361ae9ad9dbaa9a85736c5593eb3a471983d,0.0,BIBREF2 BIBREF3 BIBREF4 BIBREF,"[[    0     3  5972 25582   371   357     3  5972 25582   371   519     3
   5972 25582   371   591     3  5972 25582   371]]","GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent","[[ 9840   553    15     6   272 24203     6 12489  4892    17  1433   695
   4978    52     6     3  9164    18  4309   371     6    86  1010   134
    295     1]]"
55c8f7acbfd4f5cde634aaecd775b3bb32e9ffa3,0.0,accuracy,[[   0 7452    1]],"Phoneme Error Rate (PER), Word Error Rate (WER), Word Error Rate 100 (WER 100)","[[ 8924   526   848    52   127 13002    41  8742   201  4467   848    52
    127 13002    41   518  3316   201  4467   848    52   127 13002   910
     41   518  3316   910    61     1]]"
3d013f15796ae7fed5272183a166c45f16e24e39,6.8966,a collection of ten different types of typography from the Arab World (Arab World,"[[    0     3     9  1232    13     3   324   315  1308    13 23042 16369
     45     8  9217  1150    41   188  7093  1150]]","font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer, A separate fonts layer was introduced to preserve detailed information on the font configurations referenced in the tokens layer","[[ 4088   686    11  4088   869    41    15     5   122     5     6    34
      9  2176     7     6  8197  2281    61    13     3     9 14145    11
    165  1102    30     8  1722   543    41  1161  3948     7   163    61
     47  7173    38 12978    12     8 14145  2479    13     8 14145     7
   3760     6    71  2450  4088     7  3760    47  3665    12  8996  3117
    251    30     8  4088  5298     7  2848    26    16     8 14145     7
   3760     1]]"
692c9c5d9ff9cd3e0ce8b5e4fa68dda9bd23dec1,0.0,"20,000",[[    0     3 13922     1]],"22,880 users",[[12889   927  2079  1105     1]]
9c33b340aefbc1f15b6eb6fb3e23ee615ce5b570,0.0,the spatial aspect of the EEG signal is computed.,"[[    0     8 15208  2663    13     8   262  8579  3240    19 29216    26
      5     1]]","we use a CNN BIBREF19 , in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers.","[[   62   169     3     9 19602     3  5972 25582   371  2294     3     6
     16  1090     3     9   662    18 21281   204   308 19602  9013    53
    192   975 24817   138    11   192  1540  2895  5697  7500     5     1]]"
1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
51b1142c1d23420dbf6d49446730b0e82b32137c,0.0,Recurrent Neural Networks (RN),[[    0   419 14907  1484  9709  3426     7    41 14151    61     1]],"LastStateRNN, AvgRNN, AttentionRNN","[[ 2506   134  4748 14151   567     6    71   208   122 14151   567     6
  20748 14151   567     1]]"
97d0f9a1540a48e0b4d30d7084a8c524dd09a4c3,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],sequence of $s$ tweets,[[ 5932    13  1514     7  3229 10657     7     1]]
8a7615fc6ff1de287d36ab21bf2c6a3b2914f73d,18.1818,"Bi-LSTM, LSTM, LSTM with atomic element, LS","[[    0  2106    18  7600  2305     6     3  7600  2305     6     3  7600
   2305    28     3 20844  3282     6     3  7600]]","BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF","[[ 2106  7600  2305     6  2106  7600  2305    18   254 17235     6  2106
   7600  2305    18  4545   371     6  2106  7600  2305    18   254 17235
     18  4545   371     1]]"
bfc1de5fa4da2f0e301fd22aea19cf01e2bb5b31,40.0,"English, German",[[   0 1566    6 2968    1]],"English, Spanish, Finnish",[[ 1566     6  5093     6 28124     1]]
b7381927764536bd97b099b6a172708125364954,8.0,They evaluate their resulting word embeddings using the distributional model BIBREF,"[[    0   328  6825    70     3  5490  1448 25078    26    53     7   338
      8  3438   138   825     3  5972 25582   371]]","We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings.","[[  101    92  6825    66   874  2250    30 26804  4145    45     8  3901
     75   427  2165  3132     3  5972 25582   371  2368     3     6   338
    163     8  4145    21    84   761    11  5002   331    19 14019   347
     10 16749    53     6  6493    11   822 13774     6    11   793  1612
  10356    41 18207   196   137    37  4647  3803    45     8  3132    33
    261     6    68    62   661   163     8  3599  3803     6   213     8
  25078    26    53     7  1452    33    59     3    17  6319  8755    13
      8  2250     6 19060     8   358    12   169   163     8   251   641
     16     8 25078    26    53     7     5     1]]"
b6f466e0fdcb310ecd212fd90396d9d13e0c0504,0.0,Yes,[[   0 2163    1]],Data already contain errors,[[2747  641 3480 6854    1]]
5da26954fbcd3cf6a7dba9f8b3c9a4b0391f67d4,20.0,they combine text sequences to generate a single RNN model,"[[    0    79  5148  1499  5932     7    12  3806     3     9   712   391
  17235   825     1]]",encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model,"[[23734     7     8   251    45  2931    11  1499  5932     7   338  7013
    391 17235     7    11   258     3 15256     8   251    45   175  2836
    338     3     9  3305    18 26338 24228   825     1]]"
887c6727e9f25ade61b4853a869fe712fe0b703d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],The neural projector must be invertible.,[[   37 24228   516   127   398    36    16  3027  2317     5     1]]
f0848e7a339da0828278f6803ed7990366c975f0,0.0,Yes,[[   0 2163    1]],"No-Answer Baseline (NA), Word Count Baseline, Human Performance","[[  465    18   188    29     7  3321  8430   747    41  5999   201  4467
      3 10628  8430   747     6  3892  8233     1]]"
31b92c03d5b9be96abcc1d588d10651703aff716,0.0,3rd,[[  0 220  52  26   1]],0.7033,[[4097 2518 4201    1]]
4d706ce5bde82caf40241f5b78338ea5ee5eb01e,0.0,Cwrs for a linguistic task,[[    0   205   210    52     7    21     3     9     3 24703  2491     1]],"CCG Supertagging CCGBank , PTB part-of-speech tagging, EWT part-of-speech tagging,
Chunking, Named Entity Recognition, Semantic Tagging, Grammar Error Detection, Preposition Supersense Role, Preposition Supersense Function, Event Factuality Detection","[[  205 12150  2011    17 15242   205 12150 21347     3     6   276  9041
    294    18   858    18     7   855 10217     3    17 15242     6   262
    518   382   294    18   858    18     7   855 10217     3    17 15242
      6 16636    29  1765     6  5570    26  4443   485 31110     6   679
    348  1225   332 15242     6 30751   848    52   127     3 31636    23
    106     6  1266  4718  2011     7  5167  2158   109     6  1266  4718
   2011     7  5167 21839     6  8042   377 25481   485     3 31636    23
    106     1]]"
6b367775a081f4d2423dc756c9b65b6eef350345,100.0,No,[[  0 465   1]],No,[[465   1]]
511517efc96edcd3e91e7783821c9d6d5a6562af,0.0,"CoNLL-2010 Shared Task (BIBREF0), which had 3 different tasks","[[    0   638   567 10376    18 14926  7105    26 16107    41  5972 25582
    371   632   201    84   141   220   315  4145]]","BF, BA, SFU and Sherlock",[[    3 19780     6  8145     6   180 19813    11 31396     1]]
4477bb513d56e57732fba126944073d414d1f75f,0.0,Medical search engines,[[   0 3721  960 7277    1]],clinical notes from the CE task in 2010 i2b2/VA ,"[[ 3739  3358    45     8  9265  2491    16  2735     3    23   357   115
  15896  8230     1]]"
8eefa116e3c3d3db751423cc4095d1c4153d3a5f,0.0,CoNLL2003 shared task BIBREF15,"[[    0   638   567 10376 23948  2471  2491     3  5972 25582   371  1808
      1]]","CoNLL2003-testA, GENIA",[[  638   567 10376 23948    18  4377   188     6     3  5042 26077     1]]
afb77b11da41cd0edcaa496d3f634d18e48d7168,0.0,"LSTM with attention, LSTM with attention, LSTM with attention","[[   0    3 7600 2305   28 1388    6    3 7600 2305   28 1388    6    3
  7600 2305   28 1388    1]]","BERT based fine-tuning, Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer","[[  272 24203     3   390  1399    18    17   202    53     6 28953   529
    747   291  7500     6 28953  2106    18  7600  2305  3760     6 28953
  19602  3760     1]]"
2a3e36c220e7b47c1b652511a4fdd7238a74a68f,100.0,244,[[  0 997 591   1]],244 ,[[997 591   1]]
2c947447d81252397839d58c75ebcc71b34379b5,0.0,On the Reddit Reddit Reddit Reddit Reddit Redd,"[[   0  461    8 1624   26  155 1624   26  155 1624   26  155 1624   26
   155 1624   26  155 1624   26]]","CoinCollector , CookingWorld ",[[15589  9939  3437   127     3     6  6176    53 17954     1]]
e431661f17347607c3d3d9764928385a8f3d9650,0.0,By a margin of 0.80 compared to 0.908 on the bi-directional and,"[[    0   938     3     9  6346    13  4097  2079     3  2172    12     3
  23758  4018    30     8  2647    18 26352    11]]",They decrease MAE in 0.34,[[ 328 6313 4800  427   16 4097 3710    1]]
4d7ff4e5d06902de85b0e9a364dc455196d06a7d,0.0,"Convolutional, Recurrent, and AdaBoost","[[    0  1193 24817   138     6   419 14907     6    11  1980     9 16481
      1]]",Unanswerable,[[ 597 3247 3321  179    1]]
93beae291b455e5d3ecea6ac73b83632a3ae7ec7,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],They compute the gradient of the output at each time step with respect to the input words to decide the importance.,"[[  328 29216     8 26462    13     8  3911    44   284    97  1147    28
   1445    12     8  3785  1234    12  2204     8  3172     5     1]]"
ecd5770cf8cb12cb34285e26ab834301c17c53e1,8.6957,LSTM,[[   0    3 7600 2305    1]],"LSTM to encode the question, VGG16 to extract visual features. The outputs of LSTM and VGG16 are multiplied element-wise and sent to a softmax layer.","[[    3  7600  2305    12 23734     8   822     6   584 21320  2938    12
   5819  3176   753     5    37  3911     7    13     3  7600  2305    11
    584 21320  2938    33 15574    15    26  3282    18 10684    11  1622
     12     3     9  1835  9128  3760     5     1]]"
54e945ea4b014e11ed4e1e61abc2aa9e68fea310,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],seq2seq model with global attention gives the best results with an average target BLEU score of 29.65,"[[  142  1824   357     7    15  1824   825    28  1252  1388  1527     8
    200   772    28    46  1348  2387     3  8775 12062  2604    13  2838
      5  4122     1]]"
22714f6cad2d5c54c28823e7285dc85e8d6bc109,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"reduced the dataset by eliminating features, apply feature selection to select highest ranked features to train and test the model and rank the performance of incrementally adding features.","[[ 3915     8 17953    57 17323   753     6  1581  1451  1801    12  1738
   2030     3  8232   753    12  2412    11   794     8   825    11 11003
      8   821    13 28351   120  2651   753     5     1]]"
290ee79b5e3872e0496a6a0fc9b103ab7d8f6c30,13.7931,"3 layers of the annotation scheme: (1) target, (2) target, (3) target, (4) target","[[    0   220  7500    13     8 30729  5336    10  5637  2387     6  6499
   2387     6 10153  2387     6     3 10820  2387]]","Level A: Offensive language Detection
, Level B: Categorization of Offensive Language
, Level C: Offensive Language Target Identification
","[[ 7166    71    10  1129 23039    15  1612     3 31636    23   106     3
      6  7166   272    10  3431  6066    52  1707    13  1129 23039    15
  10509     3     6  7166   205    10  1129 23039    15 10509 12615 31474
      1]]"
37be0d479480211291e068d0d3823ad0c13321d3,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Table TABREF6, Table TABREF8","[[ 4398     3  3221 25582   371 11071  4398     3  3221 25582   371   927
      1]]"
d087539e6a38c42f0a521ff2173ef42c0733878e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.","[[  818   132    65   118  1895   161    30     3  5503   445  6892   825
  19067  4342     3  5972 25582   371  1808     6 20487   257  2097  1178
   5849   175     6   437    79  1457     8  1236    11  3145  2250    12
    698     8   337 19067    11  3911   628     5   100 13343   120  6790
     70  1055    12   856  1428   825  4342     5     1]]"
19b7312cfdddb02c3d4eaa40301a67143a72a35a,7.6923,BLEU and TER scores,[[    0     3  8775 12062    11     3  5946  7586     1]],"average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )","[[ 1348   775 20099     6 21306  3106  2766    20  4978    52  5697  2315
     44  3388 20006 24030   591  2245   826     3     9    20 16488    49
     41  3388 20006 24030   755     3    61    11  1581    46    73 23313
   9068    53  1573    41    17    18   134  4171     3  5972 25582   371
   2469     3    61     1]]"
8b3d3953454c88bde88181897a7a2c0c8dd87e23,0.0,secondâ€“order coâ€“occurrence methods,[[    0   511   104  9397   576   104 16526  2254     1]],"Skipâ€“gram, CBOW",[[25378   104  5096     6   205   279 15251     1]]
40c0f97c3547232d6aa039fcb330f142668dea4b,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
20e38438471266ce021817c6364f6a46d01564f2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.","[[  555    54   317  1514   599   536    18   102   834     2    23   536
      2    61  3229    38     3     9  1293  1968    28   284   677     6
     84  1112    38   761 14942     5    37 26207    13  2839  1514   102
    834     2    23   536     2  3229    12  1514   599   536    18   102
    834     2    23   536     2    61     3   102   834     2    23   536
      2  3229    19    12  3292   323     8  1293    13   514  4062     5
    242   514  4062     3  2544 15834    33 16009     3   632    42  1914
   1514   599   536    18   102   834     2    23   536     2    61     3
    102   834     2    23   536     2  3229   656     8   825  9129  4019
    705   992    12   135     5  7996 11376  4386   371  2773  1527  1527
     46  7295    45     8  3503    16 21875    10     8 21875    13  1514
      2  9880     2   599   536    18   102    61   102     2   536  1220
    599   536    18   102    61   102     2  3229    28  1445    12  1514
    102  3229  6315     3   632  2017   227  1514   102  3229  6315  8014
     84   598     8   825  2467     7   705    12  4062   728    79    33
   6549 12910     5     1]]"
70148c8d0f345ea36200d5ba19d021924d98e759,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],When the perception of what we hear is influenced by what we see.,"[[  366     8  8136    13   125    62  1616    19     3 12913    57   125
     62   217     5     1]]"
f85f2a532e7e700d9f8f9c09cd08d4e47b87bdd3,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],create fair systems,[[ 482 2725 1002    1]]
9176d2ba1c638cdec334971c4c7f1bb959495a8e,4.7619,"restaurants, movie reviews, and food critics",[[   0 3661    6 1974 2456    6   11  542 6800    7    1]],"we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)","[[   62   169   356   209    13     8  1391  3303    38     8   163  1391
     28  6493  3783   251   383   761     6    11    62  6825     8  4252
    825    30   356   209    13     8  2387  3303     6  3086    41   279
    439   201  9885     7    41   427   201 12587    41  9021   201    11
   3057    41   329    61     1]]"
395b61d368e8766014aa960fde0192e4196bcb85,0.0,WN16N & FB15k,[[    0     3 21170  2938   567     3   184     3 15586  1808   157     1]],1 IMDB dataset and 2 Yelp datasets,"[[  209     3  5166  9213 17953    11   204  7271    40   102 17953     7
      1]]"
74e866137b3452ec50fb6feaf5753c8637459e62,0.0,Rouge BIBREF1,[[    0 23777     3  5972 25582   371   536     1]],"following the pyramid framework, we design an annotation scheme",[[  826     8 22734  4732     6    62   408    46 30729  5336     1]]
567dc9bad8428ea9a2658c88203a0ed0f8da0dc3,25.0,Bi-LSTM with max-flow and max-flow,[[   0 2106   18 7600 2305   28 9858   18 7631   11 9858   18 7631    1]],BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS ,"[[ 2106  7600  2305  1220   254 17235   599  9413    15   526    18  4563
     61    11  2106  7600  2305  1220   254 17235   599   517    61  1220
  16034     1]]"
b3de9357c569fb1454be8f2ac5fcecaea295b967,100.0,"10,000",[[    0 13923     1]],"10,000",[[13923     1]]
70148c8d0f345ea36200d5ba19d021924d98e759,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"a perceptual illusion, where listening to a speech sound while watching a mouth pronounce a different sound changes how the audio is heard","[[    3     9   399  6873  3471 19178     6   213  5351    12     3     9
   5023  1345   298  3355     3     9  4247 29786     3     9   315  1345
   1112   149     8  2931    19  1943     1]]"
14b74ad5a6f5b0506511c9b454e9c464371ef8c4,0.0,English-German,[[    0  1566    18 24518     1]],"De-En, Ja-En, Ro-En",[[ 374   18 8532    6 2215   18 8532    6 2158   18 8532    1]]
fabf6fdcfb4c4c7affaa1e4336658c1e6635b1bf,0.0,"BIBREF15, BIBREF16","[[    0     3  5972 25582   371  1808     6     3  5972 25582   371  2938
      1]]","This dataset was created by BIBREF8, another English dataset from BIBREF8 ,  dataset from The Sarcasm Detector","[[  100 17953    47   990    57     3  5972 25582   371 11864   430  1566
  17953    45     3  5972 25582   371   927     3     6 17953    45    37
   9422  6769    51     3 31636   127     1]]"
c262d3d1c5a8b6fef6b594d5eee86bc2b09e3baf,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
c88a846197b72d25e04ec55f00ee3e72f655504c,0.0,Vote and votes,[[    0  3152    17    15    11 11839     1]],corpus of state speeches delivered during the annual UN General Debate,"[[11736   302    13   538 26147  3566   383     8  2041  4417  2146  9794
    342     1]]"
1e185a3b8cac1da939427b55bf1ba7e768c5dae4,0.0,"BERT-Base, Word2Vec, Glove","[[    0   272 24203    18 14885    15     6  4467   357   553    15    75
      6  9840   162     1]]",VAE,[[9039  427    1]]
7793805982354947ea9fc742411bec314a6998f6,0.0,manually generated,[[    0 12616  6126     1]],We performed the annotation with freely available tools for the Portuguese language.,"[[  101  3032     8 30729    28 14019   347  1339    21     8 21076  1612
      5     1]]"
e6583c60b13b87fc37af75ffc975e7e316d4f4e0,0.0,CASINO,[[    0     3 18678   196  7400     1]],"7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)","[[  489   951  3113    87     7    63   195     9 15979    41     3    87
     23    63    87     6     3    87   102    23    63    87     6     3
     87    17    23    63    87     6     3    87    26    23    63    87
      6     3    87    76   210    87     6     3    87    51    87     6
      3    87    29    87     3    61    38   168    38   314  1234   599
   4665     6   815     6  2124    11     3 11260   210    61     1]]"
8f16dc7d7be0d284069841e456ebb2c69575b32b,0.0,BIBREF13,[[    0     3  5972 25582   371  2368     1]],versions of LiLi,[[5204   13 1414  434   23    1]]
5c88d601e8fca96bffebfa9ef22331ecf31c6d75,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
bd419f4094186a5ce74ba6ac1622b24e29e553f4,31.5789,EM score of 82.16 on OpenStreetMap and 82.,"[[    0     3  6037  2604    13     3  4613     5  2938    30  2384 11500
     15    15    17 25760    11     3  4613     5]]",accuracy of 30.3% on single sentences and 0.3 on complete paragraphs,"[[ 7452    13   604     5  5170    30   712 16513    11     3 19997    30
    743  8986     7     1]]"
ddf5e1f600b9ce2e8f63213982ef4209bab01fd8,0.0,"SQuAD, ODSQA",[[    0   180  5991  6762     6   411  3592 23008     1]],Spoken-SQuAD,[[8927 2217   18  134 5991 6762    1]]
193ee49ae0f8827a6e67388a10da59e137e7769f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],recovers a masked document to its original form,"[[ 8303     7     3     9     3    51 23552  1708    12   165   926   607
      1]]"
09c86ef78e567033b725fc56b85c5d2602c1a7c3,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],simply averaging the predictions from the constituent single models,[[  914     3     9 23980     8 20099    45     8 17429   712  2250     1]]
b36f867fcda5ad62c46d23513369337352aa01d2,23.5294,WN18 and FB15k,[[    0     3 21170  2606    11     3 15586  1808   157     1]],"WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2","[[ 4467  9688     3  5972 25582   371   632     6  1443 10925     3  5972
  25582   371  4347     3 21170  2606    41     9   769  2244    13  4467
   9688    61     3  5972 25582   371  2266     3     6     3 15586  1808
    439    41     9   769  2244    13  1443 10925    61     3  5972 25582
    371   357     1]]"
375b281e7441547ba284068326dd834216e55c07,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"a setup where the seeker interacts with a real conversational interface and the wizard, an intermediary, performs actions related to the seeker's message","[[    3     9  5818   213     8  2762    49  6815     7    28     3     9
    490  3634   138  3459    11     8 25027     6    46 25960    63     6
   1912     7  2874  1341    12     8  2762    49    31     7  1569     1]]"
352bc6de5c5068c6c19062bad1b8f644919b1145,0.0,2,[[  0 204   1]],"we considered 4 different proportions i.e., INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 of the training data to train the classifier","[[   62  1702   314   315  7385     7     3    23     5    15     5     6
   3388 20006 24030   632     3     6  3388 20006 24030   536     3     6
   3388 20006 24030   357    11  3388 20006 24030   519    13     8   761
    331    12  2412     8   853  7903     1]]"
b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54,33.3333,"Bi-directional, multi-lingual",[[    0  2106    18 26352     6  1249    18 25207     1]],multilingual NMT (MNMT) BIBREF19,"[[ 1249 25207   445  7323    41   329   567  7323    61     3  5972 25582
    371  2294     1]]"
b962cc817a4baf6c56150f0d97097f18ad6cd9ed,0.0,open questions,[[  0 539 746   1]],These 8 tasks require different competencies and a different level of understanding of the document to be well answered,"[[  506   505  4145  1457   315 29514    11     3     9   315   593    13
   1705    13     8  1708    12    36   168  9986     1]]"
fc4ae12576ea3a85ea6d150b46938890d63a7d18,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
ef7212075e80bf35b7889dc8dd52fcbae0d1400a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Linked entities may be ambiguous or too common,[[    3 29806 12311   164    36     3 24621  1162    42   396  1017     1]]
cfbec1ef032ac968560a7c76dec70faf1269b27c,0.0,tuples,[[  0   3  17 413 965   1]],Knowledge Base Question Answering ,[[16113  8430 11860 11801    53     1]]
8a7615fc6ff1de287d36ab21bf2c6a3b2914f73d,0.0,"Bi-LSTM, LSTM, LSTM with atomic element, LS","[[    0  2106    18  7600  2305     6     3  7600  2305     6     3  7600
   2305    28     3 20844  3282     6     3  7600]]","BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21","[[ 2106  7600  2305  5972 25582   371  2534     6  2106  7600  2305  1220
    254 17235  5972 25582   371  1755     6  2106  7600  2305  1220  4545
    371  5972 25582   371  4347  2106  7600  2305  1220   254 17235  1220
   4545   371  5972 25582   371  4482 19602   825  5972 25582   371   632
     11 19796   205  8556   825  5972 25582   371  2658     1]]"
badc9db40adbbf2ea7bac29f2e4e3b6b9175b1f9,6.25,"Using a supervised LSTM to detect a set of puns, we","[[    0     3  3626     3     9     3 23313     3  7600  2305    12  8432
      3     9   356    13  4930     7     6    62]]",for the homographic dataset F1 score of 92.19 and 80.19 on detection and location and for the heterographic dataset F1 score of 89.76 on detection,"[[   21     8 13503 14797 17953   377   536  2604    13     3  4508     5
   2294    11  2775     5  2294    30 10664    11  1128    11    21     8
  26481 14797 17953   377   536  2604    13     3  3914     5  3959    30
  10664     1]]"
7fa3c2c0cf7f559d43e84076a9113a390c5ba03a,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
45e9533586199bde19313cd43b3d0ecadcaf7a33,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
81a35b9572c9d574a30cc2164f47750716157fc8,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10","[[ 2751    15    15    51    11  1546   208    63     3  5972 25582   371
  11116     3 23268     3    15    17   491     5     3  5972 25582   371
   1298     6    11  2751    15    15    51     3    15    17   491     5
      3  5972 25582   371  1714     1]]"
506d21501d54a12d0c9fd3dbbf19067802439a04,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Words that a user wants them to appear in the generated output.,"[[4467    7   24    3    9 1139 2746  135   12 2385   16    8 6126 3911
     5    1]]"
31735ec3d83c40b79d11df5c34154849aeb3fb47,0.0,"BIBREF5, BIBREF6, BIBREF8, BIBREF","[[    0     3  5972 25582   371 11116     3  5972 25582   371 11071     3
   5972 25582   371 11864     3  5972 25582   371]]",20 evaluators were recruited from our institution and asked to each perform 20 annotations,"[[  460     3    15  7480  6230   130 26870    45    69  6568    11  1380
     12   284  1912   460 30729     7     1]]"
d67c01d9b689c052045f3de1b0918bab18c3f174,24.0,"Compared to the previous best model, the attention-sum reader improved by almost 2% and","[[    0     3 25236    12     8  1767   200   825     6     8  1388    18
   4078  5471  3798    57   966     3  5406    11]]",Answer with content missing: (Table 2) Accuracy of best AS reader results including ensembles are 78.4 and 83.7 when trained on BookTest compared to 71.0 and 68.9 when trained on CBT for Named endity and Common noun respectively.,"[[11801    28   738  3586    10    41 20354  9266  4292  3663  4710    13
    200  6157  5471   772   379  8784     7    33     3  3940     5   591
     11   505 25168   116  4252    30  3086   382   222     3  2172    12
    489 12734    11     3  3651     5  1298   116  4252    30   205  9021
     21  5570    26   414   485    11  7155   150   202  6898     5     1]]"
5fa36dc8f7c4e65acb962fc484989d20b8fdaeec,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
044f922604b4b3f42ae381419fd5cd5624fa0637,12.9032,"in some cases, attention is seen as capturing the relevance of multiple source words where their relevance","[[    0    16   128  1488     6  1388    19   894    38     3 18147     8
  20208    13  1317  1391  1234   213    70 20208]]","most word alignments only involve one or a few words, attention can be distributed more freely","[[  167  1448 14632     7   163  7789    80    42     3     9   360  1234
      6  1388    54    36  8308    72 14019     1]]"
ed2eb4e54b641b7670ab5a7060c7b16c628699ab,0.0,"SQuAD, NLI and text classification",[[    0   180  5991  6762     6   445  8159    11  1499 13774     1]],SR,[[   3 6857    1]]
99e78c390932594bd833be0f5c890af5c605d808,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],QA PGNet and Multi-decoder QA PGNet,"[[    3 23008     3  7861  9688    11  4908    18   221  4978    52     3
  23008     3  7861  9688     1]]"
709feae853ec0362d4e883db8af41620da0677fe,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters,"[[ 3277  1388     7    12 12487  2850    11  4061     7     3     9   415
    655  1675   344     8  2850    38     3     9  3599 12520     7 10488
   1293     3 15433     8  1293 11455     7    30     8  2357   344  2850
      1]]"
71bd5db79635d48a0730163a9f2e8ef19a86cd66,0.0,"lexical overlap, word order, tenseness","[[    0     3 30949   138 21655     6  1448   455     6     3    17  5167
    655     1]]",semantics-altering grammatical modifiers,[[27632     7    18  8818    53     3  5096  4992   138 21090     7     1]]
26c2e1eb12143d985e4fb50543cf0d1eb4395e67,0.0,"stereotypes may be pervasive enough for the data to be consistently biased, stereotypes may","[[    0 26524     7   164    36   399  9856   757   631    21     8   331
     12    36  8182 30026     6 26524     7   164]]",Ethnic bias,[[  262   189  2532 14387     1]]
545e92833b0ad4ba32eac5997edecf97a366a244,0.0,word co-occurrence networks,[[    0  1448   576    18 16526  5275     1]],Removing the master node deteriorates performance across all datasets,"[[  419  7168    53     8  2325   150   221     3 18687    15     7   821
    640    66 17953     7     1]]"
78a5546e87d4d88e3d9638a0a8cd0b7debf1f09d,0.0,"conversational recommendation, question answering, movie recommendation",[[    0  3634   138 10919     6   822 18243     6  1974 10919     1]],"Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation","[[  638    18  1649 11788 19957     6     3 27569 11946     6   419  1788
     15  2165  5154     6     3 20119 11946     1]]"
d3ff2986ca8cb85a9a5cec039c266df756947b43,0.0,feature sets that are more predictive than the baselines,[[    0  1451  3369    24    33    72 27875   145     8 20726     7     1]],"words embeddings, style, and morality features","[[ 1234 25078    26    53     7     6   869     6    11  4854   485   753
      1]]"
c598028815066089cc1e131b96d6966d2610467a,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
64af7f5c109ed10eda4fb1b70ecda21e6d5b96c8,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"an intentional and potentially multicast communication, â€œthe expression of an opinion or an action by individuals or groups deliberately designed to influence the opinions or the actions of other individuals or groups with reference to predetermined ends""","[[   46 24768    11  6149  1249  5254  1901     6   105   532  3893    13
     46  3474    42    46  1041    57  1742    42  1637 24067   876    12
   2860     8  8479    42     8  2874    13   119  1742    42  1637    28
   2848    12   554 22755  5542   121     1]]"
23e16c1173b7def2c5cb56053b57047c9971e3bb,0.0,LSTM,[[   0    3 7600 2305    1]],"BIBREF15, BIBREF19, BIBREF20 ","[[    3  5972 25582   371  1808     6     3  5972 25582   371  2294     6
      3  5972 25582   371  1755     1]]"
277a7e916e65dfefd44d2d05774f95257ac946ae,0.0,"ConciergeQA, ACE, CAS, CASB, CASB,","[[    0  1193  9186   397 23008     6     3 11539     6     3 18678     6
      3 18678   279     6     3 18678   279     6]]","Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT","[[24239   138 25942  7257     7     6  2106  7600  2305    18  4545   371
      6  4908    18   382     9     7   157  6630     6  3318 12920   382
      1]]"
761de1610e934189850e8fda707dc5239dd58092,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],M2M Transformer,[[  283   357   329 31220     1]]
5cb610d3d5d7d447b4cd5736d6a7d8262140af58,0.0,using a multilingual approach,[[    0   338     3     9  1249 25207  1295     1]],by randomly alternating between languages for every new minibatch,"[[   57 21306     3 30859   344  8024    21   334   126  3016   115 14547
      1]]"
6ecb69360449bb9915ac73c0a816c8ac479cbbfc,0.0,"conversational search, conversational question answering, conversational recommendation","[[    0  3634   138   960     6  3634   138   822 18243     6  3634   138
  10919     1]]","text, speech, image, click, etc",[[1499    6 5023    6 1023    6 1214    6  672    1]]
dfca00be3284cc555a6a4eac4831471fb1f5875b,0.0,"70,000 notes",[[    0     3 28891  3358     1]],"30 terms, each term-sanse pair has around 15 samples for testing","[[ 604 1353    6  284 1657   18    7 3247   15 3116   65  300  627 5977
    21 2505    1]]"
d004ca2e999940ac5c1576046e30efa3059832fa,19.0476,multi-attention and single attention models,[[    0  1249    18 25615    11   712  1388  2250     1]],"classic RNN model, avgRNN model, attentionRNN model and multiattention RNN model with and without a projected layer","[[ 2431   391 17235   825     6     3     9   208   122 14151   567   825
      6  1388 14151   567   825    11  1249 25615   391 17235   825    28
     11   406     3     9 16037  3760     1]]"
12c50dea84f9a8845795fa8b8c1679328bd66246,0.0,"WSJ10, WSJ10 Live Chat",[[   0    3 8439  683 1714    6    3 8439  683 1714 3306 9802    1]],"CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus","[[  205 17517 17953     6   460  1506 10739     7     6 14639 12559   209
  11736   302     1]]"
5eda469a8a77f028d0c5f1acd296111085614537,0.0,Arabic$rightarrow $Spanish,[[    0 19248  3229     2  3535  6770  1514 19675  1273     1]],"De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-En, En-Ru","[[  374    18  8532     6   695    18   371    52     6  6248    18  8532
      6   695    18   427     7     6  2158    18  8532     6   695    18
   2962     6  1533    18  8532     6   695    18 17137     1]]"
a1ac2a152710335519c9a907eec60d9f468b19db,0.0,"linguistic (sentiment, readability emotion part-of-speech","[[    0     3 24703    41  5277    23   297     6   608  2020 32128 13868
  32128   294    18   858    18     7   855 10217]]",An output layer for each task,[[ 389 3911 3760   21  284 2491    1]]
a1b3e2107302c5a993baafbe177684ae88d6f505,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"ILPRL contains 548 sentences, OurNepali contains 3606 sentences","[[   27  6892 12831  2579   305  3707 16513     6   421   567    15  6459
     23  2579  9181   948 16513     1]]"
a7d020120a45c39bee624f65443e09b895c10533,7.8431,in a conversational fashion by imitating human apprehensions and in,"[[    0    16     3     9  3634   138  2934    57   256   155  1014   936
      3     9   102 22459   106     7    11    16]]","Whenever we encounter an unknown concept or relation while answering a query, we perform inference using our existing knowledge. If our knowledge does not allow us to draw a conclusion, we typically ask questions to others to acquire related knowledge and use it in inference. ","[[    3 15138    62  6326    46  7752  2077    42  4689   298 18243     3
      9 11417     6    62  1912    16 11788   338    69  1895  1103     5
    156    69  1103   405    59   995   178    12  3314     3     9  7489
      6    62  3115   987   746    12   717    12  7464  1341  1103    11
    169    34    16    16 11788     5     1]]"
f651cd144b7749e82aa1374779700812f64c8799,0.0,SVM score,[[    0   180 12623  2604     1]],"BLEU, FKGL, SARI, Simplicity","[[    3  8775 12062     6   377   439 13011     6   180 22410     6  6619
  10435   485     1]]"
c34e80fbbfda0f1786d3b00e06cef5ada78a3f3c,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
1bb7eb5c3d029d95d1abf9f2892c1ec7b6eef306,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning.","[[    3 17638  7561    21  3760    18 10684   761     6     3 22724  5170
     21 20487   257     6     3 23913  6370    21  2025  1036     5     1]]"
0810b43404686ddfe4ca84783477ae300fdd2ea4,50.0,RNN layer,[[    0   391 17235  3760     1]],The transformer layer,[[   37 19903  3760     1]]
8060a773f6a136944f7b59758d08cc6f2a59693b,0.0,849090,[[   0    3 4608 2394 2394    1]],1000 hours data,[[5580  716  331    1]]
56a8826cbee49560592b2d4b47b18ada236a12b9,0.0,We use the meta-data embedded within those tweets to look for differences between tweets ,"[[    0   101   169     8 10531    18  6757 13612   441   273 10657     7
     12   320    21  5859   344 10657     7     3]]","Exposure, Characterization, Polarization",[[13471  4334     6 20087  1707     6 19052  1707     1]]
26e2d4d0e482e6963a76760323b8e1c26b6eee91,0.0,"TIMIT-2013, TIBREF21, TIBREF22, ","[[    0     3  5494 12604    18 11138     6     3  5494 25582   371  2658
      6     3  5494 25582   371  2884     6     3]]","Once split into 8 subsets (A-H), the test set used are blocks D+H and blocks F+H","[[1447 5679  139  505  769 2244    7   41  188   18  566  201    8  794
   356  261   33 6438  309 1220  566   11 6438  377 1220  566    1]]"
73e83c54251f6a07744413ac8b8bed6480b2294f,0.0,"feature of the reviewer's name, sentimental sentiment, and the sentiment of the review","[[   0 1451   13    8 1132   49   31    7  564    6 6493  138 6493    6
    11    8 6493   13    8 1132]]","generate word embeddings specific to a domain, TDK (TÃ¼rk Dil Kurumu - â€œTurkish Language Institutionâ€) dictionary to obtain word polarities","[[ 3806  1448 25078    26    53     7   806    12     3     9  3303     6
      3 10494   439    41   382  4087   157   309   173  8333   440    76
      3    18   105   382   450   157  1273 10509 14932  7058 24297    12
   3442  1448     3  9618  2197     1]]"
203337c15bd1ee05763c748391d295a1f6415b9b,0.0,single attention model,[[   0  712 1388  825    1]],Projected Layer,[[ 2786    15    26 22697     1]]
455d4ef8611f62b1361be4f6387b222858bb5e56,5.1282,dialog policies,[[    0 13463  3101     1]],"The crowdsourcing platform CrowdFlower was used to obtain natural dialog data that prompted the user to paraphrase, explain, and/or answer a question from a Simple questions BIBREF7 dataset. The CrowdFlower users were restricted to English-speaking countries to avoid dialogs  with poor English.","[[   37  4374 19035  1585 15343    26 15390    49    47   261    12  3442
    793 13463   331    24     3 18797     8  1139    12  3856 27111     6
   3209     6    11    87   127  1525     3     9   822    45     3     9
   9415   746     3  5972 25582   371   940 17953     5    37 15343    26
  15390    49  1105   130 12103    12  1566    18 26611  1440    12  1792
  13463     7    28  2714  1566     5     1]]"
dd09db5eb321083dba16c2550676e60682f9a0cd,0.0,"labels for each utterance, Friends, EmotionPush, and Distinct","[[    0 11241    21   284     3  5108   663     6  9779     6   262  7259
    345  8489     6    11  2678 19827     1]]","Ekmanâ€™s six basic emotions,  neutral",[[ 262  157  348   22    7 1296 1857 7848    6 7163    1]]
2a564b092916f2fabbfe893cf13de169945ef2e1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The number of reviews in this movie corpus is 20,244 and the average number of words in reviews is 39. Each of these reviews has a star-rating score which is indicative of sentiment.","[[   37   381    13  2456    16    48  1974 11736   302    19 16047   357
   3628    11     8  1348   381    13  1234    16  2456    19  6352     5
   1698    13   175  2456    65     3     9  2213    18    52  1014  2604
     84    19 28618    13  6493     5     1]]"
b9d168da5321a7d7b812c52bb102a05210fe45bd,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
c01784b995f6594fdb23d7b62f20a35ae73eaa77,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game","[[12894   772    57 11795   966   985    13     8  1149    15    35  1031
      6   167    13     8  1513  1031    33    16     8 18723   356     6
    213     3     9   182   307  5932    13  2874    19   831    21  3447
      8   467     1]]"
518dae6f936882152c162058895db4eca815e649,50.0,3 layers,[[   0  220 7500    1]],eight layers,[[2641 7500    1]]
0ad4359e3e7e5e5f261c2668fe84c12bc762b3b8,1.2739,"BiLSTM BIBREF22, BERT BIBREF23, LS","[[    0  2106  7600  2305     3  5972 25582   371  2884     6   272 24203
      3  5972 25582   371  2773     6     3  7600]]","Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). 
Stanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018).","[[ 4892    17  1433 13774 20726     7    10     3 14151 11053    41  5231
   1703     3    15    17   491     5  2038   201  1980     9  3698    18
  14151 11053    41   308  2444     3    15    17   491     5  1412   201
      3  3463    18 14151 11053    41  2247    23   152     3    15    17
    491     5  1230   201   332  7645 17235    41   329  1063     3    15
     17   491     5  1230   201  7552    18  7600  2305    41   382     9
     23     6   264  1703     6    11  6362    53  1230   201  1980     9
  10966    18  7600  2305    18  5518    41   434    23    76     6  1593
     23    76     6    11 29034  1233   201  5795    18   382    60    15
   7600  2305    41   434    23    76     6  1593    23    76     6    11
  29034  1233   201     3  3463    18  7600  2305    41 13284  1468     6
   1593    23   152     6    11  1027   107    76  1233   201  2106  4302
    382    60    15    41   382  4606    11 28015  1233   201  2846    51
   2370  7552    18  7600  2305    41  3541    32    23     6  6545    32
      6    11  5531   846   201  7552  9688    41  3541  4606     3    15
     17   491     5   846   201 19602    41   439   603  1412   201  1980
      9   134   295    41   956  1024    32     6  2318     6    11 20313
   2274  1230   201     3  7600  2305    18   254 17235    41   956  9492
      3    15    17   491     5  1421   201     3 17770    18    51  7600
   2305    41   448     9    26  2590     6  2194   776    89  2381   447
    172     6    11  1923    17 11049   624  1233   201   272 10077  1768
   7435  1768   638   553    15    41   329    75 14050    29     3    15
     17   491     5  1233   201   272 10077  1768  7435  1768   262 11160
     32    41   345    15  4849     3    15    17   491     5   846   137
  19796  6869 10509    86 11788 20726     7    10   325  4669  8951 14727
   7552    18  7600  2305    41   476 19914 22713     3    15    17   491
      5  1233   201  7552    18   390 19602    41   329  1063     3    15
     17   491     5  1421   201  2846    51  2370  7552    18  7600  2305
     41  3541    32    23     6  6545    32     6    11  5531   846   201
    445  4132    41   329  6513   107    26   138     9    23    11  6214
   1233   201 14317 10880    26  8662    18 20748  3426    41 10499    35
      3    15    17   491     5   846   201  7127    23    26  3471     3
  24052 23734    52     7    10    41   567    23    15    11  5185     7
    138  1233   201  2106  7600  2305    28   879  1601  2201    53    41
   3541    35     6   301    53     6    11  1027   107    76   846   137
      1]]"
57f23dfc264feb62f45d9a9e24c60bd73d7fe563,0.0,"augmented dataset contains utterances of 103,432 sentences","[[    0     3 28984 17953  2579     3  5108   663     7    13     3 17864
      6   591  2668 16513     1]]",Unanswerable,[[ 597 3247 3321  179    1]]
b8d0e4e0e820753ffc107c1847fe1dfd48883989,0.0,Yes,[[   0 2163    1]],NCEL considers only adjacent mentions.,[[ 9187  3577  1099     7   163 12487  2652     7     5     1]]
63c3550c6fb42f41a0c93133e9fca12ac00df9b3,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
7697baf8d8d582c1f664a614f6332121061f87db,0.0,AMT,[[   0   71 7323    1]],SVMhmm ,[[  180 12623   107   635     1]]
68df324e5fa697baed25c761d0be4c528f7f5cf7,0.0,"actions, natural language interface, and conversational search",[[   0 2874    6  793 1612 3459    6   11 3634  138  960    1]],"Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation","[[  638    18  1649 11788 19957     6     3 27569 11946     6   419  1788
     15  2165  5154     6     3 20119 11946     1]]"
1170e4ee76fa202cabac9f621e8fbeb4a6c5f094,100.0,No,[[  0 465   1]],No,[[465   1]]
8de9f14c7c4f37ab103bc8a639d6d80ade1bc27b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],one additional hop,[[   80  1151 13652     1]]
cb384dc5366b693f28680374d31ff45356af0461,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
86bf75245358f17e35fc133e46a92439ac86d472,0.0,cwrs provide a more detailed representation of the syntactic structure of,"[[   0    3   75  210   52    7  370    3    9   72 3117 6497   13    8
  8953   17 2708  447 1809   13]]", the performance differences across all tasks are small enough ,[[   8  821 5859  640   66 4145   33  422  631    1]]
cb196725edc9cdb2c54b72364f3bbf7c76471490,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5daeb8d4d6f3b8543ec6309a7a35523e160437eb,0.0,German,[[   0 2968    1]],English ,[[1566    1]]
6844683935d0d8f588fa06530f5068bf3e1ed0c0,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],A finite corpora may entirely omit rare word combinations,"[[   71   361  7980 11736   127     9   164  4585     3    32  1538  3400
   1448 15617     1]]"
4e748cb2b5e74d905d9b24b53be6cfdf326e8054,0.0,sarcastic features are extracted from human eye-movement patterns and used to train binary,"[[    0     3     7  4667 10057   753    33 21527    45   936  1580    18
   7168  1194  4264    11   261    12  2412 14865]]",Unanswerable,[[ 597 3247 3321  179    1]]
8e52637026bee9061f9558178eaec08279bf7ac6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],using the machine translation platform Apertium ,[[ 338    8 1437 7314 1585   71  883   17 2552    1]]
38f58f13c7f23442d5952c8caf126073a477bac0,0.0,82.0%,[[   0    3 4613    5 6932    1]],Best results authors obtain is EM 51.10 and F1 63.11,"[[ 1648   772  5921  3442    19     3  6037 11696     5  1714    11   377
    536     3  3891     5  2596     1]]"
a5505e25ee9ae84090e1442034ddbb3cedabcf04,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],F1 of 0.8099,[[ 377  536   13 4097 2079 3264    1]]
79413ff5d98957c31866f22179283902650b5bb6,0.0,WSJ228,[[   0    3 8439  683  357 2577    1]]," E-book annotation data: editor tags, Amazon search terms, and  Amazon review keywords.","[[  262    18  2567 30729   331    10  6005 12391     6  2536   960  1353
      6    11  2536  1132 12545     5     1]]"
5bc1dc6ebcb88fd0310b21d2a74939e35a4c1a11,40.0,"English, German, French, Spanish, Japanese and Spanish",[[   0 1566    6 2968    6 2379    6 5093    6 4318   11 5093    1]],"English, Spanish, Finnish",[[ 1566     6  5093     6 28124     1]]
5b551ba47d582f2e6467b1b91a8d4d6a30c343ec,9.0909,accuracy,[[   0 7452    1]]," Distinct-1/2, UMA = User Matching Accuracy, MRR
= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)","[[ 2678 19827    18 17637     6   412  4148  3274  6674 12296    53  4292
   3663  4710     6   283 12224  3274 23045  7136    23  1409  1489     3
  22557     6     3  6158  3274 25072 10684 11633   147 20726    41    15
   7480   920    21     3 19947  2696 14152   399   825    61     1]]"
c2d1387e08cf25cb6b1f482178cca58030e85b70,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
249c805ee6f2ebe4dbc972126b3d82fb09fa3556,0.0,accuracy,[[   0 7452    1]], the average dissimilarity of all pairs of tags in the list of recommended tags,"[[    8  1348  1028 26714   485    13    66 14152    13 12391    16     8
    570    13  3024 12391     1]]"
2c7e94a65f5f532aa31d3e538dcab0468a43b264,0.0,Exchanges between a user and a task-oriented dialog system,"[[    0  8231     7   344     3     9  1139    11     3     9  2491    18
   9442 13463   358     1]]",intents are annotated manually with guidance from queries collected using a scoping crowdsourcing task,"[[ 9508     7    33    46  2264   920 12616    28  4864    45 13154  4759
    338     3     9 16190    53  4374 19035  2491     1]]"
4944cd597b836b62616a4e37c045ce48de8c82ca,2.5641,"large-scale semantic similarity comparison, clustering, and information retrieval via semantic search","[[    0   508    18  6649 27632  1126   485  4993     6  9068    53     6
     11   251 24515   138  1009 27632   960     1]]","MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.

CR: Sentiment prediction of customer product reviews BIBREF26.

SUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.

MPQA: Phrase level opinion polarity classification from newswire BIBREF28.

SST: Stanford Sentiment Treebank with binary labels BIBREF29.

TREC: Fine grained question-type classification from TREC BIBREF30.

MRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.","[[    3  9320    10  4892  2998   295 21332    21  1974  2456     3 20317
   4995     7    30     3     9   874   456  2643     3  5972 25582   371
   1828     5     3  4545    10  4892  2998   295 21332    13   884   556
   2456     3  5972 25582   371  2688     5   180 10134   683    10 19237
  10696 21332    13 16513    45  1974  2456    11  5944  4505    51  5414
      3  5972 25582   371  2555     5  5220 23008    10  3657 15447   593
   3474     3  9618   485 13774    45  1506 12931     3  5972 25582   371
   2577     5   180  4209    10 19796  4892  2998   295  7552  4739    28
  14865 11241     3  5972 25582   371  3166     5   332 20921    10 11456
      3   122 10761   822    18  6137 13774    45   332 20921     3  5972
  25582   371  1458     5     3  9320  4051    10  2803  2200  4734 27111
  10052   302    45  8449  1506  2836     3  5972 25582   371  3341     5
      1]]"
f7d0fa52017a642a9f70091a252857fccca31f12,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections","[[   41    23    61  2250    24   169  6080     3 24052     3  7600  2305
      7     6    41    23    23    61  2250    28   315  3388 20006 24030
    632     6    41    23    23    23    61  2250   406  3388 20006 24030
   4347    41    23   208    61  2250    24  9162  1364  2625     7  1009
    158    15   102  9136  5992     1]]"
932b39fd6c47c6a880621a62e6a978491d881d60,0.0,LSTM+En-Fr and TL-En-Fr,"[[    0     3  7600  2305  1220  8532    18   371    52    11     3 12733
     18  8532    18   371    52     1]]",TransE,[[4946  427    1]]
f03112b868b658c954db62fc64430bebbaa7d9e0,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
deb89bca0925657e0f91ab5daca78b9e548de2bd,0.0,five different binary classification tasks,[[    0   874   315 14865 13774  4145     1]]," presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.","[[ 3053    87 23767    13  6900   106  2366     6   951  3113 26612     6
      3  3727 15932    40     6   306    18  6849 22121  3573    11   306
     18  1549 22121  3573     5     1]]"
71a0c4f19be4ce1b1bae58a6e8f2a586e125d074,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Wikipedia articles are labelled with one of six quality classes, in descending order of quality: Featured Article (â€œFAâ€), Good Article (â€œGAâ€), B-class Article (â€œBâ€), C-class Article (â€œCâ€), Start Article (â€œStartâ€), and Stub Article (â€œStubâ€)., The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus., The arXiv dataset BIBREF2 consists of three subsets of academic articles under the arXiv repository of Computer Science (cs), from the three subject areas of: Artificial Intelligence (cs.ai), Computation and Language (cs.cl), and Machine Learning (cs.lg). In line with the original dataset formulation BIBREF2 , a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI. Failing this, it is considered to be rejected (noting that some of the papers may not have been submitted to one of these conferences). ","[[16885  2984    33     3 29506    28    80    13  1296   463  2287     6
     16     3 30960   455    13   463    10     3 16772    26  7491  8186
   4795  7058     6  1804  7491  8186  6302  7058     6   272    18  4057
   7491  8186   279  7058     6   205    18  4057  7491  8186   254  7058
      6  3273  7491  8186  7681    17  7058     6    11   180 14535  7491
   8186 13076   115 16937     6    37   463   853    13     3     9 16885
   1108    19  7604    57 16885  1132   277    42   136  3366  1139     6
    113    54  2497   190     8  1108    31     7  1350   543    12  1535
  16698     5     6    37  1584     4    23   208 17953     3  5972 25582
    371   357     3  6848    13   386   769  2244     7    13  2705  2984
    365     8  1584     4    23   208 22109    13  5491  2854    41    75
      7   201    45     8   386  1426   844    13    10 24714  5869  2825
   1433    41    75     7     5     9    23   201   638 31148    11 10509
     41    75     7     5    75    40   201    11  5879  6630    41    75
      7     5    40   122   137    86   689    28     8   926 17953 20029
      3  5972 25582   371   357     3     6     3     9  1040    19  1702
     12    43   118  4307    41    23     5    15     5    19 18294  3783
     15    26    61     3    99    34  6407     3     9  1040    16     8
      3  9213  6892  3501    42    19  2904  4307    57   136    13     8
    826 13653    10    71  8440     6     3  6037   567  6892     6 10144
    188  8440     6   262   188  8440     6     3  3221  8440     6   445
  21563     6     3  4666  6858     6     3  4666 12564     6    42 22656
    196     5   377 17446    48     6    34    19  1702    12    36 12967
     41  2264    53    24   128    13     8  5778   164    59    43   118
   5776    12    80    13   175 13653   137     1]]"
ffde866b1203a01580eb33237a0bb9da71c75ecf,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
a1645d0ba50e4c29f0feb806521093e7b1459081,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Social Honeypot dataset (public) and Weibo dataset (self-collected); yes,"[[ 2730 16354  3013 17953    41 15727    61    11   101    23   115    32
  17953    41  7703    18 22153    15    26  3670  4273     1]]"
63a1cbe66fd58ff0ead895a8bac1198c38c008aa,0.0,"BERT-Seq, MATREF12, MATREF13","[[    0   272 24203    18   134    15  1824     6     3 18169  4386   371
   2122     6     3 18169  4386   371  2368     1]]","Show&Tell model, LRCN1u",[[ 3111   184   382  3820   825     6     3 12564 10077   536    76     1]]
e35c2fa99d5c84d8cb5d83fca2b434dcd83f3851,9.5238,Twitter,[[   0 3046    1]],"public resources where suspicious Twitter accounts were annotated, list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy","[[  452  1438   213 21641  3046  3744   130    46  2264   920     6   570
     28   430  3538  3046  3744    45     3  5972 25582   371  2294    24
     33  1702 20739     1]]"
6d1217b3d9cfb04be7fcd2238666fa02855ce9c5,15.3846,"Bi-LSTM, LSTM with atomic element labeling","[[    0  2106    18  7600  2305     6     3  7600  2305    28     3 20844
   3282  3783    53     1]]","BiLSTM, BiLSTM+CNN, BiLSTM+CRF, BiLSTM+CNN+CRF, CNN, Stanford CRF","[[ 2106  7600  2305     6  2106  7600  2305  1220   254 17235     6  2106
   7600  2305  1220  4545   371     6  2106  7600  2305  1220   254 17235
   1220  4545   371     6 19602     6 19796   205  8556     1]]"
e5bc73974c79d96eee2b688e578a9de1d0eb38fd,6.6667,by comparing model performance with the available data.,[[    0    57     3 14622   825   821    28     8   347   331     5     1]], by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly,"[[   57  2505  6917    30     3     9  6504   769  2244    13   943  2650
  10409    11   943  1017   150   202 16148   746    24     8     3   102
      7    52  8784   228    59  1525  6549     1]]"
dc1cec824507fc85ac1ba87882fe1e422ff6cffb,0.0,NABEL,[[   0  445 5359 3577    1]],"Dataset of total 3500 questions from the Internet and other sources such as books of general knowledge questions, history, etc.","[[2747 2244   13  792  220 2560  746   45    8 1284   11  119 2836  224
    38 1335   13  879 1103  746    6  892    6  672    5    1]]"
4944cd597b836b62616a4e37c045ce48de8c82ca,13.3333,"large-scale semantic similarity comparison, clustering, and information retrieval via semantic search","[[    0   508    18  6649 27632  1126   485  4993     6  9068    53     6
     11   251 24515   138  1009 27632   960     1]]","Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification.","[[  679   348  1225  5027  3471 18347   485     6  6493 21332     6  1426
  10696 21332     6  9261   593  3474     3  9618   485 13774     6 19796
   4892  2998   295  7552  4739     6  1399     3   122 10761   822    18
   6137 13774     5     1]]"
1bb7eb5c3d029d95d1abf9f2892c1ec7b6eef306,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Their best model achieved a 2.49% Character Error Rate.,"[[ 2940   200   825  5153     3     9     3 17638  7561 20087   848    52
    127 13002     5     1]]"
f3c204723da53c7c8ef4dc1018ffbee545e81056,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
149da739b1c19a157880d9d4827f0b692006aa2c,0.0,BIBREF23,[[    0     3  5972 25582   371  2773     1]],"SVM, MLP, FastText, CNN, BERT, DialogFlow, Rasa NLU","[[  180 12623     6   283  6892     6  6805 13598    17     6 19602     6
    272 24203     6 25843 15390     6  9053     9   445  9138     1]]"
2f901dab6b757e12763b23ae8b37ae2e517a2271,0.0,English-Japanese,[[   0 1566   18  683 9750 1496   15    1]],Germanâ€“English,[[ 2968   104 26749     1]]
82595ca5d11e541ed0c3353b41e8698af40a479b,8.6957,inorganic and organic ways,[[    0    16 11127   447    11  3648  1155     1]],"Organic: mention of political parties names in the profile attributes, specific mentions of political handles in the profile attributes.
Inorganic:  adding Chowkidar to the profile attributes, the effect of changing the profile attribute in accordance with Prime Minister's campaign, the addition of election campaign related keywords to the profile.","[[15045    10  2652    13  1827  2251  3056    16     8  3278 12978     6
    806  2652     7    13  1827 13338    16     8  3278 12978     5    86
  11127   447    10  2651   205  4067  2168  3439    12     8  3278 12978
      6     8  1504    13  2839     8  3278 15816    16  4408   663    28
   5923  3271    31     7  2066     6     8   811    13  4356  2066  1341
  12545    12     8  3278     5     1]]"
7e38e0279a620d3df05ab9b5e2795044f18d4471,22.2222,"Race, Religion, and Gender",[[    0 10949     6 18182     6    11   350  3868     1]],"personal attack, racism, and sexism",[[  525  3211     6 21681     6    11     3     7   994   159    51     1]]
16535db1d73a9373ffe9d6eedaa2369cefd91ac4,0.0,"Wikipedea Corpus, Wikipedia",[[    0  2142  2168  3138    15     9 10052   302     6 16885     1]],PubMed+PMC,[[22057 20123  1220   345  3698     1]]
012b8a89aea27485797373adbcda32f16f9d7b54,8.6957,lexical model with a n-gram model,"[[    0     3 30949   138   825    28     3     9     3    29    18  5096
    825     1]]","BIBREF11 that uses a character level n-gram language model, 'shallow' naive Bayes classifiers BIBREF12, BIBREF8, BIBREF13, BIBREF14, SVMs BIBREF15, BIBREF16 used an SVM with character n-gram, parts of speech tag features and some other engineered features, The winning approach for DSL 2015 used an ensemble naive Bayes classifier, The fasttext classifier BIBREF17, hierarchical stacked classifiers (including lexicons), bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24","[[    3  5972 25582   371  2596    24  2284     3     9  1848   593     3
     29    18  5096  1612   825     6     3    31     7   107 18912    31
      3    29     9   757  2474    15     7   853  7903     7     3  5972
  25582   371  2122     6     3  5972 25582   371 11864     3  5972 25582
    371  2368     6     3  5972 25582   371  2534     6   180 12623     7
      3  5972 25582   371  1808     6     3  5972 25582   371  2938   261
     46   180 12623    28  1848     3    29    18  5096     6  1467    13
   5023  7860   753    11   128   119  1948  3737   753     6    37  3447
   1295    21   309  5629  1230   261    46  8784     3    29     9   757
   2474    15     7   853  7903     6    37  1006  6327   853  7903     3
   5972 25582   371  2517     6  1382  7064  1950     3 24052   853  7903
      7    41  5751     3 30949   106     7   201  2647 26352     3    60
  14907 24228  5275     3  5972 25582   371  2773    42  8784     7    13
      3    60 14907 24228  5275     3  5972 25582   371  2266     1]]"
66c96c297c2cffdf5013bab5e95b59101cb38655,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"F1 scores are:
HUBES-PHI: Detection(0.965), Classification relaxed (0.95), Classification strict (0.937)
Medoccan: Detection(0.972), Classification (0.967)","[[  377   536  7586    33    10   454 10134  3205    18  8023   196    10
      3 31636    23   106   599 23758  4122   201  4501  2420  9904    41
  23758  9120     6  4501  2420  6926    41 23758  4118    61  1212  7171
   1608    10     3 31636    23   106   599 23758  5865   201  4501  2420
     41 23758  3708    61     1]]"
c0af8b7bf52dc15e0b33704822c4a34077e09cd1,16.6667,LSTM,[[   0    3 7600 2305    1]],"Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers.","[[ 5645 26352     3  7600  2305  5275    28  3547  8580  7973  9478    11
    668  7500     5     1]]"
d78f7f84a76a07b777d4092cb58161528ca3803c,0.0,Bi-LSTM with a corresponding equivalence of a pair of,"[[   0 2106   18 7600 2305   28    3    9    3 9921    3   15 1169 2165
  1433   13    3    9 3116   13]]",backward greedy search over each sentence's label sequence to identify word boundaries,"[[  223  2239 30337    63   960   147   284  7142    31     7  3783  5932
     12  2862  1448 11814     1]]"
126ff22bfcc14a2f7e1a06a91ba7b646003e9cf0,0.0,BLEU score,[[    0     3  8775 12062  2604     1]], MT system on the data released by BIBREF11,"[[    3  7323   358    30     8   331  1883    57     3  5972 25582   371
   2596     1]]"
5529f26f72ce47440c2a64248063a6d5892b9fde,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Answer with content missing: (Evaluation section) Given that in CLIR the primary goal is to get a better ranked list of documents against a translated query, we only report Mean Average Precision (MAP).","[[11801    28   738  3586    10    41   427  7480   257  1375    61  9246
     24    16 11175  5705     8  2329  1288    19    12   129     3     9
    394     3  8232   570    13  2691   581     3     9 15459 11417     6
     62   163   934 23045 23836 28464    41 25790   137     1]]"
ecd5770cf8cb12cb34285e26ab834301c17c53e1,3.2258,LSTM,[[   0    3 7600 2305    1]],"random forest, The question is encoded with a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question. The image is described with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 BIBREF25 . The system performs an element-wise multiplication of the image and question features, after linearly transforming the image descriptor to 1024 dimensions. The final layer of the architecture is a softmax layer.","[[ 6504  5827     6    37   822    19 23734    26    28     3     9   335
   2266    18 11619     3  7600  2305   825    24  1217    16     3     9
     80    18 10718    20 11815   127    13   284  1448    16     8   822
      5    37  1023    19  3028    28     8  1283  4314    18 11619  3911
     45     8   336  1540  2895  3760    13     8  1193 24817   138  1484
   9709  3426    41   254 17235   201   584 21320  2938     3  5972 25582
    371  1828     3     5    37   358  1912     7    46  3282    18 10684
   1249 13555    13     8  1023    11   822   753     6   227 13080   120
      3 21139     8  1023    20 11815   127    12   335  2266  8393     5
     37   804  3760    13     8  4648    19     3     9  1835  9128  3760
      5     1]]"
30eacb4595014c9c0e5ee9669103d003cfdfe1e5,36.3636,SemEval 2010 benchmark,[[    0   679    51   427  2165  2735 15705     1]],relation classification dataset of the SemEval 2010 task 8,"[[ 4689 13774 17953    13     8   679    51   427  2165  2735  2491   505
      1]]"
0c234db3b380c27c4c70579a5d6948e1e3b24ff1,100.0,LSTM,[[   0    3 7600 2305    1]],LSTM,[[   3 7600 2305    1]]
5b551ba47d582f2e6467b1b91a8d4d6a30c343ec,5.5556,accuracy,[[   0 7452    1]],"BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence","[[    3  8775 12062  2292 13572    11   391 26260   427    18   434     6
  17902    13  6126  5459   338 12022  3785 10364    68     3 19657    30
      3   324   315  1139 10958     6  1139  8150  7452    41  6122   188
    201 23045  7136    23  1409  1489     3 22557    41   329 12224   201
  24228 10389   825    45     3  5972 25582   371  4201    12  3613  2696
     18  4563   576   760  1433     1]]"
f8da63df16c4c42093e5778c01a8e7e9b270142e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Segmentation quality is evaluated by calculating the precision, recall, and F-score of the automatic segmentations in comparison to the segmentations made by expert annotators from the ANNODIS subcorpus.","[[15696   297   257   463    19 14434    57     3 25956     8 11723     6
   7881     6    11   377    18     7  9022    13     8  6569  5508  1628
     16  4993    12     8  5508  1628   263    57  2205    46  2264  6230
     45     8     3 21478   667 15438   769 14723   302     5     1]]"
d915b401bb96c9f104a0353bef9254672e6f5a47,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions,"[[   12   856 27354     8   825    30     8   331  1809    16   455    12
   1709 27801    13   237 21454   127    63 15293     1]]"
2a6003a74d051d0ebbe62e8883533a5f5e55078b,40.0,neural embedding model,[[    0 24228 25078    26    53   825     1]],3C model,[[220 254 825   1]]
051df74dc643498e95d16e58851701628fdfd43e,0.0,"They collected online conversations from a variety of sources including ReachOut.com, ReachOut.","[[    0   328  4759   367  9029    45     3     9  1196    13  2836   379
  23202 15767     5   287     6 23202 15767     5]]",crawling and pre-processing an OSG web forum,"[[18639    53    11   554    18 15056    53    46  6328   517   765  5130
      1]]"
bf52c01bf82612d0c7bbf2e6a5bb2570c322936f,7.4074,high correlation results are observed for BIBREF1 and BIBREF2 respectively,"[[    0   306 18712   772    33  6970    21     3  5972 25582   371   536
     11     3  5972 25582   371   357  6898     1]]",we observe that many variants of Rouge scores do not have high correlations with human pyramid scores,"[[   62  7743    24   186  6826     7    13 23777  7586   103    59    43
    306 18712     7    28   936 22734  7586     1]]"
0ab3df10f0b7203e859e9b62ffa7d6d79ffbbe50,0.0,"precision, recall, F-measure",[[    0 11723     6  7881     6   377    18 31038     1]],"average classification accuracy, execution performance",[[ 1348 13774  7452     6  9328   821     1]]
01a41c0a4a7365cd37d28690735114f2ff5229f2,0.0,Google Analytics,[[    0  1163 13926     1]],http://www.blogger.com,[[ 2649  1303  1986     5   115 27282     5   287     1]]
f5e6f43454332e0521a778db0b769481e23e7682,0.0,LSTMs,[[   0    3 7600 2305    7    1]],"pivoting, pivoting$_{\rm m}$","[[16959    53     6 16959    53  3229   834     2    52    51     3    51
      2  3229     1]]"
30eacb4595014c9c0e5ee9669103d003cfdfe1e5,50.0,SemEval 2010 benchmark,[[    0   679    51   427  2165  2735 15705     1]],SemEval 2010 task 8 BIBREF8,"[[  679    51   427  2165  2735  2491   505     3  5972 25582   371   927
      1]]"
b9d07757e2d2c4be41823dd1ea3b9c7f115b5f72,18.1818,bilingual dictionary of Chinese words,[[    0 30521 24297    13  2830  1234     1]],ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era,"[[ 4913  2830   892  3187    16   633     3 24805     7  3010    41  7932
   5580  7645    18  3632  7645    61    11  2984  1545    57 20076    13
     24     3  1498     1]]"
664db503509b8236bc4d3dc39cebb74498365750,0.0,"best variant is the one that achieves the highest accuracy and fluency, the best variant is","[[   0  200 6826   19    8   80   24 1984    7    8 2030 7452   11 6720
  4392    6    8  200 6826   19]]",Hierarchical-k,[[3204 7064 1950   18  157    1]]
df0257ab04686ddf1c6c4d9b0529a7632330b98e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"On Coin Collector, proposed model finds shorter path in fewer number of interactions with enironment.
On Cooking World, proposed model uses smallest amount of steps and on average has bigger score and number of wins by significant margin.","[[  461 15589 30275     6  4382   825 12902 10951  2071    16     3 10643
    381    13  9944    28     3    35 17773   297     5   461  6176    53
   1150     6  4382   825  2284     3 17924   866    13  2245    11    30
   1348    65  4038  2604    11   381    13  9204    57  1516  6346     5
      1]]"
3fff37b9f68697d080dbd9d9008a63907137644e,0.0,comparing to state-of-the-art results for offensive language identification,"[[    0     3 14622    12   538    18   858    18   532    18  1408   772
     21 12130  1612 10356     1]]",Unanswerable,[[ 597 3247 3321  179    1]]
357eb9f0c07fa45e482d998a8268bd737beb827f,0.0,"Character Identification, Language Style Recovery",[[    0 20087 31474     6 10509  7936 16532     1]],"Kvmemnn,  Feed Yourself, Poly-encoder, BERT bi-ranker","[[  480   208   526    51    29    29     6  8495    26   696  7703     6
   7945    18    35  4978    52     6   272 24203  2647    18  6254    49
      1]]"
8d4ac4afbf5b14f412171729ceb5e822afcfa3f4,100.0,No,[[  0 465   1]],No,[[465   1]]
2c947447d81252397839d58c75ebcc71b34379b5,0.0,On the Reddit Reddit Reddit Reddit Reddit Redd,"[[   0  461    8 1624   26  155 1624   26  155 1624   26  155 1624   26
   155 1624   26  155 1624   26]]","CoinCollector, CookingWorld",[[15589  9939  3437   127     6  6176    53 17954     1]]
a6d3e57de796172c236e33a6ceb4cca793dc2315,0.0,"LSTM+attention, LSTM+attention, LSTM+attention","[[    0     3  7600  2305  1220 25615     6     3  7600  2305  1220 25615
      6     3  7600  2305  1220 25615     1]]","Answer with content missing: (Experimental Setup missing subsections)
To be selected: We compared REFRESH against a baseline which simply selects the first m leading sentences from each document (LEAD) and two neural models similar to ours (see left block in Figure 1), both trained with cross-entropy loss.
Answer: LEAD","[[11801    28   738  3586    10    41  5420  4267 13974  2821   413  3586
  29233     7    61   304    36  2639    10   101     3  2172  4083   371
  12200   566   581     3     9 20726    84   914  1738     7     8   166
      3    51  1374 16513    45   284  1708    41   434 19552    61    11
    192 24228  2250  1126    12    69     7    41  2338   646  2463    16
   7996  8925     6   321  4252    28  2269    18    35 12395    63  1453
      5 11801    10   301 19552     1]]"
249c805ee6f2ebe4dbc972126b3d82fb09fa3556,0.0,accuracy,[[   0 7452    1]],average dissimilarity of all pairs of tags in the list of recommended tags,"[[ 1348  1028 26714   485    13    66 14152    13 12391    16     8   570
     13  3024 12391     1]]"
3f856097be2246bde8244add838e83a2c793bd17,0.0,Rouge BIBREF1,[[    0 23777     3  5972 25582   371   536     1]],The content relevance between the candidate summary and the human summary is evaluated using information retrieval - using the summaries as search queries and compare the overlaps of the retrieved results. ,"[[   37   738 20208   344     8  4775  9251    11     8   936  9251    19
  14434   338   251 24515   138     3    18   338     8  4505    51  5414
     38   960 13154    11  4048     8 21655     7    13     8     3 31340
    772     5     1]]"
9b76f428b7c8c9fc930aa88ee585a03478bff9b3,0.0,103,[[    0     3 17864     1]],53 documents,[[12210  2691     1]]
ffa7f91d6406da11ddf415ef094aaf28f3c3872d,10.8108,BLEU correlates by 0.81 compared to HEALTH by 0.81 ,"[[    0     3  8775 12062 30575     7    57  4097  4959     3  2172    12
      3  6021  4090  4611    57  4097  4959     3]]",Best proposed metric has average correlation with human judgement of 0.913 and 0.846 compared to best compared metrics result of 0.758 and 0.829 on WikiBio and WebNLG challenge.,"[[ 1648  4382     3  7959    65  1348 18712    28   936 22555    13     3
  23758  2368    11     3 22384  4448     3  2172    12   200     3  2172
  15905   741    13     3 22426  3449    11     3 22384  3166    30  2142
   2168 26475    11  1620 18207   517  1921     5     1]]"
df95b3cb6aa0187655fd4856ae2b1f503d533583,25.0,n-grams,[[   0    3   29   18 5096    7    1]],simple n-grams (like fastText) and unsupervised morphemes,"[[  650     3    29    18  5096     7    41  2376  1006 13598    17    61
     11    73 23313     3  8886    15  2687     1]]"
05c49b9f84772e6df41f530d86c1f7a1da6aa489,17.3913,user interface to structured and semi-structured data,[[    0  1139  3459    12 14039    11  4772    18 16180    26   331     1]],"The current implementation of Macaw supports a command line interface as well as mobile, desktop, and web apps.","[[  37  750 4432   13 2143    9  210 4951    3    9 4106  689 3459   38
   168   38 1156    6 6555    6   11  765 4050    5    1]]"
23b2901264bda91045258b5d4120879ae292e950,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP,"[[    3  3626   309  4112  1453  1172     7     8   377   536  2604    57
   1768 12100   927    21     3  9320  4051    11  1768 22426   519    21
   1593  2247   345     1]]"
d28d86524292506d4b24ae2d486725a6d57a3db3,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],33.33,[[5400    5 4201    1]]
392fb87564c4f45d0d8d491a9bb217c4fce87f03,0.0,Attention-based multi-attention,[[    0 20748    18   390  1249    18 25615     1]]," LastStateRNN, AvgRNN, AttentionRNN","[[ 2506   134  4748 14151   567     6    71   208   122 14151   567     6
  20748 14151   567     1]]"
6cd25c637c6b772ce29e8ee81571e8694549c5ab,0.0,Wikidata,[[   0 2142 2168 6757    1]],"WikiBio dataset,  introduce two new biography datasets, one in French and one in German","[[ 2142  2168 26475 17953     6  4277   192   126 25705 17953     7     6
     80    16  2379    11    80    16  2968     1]]"
90d946ccc3abf494890e147dd85bd489b8f3f0e8,0.0,accuracy,[[   0 7452    1]],Unanswerable,[[ 597 3247 3321  179    1]]
e3c9e4bc7bb93461856e1f4354f33010bc7d28d5,0.0,"Bi-LSTM, BERT",[[    0  2106    18  7600  2305     6   272 24203     1]],"SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ","[[  180 12623     3     6 19602     3     6   350  8503     3     6 19602
     87   517  8503  1220  4207     6     3    52    18  1582     3     6
     71    32   188     1]]"
6a633811019e9323dc8549ad540550d27aa6d972,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],No,[[465   1]]
de3b1145cb4111ea2d4e113f816b537d052d9814,0.0,BERT,[[    0   272 24203     1]]," Wang et al. BIBREF21, paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets","[[18102     3    15    17   491     5     3  5972 25582   371  2658     6
   1040    57     3  5972 25582   371  4201    16    84    79   261  2411
      3    35 12395    63   853  7903    28  2182    13  1234   825    12
    853  4921   796  3973 17953     7     1]]"
009ce6f2bea67e7df911b3f93443b23467c9f4a1,0.0,BiLSTM with max pooling,[[   0 2106 7600 2305   28 9858 2201   53    1]],pre-trained multi-BERT,[[  554    18    17 10761  1249    18 12920   382     1]]
43eecc576348411b0634611c81589f618cd4fddf,0.0,"LSTMs, LSTMs with a minimum of 256 possible inputs","[[    0     3  7600  2305     7     6     3  7600  2305     7    28     3
      9  2559    13     3 19337   487  3785     7]]","MLE, SeqGAN, LeakGAN, MaliGAN, IRL, RAML, DialogGAN, DPGAN","[[  283  3765     6   679  1824   517  5033     6   312  1639   517  5033
      6  2148    23   517  5033     6    27 12831     6 12118   434     6
  25843   517  5033     6   309 24127   567     1]]"
7e38e0279a620d3df05ab9b5e2795044f18d4471,0.0,"Race, Religion, and Gender",[[    0 10949     6 18182     6    11   350  3868     1]],"racism, sexism, personal attack, not specifically about any single topic","[[21681     6     3     7   994   159    51     6   525  3211     6    59
   3346    81   136   712  2859     1]]"
06eb9f2320451df83e27362c22eb02f4a426a018,18.1818,3 incremental levels of document preprocessing,[[    0   220 28351  1425    13  1708   554 15056    53     1]],"raw text, text cleaning through document logical structure detection, removal of keyphrase sparse sections of the document","[[ 5902  1499     6  1499  2327   190  1708     3  6207  1809 10664     6
   4614    13   843 27111 14144     7    15  6795    13     8  1708     1]]"
03ebb29c08375afc42a957c7b2dc1a42bed7b713,0.0,accuracy,[[   0 7452    1]],"proposed ontology, which, in our evaluation procedure, was populated with 3121 events entries from 51 documents.","[[ 4382    30    17  1863     6    84     6    16    69  5002  3979     6
     47     3 23606    28   220 22011   984 10066    45 11696  2691     5
      1]]"
8e44c02c2d9fa56fb74ace35ee70a5add50b52ae,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
cd2878c5a52542ddf080b20bec005d9a74f2d916,0.0,occupational class,[[    0 25082   853     1]],"Technology, Religion, Fashion, Publishing, Sports coach, Real Estate, Law, Environment, Tourism, Construction, Museums, Banking, Security, Automotive.","[[ 3669     6 18182     6 11256     6 17511     6  5716  3763     6  2977
   6925     6  2402     6 13706     6 14691     6  8663     6  3312     7
      6 20460     6  3684     6 20378     5     1]]"
b0dbe75047310fec4d4ce787be5c32935fc4e37b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],By evaluating their model on adversarial sets containing misleading sentences,"[[  938     3 17768    70   825    30 23210    23   138  3369     3  6443
  23828 16513     1]]"
70a1b0f9f26f1b82c14783f1b76dfb5400444aa4,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Their accuracy in word segmentation is about 94%-97%.,[[2940 7452   16 1448 5508  257   19   81  668 5988 7141 6170    5    1]]
357eb9f0c07fa45e482d998a8268bd737beb827f,0.0,"Character Identification, Language Style Recovery",[[    0 20087 31474     6 10509  7936 16532     1]],"the Poly-encoder from BIBREF7 humeau2019real, Feed Yourself BIBREF13 is an open-domain dialogue agent with a self-feeding model, Kvmemnn BIBREF14 is a key-value memory network with a knowledge base that uses a key-value retrieval mechanism to train over multiple domains simultaneously, We compare against four dialogue system baselines: Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline trained on the Persona-Chat dataset using the same training hyperparameters (including learning rate scheduler and length capping settings) described in Section SECREF20., a BERT bi-ranker","[[    8  7945    18    35  4978    52    45     3  5972 25582   371   940
      3  4884  1607  8584  6644     6  8495    26   696  7703     3  5972
  25582   371  2368    19    46   539    18 22999  7478  3102    28     3
      9  1044    18 17012    53   825     6   480   208   526    51    29
     29     3  5972 25582   371  2534    19     3     9   843    18 12097
   2594  1229    28     3     9  1103  1247    24  2284     3     9   843
     18 12097 24515   138  8557    12  2412   147  1317  3303     7 11609
      6   101  4048   581   662  7478   358 20726     7    10   480   208
    526    51    29    29     6  8495    26   696  7703     6  7945    18
     35  4978    52     6    11     3     9   272 24203  2647    18  6254
     49 20726  4252    30     8  5780     9    18   254   547 17953   338
      8   337   761  6676  6583  4401     7    41  5751  1036  1080  2023
     52    11  2475   212  5341  3803    61  3028    16  5568  5985  4545
   9976  1755     5     6     3     9   272 24203  2647    18  6254    49
      1]]"
a9337636b52de375c852682a2561af2c1db5ec63,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
01e2d10178347d177519f792f86f25575106ddc7,20.0,WSJ and WSJX,[[   0    3 8439  683   11    3 8439  683    4    1]],"LORELEI datasets of Uzbek, Mandarin and Turkish","[[  301  2990 16479   196 17953     7    13   412   172   346   157     6
  31057    11 15423     1]]"
f1f1dcc67b3e4d554bfeb508226cdadb3c32d2e9,0.0,"WSJ2V, WSJ3V, WSJ4V","[[   0    3 8439  683  357  553    3    6    3 8439  683  519  553    3
     6    3 8439  683  591  553]]","SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28","[[  679    51   427  2165    18 11505  7729 16107   305     3  5972 25582
    371  2555     3     6     3  5972 25582   371  2577     1]]"
f03df5d99b753dc4833ef27b32bb95ba53d790ee,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Accounts that spread fake news are mostly unverified, recently created and have on average high friends/followers ratio","[[ 6288     7    24  3060  9901  1506    33  3323 13705  3676     6  1310
    990    11    43    30  1348   306   803    87 25278   277  5688     1]]"
443d2448136364235389039cbead07e80922ec5c,0.0,"BIBREF16, BIBREF17","[[    0     3  5972 25582   371  2938     6     3  5972 25582   371  2517
      1]]","LSA, TextRank, LexRank and ILP-based summary.","[[  301  4507     6  5027 22557     6 17546 22557    11    27  6892    18
    390  9251     5     1]]"
c2b8ee872b99f698b3d2082d57f9408a91e1b4c1,0.0,CoNLL2003 shared task BIBREF15,"[[    0   638   567 10376 23948  2471  2491     3  5972 25582   371  1808
      1]]","Babelfy, DBpedia Spotlight, Entityclassifier.eu, FOX, LingPipe MUC-7, NERD-ML, Stanford NER, TagMe 2","[[11563 10386    63     6     3  9213 24477  8927    17  2242     6  4443
    485  4057  7903     5    15    76     6 12408     4     6   301    53
    345    23   855   283  6463  6832     6     3 18206   308    18  6858
      6 19796     3 18206     6  3284   329    15   204     1]]"
7ae38f51243cb80b16a1df14872b72a1f8a2048f,13.7931,"Using a combination of EEG and MR data, we can determine whether the E","[[   0    3 3626    3    9 2711   13  262 8579   11    3 9320  331    6
    62   54 2082  823    8  262]]",we plot T-distributed Stochastic Neighbor Embedding (tSNE) corresponding to INLINEFORM0 and V/C classification tasks in Fig. FIGREF8 .,"[[   62  5944   332    18    26   159  5135  1054   472  6322 10057  1484
   9031  6693     3 17467    15  7249    41    17   134  4171    61     3
   9921    12  3388 20006 24030   632    11   584    87   254 13774  4145
     16     3 12286     5 11376  4386   371   927     3     5     1]]"
fd2c6c26fd0ab3c10aae4f2550c5391576a77491,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
2c78993524ca62bf1f525b60f2220a374d0e3535,0.0,"scientific articles, online reviews, forum posts, blogs, microblogs","[[    0  4290  2984     6   367  2456     6  5130  3489     6  9407     6
   2179 12139     7     1]]",rupnik2016news,[[    3    52   413  4953 11505 15808     1]]
2df4a045a9cd7b44874340b6fdf9308d3c55327a,0.0,Microsoft Azure (CBizSphere),[[    0  2803 21808    41   254   279    23   172 31722    61     1]],Unanswerable,[[ 597 3247 3321  179    1]]
e86130c5b9ab28f0ec539c2bed1b1ae9efb99b7d,0.0,"238,432 sentences",[[    0   204  3747     6   591  2668 16513     1]],Unanswerable,[[ 597 3247 3321  179    1]]
ebf0d9f9260ed61cbfd79b962df3899d05f9ebfb,12.1212,"two sets of test sentences, one for a novel'simplified' version of","[[    0   192  3369    13   794 16513     6    80    21     3     9  3714
      3    31     7 10296  3676    31   988    13]]","training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing","[[  761   356    65     3  3914     6  6348   357  7142 14152     6    11
      8   794   356    65   910 14152     6   761   356  2579  2838 11071
   2445  4482     3  8630    21   606    11   220  3390    21  2505     1]]"
869feb7f47606105005efdb6bea1c549824baea0,0.0,"TweetQA dataset contains 57,432 news articles and 4,714 questions and answers","[[    0 25335 23008 17953  2579     3  3436     6   591  2668  1506  2984
     11  6464   940  2534   746    11  4269     1]]","13,757",[[10670  3072   940     1]]
cfcdd73e712caf552ba44d0aa264d8dace65a589,7.2289,"23,700 queries that are short and unstructured, in the same style made by real users","[[    0 12992  9295 13154    24    33   710    11    73 16180    26     6
     16     8   337   869   263    57   490  1105]]","For ins scope data collection:crowd workers which provide questions and commands related to topic domains and additional data the rephrase and scenario crowdsourcing tasks proposed by BIBREF2 is used. 
For out of scope data collection:  from workers mistakes-queries written for one of the 150 intents that did not actually match any of the intents and using scoping and scenario tasks with prompts based on topic areas found on Quora, Wikipedia, and elsewhere.","[[  242    16     7  7401   331  1232    10    75  3623    26  2765    84
    370   746    11 16565  1341    12  2859  3303     7    11  1151   331
      8     3    60 27111    11  8616  4374 19035  4145  4382    57     3
   5972 25582   371   357    19   261     5   242    91    13  7401   331
   1232    10    45  2765  8176    18   835  2593  1545    21    80    13
      8  4261  9508     7    24   410    59   700  1588   136    13     8
   9508     7    11   338 16190    53    11  8616  4145    28  9005     7
      3   390    30  2859   844   435    30  2415   127     9     6 16885
      6    11  8975     5     1]]"
11e8bd4abf5f8bdabad3e8f0691e6d0ad6c326af,0.0,strong neural baseline,[[    0  1101 24228 20726     1]],"NO-MOVE, RANDOM, JUMP",[[ 5693    18   329 26479     6     3 16375 27415     6   446 28468     1]]
808f0ad46ca4eb4ea5492f9e14ca043fe1e206cc,0.0,3,[[  0 220   1]],"45,821 characters",[[3479    6 4613  536 2850    1]]
a74190189a6ced2a2d5b781e445e36f4e527e82a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],significant improvements clearly demonstrate that our approach is effective at improving model performance,[[1516 6867 3133 5970   24   69 1295   19 1231   44 4863  825  821    1]]
bc730e4d964b6a66656078e2da130310142ab641,0.0,"Bi-LSTM, BERT",[[    0  2106    18  7600  2305     6   272 24203     1]],probabilistic model,[[9551 3040  825    1]]
96b07373756d7854bccc3c12e8d41454ab8741f5,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
3f856097be2246bde8244add838e83a2c793bd17,0.0,Rouge BIBREF1,[[    0 23777     3  5972 25582   371   536     1]],"On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval.","[[  461   306   593     6    62 25509  6825     8   738 20208   344     8
   4775  9251    11     8   936  9251   338   251 24515   138     5     1]]"
07d15501a599bae7eb4a9ead63e9df3d55b3dc35,22.2222,using the psychological dimensions of people,[[    0   338     8 11041  8393    13   151     1]],using the Meaning Extraction Method,[[  338     8 25148  1881 10559  7717     1]]
2007bfb8f66e88a235c3a8d8c0a3b3dd88734706,16.3265,We use topic modeling to analyze the similarities and differences between extremist and non-extremist articles,"[[    0   101   169  2859 15309    12  8341     8 25758    11  5859   344
   5273   343    11   529    18 30650   343  2984]]","A comparison of common words, We aggregated the score for each word and normalized each article by emotions. To better compare the result, we added a baseline of 100 random articles from a Reuters news dataset as a non-religious general resource","[[   71  4993    13  1017  1234     6   101 12955    26     8  2604    21
    284  1448    11  1389  1601   284  1108    57  7848     5   304   394
   4048     8   741     6    62   974     3     9 20726    13   910  6504
   2984    45     3     9     3 18844  1506 17953    38     3     9   529
     18    60  2825  2936   879  3487     1]]"
2c59528b6bc5b5dc28a7b69b33594b274908cca6,7.6923,character-level model with a minimum of four attainable changes,"[[    0  1848    18  4563   825    28     3     9  2559    13   662     3
  31720  1112     1]]","A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters","[[   71  4772    18 31886     3   390   391 17235    41   134    75 14151
    567    61 11954     8   166    11   336  2850 12011     6    11    19
      3     9  6715     7  1225    12     8 12320    13     8  3224  2850
      1]]"
c70bafc35e27be9d1efae60596bc0dd390c124c0,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
551457ed34ca7fc0878c85bc664b135c21059b58,0.0,SST-2 dataset,[[    0   180  4209  4949 17953     1]],190 hours ( INLINEFORM1 100K instances),"[[    3 11776   716    41  3388 20006 24030   536   910   439 10316    61
      1]]"
395b61d368e8766014aa960fde0192e4196bcb85,0.0,WN16N & FB15k,[[    0     3 21170  2938   567     3   184     3 15586  1808   157     1]],three datasets based on IMDB reviews and Yelp reviews,"[[  386 17953     7     3   390    30     3  5166  9213  2456    11  7271
     40   102  2456     1]]"
03e9ac1a2d90152cd041342a11293a1ebd33bcc3,0.0,No,[[  0 465   1]],NLG datasets,[[  445 24214 17953     7     1]]
b7c3f3942a07c118e57130bc4c3ec4adc431d725,0.0,Fast.ai,[[   0 6805    5    9   23    1]],Unanswerable,[[ 597 3247 3321  179    1]]
bc16ce6e9c61ae13d46970ebe6c4728a47f8f425,0.0,average dialog length is 8.7 min.,[[    0  1348 13463  2475    19  4848   940  3519     5     1]],4.49 turns,[[2853 3647 5050    1]]
2275b0e195cd9cb25f50c5c570da97a4cce5dca8,0.0,Yes,[[   0 2163    1]],"Build a bilingual language model,   learn the target language specific parameters starting from a pretrained English LM , fine-tune both English and target model to obtain the bilingual LM.","[[14025     3     9 30521  1612   825     6   669     8  2387  1612   806
   8755  1684    45     3     9  7140 10761  1566     3 11160     3     6
   1399    18    17   444   321  1566    11  2387   825    12  3442     8
  30521     3 11160     5     1]]"
82642d3111287abf736b781043d49536fe48c350,0.0,"9,473 annotations for 9,300 tweets.","[[    0  9902  4177   519 30729     7    21  9902  5426 10657     7     5
      1]]","no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy","[[  150  2084    13  7562     6    20  8918  6526     6 30371  2085     6
  13034    42  1453    13   827     1]]"
e025061e199b121f2ac8f3d9637d9bf987d65cd5,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],15.5,[[  209 15938     1]]
73906462bd3415f23d6378590a5ba28709b17605,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length, NLI models tend to predict entailment for sentence pairs with a high lexical overlap","[[22455    18  9926 20726  1912     7   394   145  1253   788    12   123
     15     7    30    70     3 30949   138  1160    11  7142  2475     6
    445  8159  2250  2134    12  9689     3    35  5756   297    21  7142
  14152    28     3     9   306     3 30949   138 21655     1]]"
c2e475adeddcdc4d637ef0d4f5065b6a9b299827,0.0,accuracy,[[   0 7452    1]],"BLEU-4, NIST-4, ROUGE-4","[[    3  8775 12062  4278     6   445 13582  4278     6   391 26260   427
   4278     1]]"
d98847340e46ffe381992f1a594e75d3fb8d385e,0.0,Deep Learning,[[   0 9509 6630    1]],"Logistic Regression, neural networks",[[ 7736  3040   419 22430     6 24228  5275     1]]
9b4dc790e4ff49562992aae4fad3a38621fadd8b,0.0,"Using a combination of vector space embeddings and word embeddings,","[[    0     3  3626     3     9  2711    13 12938   628 25078    26    53
      7    11  1448 25078    26    53     7     6]]","BOW-Tags, BOW-KL(Tags), BOW-All, GloVe","[[  272 15251    18 23593     7     6   272 15251    18   439   434   599
  23593     7   201   272 15251    18  6838     6  9840   553    15     1]]"
e8a32460fba149003566969f92ab5dd94a8754a4,0.0,"BOC, INLINEFORM1",[[    0   272  5618     6  3388 20006 24030   536     1]],"Concept Raw Context model, Concept-Concept Context model","[[16688 19401  1193  6327   825     6 16688    18  4302  6873  1193  6327
    825     1]]"
6bf93968110c6e3e3640360440607744007a5228,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],we do not know exactly,[[  62  103   59  214 1776    1]]
14b74ad5a6f5b0506511c9b454e9c464371ef8c4,0.0,English-German,[[    0  1566    18 24518     1]],"De-En, Ja-En, Ro-En",[[ 374   18 8532    6 2215   18 8532    6 2158   18 8532    1]]
f1e90a553a4185a4b0299bd179f4f156df798bce,0.0,"BIBREF10, BIBREF10","[[    0     3  5972 25582   371  1714     6     3  5972 25582   371  1714
      1]]","CopyRNN BIBREF0, KEA BIBREF4 and Maui BIBREF8, CopyRNN*","[[20255 14151   567     3  5972 25582   371   632     6     3  9914   188
      3  5972 25582   371   591    11  7758    23     3  5972 25582   371
  11864 20255 14151   567  1935     1]]"
2ee715c7c6289669f11a79743a6b2b696073805d,3.3898,LSTM with a corresponding LDVM model,"[[    0     3  7600  2305    28     3     9     3  9921     3  9815 12623
    825     1]]","B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .

, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0","[[  272  5411    37   166 20726  2284   163     8  5394    23  1433    18
    390   753    57  6393    23  6706    11 18584  3142     3  5972 25582
    371  2596     3     5     6   272  4416    37   511 20726 12317     7
      8   701  2193    12     3     9  3116  3388 20006 24030   632     3
      6     3    99    11   163     3    99  3388 20006 24030   536  3475
     16     8  2233    13  3388 20006 24030   357     3     5     3     6
    180   536    10  8356     8  1375    45  3847  3388 20006 24030   632
     28     8  2030     3 30949   138  1126   485    12  3388 20006 24030
    536     3    10   180   536  3388 20006 24030  4482   180   357    10
   3399     8  1506   139     8   167  8325  1375    16  3388 20006 24030
    632     1]]"
77c34f1033702278f7f044806c1eba0c6ecb8b04,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
5e324846a99a5573cd2e843d1657e87f4eb22fa6,7.1429,"human labour is eliminated at expense of accuracy, as the context of each relative only approximates","[[    0   936 12568    19 17809    44 32128  8225    13  7452     6    38
      8  2625    13   284  5237   163 24672     7]]",The NÃ¤ive-Bayes classifier is corrected so it is not biased to most frequent classes,"[[   37   445  1864   757    18   279     9 10070   853  7903    19 23006
     78    34    19    59 30026    12   167  8325  2287     1]]"
8a1c0ef69b6022a0642ca131a8eacb5c97016640,0.0,"vulgarity, profanity, slang, slang, slang,","[[    0 31648   485     6  7108   152   485     6     3     7  4612     6
      3     7  4612     6     3     7  4612     6]]",using tweets that one has replied or quoted to as contextual information,"[[  338 10657     7    24    80    65 18606    42 16854    12    38 28131
    251     1]]"
ababb79dd3c301f4541beafa181f6a6726839a10,28.5714,IQ2,[[    0     3 20835   357     1]],â€œIntelligence Squared Debatesâ€ (IQ2 for short),"[[  105  1570  1625  2825  1433  7120    26  9794  6203   153    41 20835
    357    21   710    61     1]]"
45f7c03a686b68179cadb1413c5f3c1d373328bd,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses","[[   84  2579   147   314  5898     3 26892  2984     6   379   147   220
  11212    28   423  1499     6    81  2847  7765   308  4481     6   180
  25210    18  3881   553  4949     6    11  1341  4301   106     9 18095
     15     7     1]]"
bd5bd1765362c2d972a762ca12675108754aa437,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 )., full paragraph-level neural network model achieves the best macro-average F1-score of 48.82% in predicting implicit discourse relations, which outperforms previous neural tensor network models (e.g., BIBREF18 ) by more than 2 percents and outperforms the best previous system BIBREF19 by 1 percent., Then we also created ensemble models by applying majority voting to combine results of ten runs. From table 5 , each ensemble model obtains performance improvements compared with single model. The full model achieves performance boosting of (51.84 - 48.82 = 3.02) and (94.17 - 93.21 = 0.96) in macro F1-scores for predicting implicit and explicit discourse relations respectively. ","[[    8  1857   825  6339     7   207   821    21     3 22873 17623 22739
   5836    38   168     6    84    19 13289    28  1767   200   741 14156
  24273  2712 11663   377   536    18     7  9022    11     3  4271     5
    632  7561  7452    38  2196    16     3  5972 25582   371  2596     3
    137     6   423  8986    18  4563 24228  1229   825  1984     7     8
    200 11663    18 28951   377   536    18     7  9022    13  4678     5
    927  5406    16     3 29856 21773 22739  5836     6    84    91   883
   2032     7  1767 24228     3   324     7   127  1229  2250    41    15
      5   122     5     6     3  5972 25582   371  2606     3    61    57
     72   145   204  1093     7    11    91   883  2032     7     8   200
   1767   358     3  5972 25582   371  2294    57   209  1093     5     6
     37    29    62    92   990  8784  2250    57  6247  2942 10601    12
   5148   772    13     3   324  3154     5  1029   953   305     3     6
    284  8784   825  3442     7   821  6867     3  2172    28   712   825
      5    37   423   825  1984     7   821     3 24220    13    41  5553
      5  4608     3    18  4678     5  4613  3274  1877  4305    61    11
     41  4240     5  2517     3    18     3  4271     5  2658  3274     3
  23758 10938    16 11663   377   536    18     7  9022     7    21     3
  29856 21773    11 17623 22739  5836  6898     5     1]]"
1d9aeeaa6efa1367c22be0718f5a5635a73844bd,0.0,Unanswerable,[[   0  597 3247 3321  179    1]], the context and sequential nature of the text,[[    8  2625    11 29372  1405    13     8  1499     1]]
96a4091f681872e6d98d0efee777d9e820cb8dae,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters,"[[ 2747 30729 14387    15     7   213 10657     3  6443 31973  1329  1234
     33    46  2264   920    38  5591    42 12130   406   136   554  4078
    102  1575    81     8   569  2625    13 10657   277     1]]"
e5bc73974c79d96eee2b688e578a9de1d0eb38fd,0.0,by comparing model performance with the available data.,[[    0    57     3 14622   825   821    28     8   347   331     5     1]],majority of questions that our system could not answer so far are in fact answerable,"[[2942   13  746   24   69  358  228   59 1525   78  623   33   16  685
  1525  179    1]]"
ab0fd94dfc291cf3e54e9b7a7f78b852ddc1a797,0.0,SVM,[[    0   180 12623     1]],BIBREF26,[[    3  5972 25582   371  2688     1]]
7cd22ca9e107d2b13a7cc94252aaa9007976b338,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
b8d7d055ddb94f5826a9aad7479b4a92a9c8a2f0,0.0,LSTM,[[   0    3 7600 2305    1]],RNN,[[  391 17235     1]]
0d9fcc715dee0ec85132b3f4a730d7687b6a06f4,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],The expected number of unique outputs a word recognition system assigns to a set of adversarial perturbations ,"[[   37  1644   381    13   775  3911     7     3     9  1448  5786   358
  12317     7    12     3     9   356    13 23210    23   138 29404  1628
      1]]"
4dc268e3d482e504ca80d2ab514e68fd9b1c3af1,0.0,30,[[  0 604   1]],Unanswerable,[[ 597 3247 3321  179    1]]
81d607fc206198162faa54a796717c2805282d9b,0.0,Privacy experts from Google,[[    0 17865  2273    45  1163     1]],Individuals with legal training,[[10963     7    28  1281   761     1]]
b1a068c1050e2bed12d5c9550c73e59cd5b1f78d,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
9a9d225f9ac35ed35ea02f554f6056af3b42471d,18.1818,"collocations, tense errors, idioms","[[    0  7632 14836     7     6     3    17  5167  6854     6     3 19916
     51     7     1]]",patterns for generating all types of errors,[[ 4264    21     3 11600    66  1308    13  6854     1]]
3fff37b9f68697d080dbd9d9008a63907137644e,0.0,comparing to state-of-the-art results for offensive language identification,"[[    0     3 14622    12   538    18   858    18   532    18  1408   772
     21 12130  1612 10356     1]]",Unanswerable,[[ 597 3247 3321  179    1]]
568fb7989a133564d84911e7cb58e4d8748243ef,17.3913,high-reward trajectories of states and actions in the game using the exploration,"[[    0   306    18    60  2239     3  1313 11827    32  2593    13  2315
     11  2874    16     8   467   338     8  9740]]",explores the state space through keeping track of previously visited states by maintaining an archive,"[[ 2075     7     8   538   628   190  2627  1463    13  3150  5251  2315
     57  6011    46 13269     1]]"
3d7a982c718ea6bc7e770d8c5da564fbb9d11951,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions, supervision signal provided by the discriminator will help generator to capture the event-related patterns","[[ 6461    13 24228  5275     6     8  9877    19  3919    13  1036  6446
    529   747   291  3438     7     6 15520  3240   937    57     8  9192
   1016    56   199  9877    12  4105     8   605    18  3897  4264     1]]"
c59078efa7249acfb9043717237c96ae762c0a8c,0.0,LSTM with he and she word pairs,[[    0     3  7600  2305    28     3    88    11   255  1448 14152     1]],Unanswerable,[[ 597 3247 3321  179    1]]
0d9fcc715dee0ec85132b3f4a730d7687b6a06f4,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"the number of distinct word recognition outputs that an attacker can induce, not just the number of words on which the model is â€œfooledâ€","[[    8   381    13  6746  1448  5786  3911     7    24    46 22600    54
  21151     6    59   131     8   381    13  1234    30    84     8   825
     19   105    89    32    32  1361   153     1]]"
b68f72aed961d5ba152e9dc50345e1e832196a76,18.1818,BLEU score improves by 0.86 on all metrics,"[[    0     3  8775 12062  2604  1172     7    57  4097  3840    30    66
  15905     1]]",On average 0.64 ,[[ 461 1348 4097 4389    1]]
8eefa116e3c3d3db751423cc4095d1c4153d3a5f,13.3333,CoNLL2003 shared task BIBREF15,"[[    0   638   567 10376 23948  2471  2491     3  5972 25582   371  1808
      1]]","GENIA Corpus BIBREF3, CoNLL2003 BIBREF14, KORE50 BIBREF21 , ACE2004 BIBREF22 and MSNBC","[[    3  5042 26077 10052   302     3  5972 25582   371  6355   638   567
  10376 23948     3  5972 25582   371  2534     6   480 20888  1752     3
   5972 25582   371  2658     3     6     3 11539 21653     3  5972 25582
    371  2884    11  5266 15829     1]]"
fb3d30d59ed49e87f63d3735b876d45c4c6b8939,25.0,"accuracy, reliability, FAST, METEOR",[[    0  7452     6 10581     6   377 12510     6  7934  3463  2990     1]],"Precision, Recall, F-measure, accuracy",[[28464     6   419 16482     6   377    18 31038     6  7452     1]]
af5730d82535464cedfa707a03415ac2e7a21295,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set, formal spoken text (excerpted from court and presidential debate transcripts in the Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus), causal or procedural text, which describes sequences of events or actions, extracted from WikiHow, annotations using the longer contexts present in the GLUE RTE training data, which came from the RTE5 dataset","[[16885    41   858  5986    18  6007  2850    61    45     8 12616     3
  22579  5396  3013 23008   761   356     6  4727 11518  1499    41   994
   2110   102  1054    45  1614    11 13074  5054 20146     7    16     8
   9950   120   389  2264   920  3325    18 13026  7800    41 23010   254
     61    13     8  2384   797   868 10052   302   201 31161    42 19291
     40  1499     6    84  8788  5932     7    13   984    42  2874     6
  21527    45  2142  2168  7825     6 30729     7   338     8  1200  2625
      7   915    16     8     3 13011  5078   391  3463   761   331     6
     84   764    45     8   391  3463   755 17953     1]]"
876700622bd6811d903e65314ac75971bbe23dcc,0.0,"BIBREF13, BIBREF18","[[    0     3  5972 25582   371  2368     6     3  5972 25582   371  2606
      1]]", high-quality datasets  from SemEval-2016 â€œSentiment Analysis in Twitterâ€ task,"[[  306    18  4497 17953     7    45   679    51   427  2165    18 11505
    105   134   295    23   297 10582    16  3046   153  2491     1]]"
5fda8539a97828e188ba26aad5cda1b9dd642bc8,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"MSR: 97.7 compared to 97.5 of baseline
AS: 95.7 compared to 95.6 of baseline","[[  283  6857    10     3  4327     5   940     3  2172    12   668 15731
     13 20726  6157    10   668 27220     3  2172    12   668 25134    13
  20726     1]]"
ee2ad0ab64579cffb60853db6a8c0f971d7cf0ff,0.0,the author and the supervisor,[[    0     8  2291    11     8 14640     1]],Unanswerable,[[ 597 3247 3321  179    1]]
6bfba3ddca5101ed15256fca75fcdc95a53cece7,4.6512,"logical fallacy, emotion-based argumentation",[[    0     3  6207  1590  4710     6 13868    18   390  5464   257     1]],"Loaded language, Name calling or labeling, Repetition, Exaggeration or minimization, Doubt, Appeal to fear/prejudice, Flag-waving, Causal oversimplification, Slogans,  Appeal to authority, Black-and-white fallacy, dictatorship, Thought-terminating clichÃ©, Whataboutism, Reductio ad Hitlerum, Red herring, Bandwagon, Obfuscation, intentional vagueness, confusion, Straw man","[[ 1815 15624  1612     6  5570  3874    42  3783    53     6   419  4995
   4749     6  1881     9  6938   257    42  8984  1707     6 17159   115
     17     6 25024    12  2971    87  2026 14312   867     6 17016    18
    210     9  3745     6 16371     7   138   147     7    23 27717     6
  18043  2565     7     6 25024    12  5015     6  1589    18   232    18
  13698  1590  4710     6 25280  2009     6  4229    17    18  6544  1014
  27319     6   363  7932   159    51     6   419  7472    23    32     3
      9    26 22640   440     6  1624   160  1007     6  4483 15238   106
      6  4249    89   302    75   257     6 24768 15986   655     6 12413
      6  5438   210   388     1]]"
bbb77f2d6685c9257763ca38afaaef29044b4018,0.0,Arcane Detector with a Distinction Detector with Dist,"[[    0  1533  1608    15     3 31636   127    28     3     9  2678    17
     77  4985     3 31636   127    28  2678    17]]",Gaze Sarcasm using Multi Instance Logistic Regression.,"[[16709    15  9422  6769    51   338  4908    86  8389  7736  3040   419
  22430     5     1]]"
0f12dc077fe8e5b95ca9163cea1dd17195c96929,0.0,Gender classification criteria,[[    0   350  3868 13774  6683     1]],"Sentences involving at least one race- or gender-associated word,  sentence  have to be short and grammatically simple,  sentence have to  include expressions of sentiment and emotion.","[[ 4892   324  2319     3  6475    44   709    80  1964    18    42  7285
     18 30582  1448     6  7142    43    12    36   710    11     3 16582
    144  6402   650     6  7142    43    12   560  3893     7    13  6493
     11 13868     5     1]]"
0689904db9b00a814e3109fb1698086370a28fa2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Doc2Vec, PV-DBOW model",[[17268   357   553    15    75     6 16303    18  9213 15251   825     1]]
3de0487276bb5961586acc6e9f82934ef8cb668c,0.0,"Medical records, BIBREF0",[[    0  3721  3187     6     3  5972 25582   371   632     1]],"MEDDOCAN, NUBes-PHI",[[    3 21357  9857 11425     6 13046  2703     7    18  8023   196     1]]
70e9210fe64f8d71334e5107732d764332a81cb1,0.0,fully convolutional front-end model,[[    0  1540   975 24817   138   851    18   989   825     1]],HMM-based system,[[ 454 8257   18  390  358    1]]
28067da818e3f61f8b5152c0d42a531bf0f987d4,0.0,length of the N-grams is calculated as the mean of the mean of the mean of,"[[    0  2475    13     8   445    18  5096     7    19 11338    38     8
   1243    13     8  1243    13     8  1243    13]]",Unanswerable,[[ 597 3247 3321  179    1]]
600b097475b30480407ce1de81c28c54a0b3b2f8,0.0,general adversarial network,[[    0   879 23210    23   138  1229     1]],Unanswerable,[[ 597 3247 3321  179    1]]
5eda469a8a77f028d0c5f1acd296111085614537,8.0,Arabic$rightarrow $Spanish,[[    0 19248  3229     2  3535  6770  1514 19675  1273     1]],"French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation","[[ 2379    18 26749    18 19675  1273    41   371    52    18  8532    18
    427     7   201  2968    18 26749    18   371    60  5457    41  2962
     18  8532    18   371    52    61    11  3871    29    18 26749    18
  24518    41   448    32    18  8532    18  2962   201 19248    41   188
     52   201  5093    41   427     7   201    11  4263    41 17137   201
     11  8543  7314   344  1452 11708     7  1296  5733    18 11159  7314
      1]]"
cd2878c5a52542ddf080b20bec005d9a74f2d916,0.0,occupational class,[[    0 25082   853     1]],"technology, religion, fashion, publishing, sports or recreation, real estate, agriculture/environment, law, security/military, tourism, construction, museums or libraries, banking/investment banking, automotive","[[  748     6  5562     6  2934     6  9002     6  2100    42 17711     6
    490  2052     6 11402    87 19659   297     6   973     6  1034    87
   5952    17  1208     6  8676     6  1449     6 17385    42 12256     6
   8175    87 15601   297  8175     6 11106     1]]"
f0b2289cb887740f9255909018f400f028b1ef26,50.0,sexual harassment,[[    0  6949 23556     1]],"indirect harassment, sexual and physical harassment",[[16335 23556     6  6949    11  1722 23556     1]]
9ec1f88ceec84a10dc070ba70e90a792fba8ce71,0.0,3rd,[[  0 220  52  26   1]],0.6103,[[    3 22787 17864     1]]
61a9ea36ddc37c60d1a51dabcfff9445a2225725,0.0,WSJ news articles from major news outlets,[[    0     3  8439   683  1506  2984    45   779  1506 14290     1]],Unanswerable,[[ 597 3247 3321  179    1]]
3de0487276bb5961586acc6e9f82934ef8cb668c,0.0,"Medical records, BIBREF0",[[    0  3721  3187     6     3  5972 25582   371   632     1]],"MEDDOCAN, NUBes ",[[    3 21357  9857 11425     6 13046  2703     7     1]]
df95b3cb6aa0187655fd4856ae2b1f503d533583,0.0,n-grams,[[   0    3   29   18 5096    7    1]],"n-gram subwords, unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords ","[[    3    29    18  5096   769  6051     7     6    73 23313     3  8886
     15  2687  4313   338  4574    89 24901     3  5972 25582   371  2596
     12   669   823    72     3 24703  1427 11361   769  6051     7     1]]"
29923a824c98b3ba85ced964a0e6a2af35758abe,0.0,BLEU scores,[[    0     3  8775 12062  7586     1]],"Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets","[[23545   920 20146     7   130  1622    12   771 22770     7     5     6
    796     3     7   152   485 11642    12     8  7314     7     6     3
      7   152   485   691     8 21655     7    13  2412     6   606    11
    794  3369     1]]"
9658b5ffb5c56e5a48a3fea0342ad8fc99741908,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
fff5c24dca92bc7d5435a2600e6764f039551787,0.0,StackExchange,[[    0     3 19814  5420 13073     1]],they obtained computer science related topics by looking at titles and user-assigned tags,"[[   79  5105  1218  2056  1341  4064    57   479    44  8342    11  1139
     18     9     7 15532 12391     1]]"
08b57deb237f15061e4029b6718f1393fa26acce,0.0,political pundits of the Washington Post,[[   0 1827 4930   26 7085   13    8 2386 1844    1]],"located in the US, hired on the BIBREF22 platform","[[ 1069    16     8   837     6 10626    30     8     3  5972 25582   371
   2884  1585     1]]"
45be665a4504f0c7f458cf3f75a95d5a75eefd42,0.0,220,[[   0  204 1755    1]],Unanswerable,[[ 597 3247 3321  179    1]]
38e4aaeabf06a63a067b272f8950116733a7895c,31.5789,a tagging scheme that uses a combination of word and phonological similarity,"[[    0     3     9     3    17 15242  5336    24  2284     3     9  2711
     13  1448    11     3  9621  4478  1126   485]]","a new tagging scheme consisting of three tags, namely { INLINEFORM0 }","[[    3     9   126     3    17 15242  5336  5608    53    13   386 12391
      6     3 17332     3     2  3388 20006 24030   632     3     2     1]]"
c88a846197b72d25e04ec55f00ee3e72f655504c,0.0,Vote and votes,[[    0  3152    17    15    11 11839     1]],corpus of state speeches delivered during the annual UN General Debate,"[[11736   302    13   538 26147  3566   383     8  2041  4417  2146  9794
    342     1]]"
eda4869c67fe8bbf83db632275f053e7e0241e8c,0.0,WikiText-TL-02,[[    0  2142  2168 13598    17    18 12733    18  4305     1]],Unanswerable,[[ 597 3247 3321  179    1]]
fabcd71644bb63559d34b38d78f6ef87c256d475,0.0,"BERT-Base, X-Base, X-Base","[[    0   272 24203    18 14885    15     6     3     4    18 14885    15
      6     3     4    18 14885    15     1]]","Baseline models are:
- Chen et al., 2015a
- Chen et al., 2015b
- Liu et al., 2016
- Cai and Zhao, 2016
- Cai et al., 2017
- Zhou et al., 2017
- Ma et al., 2018
- Wang et al., 2019","[[ 8430   747  2250    33    10     3    18 15570     3    15    17   491
      5     6  1230     9     3    18 15570     3    15    17   491     5
      6  1230   115     3    18  1414    76     3    15    17   491     5
      6  1421     3    18  1336    23    11 30680     6  1421     3    18
   1336    23     3    15    17   491     5     6  1233     3    18  1027
   9492     3    15    17   491     5     6  1233     3    18  1534     3
     15    17   491     5     6   846     3    18 18102     3    15    17
    491     5     6  1360     1]]"
09a1173e971e0fcdbf2fbecb1b077158ab08f497,33.3333,0.8 accuracy points,[[    0     3 22384  7452   979     1]],5 percent points.,[[ 305 1093  979    5    1]]
4ef11518b40cc55d86c485f14e24732123b0d907,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4","[[  377  7381     6     3 22807 12623     6  9509   371    32    32    40
      3  5972 25582   371 11116  5396   371  7446     3  5972 25582   371
   5268    11     3 12016   254     3  5972 25582   371   591     1]]"
88ab7811662157680144ed3fdd00939e36552672,6.8966,Action-Space,[[    0  6776    18 24722     1]],"a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state, to leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-space","[[    3     9  1573    24  8432     7  4782 14694     7    16  1499    18
   7261     7   338     8  1879  9676  6886    11     8  1103  8373   538
      6    12 11531  1103  8373     7    12  1172  1895  9740 16783    21
   4945    28 10374  7265   138  1041    18  6633     1]]"
4e1a67f8dc68b55a5ce18e6cd385ae9ab90d891f,100.0,No,[[  0 465   1]],No,[[465   1]]
f2155dc4aeab86bf31a838c8ff388c85440fce6e,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
0689904db9b00a814e3109fb1698086370a28fa2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Document to Vector (Doc2Vec),[[11167    12 29011    41  4135    75   357   553    15    75    61     1]]
bb2de20ee5937da7e3e6230e942bec7b6e8f61ee,33.3333,The dataset is collected from various daily news sources from Nepal around the year 2015-2016,"[[    0    37 17953    19  4759    45   796  1444  1506  2836    45 17029
    300     8   215  1230    18 11505     1]]",daily newspaper of the year 2015-2016,[[ 1444  8468    13     8   215  1230    18 11505     1]]
90d946ccc3abf494890e147dd85bd489b8f3f0e8,0.0,accuracy,[[   0 7452    1]],"gender bias, normalized version of INLINEFORM0, ratio of occurrence of male and female words in the model generated text, Causal occupation bias conditioned on occupation, causal occupation bias conditioned on gender, INLINEFORM1","[[ 7285 14387     6  1389  1601   988    13  3388 20006 24030   632     6
   5688    13     3 16526    13  5069    11  3955  1234    16     8   825
   6126  1499     6 16371     7   138 13792 14387     3 19657    30 13792
      6 31161 13792 14387     3 19657    30  7285     6  3388 20006 24030
    536     1]]"
50be4a737dc0951b35d139f51075011095d77f2a,0.0,a wealth of prior knowledge about many natural language processing tasks,[[   0    3    9 5987   13 1884 1103   81  186  793 1612 3026 4145    1]],labeled features,[[3783   15   26  753    1]]
d604f5fb114169f75f9a38fab18c1e866c5ac28b,40.0,accuracy,[[   0 7452    1]],"F1, precision, recall, accuracy",[[  377  4347 11723     6  7881     6  7452     1]]
51fe4d44887c5cc5fc98b65ca4cb5876f0a56dad,0.0,NNN,[[    0   445 17235     1]],Bert + Unanswerable,[[20612  1768   597  3247  3321   179     1]]
5a81732d52f64e81f1f83e8fd3514251227efbc7,8.3333,"a Spanish dataset containing 9,473 annotations for 9,300 tweets","[[    0     3     9  5093 17953     3  6443  9902  4177   519 30729     7
     21  9902  5426 10657     7     1]]","an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13","[[   46  1895     6    46  2264   920  3046 17953    24    47  8520     3
    390    30     3     9  1382  7064  1950   825    13  7562    18  3897
   3976     3  5972 25582   371  2122     3     6     3  5972 25582   371
   2368     1]]"
5a8cc8f80509ea77d8213ed28c5ead501c68c725,0.0,Unanswerable,[[   0  597 3247 3321  179    1]]," Most prior work focuses on a different aspect of offensive language such as abusive language BIBREF0 , BIBREF1 , (cyber-)aggression BIBREF2 , (cyber-)bullying BIBREF3 , BIBREF4 , toxic comments INLINEFORM0 , hate speech BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , and offensive language BIBREF11 . Prior work has focused on these aspects of offensive language in Twitter BIBREF3 , BIBREF7 , BIBREF8 , BIBREF11 , Wikipedia comments, and Facebook posts BIBREF2 .","[[ 1377  1884   161     3  6915    30     3     9   315  2663    13 12130
   1612   224    38 27031  1612     3  5972 25582   371   632     3     6
      3  5972 25582   371   536     3     6    41    75    63  1152    18
     61     9   122 22430     3  5972 25582   371   357     3     6    41
     75    63  1152    18    61 20638  8149     3  5972 25582   371   519
      3     6     3  5972 25582   371   591     3     6 12068  2622  3388
  20006 24030   632     3     6  5591  5023     3  5972 25582   371   755
      3     6     3  5972 25582   371   948     3     6     3  5972 25582
    371   940     3     6     3  5972 25582   371   927     3     6     3
   5972 25582   371  1298     3     6     3  5972 25582   371  1714     3
      6    11 12130  1612     3  5972 25582   371  2596     3     5  6783
    161    65  2937    30   175  3149    13 12130  1612    16  3046     3
   5972 25582   371   519     3     6     3  5972 25582   371   940     3
      6     3  5972 25582   371   927     3     6     3  5972 25582   371
   2596     3     6 16885  2622     6    11  1376  3489     3  5972 25582
    371   357     3     5     1]]"
7784d321ccc64db5141113b6783e4ba92fdd4b20,0.0,LSTM-based LSTM with a corresponding syllable structure,"[[   0    3 7600 2305   18  390    3 7600 2305   28    3    9    3 9921
     3    7   63  195  179 1809]]",Unanswerable,[[ 597 3247 3321  179    1]]
b5e4866f0685299f1d7af267bbcc4afe2aab806f,9.0909,"The news articles were collected from the Armenian government website, Armenian News Corp.","[[    0    37  1506  2984   130  4759    45     8 18715    29   789   475
      6 18715    29  3529 10052     5     1]]",links between Wikipedia articles to generate sequences of named-entity annotated tokens,"[[ 2416   344 16885  2984    12  3806  5932     7    13  2650    18   295
    485    46  2264   920 14145     7     1]]"
b4f881331b975e6e4cab1868267211ed729d782d,0.0,30%,[[    0 10738     1]],"33,663 distinct review keywords ",[[ 5400     6  3539   519  6746  1132 12545     1]]
6c91d44d5334a4ac80100eead4e105d34e99a284,22.2222,"two representative model architectures: English-French, Chinese-English","[[    0   192  6978   825  4648     7    10  1566    18   371    60  5457
      3     6  2830    18 26749     1]]","Transformer, RNN-Search model",[[31220     6   391 17235    18 25001   825     1]]
ef4dba073d24042f24886580ae77add5326f2130,0.0,82.0%,[[   0    3 4613    5 6932    1]],F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain),"[[  377   536    13 11989 14990    30     8     3 10013    18  4176 17953
     41    26    23     9  2152  3303  3670  6374     5  1808    30     3
   3073    18  7323    11     3  4450     5  4867    30     3  3073    18
   1265  2247    41    15    18  7949  3303    61     1]]"
4eaf9787f51cd7cdc45eb85cf223d752328c6ee4,0.0,WSJ5 and jyoti2016,"[[    0     3  8439   683   755    11     3   354    63    32    17    23
  11505     1]]",multilingual pronunciation corpus collected by deri2016grapheme,"[[ 1249 25207 30637 11736   302  4759    57    74    23 11505  9413    15
    526     1]]"
23e16c1173b7def2c5cb56053b57047c9971e3bb,66.6667,LSTM,[[   0    3 7600 2305    1]],LSTM model,[[   3 7600 2305  825    1]]
62ea141d0fb342dfb97c69b49d1c978665b93b3c,36.3636,"erroneous text segments, grammatical errors","[[    0     3    49    52   782  1162  1499 15107     6     3  5096  4992
    138  6854     1]]","spelling, word order and grammatical errors",[[19590     6  1448   455    11     3  5096  4992   138  6854     1]]
d7b60abb0091246e29d1a9c28467de598e090c20,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"stochastic gradient descent, naive bayes, decision tree","[[13564   107 10057 26462 19991     6     3    29     9   757 10210    15
      7     6  1357  2195     1]]"
dd76130ec5fac477123fe8880472d03fbafddef6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"ELIZA,  PARRY, A.L.I.C.E., Cleverbot","[[    3 22590 19873     6 17917 11824     6    71     5   434     5   196
      5   254     5   427     5     6  4779  3258  4045     1]]"
8bf7f1f93d0a2816234d36395ab40c481be9a0e0,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
0a75a52450ed866df3a304077769e1725a995bb7,7.6923,the decoder is programmed to generate a log-rank speech using a set,"[[   0    8   20 4978   52   19 2486   26   12 3806    3    9 4303   18
  6254 5023  338    3    9  356]]","decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information","[[   20  4978    52  2491     6    84  9689     7     8  2387  5932 15834
     44    97  3388 20006 24030   519     3   390    30  1767  3911    11
   2625   251     1]]"
0810b43404686ddfe4ca84783477ae300fdd2ea4,0.0,RNN layer,[[    0   391 17235  3760     1]],Transformer over BERT (ToBERT),[[31220   147   272 24203    41  3696 12920   382    61     1]]
8db6f8714bda7f3781b4fbde5ebb3794f2a60cfe,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
cc354c952b5aaed2d4d1e932175e008ff2d801dd,0.0,Unanswerable,[[   0  597 3247 3321  179    1]]," the number of systems consistently giving higher scores to sentences with female noun phrases, higher scores to sentences with African American names on the tasks of anger, fear, and sadness,  joy and valence tasks, most submissions tended to assign higher scores to sentences with European American names","[[    8   381    13  1002  8182  1517  1146  7586    12 16513    28  3955
    150   202 15101     6  1146  7586    12 16513    28  3850   797  3056
     30     8  4145    13 11213     6  2971     6    11 24784     6  3922
     11     3  2165  1433  4145     6   167  8121     7     3 15443    12
  12317  1146  7586    12 16513    28  1611   797  3056     1]]"
a66a275a817f980c36e0b67d2e00bd823f63abf8,0.0,accuracy,[[   0 7452    1]],Unanswerable,[[ 597 3247 3321  179    1]]
d509081673f5667060400eb325a8050fa5db7cc8,0.0,"38,432",[[   0 6654    6  591 2668    1]],"2174000000, 989000000",[[ 1401  4581  2313  2313     6     3  3916  7015 19568     1]]
932b39fd6c47c6a880621a62e6a978491d881d60,0.0,LSTM+En-Fr and TL-En-Fr,"[[    0     3  7600  2305  1220  8532    18   371    52    11     3 12733
     18  8532    18   371    52     1]]",TransE,[[4946  427    1]]
dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"support vector machine BIBREF18 , random forest, extra tree and naive bayes classifier BIBREF19","[[  380 12938  1437     3  5972 25582   371  2606     3     6  6504  5827
      6   996  2195    11     3    29     9   757 10210    15     7   853
   7903     3  5972 25582   371  2294     1]]"
78292bc57ee68fdb93ed45430d80acca25a9e916,19.3548,They extend LAMA evaluation framework to focus on negation,"[[    0   328  4285   301 21250  5002  4732    12   992    30 14261   257
      1]]","To this end, we introduce the negated LAMA dataset. We construct it by simply inserting negation elements (e.g., â€œnotâ€) in LAMA cloze statement","[[  304    48   414     6    62  4277     8 14261   920   301 21250 17953
      5   101  6774    34    57   914  8722    53 14261   257  2479    41
     15     5   122     5     6   105  2264  7058    16   301 21250     3
   3903   776  2493     1]]"
afb77b11da41cd0edcaa496d3f634d18e48d7168,0.0,"LSTM with attention, LSTM with attention, LSTM with attention","[[   0    3 7600 2305   28 1388    6    3 7600 2305   28 1388    6    3
  7600 2305   28 1388    1]]","BERT based fine-tuning, Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer","[[  272 24203     3   390  1399    18    17   202    53     6 28953   529
    747   291  7500     6 28953  2106    18  7600  2305  3760     6 28953
  19602  3760     1]]"
37edc25e39515ffc2d92115d2fcd9e6ceb18898b,0.0,a five-point scale ranging from VeryNegative to VeryPos,"[[   0    3    9  874   18 2700 2643    3 6836   45 4242  567   15  122
  1528   12 4242  345   32    7]]","SVMs, LR, BIBREF2","[[  180 12623     7     6     3 12564     6     3  5972 25582   371   357
      1]]"
126ff22bfcc14a2f7e1a06a91ba7b646003e9cf0,0.0,BLEU score,[[    0     3  8775 12062  2604     1]],"Transformer base, two-pass CADec model",[[31220  1247     6   192    18  3968     3 12926    15    75   825     1]]
7d483077ed7f2f504d59f4fc2f162741fa5ac23b,0.0,seasonal variations in the patterns of co-purchase of items can occur across different seasons of the year,"[[    0 10852 10914    16     8  4264    13   576    18 29446    13  1173
     54  4093   640   315  9385    13     8   215]]",Unanswerable,[[ 597 3247 3321  179    1]]
fd0a3e9c210163a55d3ed791e95ae3875184b8f8,0.0,WSJ 2013,[[   0    3 8439  683 2038    1]],"WSJ-SI84, WSJ-SI284","[[   3 8439  683   18  134  196 4608    6    3 8439  683   18  134  196
   357 4608    1]]"
792f6d76d2befba2af07198584aac1b189583ae4,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Hashtag prediction for social media has been addressed earlier,[[ 4498   107  2408 21332    21   569   783    65   118  8705  2283     1]]
9cf070d6671ee4a6353f79a165aa648309e01295,50.0,"1,200 sentences",[[    0  1914  3632 16513     1]],1500 sentences,[[15011 16513     1]]
a5e49cdb91d9fd0ca625cc1ede236d3d4672403c,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
861187338c5ad445b9acddba8f2c7688785667b1,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
660284b0a21fe3801e64dc9e0e51da5400223fe3,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],GM$\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset.,"[[    3  7381  3229     2   834  3229   439   434  1984     7   394 18712
    145  1895  6315    21   796 15905    30  6508  8439 17953     5     1]]"
ef396a34436072cb3c40b0c9bc9179fee4a168ae,20.0,"Noun-Noun Matching, Adverb-Noun Matching","[[    0   465   202    18  4168   202 12296    53     6  1980 11868    18
   4168   202 12296    53     1]]",text classification and text semantic matching,[[ 1499 13774    11  1499 27632  8150     1]]
3116453e35352a3a90ee5b12246dc7f2e60cfc59,8.3333,"LSTM BIBREF13, LSTM BIBREF18","[[    0     3  7600  2305     3  5972 25582   371  2368     6     3  7600
   2305     3  5972 25582   371  2606     1]]","support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self","[[  380 12938  1437   853  7903    41   134 12623   201 28820 26625   853
   7903    41 12564   201  1823   757  2474    15     7   853  7903    41
  14972   201  6504  5827    41  8556   201 19602     6     3  7600  2305
      3     6     3  7600  2305    18 12369     6     3  7600  2305    18
   7703     1]]"
37861be6aecd9242c4fdccdfcd06e48f3f1f8f81,14.2857,"On the assisting-target language pair (child task) there are 57,813","[[    0   461     8     3 16881    18 24315  1612  3116    41 11495  2491
     61   132    33     3  3436     6   927  2368]]","Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks.","[[20008    23     6 30812    23     6  1571     9  7436     6   283 22858
     40   265    11 22503    33     8  2329  1391  8024     6    11  7314
     45   175    12 25763 11708     8   861  4145     5     1]]"
d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751,0.0,"toxic comments, hate speech, profanity, insult, or abuse","[[    0 12068  2622     6  5591  5023     6  7108   152   485     6 21548
      6    42  5384     1]]","offensive (OFF) and non-offensive (NOT), targeted (TIN) and untargeted (INT) insults, targets of insults and threats as individual (IND), group (GRP), and other (OTH)","[[12130    41 15316    61    11   529    18   858 23039    15    41  7400
    382   201  7774    41 25424    61    11    73 24315    15    26    41
  13777    61 21548     7     6  8874    13 21548     7    11 11262    38
    928    41 13885   201   563    41  8727   345   201    11   119    41
    667  4611    61     1]]"
46c9e5f335b2927db995a55a18b7c7621fd3d051,0.0,57,[[   0    3 3436    1]],15 clinical patient phenotypes,[[  627  3739  1868     3 19017    32  6137     7     1]]
c9bc6f53b941863e801280343afa14248521ce43,100.0,English,[[   0 1566    1]],English,[[1566    1]]
ba28ce9a2f7e8524243adf288cc3f11055e667bb,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
75df70ce7aa714ec4c6456d0c51f82a16227f2cb,23.5294,"Hindi, English, Tamil, Punjabi",[[    0 25763     6  1566     6 22503     6 27864    23     1]],"Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)","[[ 4540 18089     6 25763     6  2255  8076    76     6   283 22858    40
    265     6 20008    23     6  1566    11   282     7     9  2687    15
     41    77   953     6  3586    16  1499    61     1]]"
1269c5d8f61e821ee0029080c5ba2500421d5fa6,0.0,LSTMs,[[   0    3 7600 2305    7    1]],"Back Translation, Mix-Source Approach",[[ 3195 24527     6  7382    18 23799 24500     1]]
8dc707a0daf7bff61a97d9d854283e65c0c85064,100.0,No,[[  0 465   1]],No,[[465   1]]
3e1829e96c968cbd8ad8e9ce850e3a92a76b26e4,0.0,"70,000",[[    0     3 28891     1]],212 accounts,[[    3 24837  3744     1]]
35b3ce3a7499070e9b280f52e2cb0c29b0745380,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
784ce5a983c5f2cc95a2c60ce66f2a8a50f3636f,100.0,No,[[  0 465   1]],No,[[465   1]]
b2ecfd5480a2a4be98730e2d646dfb84daedab17,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
bf3b27a4f4be1f9ae31319877fd0c75c03126fd5,0.0,62,[[   0    3 4056    1]],Unanswerable,[[ 597 3247 3321  179    1]]
efe9bad55107a6be7704ed97ecce948a8ca7b1d2,0.0,BERT compression techniques: BERTBASE quantization and XLNet compression techniques:,"[[    0   272 24203 16685  2097    10     3 12920  9041 17892 13500  1707
     11     3     4   434  9688 16685  2097    10]]","baseline without knowledge distillation (termed NoKD), Patient Knowledge Distillation (PKD)","[[20726   406  1103 20487   257    41   449  2726   465   439   308   201
  17656 16113  2043 11656   257    41 16782   308    61     1]]"
7e62a53823aba08bc26b2812db016f5ce6159565,20.0,Parallel English-Japanese,[[    0 27535  1566    18   683  9750  1496    15     1]],"IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus","[[ 2466  9041  1566    18   566  8482  8449 11736   302     6     3  3502
   3597  1566    18   566  8482  8449 11736   302     1]]"
8dd8e5599fc56562f2acbc16dd8544689cddd938,19.0476,they are defined as singletons in the context of other words,[[   0   79   33 4802   38  712 8057   16    8 2625   13  119 1234    1]],By using Euclidean distance computed between the context vector representations of the equations,"[[  938   338  4491 14758   221   152  2357 29216    26   344     8  2625
  12938  6497     7    13     8 13850     7     1]]"
a87a009c242d57c51fc94fe312af5e02070f898b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features.","[[28820 26625  2250     3   390    30    73    23  5096  2182    18   858
     18  6051     7   753    41   279 15251   201  6493  9650    41   134
   6431   201     8     3 24703   753    45    69  2283 15282    41   434
   2365   201    11 15617    13   175   753     5     1]]"
b1a068c1050e2bed12d5c9550c73e59cd5b1f78d,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
8910ee2236a497c92324bbbc77c596dba39efe46,12.5,"spelling correction, word piece, rewording, and word fragmentation","[[    0 19590 11698     6  1448  1466     6     3    60  6051    53     6
     11  1448 12071   257     1]]",Sentiment analysis and paraphrase detection under adversarial attacks,"[[ 4892  2998   295  1693    11  3856 27111 10664   365 23210    23   138
   6032     1]]"
ac7f6497be4bcca64e75f28934b207c9e8097576,8.0,sentences that are based on a particular category or category of text,"[[    0 16513    24    33     3   390    30     3     9  1090  3295    42
   3295    13  1499     1]]","seven of the originally defined relation types: political_affiliation, education, founder, wife/husband, job_title, nationality, and employer","[[ 2391    13     8  5330  4802  4689  1308    10  1827   834  4127   173
     23   257     6  1073     6  7174     6  2512    87 11823  3348     6
    613   834 21869     6  1157   485     6    11  6152     1]]"
efe9bad55107a6be7704ed97ecce948a8ca7b1d2,14.2857,BERT compression techniques: BERTBASE quantization and XLNet compression techniques:,"[[    0   272 24203 16685  2097    10     3 12920  9041 17892 13500  1707
     11     3     4   434  9688 16685  2097    10]]","NoKD, PKD, BERTBASE teacher model","[[  465   439   308     6     3 16782   308     6     3 12920  9041 17892
   3145   825     1]]"
ce2b921e4442a21555d65d8ce4ef7e3bde931dfc,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)","[[ 2379    41    89    52   201  4263    41    52    76   201 19248    41
    291   201  2830    41   172   107   201 25763    41   107    23   201
     11 24532    41  2099    61     1]]"
0602a974a879e6eae223cdf048410b5a0111665e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"K-means, LEM, DPEMM",[[ 480   18  526 3247    6  301 6037    6  309 5668 8257    1]]
77c34f1033702278f7f044806c1eba0c6ecb8b04,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
8ce11515634236165cdb06ba80b9a36a8b9099a2,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
52f9cd05d8312ae3c7a43689804bac63f7cac34b,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
a103636c8d1dbfa53341133aeb751ffec269415c,0.0,"BIBREF13, BIBREF18","[[    0     3  5972 25582   371  2368     6     3  5972 25582   371  2606
      1]]","majority baseline, lexicon-based approach",[[ 2942 20726     6     3 30949   106    18   390  1295     1]]
2c7e94a65f5f532aa31d3e538dcab0468a43b264,0.0,Exchanges between a user and a task-oriented dialog system,"[[    0  8231     7   344     3     9  1139    11     3     9  2491    18
   9442 13463   358     1]]",manually ,[[12616     1]]
ef4dba073d24042f24886580ae77add5326f2130,0.0,82.0%,[[   0    3 4613    5 6932    1]],"F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ","[[  377   536  7586    13 11989 14990    30     8     3 10013    18  4176
    331     6  6374     5  1808    30     8     3  3073    18  7323   331
     11     3  4450     5  4867    30     8     3  3073    18  1265  2247
    331     1]]"
5c6fa86757410aee6f5a0762328637de03a569e9,29.6296,F1 score of 0.91 for fat' and 0.91 for woman',"[[    0   377   536  2604    13  4097  4729    21     3     2  6589    31
     11  4097  4729    21     3     2 15893    31]]",best model achieves 0.94 F1 score for Wikipedia and Twitter datasets and 0.95 F1 on Formspring dataset,"[[  200   825  1984     7  4097  4240   377   536  2604    21 16885    11
   3046 17953     7    11  4097  3301   377   536    30  3025 14662 17953
      1]]"
4d47bef19afd70c10bbceafd1846516546641a2f,42.1053,"sequence to sequence model, LSTM",[[   0 5932   12 5932  825    6    3 7600 2305    1]],"bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder","[[ 2647    18 26352  1612   825    12 15189     8  5932    12  5932 23734
     52     3     6    73    23    18 26352   825    12 15189     8    20
   4978    52     1]]"
beac555c4aea76c88f19db7cc901fa638765c250,8.3333,"relevance of the attended parts, especially cases where attention is smeared out'","[[    0 20208    13     8  5526  1467     6   902  1488   213  1388    19
      3     2     7   526     9  1271    91    31]]",it captures other information rather than only the translational equivalent in the case of verbs,"[[  34 4105    7  119  251 1066  145  163    8 7314  138 7072   16    8
   495   13 7375    7    1]]"
2236386729105f5cf42f73cc055ce3acdea2d452,0.0,English,[[   0 1566    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5152b78f5dfee26f1b13f221c1405ffa9b9ba3a4,0.0,F1 score of 82.56,[[   0  377  536 2604   13    3 4613    5 4834    1]],Unanswerable,[[ 597 3247 3321  179    1]]
483a699563efcb8804e1861b18809279f21c7610,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
7b4992e2d26577246a16ac0d1efc995ab4695d24,0.0,CoNLL 2014,[[    0   638   567 10376  1412     1]],error detection system by Rei2016,[[ 3505 10664   358    57   419    23 11505     1]]
c81f215d457bdb913a5bade2b4283f19c4ee826c,0.0,WikiText-TL-201,[[    0  2142  2168 13598    17    18 12733    18 22772     1]],"Waseem-dataset, Davidson-dataset,","[[ 2751    15    15    51    18  6757  2244     6     3 23268    18  6757
   2244     6     1]]"
fc1679c714eab822431bbe96f0e9cf4079cd8b8d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers","[[    3  3390     5  5988    30     3 29474 17953     6     3  4271     5
   5988    30 11409    18    60  4931 13269  7833  5778     6     3  4013
      5  4704    30 11409    18    60  4931 13269   638 31148    11 10509
   5778     6    11     3  4440     5  7561    30 11409    18    60  4931
  13269  5879  6630  5778     1]]"
ba539cab80d25c3e20f39644415ed48b9e4e4185,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.","[[ 3424    18 11258  9016     8  3673  3041 14528  1448    38    19     6
    223  1647    12  7163  1448   223     7   326    12     3     9  1448
     28  1126  3438   640  2287    11   223  1647    12  2458   825   223
      7   326    12     3     9    72  8165  1448  5786   825  4252    28
   2186    11   705     3  8689 11736   302     5     1]]"
1097768b89f8bd28d6ef6443c94feb04c1a1318e,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
2cd37743bcc7ea3bd405ce6d91e79e5339d7642e,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
41b70699514703820435b00efbc3aac4dd67560a,66.6667,"divorce, custody",[[    0  7759     6 16701     1]],divorce ,[[7759    1]]
585626d18a20d304ae7df228c2128da542d248ff,0.0,accuracy,[[   0 7452    1]],"strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score","[[ 1998 20029  1418     6    62  4277     3     9  3613   718  5620   545
    599  3388 20006 24030   632     3   201   304  6825     8 27875   821
      6    62   169    71   208   122     5   283  2823    11     3     9
    208   122     5  1768   162   377   536  2604     1]]"
2c78993524ca62bf1f525b60f2220a374d0e3535,0.0,"scientific articles, online reviews, forum posts, blogs, microblogs","[[    0  4290  2984     6   367  2456     6  5130  3489     6  9407     6
   2179 12139     7     1]]","rupnik2016news, Deutsche Welle's news website","[[    3    52   413  4953 11505 15808     6 15024  1548    15    31     7
   1506   475     1]]"
c74185bced810449c5f438f11ed6a578d1e359b4,5.6338,"labels for antisocial events include hate speech, harassment, personal attacks, toxicity, and ","[[    0 11241    21  1181 15745   984   560  5591  5023     6 23556     6
    525  6032     6     3 27147     6    11     3]]","The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: ""Don't be rude or hostile to others users.""","[[   37 28941     7   350   782    71   210   651 17953    19     3 29506
     38   893     3  6443     3     9   525  3211    45   441    17    41
     23     5    15     5 24550  3889    57    80  1139    16     8  3634
   6640  1587   430    61    42  4080  3095  1019     5    37  1624    26
    155  5968   499  4197 17953    19     3 29506    28   823    42    59
      3     9  3792   257  3725   141     3     9  1670  3641    57     3
      9  1794    49  1016    21 12374    13 17104   204    10    96 13843
     31    17    36 19986    42 24550    12   717  1105   535     1]]"
06be47e2f50b902b05ebf1ff1c66051925f5c247,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
545e92833b0ad4ba32eac5997edecf97a366a244,0.0,word co-occurrence networks,[[    0  1448   576    18 16526  5275     1]],Increasing number of message passing iterations showed consistent improvement in performance - around 1 point improvement compared between 1 and 4 iterations,"[[    3 31372   381    13  1569  5792    34    49  1628  3217  4700  4179
     16   821     3    18   300   209   500  4179     3  2172   344   209
     11   314    34    49  1628     1]]"
565189b672efee01d22f4fc6b73cd5287b2ee72c,0.0,WN15K,[[    0     3 21170  1808   439     1]],"Europarl tests from 2006, 2007, 2008; WMT newstest 2014.","[[ 5578    52    40  3830    45  3581     6 11979  2628   117   549  7323
   1506  4377  5832     1]]"
76121e359dfe3f16c2a352bd35f28005f2a40da3,22.2222,"classification, classification of news articles, sentiment classification",[[    0 13774     6 13774    13  1506  2984     6  6493 13774     1]],"text classification for themes including sentiment, web-page, science, medical and healthcare","[[ 1499 13774    21  8334   379  6493     6   765    18  6492     6  2056
      6  1035    11  4640     1]]"
27275fe9f6a9004639f9ac33c3a5767fea388a98,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Hyperparameters explored were: dimension size, window size, architecture, algorithm and epochs.","[[15889  6583  4401     7 15883   130    10  9340   812     6  2034   812
      6  4648     6 12628    11     3    15   102  6322     7     5     1]]"
27cf16bc9ef71761b9df6217f00f39f21130ce15,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
e587559f5ab6e42f7d981372ee34aebdc92b646e,0.0,"Result: 62.0%, 62.0%, 63.0%","[[    0     3 20119    10     3  4056     5  6932     3     6     3  4056
      5  6932     6     3  3891     5  6932     1]]","In case of read speech datasets,  their best model got the highest nov93 score of 16.1 and the highest nov92 score of 13.3.
In case of Conversational Speech, their best model got the highest SWB of 8.3 and the highest CHM of 19.3. ","[[   86   495    13   608  5023 17953     7     6    70   200   825   530
      8  2030     3  5326  4271  2604    13 10128   536    11     8  2030
      3  5326  4508  2604    13  8808  5787    86   495    13 28941   138
  26351     6    70   200   825   530     8  2030 12222   279    13  4848
    519    11     8  2030  9302   329    13  9997  5787     1]]"
0c234db3b380c27c4c70579a5d6948e1e3b24ff1,100.0,LSTM,[[   0    3 7600 2305    1]],LSTM,[[   3 7600 2305    1]]
2ea4347f1992b0b3958c4844681ff0fe4d0dd1dd,16.6667,"module embedding, CNN/RNN, Transformer","[[    0  6008 25078    26    53     6 19602    87 14151   567     6 31220
      1]]","Embedding Layer, Neural Network Layers, Loss Function, Metrics","[[    3 17467    15  7249 22697     6  1484  9709  3426 22697     7     6
   3144     7 21839     6  1212  3929     7     1]]"
ee2c9bc24d70daa0c87e38e0558e09ab97feb4f2,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
133eb4aa4394758be5f41744c60c99901b2bc01c,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
12f7fac818f0006cf33269c9eafd41bbb8979a48,5.7143,biLSTM,[[   0 2647 7600 2305    1]],"visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models","[[ 3176   825    19     3   390    30  1399    18    17   202    53    46
     86  7239   584   519   825     3  5972 25582   371   536   147  3176
  18968     7    13  2691     6   298    69  1499  3471   825    19     3
    390    30     3     9  1382  7064  1950  2647  7600  2305     5   101
    856  5148     8   192   139     3     9  4494   825     5     3     6
  24228  1229  2250     1]]"
3355918bbdccac644afe441f085d0ffbbad565d7,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"(+1 or -1), words of opposite polarities (e.g. â€œhappy"" and â€œunhappy"") get far away from each other","[[   41 18446    42     3  2292   201  1234    13  6401     3  9618  2197
     41    15     5   122     5   105  1024  9632   121    11   105   202
   1024  9632  8512   129   623   550    45   284   119     1]]"
206739417251064b910ae9e5ff096e867ee10fb8,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
b5e4866f0685299f1d7af267bbcc4afe2aab806f,0.0,"The news articles were collected from the Armenian government website, Armenian News Corp.","[[    0    37  1506  2984   130  4759    45     8 18715    29   789   475
      6 18715    29  3529 10052     5     1]]",ilur.am,[[  3 173 450   5 265   1]]
3cf1edfa6d53a236cf4258afd87c87c0a477e243,100.0,English,[[   0 1566    1]],English,[[1566    1]]
692c9c5d9ff9cd3e0ce8b5e4fa68dda9bd23dec1,100.0,"20,000",[[    0     3 13922     1]],"20,000",[[    3 13922     1]]
aa7d327ef98f9f9847b447d4def04889b4508d7a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset","[[ 1914  9295    18  5842    41  3388 20006 24030   357     3 11039   329
  10316    61    73  9339   400    26 17953     1]]"
85590bb26fed01a802241bc537d85ba5ef1c6dc2,7.2727,"they are controlled by a set of annotators, they are given annotated annotation","[[    0    79    33  6478    57     3     9   356    13    46  2264  6230
      6    79    33   787    46  2264   920 30729]]","Choice-Only model, which is a variant of the well-known hypothesis-only baseline, Choice-to-choice model, tries to single out a given answer choice relative to other choices, Question-to-choice model, in contrast, uses the contextual representations for each question and individual choice and an attention model Att model to get a score","[[13745    18  7638   120   825     6    84    19     3     9  6826    13
      8   168    18  5661 22455    18  9926 20726     6 13745    18   235
     18  3995   867   825     6     3  9000    12   712    91     3     9
    787  1525  1160  5237    12   119  3703     6 11860    18   235    18
   3995   867   825     6    16  4656     6  2284     8 28131  6497     7
     21   284   822    11   928  1160    11    46  1388   825   486    17
    825    12   129     3     9  2604     1]]"
cfbec1ef032ac968560a7c76dec70faf1269b27c,0.0,tuples,[[  0   3  17 413 965   1]],Knowledge Base Question Answering,[[16113  8430 11860 11801    53     1]]
45a5961a4e1d1c22874c4918e5c98bd3c0a670b3,0.0,2 types,[[   0  204 1308    1]],7,[[489   1]]
2c6b50877133a499502feb79a682f4023ddab63e,0.0,Chinese,[[   0 2830    1]],English,[[1566    1]]
78a4ec72d76f0a736a4a01369a42b092922203b6,40.0,"EmotionLines, FriendsBERT, ChatBERT","[[    0   262  7259 21022     7     6  9779 12920   382     6  9802 12920
    382     1]]",EmotionLines BIBREF6,[[  262  7259 21022     7     3  5972 25582   371   948     1]]
5c88d601e8fca96bffebfa9ef22331ecf31c6d75,100.0,No,[[  0 465   1]],No,[[465   1]]
c77359fb9d3ef96965a9af0396b101f82a0a9de6,0.0,they obtain the interactions via email and phone conversations via text message exchanges,"[[   0   79 3442    8 9944 1009  791   11  951 9029 1009 1499 1569 2509
     7    1]]",from Food.com,[[  45 3139    5  287    1]]
1e4dbfc556cf237accb8b370de2f164fa723687b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set","[[ 1348   775 20099     6 11485     8  1750    13 20099   344    69  4382
   2250     6    62   504    46   677  3934    45     8   480   345  1755
    157 16148   356     1]]"
fe2666ace293b4bfac3182db6d0c6f03ea799277,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"to acquire very large Vietnamese corpus and to use them in building a classifier,  design and development of big data warehouse and analytic framework for Vietnamese documents, to building a system, which is able to incrementally learn new corpora and interactively process feedback","[[   12  7464   182   508 24532 11736   302    11    12   169   135    16
    740     3     9   853  7903     6   408    11   606    13   600   331
  11625    11    46     9 14991  4732    21 24532  2691     6    12   740
      3     9   358     6    84    19     3   179    12 28351   120   669
    126 11736   127     9    11  6076   120   433  3160     1]]"
aefa333b2cf0a4000cd40566149816f5b36135e7,0.0,accuracy,[[   0 7452    1]],ratio of correct `translations',[[5688   13 2024    3    2 7031 6105    7   31    1]]
9ecde59ffab3c57ec54591c3c7826a9188b2b270,0.0,"standard SQuAD, standard WSJ5, standard WSJ10","[[    0  1068   180  5991  6762     6  1068     3  8439   683 11116  1068
      3  8439   683  1714     1]]","MSMARCO,  HOTPOTQA, RECORD,  MULTIRC, NEWSQA, and DROP.","[[ 5266 13845  5911     6 27739   345  6951 23008     6     3 20921 18400
      6   283  4254  5494  4902     6  8747   134 23008     6    11     3
   3913  4652     5     1]]"
55139fcfe04ce90aad407e2e5a0067a45f31e07e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Google translation API,[[1163 7314 6429    1]]
5f60defb546f35d25a094ff34781cddd4119e400,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Using INLINEFORM0 and INLINEFORM1,[[    3  3626  3388 20006 24030   632    11  3388 20006 24030   536     1]]
2e89ebd2e4008c67bb2413699589ee55f59c4f36,0.0,using a modified version of BERT,[[    0   338     3     9  8473   988    13   272 24203     1]],Unanswerable,[[ 597 3247 3321  179    1]]
feafcc1c4026d7f55a2c8ce7850d7e12030b5c22,0.0,Yes,[[   0 2163    1]],the model can be trained end-to-end,[[   8  825   54   36 4252  414   18  235   18  989    1]]
b1cf5739467ba90059add58d11b73d075a11ec86,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
b6ae8e10c6a0d34c834f18f66ab730b670fb528c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. ","[[6525    6  268    6 2056    6   11 8366 1649   26   26  155    6   11
  5580 1151 3489   45    8 1624   26  155  851 6492    5    1]]"
021bfb7e180d67112b74f05ecb3fa13acc036c86,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Zero Resource Toolkit (ZRTools) BIBREF7,"[[17971 16154 11640  9229    41   956   448 23527     7    61     3  5972
  25582   371   940     1]]"
a1dac888f63c9efaf159d9bdfde7c938636f07b1,0.0,dataset from the Greenland Institute for Environmental Research (GREENL) BIBREF,"[[    0 17953    45     8  1862    40   232  2548    21  9185  2200    41
   8727 23394   434    61     3  5972 25582   371]]", the same datasets as BIBREF7,[[    8   337 17953     7    38     3  5972 25582   371   940     1]]
3f0ae9b772eeddfbfd239b7e3196dc6dfa21365f,0.0,"reward for generating an ironic sentence, which is inverted from the intended meaning of the","[[    0  9676    21     3 11600    46  3575   447  7142     6    84    19
     16 19825    45     8  3855  2530    13     8]]", irony accuracy and sentiment preservation,[[ 3575    63  7452    11  6493 19368     1]]
4dcf67b5e7bd1422e7e70c657f6eacccd8de06d3,0.0,two,[[  0 192   1]],16,[[898   1]]
3d7a982c718ea6bc7e770d8c5da564fbb9d11951,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],by learning a projection function between the document-event distribution and four event related word distributions ,"[[   57  1036     3     9 13440  1681   344     8  1708    18    15  2169
   3438    11   662   605  1341  1448  3438     7     1]]"
a6d37b5975050da0b1959232ae756fc09e5f87e8,0.0,BIBREF13,[[    0     3  5972 25582   371  2368     1]],"a simple word-level encoder, with the input as words instead of characters","[[    3     9   650  1448    18  4563 23734    52     6    28     8  3785
     38  1234  1446    13  2850     1]]"
bd6dc38a9ac8d329114172194b0820766458dacc,0.0,"BIBREF13, BIBREF18","[[    0     3  5972 25582   371  2368     6     3  5972 25582   371  2606
      1]]","Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.
Select:
- ERP data collected and computed by Frank et al. (2015)
- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)","[[11801    28   738  3586    10    41 20754   109  7717    11 12772  6795
     61    37  2329 17953    62   169    19     8 22568   331  4759    11
  29216    26    57  4937     3    15    17   491     5 25695     6    11
     62    92   169 17340   331    41 13370    18    17 22499   331    11
   1044    18 18789  1183   648    61    45  4937     3    15    17   491
      5 25195    84   130  4759    30     8   337   356    13     3 23201
  16513     5  6185    10     3    18 22568   331  4759    11 29216    26
     57  4937     3    15    17   491     5 25695     3    18 17340   331
     41 13370    18    17 22499   331    11  1044    18 18789  1183   648
     61    45  4937     3    15    17   491     5 25195     1]]"
a9337636b52de375c852682a2561af2c1db5ec63,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
2a6003a74d051d0ebbe62e8883533a5f5e55078b,40.0,neural embedding model,[[    0 24228 25078    26    53   825     1]],the CRX model,[[   8    3 4545    4  825    1]]
8e44c02c2d9fa56fb74ace35ee70a5add50b52ae,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751,22.2222,"toxic comments, hate speech, profanity, insult, or abuse","[[    0 12068  2622     6  5591  5023     6  7108   152   485     6 21548
      6    42  5384     1]]","non-targeted profanity and swearing, targeted insults such as cyberbullying, offensive content related to ethnicity, gender or sexual orientation, political affiliation, religious belief, and anything belonging to hate speech","[[  529    18 24315    15    26  7108   152   485    11 23782    53     6
   7774 21548     7   224    38  9738 20638  8149     6 12130   738  1341
     12 11655   485     6  7285    42  6949 12602     6  1827 24405     6
   4761  7750     6    11   959 12770    12  5591  5023     1]]"
893ec40b678a72760b6802f6abf73b8f487ae639,14.2857,The model does not capture any biases in data annotation and collection.,"[[    0    37   825   405    59  4105   136 14387    15     7    16   331
  30729    11  1232     5     1]]",The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate,"[[   37  5921  3217   360 10657     7   213  7598    11 21773 27983   738
   3223    68     8   825    47     3   179    12  9192   342     1]]"
9b7655d39c7a19a23eb8944568eb5618042b9026,0.0,"NLTK, Stanford CoreNLP, TwitterNLP","[[    0     3 18207 22110     6 19796  9020   567  6892     6  3046   567
   6892     1]]","BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26","[[    3  5972 25582   371  2773     6     3  5972 25582   371  2517     6
      3  5972 25582   371  2606     6     3  5972 25582   371  2294     6
      3  5972 25582   371  2266     6     3  5972 25582   371  1828     6
      3  5972 25582   371  2688     1]]"
1b23c4535a6c10eb70bbc95313c465e4a547db5e,9.8361,"LVCSR architectures are composed of encoder, decoder, and decoder","[[    0   301  7431  6857  4648     7    33 10431    13 23734    52     6
     20  4978    52     6    11    20  4978    52]]","we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part, On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP)","[[   62  6774    46 23734    52    28   633   975 24817   138  7500     3
   5972 25582   371  2534  2348    57   445  3162  7500     3  5972 25582
    371  1808    38     8  1364   294    16     8 23734    52    11  9162
    135    28  1659  2647 26352   307   710    18  1987  2594    41   279
     23    18  7600  2305    61     3  5972 25582   371  2938    44     8
   1146   294     6   461     8    20  4978    52   596     6    62   169
      3     9  1068  1659    73    23 26352     3  7600  2305    28  1252
   1388     3  5972 25582   371  2368    24    19 11338    57     3     9
   1249    18 18270   399  6873    52   106    41   329  6892    61     1]]"
7561a968470a8936d10e1ba722d2f38b5a9a4d38,70.5882,30K images with 5 crowdsourced descriptions each,[[    0   604   439  1383    28   305  4374 15551 15293   284     1]],"collection of over 30,000 images with 5 crowdsourced descriptions each","[[ 1232    13   147     3 17093  1383    28   305  4374 15551 15293   284
      1]]"
f94b53db307685d572aefad52cd55f53d23769c2,7.4074,by calculating variance from the outputs,[[    0    57     3 25956 27154    45     8  3911     7     1]],"reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3","[[    3  5503     8 27154    13    46 12182  1016     6   262 13011  1573
     16     3  5972 25582   371   519    19   966     8   337    38   262
   1824     5    41     3 23346  4386   371   927     3   201  3578     8
  26462    31     7  7982    19    59  2812    26    16     3  5972 25582
    371   519     1]]"
1269c5d8f61e821ee0029080c5ba2500421d5fa6,0.0,LSTMs,[[   0    3 7600 2305    7    1]],data augmentation,[[  331     3 19260     1]]
b06512c17d99f9339ffdab12cedbc63501ff527e,100.0,No,[[  0 465   1]],No,[[465   1]]
78a4ec72d76f0a736a4a01369a42b092922203b6,0.0,"EmotionLines, FriendsBERT, ChatBERT","[[    0   262  7259 21022     7     6  9779 12920   382     6  9802 12920
    382     1]]","Friends, EmotionPush",[[9779    6  262 7259  345 8489    1]]
56b034c303983b2e276ed6518d6b080f7b8abe6a,0.0,"Wuhan University, Xinhua University, Xinhua University","[[    0 17792  2618   636     6     3     4    77   107    76     9   636
      6     3     4    77   107    76     9   636]]","FSD dataset, Twitter dataset, Google dataset",[[  377  7331 17953     6  3046 17953     6  1163 17953     1]]
2ccc26e11df4eb26fcccdd1f446dc749aff5d572,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
a3d9b101765048f4b61cbd3eaa2439582ebb5c77,26.6667,"English, Chinese",[[   0 1566    6 2830    1]],"En-Fr, En-Zh, En-Jp, En-Kr, Zh-En, Zh-Fr, Zh-Jp, Zh-Kr to English, Chinese or Korean","[[ 695   18  371   52    6  695   18  956  107    6  695   18  683  102
     6  695   18  439   52    6 1027  107   18 8532    6 1027  107   18
   371   52    6 1027  107   18  683  102    6 1027  107   18  439   52
    12 1566    6 2830   42 9677    1]]"
007b13f05d234d37966d1aa7d85b5fd78564ff45,0.0,No,[[  0 465   1]],Unanswerable,[[ 597 3247 3321  179    1]]
4d706ce5bde82caf40241f5b78338ea5ee5eb01e,16.0,Cwrs for a linguistic task,[[    0   205   210    52     7    21     3     9     3 24703  2491     1]],Probes are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases.,"[[22677     7    33 13080  2250  4252    30 10451     3    75   210    52
      7    12   143 20099    81     3 24703    41     7    63    29    17
   2708   447    11 27632    61  2605    13  1234    11 15101     5     1]]"
bfc2dc913e7b78f3bd45e5449d71383d0aa4a890,5.8824,general knowledge learning engine (GKD),[[   0  879 1103 1036 1948   41  517  439  308   61    1]],"Answer with content missing: (list)
LiLi should have the following capabilities:
1. to formulate an inference strategy for a given query that embeds processing and interactive actions.
2. to learn interaction behaviors (deciding what to ask and when to ask the user).
3. to leverage the acquired knowledge in the current and future inference process.
4. to perform 1, 2 and 3 in a lifelong manner for continuous knowledge learning.","[[11801    28   738  3586    10    41  3350    61  1414   434    23   225
     43     8   826  5644    10  1300    12 28156    46    16 11788  1998
     21     3     9   787 11417    24 25078     7  3026    11  6076  2874
      5  1682    12   669  6565 15400    41 12053   125    12   987    11
    116    12   987     8  1139   137  1877    12 11531     8  7347  1103
     16     8   750    11   647    16 11788   433     5  2853    12  1912
   1914   204    11   220    16     3     9   280  2961  3107    21  7558
   1103  1036     5     1]]"
81a35b9572c9d574a30cc2164f47750716157fc8,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, Waseem et al. BIBREF10","[[ 2751    15    15    51    11  1546   208    63     3  5972 25582   371
  11116     3 23268     3    15    17   491     5     3  5972 25582   371
   1298     6  2751    15    15    51     3    15    17   491     5     3
   5972 25582   371  1714     1]]"
1ec152119cf756b16191b236c85522afeed11f59,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"They plot the average cosine similarity between uniformly random words increases exponentially from layers 8 through 12.  
They plot the average self-similarity of uniformly randomly sampled words in each layer of BERT, ELMo, and GPT-2 and shown that the higher layer produces more context-specific embeddings.
They plot that word representations in a sentence become more context-specific in upper layers, they drift away from one another.","[[  328  5944     8  1348   576     7   630  1126   485   344  7117   120
   6504  1234  5386 25722   120    45  7500   505   190  8013   328  5944
      8  1348  1044    18 26714   485    13  7117   120 21306  3106    26
   1234    16   284  3760    13   272 24203     6   262 11160    32     6
     11   350  6383  4949    11  2008    24     8  1146  3760  9560    72
   2625    18  9500 25078    26    53     7     5   328  5944    24  1448
   6497     7    16     3     9  7142   582    72  2625    18  9500    16
   4548  7500     6    79 16901   550    45    80   430     5     1]]"
37f8c034a14c7b4d0ab2e0ed1b827cc0eaa71ac6,0.0,"document classification, named entity recognition, part-of-speech tagging, dependency","[[    0  1708 13774     6  2650 10409  5786     6   294    18   858    18
      7   855 10217     3    17 15242     6 27804]]","translation probabilities, Labeled Attachment Scores (LAS)","[[ 7314  9551  2197     6 16229    15    26 28416   297 17763     7    41
  20245    61     1]]"
d93c0e78a3fe890cd534a11276e934be68583f4b,0.0,58-79,[[   0    3 3449   18 4440    1]],Unanswerable,[[ 597 3247 3321  179    1]]
8c852fc29bda014d28c3ee5b5a7e449ab9152d35,16.6667,"Bi-LSTM, BERT",[[    0  2106    18  7600  2305     6   272 24203     1]],"linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)","[[13080   180 12623     6  2647 26352  3230  7110    18 11679    18   329
     15  2528    63    41   279    23  7600  2305   201  1193 24817   138
   1484  9709  3426    41   254 17235    61     1]]"
1088255980541382a2aa2c0319427702172bbf84,15.0943,"attention mechanism acts upon the attention of two separate entities, attention to the text and attention to the","[[    0  1388  8557  6775  1286     8  1388    13   192  2450 12311     6
   1388    12     8  1499    11  1388    12     8]]","At the macro level, it is important to decide which is the appropriate field to attend to next, micro level (i.e., within a field) it is important to know which values to attend to next, fuse the attention weights at the two levels","[[  486     8 11663   593     6    34    19   359    12  2204    84    19
      8  2016  1057    12  2467    12   416     6  2179   593    41    23
      5    15     5     6   441     3     9  1057    61    34    19   359
     12   214    84  2620    12  2467    12   416     6 17165     8  1388
   1293     7    44     8   192  1425     1]]"
53014cfb506f6fffb22577bf580ae6f4d5317ce5,0.0,WN18 and FB15k,[[    0     3 21170  2606    11     3 15586  1808   157     1]],"the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22","[[    8 19602    87   308     9  9203  7098  1506  8285 17953     3  5972
  25582   371  2266     6     8   368  1060  5324   389  2264   920 10052
    302    41 12056   382   117     3  5972 25582   371  1828   201     3
      4   134   440     3  5972 25582   371  2884     1]]"
498c0229f831c82a5eb494cdb3547452112a66a0,19.0476,They use a combination of lay and expert annotations during training,"[[    0   328   169     3     9  2711    13  8260    11  2205 30729     7
    383   761     1]]",Annotations from experts are used if they have already been collected.,"[[ 389 2264 1628   45 2273   33  261    3   99   79   43  641  118 4759
     5    1]]"
44497509fdf5e87cff05cdcbe254fbd288d857ad,0.0,"Compared to the baseline, the merged words improve by about 1%.","[[    0     3 25236    12     8 20726     6     8     3 21726  1234  1172
     57    81     3  4704     5     1]]",Unanswerable,[[ 597 3247 3321  179    1]]
e025061e199b121f2ac8f3d9637d9bf987d65cd5,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],average:15.5,[[ 1348    10   536 15938     1]]
0cfe0e33fbb100751fc0916001a5a19498ae8cb5,19.6078,They use a simple voting scheme to obtain the new context represetation.,"[[    0   328   169     3     9   650 10601  5336    12  3442     8   126
   2625  3852    60  2244   257     5     1]]","They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation.","[[  328   169   192  2547   975 24817   138    11  9858    18 13194    53
   7500    30  5637     3     9  2711    13     8   646  2625     6     8
    646 10409    11     8  2214  2625   117    11  6499     3     9  2711
     13     8  2214  2625     6     8   269 10409    11     8   269  2625
      5   328   975  2138    35   920     8   192   772   227  2201    53
     12   129     8   126  2625  6497     5     1]]"
e44a6bf67ce3fde0c6608b150030e44d87eb25e3,28.5714,"abortion, medicine usage, marijuana",[[    0 20526     6  4404  4742     6 10434     1]],"abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR)","[[20526    41   188  8471   201 16998  2166    41  6302   476   201  4534
     41 10539   188   201    11 10434    41 13845    61     1]]"
3d013f15796ae7fed5272183a166c45f16e24e39,5.7143,a collection of ten different types of typography from the Arab World (Arab World,"[[    0     3     9  1232    13     3   324   315  1308    13 23042 16369
     45     8  9217  1150    41   188  7093  1150]]","font type, font style, Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page","[[ 4088   686     6  4088   869     6  2784    30     8  4088   686    11
   4088   869    41    15     5   122     5     6    34     9  2176     7
      6  8197  2281    61    13     3     9 14145    11   165  1102    30
      8  1722   543     1]]"
9193006f359c53eb937deff1248ee3317978e576,0.0,10 classification datasets from the Wikipedea corpus,"[[    0   335 13774 17953     7    45     8  2142  2168  3138    15     9
  11736   302     1]]","Reuters,  BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2, Yelp2013","[[    3 18844     6  9938 17682     6 19052   485     6 19237 10696     6
   5220 23008     6     3  5166  9213     6   332 20921     6   180  4209
   2292     6   180  4209  4949     6  7271    40   102 11138     1]]"
52f9cd05d8312ae3c7a43689804bac63f7cac34b,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
876700622bd6811d903e65314ac75971bbe23dcc,0.0,"BIBREF13, BIBREF18","[[    0     3  5972 25582   371  2368     6     3  5972 25582   371  2606
      1]]", SemEval-2016 â€œSentiment Analysis in Twitterâ€,"[[  679    51   427  2165    18 11505   105   134   295    23   297 10582
     16  3046   153     1]]"
8ce11515634236165cdb06ba80b9a36a8b9099a2,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
1ff0fccf0dca95a6630380c84b0422bed854269a,4.5455,LSTM-based model,[[   0    3 7600 2305   18  390  825    1]],"efficiency of a communication scheme $(q_{\alpha },p_{\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence","[[ 3949    13     3     9  1901  5336  1514   599  1824   834     2   138
   6977     3     2     6   102   834     2   346    17     9     3     2
     61  3229    57     8 14344  1080    13 14145     7     6    84    19
   8413    38     8 12211    13 14145     7    24    33  2697    16     8
  12545     6  7452    13     3     9  5336    19  8413    38     8 12211
     13 16513  6126    57 30337  9203    20  9886     8   825    24  1776
   6407     8  2387  7142     1]]"
72ed5fed07ace5e3ffe9de6c313625705bc8f0c7,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Most texts, however, range roughly from 150 to 250 tokens.","[[ 1377 14877     6   983     6   620 10209    45  4261    12  5986 14145
      7     5     1]]"
2e89ebd2e4008c67bb2413699589ee55f59c4f36,0.0,using a modified version of BERT,[[    0   338     3     9  8473   988    13   272 24203     1]],"update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity., Classification Objective Function, Regression Objective Function, Triplet Objective Function","[[ 2270     8  1293     7   224    24     8  2546  7142 25078    26    53
      7    33 27632  1427  7892    11    54    36     3  2172    28   576
      7   630    18 26714   485     5     6  4501  2420 27919 21839     6
    419 22430 27919 21839     6 22709    17 27919 21839     1]]"
55c8f7acbfd4f5cde634aaecd775b3bb32e9ffa3,0.0,accuracy,[[   0 7452    1]],"PER, WER, WER 100",[[   3 8742    6  549 3316    6  549 3316  910    1]]
d5a8fd8bb48dd1f75927e874bdea582b4732a0cd,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
58ee0cbf1d8e3711c617b1cd3d7aca8620e26187,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer","[[ 1541    62   103    59    43    46   414    18   235    18   989 17953
      6     8  6126  1566 14193   164    59   161   168    28 18191   869
   2025     1]]"
2c85865a65acd429508f50b5e4db9674813d67f2,0.0,"dataset on the HCI web site, a dataset on the IMDB web site,","[[    0 17953    30     8   454  3597   765   353     6     3     9 17953
     30     8     3  5166  9213   765   353     6]]","A sample from nurse-initiated telephone conversations for congestive heart failure patients undergoing telepmonitoring, post-discharge from the Health Management Unit at Changi General Hospital","[[   71  3106    45 10444    18    77   155    23   920  6596  9029    21
    975  2897  3268   842  3338  1221     3 22725     3  1931   102  8823
    235  1007     6   442    18    26   159  7993    45     8  1685  2159
   5579    44 24187    23  2146  4457     1]]"
b1bc9ae9d40e7065343c12f860a461c7c730a612,0.0,"WSJ11, WSJ12",[[   0    3 8439  683 2596    6    3 8439  683 2122    1]],"ShapeWorldICE datasets: OneShape, MultiShapes, TwoShapes, MultiShapes, Count, and Ratio","[[23890 17954  8906 17953     7    10   555   134  9516    15     6  4908
    134  9516    15     7     6  2759   134  9516    15     7     6  4908
    134  9516    15     7     6     3 10628     6    11  6455    23    32
      1]]"
a253749e3b4c4f340778235f640ce694642a4555,0.0,CSA broadcasts,[[    0     3 24135  6878     7     1]],"ESTER1, ESTER2, ETAPE, REPERE","[[  262 20727  4347   262 20727  4482     3 25747  5668     6  4083  8742
    427     1]]"
06eb9f2320451df83e27362c22eb02f4a426a018,15.3846,3 incremental levels of document preprocessing,[[    0   220 28351  1425    13  1708   554 15056    53     1]],"Level 1, Level 2 and Level 3.",[[7166 1914 7166  204   11 7166 1877    1]]
326588b1de9ba0fd049ab37c907e6e5413e14acd,0.0,LSTM-based MT-based methods,[[   0    3 7600 2305   18  390    3 7323   18  390 2254    1]],"OpenNMT, PBMT-R, Hybrid, SBMT-SARI, Dress","[[ 2384   567  7323     6     3 13970  7323    18   448     6  5555  2160
     26     6 16757  7323    18   134 22410     6 12169     1]]"
f0848e7a339da0828278f6803ed7990366c975f0,0.0,Yes,[[   0 2163    1]],"SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance","[[  180 12623     6   465    18   188    29     7  3321  8430   747    41
   5999    61     3     6  4467     3 10628  8430   747     6  3892  8233
      1]]"
5a22293b055f5775081d6acdc0450f7bd5f5de04,11.7647,BiLSTM with max dependency density of 82.61,"[[    0  2106  7600  2305    28  9858 27804 11048    13     3  4613     5
   4241     1]]","OpATT BIBREF6, Neural Content Planning with conditional copy (NCP+CC) BIBREF4","[[ 4495 24642     3  5972 25582   371 11071  1484  9709  7185  9557    28
   1706   138  2405    41   567  4184  1220  2823    61     3  5972 25582
    371   591     1]]"
f03df5d99b753dc4833ef27b32bb95ba53d790ee,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"have a larger proportion of friends/followers (i.e. they have, on average, the same number of friends but a smaller number of followers) than those spreading viral content only","[[   43     3     9  2186  7385    13   803    87 25278   277    41    23
      5    15     5    79    43     6    30  1348     6     8   337   381
     13   803    68     3     9  2755   381    13 10076    61   145   273
  16436 16686   738   163     1]]"
2ceced87af4c8fdebf2dc959aa700a5c95bd518f,100.0,No,[[  0 465   1]],No,[[465   1]]
729694a9fe1e05d329b7a4078a596fe606bc5a95,0.0,"Using the above methods, we achieve a 98.99% accuracy and 98","[[   0    3 3626    8  756 2254    6   62 1984    3    9    3 3916    5
  1298 7561 7452   11    3 3916]]"," total F-1 score on the OntoNotes dataset is 88%, total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%","[[  792   377  2292  2604    30     8   461   235 10358    15     7 17953
     19   505  5953     6   792   377  2292  2269    18 27769   257  2604
     30     8   850   357   853  2142  2168   599 14910    61 17953    19
    305  5170     1]]"
d6e353e0231d09fd5dcba493544d53706f3fe1ab,0.0,The average accuracy of all the proposed methods is metric of the quality of the singing voice,"[[   0   37 1348 7452   13   66    8 4382 2254   19    3 7959   13    8
   463   13    8 8782 2249    1]]","To compare the conversions between USVC and PitchNet, we employed an automatic evaluation score and a human evaluation score.","[[ 304 4048    8 6113    7  344  837 7431   11  276 7059 9688    6   62
  8152   46 6569 5002 2604   11    3    9  936 5002 2604    5    1]]"
02428a8fec9788f6dc3a86b5d5f3aa679935678d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Low sensitivity to bias in prior knowledge,[[ 5586     3 13398    12 14387    16  1884  1103     1]]
003d6f9722ddc2ee13e879fefafc315fb8e87cb9,0.0,a node in the network approach,[[   0    3    9  150  221   16    8 1229 1295    1]],Unanswerable,[[ 597 3247 3321  179    1]]
9b7655d39c7a19a23eb8944568eb5618042b9026,23.5294,"NLTK, Stanford CoreNLP, TwitterNLP","[[    0     3 18207 22110     6 19796  9020   567  6892     6  3046   567
   6892     1]]","BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21","[[    3  5972 25582   371  2517     6     3  5972 25582   371  2606     6
   4738     7    23 11500  4606   189     3  5972 25582   371  2368     6
   3046   567  6892     3  5972 25582   371 11071     3  5972 25582   371
   2294     6   638   122  5890   102    18   567  6892     3  5972 25582
    371  1755     6 19796   445  6892     3 18206     3  5972 25582   371
   2658     1]]"
197b276d0610ebfacd57ab46b0b29f3033c96a40,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK and Pattern-based","[[ 7736  3040   419 22430     6  4908  3114    23   138  1823   757  2474
     15     7     6 25942  6944     6  1980     9 16481     6  4919   291
    180 12623     6   180 12623    28    71 20293 10047    11 20918    18
    390     1]]"
55569d0a4586d20c01268a80a7e31a17a18198e2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"we simply adopted the official training script of BERT, with default hyperparameters, to fine-tune each model until training loss converged","[[   62   914  7546     8  2314   761  4943    13   272 24203     6    28
   4647  6676  6583  4401     7     6    12  1399    18    17   444   284
    825   552   761  1453     3 28209    26     1]]"
7cd22ca9e107d2b13a7cc94252aaa9007976b338,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
05c49b9f84772e6df41f530d86c1f7a1da6aa489,0.0,user interface to structured and semi-structured data,[[    0  1139  3459    12 14039    11  4772    18 16180    26   331     1]],"File IO, Standard IO, Telegram",[[7344    3 7550    6 5150    3 7550    6 7338 5096    1]]
b1cf5739467ba90059add58d11b73d075a11ec86,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
50e80cfa84200717921840fddcf3b051a9216ad8,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
ceb767e33fde4b927e730f893db5ece947ffb0d8,0.0,"Surgical, Dental, and Sports",[[    0     3 31186     6 11826     6    11  5716     1]],"Demographics Age, DiagnosisHistory, MedicationHistory, ProcedureHistory, Symptoms/Signs, Vitals/Labs, Procedures/Results, Meds/Treatments, Movement, Other.","[[10007 16587     7  7526     6  5267  6715     7   159 12146 13029     6
   1212 17530 12146 13029     6 25266 12146 13029     6     3 21828     7
     87   134  3191     7     6 23736     7    87 18506     7     6 25266
      7    87 20119     7     6  8067     7    87   382    60   144  4128
      6 19954     6  2502     5     1]]"
127d5ddfabec5c58832e5865cbd8ed0978c25a13,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"a simple word-level encoder, The encoder is essentially the same as tweet2vec, with the input as words instead of characters.","[[    3     9   650  1448    18  4563 23734    52     6    37 23734    52
     19     3  8317     8   337    38 10657   357   162    75     6    28
      8  3785    38  1234  1446    13  2850     5     1]]"
7438b6b146e41c08cf8f4c5e1d130c3b4cfc6d93,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
a267d620af319b48e56c191aa4c433ea3870f6fb,18.1818,"labels in a car-buying data set that contain the words ""car"", ""family","[[    0 11241    16     3     9   443    18 14584    53   331   356    24
   3480     8  1234    96  1720  1686    96 15474]]",the car,[[  8 443   1]]
4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94,0.0,"LSTM, LSTM-ID",[[   0    3 7600 2305    6    3 7600 2305   18 4309    1]],state-of-the-art PDTB taggers,[[ 538   18  858   18  532   18 1408    3 6251 9041 7860 1304    7    1]]
b8d7d055ddb94f5826a9aad7479b4a92a9c8a2f0,0.0,LSTM,[[   0    3 7600 2305    1]], Recurrent Neural Network (RNN),[[  419 14907  1484  9709  3426    41 14151   567    61     1]]
53aa07cc4cc4e7107789ae637dbda8c9f6c1e6aa,0.0,"high-quality speech translation requires hundreds of hours of transcribed audio, while high-quality","[[    0   306    18  4497  5023  7314  2311  3986    13   716    13     3
  11665 22573  2931     6   298   306    18  4497]]","Assigning wrong words to a cluster, Splitting words across different clusters, sparse, giving low coverage","[[  282  6732    53  1786  1234    12     3     9  9068     6 23575  1222
   1234   640   315  9068     7     6 14144     7    15     6  1517   731
   2591     1]]"
682e26262abba473412f68cbeb5f69aa3b9968d7,11.1111,"No toxic comments, hate speech, profanity, insult, or abuse","[[    0   465 12068  2622     6  5591  5023     6  7108   152   485     6
  21548     6    42  5384     1]]",no prior work has explored the target of the offensive language,[[  150  1884   161    65 15883     8  2387    13     8 12130  1612     1]]
551457ed34ca7fc0878c85bc664b135c21059b58,9.0909,SST-2 dataset,[[    0   180  4209  4949 17953     1]],"trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data, selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset","[[ 4252    30     3 11776   716    41  3388 20006 24030   536   910   439
  10316    61    13     3 11665 22573  5023   331     6  1738     7     3
      9   769  2244    13     3     9  1914  9295    18  5842    41  3388
  20006 24030   357     3 11039   329 10316    61    73  9339   400    26
  17953     1]]"
c35806cf68220b2b9bb082b62f493393b9bdff86,0.0,Best performance achieved was 0.83 F1 score,[[   0 1648  821 5153   47 4097 4591  377  536 2604    1]],accuracy of 87.0%,[[7452   13    3 4225    5 6932    1]]
f9f59c171531c452bd2767dc332dc74cadee5120,0.0,82,[[   0    3 4613    1]],14,[[968   1]]
6b7354d7d715bad83183296ce2f3ddf2357cb449,0.0,CNN,[[    0 19602     1]],BERT,[[  272 24203     1]]
8acab64ba72831633e8cc174d5469afecccf3ae9,0.0,Yes,[[   0 2163    1]],telephone calls,[[6596 3088    1]]
bc31a3d2f7c608df8c019a64d64cb0ccc5669210,0.0,BiLSTM BIBREF2,[[    0  2106  7600  2305     3  5972 25582   371   357     1]],BERTbase,[[  272 24203 10925     1]]
6b6d498546f856ac20958f666fc3fd55811347e2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],They use the embedding layer with a size 35 and embedding dimension of 300. They use a dense layer with 70 units and a dropout layer with a rate of 50%.,"[[  328   169     8 25078    26    53  3760    28     3     9   812  3097
     11 25078    26    53  9340    13  3147     5   328   169     3     9
  13809  3760    28  2861  3173    11     3     9  2328   670  3760    28
      3     9  1080    13  5743     5     1]]"
2317ca8d475b01f6632537b95895608dc40c4415,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account,"[[16749     3  6848    13     3     9     3 14504  5932    13 10657     7
   3783    15    26    57     8  3783    13   165     3  9921   905     1]]"
beac555c4aea76c88f19db7cc901fa638765c250,12.5,"relevance of the attended parts, especially cases where attention is smeared out'","[[    0 20208    13     8  5526  1467     6   902  1488   213  1388    19
      3     2     7   526     9  1271    91    31]]",Alignment points of the POS tags.,[[  901  3191   297   979    13     8     3 16034 12391     5     1]]
a29c071065d26e5ee3c3bcd877e7f215c59d1d33,0.0,"cosine similarity, Manhatten / Euclidean distance","[[    0   576     7   630  1126   485     6  1140   547   324     3    87
   4491 14758   221   152  2357     1]]",Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels,"[[ 8974   291   348    31     7 11003 18712   344     8   576     7   630
     18 26714   485    13     8  7142 25078    26    53     7    11     8
   2045 11241     1]]"
f318a2851d7061f05a5b32b94251f943480fbd15,15.3846,The authors draw from their findings that the emotional appeal of ISIS and non-violent religious,"[[    0    37  5921  3314    45    70  7469    24     8  3973  3958    13
     27 14408    11   529    18 11275   295  4761]]","By comparing scores for each word calculated using Depechemood dictionary and normalize emotional score for each article, they found Catholic and ISIS materials show similar scores","[[  938     3 14622  7586    21   284  1448 11338   338   374   855  6482
     32    32    26 24297    11  1389  1737  3973  2604    21   284  1108
      6    79   435  6502    11    27 14408  1397   504  1126  7586     1]]"
97d1ac71eed13d4f51f29aac0e1a554007907df8,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically","[[20612   825    43     3     9  2411  2475    13     3 24163   117    62
   8269    48 14130    57  2651    72  1102 25078    26    53     7     6
     62  8722  3866   784    75    40     7   908 14145     7    44     8
    456    13   284  7142     6    11   284   784    75    40     7   908
   6083  2868     7   753    21     8  7142 20799    53    34     6  1708
   6497     7    33  2525  1382  7064  6402     1]]"
3321d8d0e190d25958e5bfe0f3438b5c2ba80fd1,10.0,"5,952 short factoid questions paired with a taxnomie developed by","[[    0  7836  3301   357   710   685    32    23    26   746     3 13804
     28     3     9  1104 32134 23140  1597    57]]",Used from  science exam questions of the Aristo Reasoning Challenge (ARC) corpus.,"[[12504    45  2056  3631   746    13     8    71 17149 21272    53  7729
     41 18971    61 11736   302     5     1]]"
218bc82796eb8d91611996979a4a42500131a936,0.0,LSTMs,[[   0    3 7600 2305    7    1]],"Eusboost, MWMOTE",[[ 4491     7 12840     7    17     6     3 16027  5365  3463     1]]
66c96c297c2cffdf5013bab5e95b59101cb38655,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"BERT remains only 0.3 F1-score points behind, and would have achieved the second position among all the MEDDOCAN shared task competitors. Taking into account that only 3% of the gold labels remain incorrectly annotated,  Table ","[[  272 24203  3048   163     3 19997   377   536    18     7  9022   979
   1187     6    11   133    43  5153     8   511  1102   859    66     8
      3 21357  9857 11425  2471  2491  9216     5     3 12297   139   905
     24   163     3  5170    13     8  2045 11241  2367 12153   120    46
   2264   920     6  4398     1]]"
c571deefe93f0a41b60f9886db119947648e967c,0.0,"BIBREF14, BIBREF16","[[    0     3  5972 25582   371  2534     3     6     3  5972 25582   371
   2938     1]]",MIMIC-III,[[ 8161 24896    18 13671     1]]
1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
56b7319be68197727baa7d498fa38af0a8440fe4,0.0,"linguistic (sentiment, readability emotion part-of-speech","[[    0     3 24703    41  5277    23   297     6   608  2020 32128 13868
  32128   294    18   858    18     7   855 10217]]",BERT,[[  272 24203     1]]
5b551ba47d582f2e6467b1b91a8d4d6a30c343ec,0.0,accuracy,[[   0 7452    1]],"Byte-Pair Encoding perplexity  (BPE PPL),
BLEU-1,
BLEU-4,
ROUGE-L,
percentage of distinct unigram (D-1),
percentage of distinct bigrams(D-2),
user matching accuracy(UMA),
Mean Reciprocal Rank(MRR)
Pairwise preference over baseline(PP)","[[  938    17    15    18   345  2256   695  9886   399  9247   485    41
    279  5668   276  5329   201     3  8775 12062  2292     6     3  8775
  12062  4278     6   391 26260   427    18   434     6  5294    13  6746
     73    23  5096    41   308  2292   201  5294    13  6746   600  2375
      7   599   308  4949   201  1139  8150  7452   599  6122   188   201
  23045  7136    23  1409  1489     3 22557   599   329 12224    61 25072
  10684 11633   147 20726   599  6158    61     1]]"
7f207549c75f5c4388efc15ed28822672b845663,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],20 minutes,[[460 676   1]]
f9f59c171531c452bd2767dc332dc74cadee5120,0.0,82,[[   0    3 4613    1]],14 participants,[[ 968 3008    1]]
c4628d965983934d7a2a9797a2de6a411629d5bc,0.0,Yes,[[   0 2163    1]],It learns a representation of medical records. The learned representation (embeddings) can be used for other predictive tasks involving information from electronic health records.,"[[   94   669     7     3     9  6497    13  1035  3187     5    37  2525
   6497    41    15    51   346  7249     7    61    54    36   261    21
    119 27875  4145     3  6475   251    45  3031   533  3187     5     1]]"
e35a7f9513ff1cc0f0520f1d4ad9168a47dc18bb,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"traditional phrase-based statistical machine translation (SMT), NMT system","[[ 1435  9261    18   390 11775  1437  7314    41   134  7323   201   445
   7323   358     1]]"
e101e38efaa4b931f7dd75757caacdc945bb32b4,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, Waseem et al. BIBREF10","[[ 2751    15    15    51    11  1546   208    63     3  5972 25582   371
  11116     3 23268     3    15    17   491     5     3  5972 25582   371
   1298     6  2751    15    15    51     3    15    17   491     5     3
   5972 25582   371  1714     1]]"
61652a3da85196564401d616d251084a25ab4596,0.0,"70,000",[[    0     3 28891     1]],26972,[[2208 4327  357    1]]
de5b6c25e35b3a6c5e40e350fc5e52c160b33490,34.4828,their model outperforms by 0.86 on fluency and 0.98 on grammar,"[[    0    70   825    91   883  2032     7    57  4097  3840    30  6720
   4392    11  4097  3916    30 19519     1]]","On arXiv dataset, the proposed model outperforms baselie model by (ROUGE-1,2,L)  0.67 0.72 0.77 respectively and by Meteor 0.31.
","[[  461  1584     4    23   208 17953     6     8  4382   825    91   883
   2032     7  1247  1896   825    57    41   448 26260   427  2292     6
   4482   434    61  4097  3708  4097  5865  4097  4013  6898    11    57
   8146    15   127  4097  3341     5     1]]"
9bb7ae50bff91571a945c1af025ed2e67714a788,0.0,"attention modeling the entire context, i.e., the series of posts that trigger an","[[    0  1388 15309     8  1297  2625     6     3    23     5    15     5
      6     8   939    13  3489    24  7294    46]]",hLSTM,[[   3  107 7600 2305    1]]
93beae291b455e5d3ecea6ac73b83632a3ae7ec7,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. ","[[ 9246     8  6275 16826     6    62    54  3442     8  1448  3172    13
    284  3785  1448    12     8  1297  3911  7142     5     1]]"
bc730e4d964b6a66656078e2da130310142ab641,0.0,"Bi-LSTM, BERT",[[    0  2106    18  7600  2305     6   272 24203     1]],"Logistic Regression, Multilayer Perceptron",[[ 7736  3040   419 22430     6  4908 18270  1915  6873    52   106     1]]
d6191c4643201262a770947fc95a613f57bedb6b,0.0,dozens of thousands of web pages,[[    0     3 14903    13  2909    13   765  1688     1]],DIP corpus BIBREF37,[[  309  4629 11736   302     3  5972 25582   371  4118     1]]
cfcdd73e712caf552ba44d0aa264d8dace65a589,0.0,"23,700 queries that are short and unstructured, in the same style made by real users","[[    0 12992  9295 13154    24    33   710    11    73 16180    26     6
     16     8   337   869   263    57   490  1105]]",crowsourcing platform,[[    3    75  3623 19035  1585     1]]
fa527becb8e2551f4fd2ae840dbd4a68971349e0,100.0,LSTM,[[   0    3 7600 2305    1]],LSTM,[[   3 7600 2305    1]]
14b8ae5656e7d4ee02237288372d9e682b24fdb8,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],ironies are often obscure and hard to understand,[[ 3575   725    33   557 21634    11   614    12   734     1]]
50cc6c5f2dcf5fb87b56007f6a825fa7c90b64ed,11.1111,"individual Bayesian models for each language BIBREF3, crosslingual latent","[[    0   928  2474    15 10488  2250    21   284  1612     3  5972 25582
    371   519     3     6  2269 25207    50  4669]]",Bayesian model of garg2012unsupervised as our base monolingual model,"[[ 2474    15 10488   825    13  5260   122 12172   202 23313    38    69
   1247  7414 25207   825     1]]"
9c44df7503720709eac933a15569e5761b378046,33.3333,"English, Spanish, Japanese and Arabic",[[    0  1566     6  5093     6  4318    11 19248     1]],English,[[1566    1]]
22744c3bc68f120669fc69490f8e539b09e34b94,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
0752d71a0a1f73b3482a888313622ce9e9870d6e,0.0,wFST-based model,[[   0    3  210  371 4209   18  390  825    1]],system presented by deri2016grapheme,[[  358  2569    57    74    23 11505  9413    15   526     1]]
f428618ca9c017e0c9c2a23515dab30a7660f65f,17.3913,"NBC, K-NN, Random Forest",[[    0   445  7645     6   480    18 17235     6 25942  6944     1]],"Multi-Layer Perceptron, Naive Bayes Classifier, Support Vector Machine, Gradient Boosting Classifier, Stochastic Gradient Descent, K Nearest Neighbour, Random Forest","[[ 4908    18  3612  7975  1915  6873    52   106     6  1823   757  2474
     15     7  4501  7903     6  4224 29011  5879     6 10771  4741     3
  16481    53  4501  7903     6   472  6322 10057 10771  4741   309 11719
      6   480 10455   222  1484  9031   115  1211     6 25942  6944     1]]"
46c9e5f335b2927db995a55a18b7c7621fd3d051,0.0,57,[[   0    3 3436    1]],Thirteen different phenotypes are present in the dataset.,"[[21564  6808   315     3 19017    32  6137     7    33   915    16     8
  17953     5     1]]"
00c57e45ac6afbdfa67350a57e81b4fad0ed2885,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
9b97805a0c093df405391a85e4d3ab447671c86a,0.0,accuracy,[[   0 7452    1]],"Exact Match (EM), Macro-averaged F1 scores (F1)","[[ 1881  2708 12296    41  6037   201  1534  2771    18 28951    26   377
    536  7586    41   371  6982     1]]"
0a5ffe4697913a57fda1fd5a188cd5ed59bdc5c7,21.0526,"German, English, Chinese",[[   0 2968    6 1566    6 2830    1]],"Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish","[[15536    29     6 19789    29     6 16870     6 23124     6  1566     6
   2379     6  2968     6  9995    29     6  4338     6 21894     6 25518
      6 16073     6 21076     6 27425    29     6  5093     3     6 16531
      1]]"
7e4ef0a4debc048b244b61b4f7dc2518b5b466c0,40.0,"VP ellipsis, lexical cohesion, VP elli","[[    0     3 11527     3  7999   102     7   159     6     3 30949   138
    576    88  1938     6     3 11527     3  7999]]","Four discourse phenomena - deixis, lexical cohesion, VP ellipsis, and ellipsis which affects NP inflection.","[[ 5933 22739 24666     3    18    20  2407   159     6     3 30949   138
    576    88  1938     6     3 11527     3  7999   102     7   159     6
     11     3  7999   102     7   159    84  2603     7     3  9082    16
     89 12252     5     1]]"
5cc2daca2a84ddccba9cdd9449e51bb3f64b3dde,0.0,"LSTM, Deep Neural Network (RNN)","[[    0     3  7600  2305     6  9509  1484  9709  3426    41 14151   567
     61     1]]","state-of-the-art Transformer architecture, Kaldi, speech clustergen statistical speech synthesizer","[[  538    18   858    18   532    18  1408 31220  4648     6  5740    26
     23     6  5023  9068   729 11775  5023 13353    15     7  8585     1]]"
c59078efa7249acfb9043717237c96ae762c0a8c,0.0,LSTM with he and she word pairs,[[    0     3  7600  2305    28     3    88    11   255  1448 14152     1]],"CDA, REG",[[3190  188    6 4083  517    1]]
220d11a03897d85af91ec88a9b502815c7d2b6f3,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Advanced neural architectures and contextualized embedding models learn how to handle spelling and morphology variations.,"[[ 8590 24228  4648     7    11 28131  1601 25078    26    53  2250   669
    149    12  2174 19590    11     3  8886  1863 10914     5     1]]"
ab0fd94dfc291cf3e54e9b7a7f78b852ddc1a797,0.0,SVM,[[    0   180 12623     1]],BIBREF26 ,[[    3  5972 25582   371  2688     1]]
49764eee7fb523a6a28375cc699f5e0220b81766,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
e6583c60b13b87fc37af75ffc975e7e316d4f4e0,0.0,CASINO,[[    0     3 18678   196  7400     1]],"KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)","[[  480 21159     3  7894     3  5972 25582   371  2517     3     6 10431
     13  1249 20226   331    21 26544    18   390     6 18534    11 21610
     26  5023   538     3  9921    12   489   951  3113    87     7    63
    195     9 15979    41     3    87    23    63    87     6     3    87
    102    23    63    87     6     3    87    17    23    63    87     6
      3    87    26    23    63    87     6     3    87    76   210    87
      6     3    87    51    87     6     3    87    29    87     3    61
     38   168    38   314  1234   599  4665     6   815     6  2124    11
      3 11260   210    61     1]]"
cb77d6a74065cb05318faf57e7ceca05e126a80d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec","[[  272   265     3    15    17   491     5   180 12623     6  1534    11
   1546   208    63     3   210    87  9680   162     6 13667   109     3
     15    17   491     5     3   210    87 11584 13598    17     6 13667
    109     3    15    17   491     5     3   210    87  6051   357   162
     75     1]]"
b21bc09193699dc9cfad523f3d5542b0b2ff1b8e,0.0,LSTM,[[   0    3 7600 2305    1]],MLP,[[ 283 6892    1]]
14b8ae5656e7d4ee02237288372d9e682b24fdb8,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"obscure and hard to understand,  lack of previous work and baselines on irony generation","[[21634    11   614    12   734     6  2136    13  1767   161    11 20726
      7    30  3575    63  3381     1]]"
154a721ccc1d425688942e22e75af711b423e086,0.0,MCScript,[[    0     3  3698 18255     1]],Amazon Mechanical Turk,[[ 2536 24483 23694     1]]
22b740cc3c8598247ee102279f96575bdb10d53f,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
4d47bef19afd70c10bbceafd1846516546641a2f,40.0,"sequence to sequence model, LSTM",[[   0 5932   12 5932  825    6    3 7600 2305    1]],uni-directional model to augment the decoder,[[   73    23    18 26352   825    12 15189     8    20  4978    52     1]]
132f752169adf6dc5ade3e4ca773c11044985da4,14.2857,a dataset of 280 million sentences,[[    0     3     9 17953    13     3 17518   770 16513     1]]," tweet dataset created by Wang et al. , CrowdFlower dataset","[[10657 17953   990    57 18102     3    15    17   491     5     3     6
  15343    26 15390    49 17953     1]]"
58355e2a782bf145b61ee2a3e0e426119985c179,23.5294,Twitter dataset for the ECML PKDD 2019 Conference,"[[    0  3046 17953    21     8     3  3073  6858     3 16782 11253  1360
   4379     1]]",Twitter dataset provided by organizers containing harassment and non-harassment tweets,"[[ 3046 17953   937    57 14250     7     3  6443 23556    11   529    18
  14888     7     7   297 10657     7     1]]"
3ddff6b707767c3dd54d7104fe88b628765cae58,0.0,"Bigram, trigram hidden Markov, decision trees, MEMMs, CRFs","[[   0 2734 2375    6 6467 5096 5697 1571 9789    6 1357 3124    6 7934
  8257    7    6  205 8556    7]]","Universal Dependencies v1.2 treebanks BIBREF21 , hereafter UD1.2","[[12489 30718 11573     3   208 10917  2195  4739     7     3  5972 25582
    371  2658     3     6   270 10245     3 10161 10917     1]]"
dac087e1328e65ca08f66d8b5307d6624bf3943f,100.0,No,[[  0 465   1]],No,[[465   1]]
8e12b5c459fa963b3e549deadb864c244879fe82,0.0,four layers,[[   0  662 7500    1]],Unanswerable,[[ 597 3247 3321  179    1]]
0f567251a6566f65170a1329eeeb5105932036b2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"current state-of-the-art approach BIBREF14 , BIBREF15","[[  750   538    18   858    18   532    18  1408  1295     3  5972 25582
    371  2534     3     6     3  5972 25582   371  1808     1]]"
4944cd597b836b62616a4e37c045ce48de8c82ca,0.0,"large-scale semantic similarity comparison, clustering, and information retrieval via semantic search","[[    0   508    18  6649 27632  1126   485  4993     6  9068    53     6
     11   251 24515   138  1009 27632   960     1]]","MR, CR, SUBJ, MPQA, SST, TREC, MRPC","[[    3  9320     6     3  4545     6   180 10134   683     6  5220 23008
      6   180  4209     6   332 20921     6     3  9320  4051     1]]"
1b1b0c71f1a4b37c6562d444f75c92eb2c727d9b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],The number of dimensions can be reduced by up to 212 times.,"[[   37   381    13  8393    54    36  3915    57    95    12     3 24837
    648     5     1]]"
1beb4a590fa6127a138f4ed1dd13d5d51cc96809,0.0,"sarcastic, ironic, and metaphoric expressions","[[    0     3     7  4667 10057     6  3575   447     6    11 21253   447
   3893     7     1]]", The features extracted from CNN.,[[   37   753 21527    45 19602     5     1]]
82595ca5d11e541ed0c3353b41e8698af40a479b,20.0,inorganic and organic ways,[[    0    16 11127   447    11  3648  1155     1]],Mentioning of political parties names and political twitter handles is the organic way to show political affiliation; adding Chowkidar or its variants to the profile is the inorganic way.,"[[ 3137  1575    53    13  1827  2251  3056    11  1827 19010 13338    19
      8  3648   194    12   504  1827 24405   117  2651   205  4067  2168
   3439    42   165  6826     7    12     8  3278    19     8    16 11127
    447   194     5     1]]"
fb1227b3681c69f60eb0539e16c5a8cd784177a7,15.3846,relative authority of all citations,[[    0  5237  5015    13    66     3 13903     7     1]],"positional features, occurrence frequency, internal POS structure of the entity and the sentence it occurs in, relative entity frequency, centrality measures like PageRank ","[[ 1102   138   753     6     3 16526  7321     6  3224     3 16034  1809
     13     8 10409    11     8  7142    34  6986    16     6  5237 10409
   7321     6  2069   485  3629   114  5545 22557     1]]"
10fb7dc031075946153baf0a0599e126de29e3a4,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"converts WSD to a sequence learning task,  leverage gloss knowledge, by extending gloss knowledge","[[ 5755     7   549  7331    12     3     9  5932  1036  2491     6 11531
  20666  1103     6    57     3 16878 20666  1103     1]]"
521280a87c43fcdf9f577da235e7072a23f0673e,0.0,3,[[  0 220   1]],five annotators,[[ 874   46 2264 6230    1]]
c5abe97625b9e1c8de8208e15d59c704a597b88c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.","[[  419  2239    13   209 16253    21     8    71   357   254    18 19836
     15    26   825     6   314 16253    21     8     3 18256    18   188
    357   254    18 19836    15    26   825     6  1283    21    71   357
    254    18 12882   322    15    11  8537    21     3 18256    18   188
    357   254    18 12882   322    15     5     1]]"
bdc91d1283a82226aeeb7a2f79dbbc57d3e84a1a,9.0909,performance improved by +3.5% on the QNLI task and +3.5% on the ,"[[    0   821  3798    57  1768  5787  2712    30     8  1593 18207   196
   2491    11  1768  5787  2712    30     8     3]]"," improvement on the RTE dataset is significant, i.e., 4% absolute gain over the BERTBase","[[ 4179    30     8   391  3463 17953    19  1516     6     3    23     5
     15     5     6     3  5988  6097  2485   147     8   272 24203 14885
     15     1]]"
fb2b536dc8e442dffab408db992b971e86548158,0.0,agreement of 0.88 and 0.88,[[   0 2791   13 4097 4060   11 4097 4060    1]],Unanswerable,[[ 597 3247 3321  179    1]]
6cd8bad8a031ce6d802ded90f9754088e0c8d653,14.2857,"their best model outperforms by 0.61, 0.61, 0.61 ","[[   0   70  200  825   91  883 2032    7   57 4097 4241    3    6 4097
  4241    3    6 4097 4241    3]]",0.8% F1 better than the best state-of-the-art,"[[4097 5953  377  536  394  145    8  200  538   18  858   18  532   18
  1408    1]]"
0a75a52450ed866df3a304077769e1725a995bb7,6.6667,the decoder is programmed to generate a log-rank speech using a set,"[[   0    8   20 4978   52   19 2486   26   12 3806    3    9 4303   18
  6254 5023  338    3    9  356]]",Decoder predicts the sequence of phoneme or grapheme at each time based on the previous output and context information with a beam search strategy,"[[ 4451 13487  9689     7     8  5932    13   951   526    42  8373    15
    526    44   284    97     3   390    30     8  1767  3911    11  2625
    251    28     3     9 11638   960  1998     1]]"
57f23dfc264feb62f45d9a9e24c60bd73d7fe563,0.0,"augmented dataset contains utterances of 103,432 sentences","[[    0     3 28984 17953  2579     3  5108   663     7    13     3 17864
      6   591  2668 16513     1]]",609,[[ 431 4198    1]]
9bfebf8e5bc0bacf0af96a9a951eb7b96b359faa,17.3913,BLEU scores of 93.0% and 93.0%,"[[    0     3  8775 12062  7586    13     3  4271     5  6932    11     3
   4271     5  6932     1]]","average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time","[[ 1348  2696    18  4563   576   760  1433  7586    13  1300  3940    18
  16253  4482   936     3    15  7480  6230  6241  9354   825  3911     7
     12 20726   431  5170    13     8    97     1]]"
1c997c268c68149ae6fb43d83ffcd53f0e7fe57e,15.3846,"a LSTM with a corresponding knowledge base, a LSTM with","[[   0    3    9    3 7600 2305   28    3    9    3 9921 1103 1247    6
     3    9    3 7600 2305   28]]",ELMo embeddings are then used with a residual LSTM to learn informative morphological representations from the character sequence of each token,"[[  262 11160    32 25078    26    53     7    33   258   261    28     3
      9 27687     3  7600  2305    12   669 11152     3  8886  4478  6497
      7    45     8  1848  5932    13   284 14145     1]]"
f651cd144b7749e82aa1374779700812f64c8799,0.0,SVM score,[[    0   180 12623  2604     1]],"BLEU , FKGL , SARI ","[[    3  8775 12062     3     6   377   439 13011     3     6   180 22410
      1]]"
5913930ce597513299e4b630df5e5153f3618038,21.0526,Adaptive version of sparse attention,[[    0     3 14808   757   988    13 14144     7    15  1388     1]],the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence,"[[    8  1388  7701    16     8  4382 25326   120 14144     7    15 31220
     54 14721    72    11    28  1146  3410     1]]"
fbaf060004f196a286fef67593d2d76826f0304e,7.1429,"a training set that contains reviews from different domains in English (e.g.,","[[   0    3    9  761  356   24 2579 2456   45  315 3303    7   16 1566
    41   15    5  122    5    6]]","Amazon reviews BIBREF23 , BIBREF24, Yelp restaurant reviews dataset,  restaurant reviews dataset as part of a Kaggle competition BIBREF26","[[ 2536  2456     3  5972 25582   371  2773     3     6     3  5972 25582
    371  2266     6  7271    40   102  2062  2456 17953     6  2062  2456
  17953    38   294    13     3     9  2209   122  3537  2259     3  5972
  25582   371  2688     1]]"
37861be6aecd9242c4fdccdfcd06e48f3f1f8f81,0.0,"On the assisting-target language pair (child task) there are 57,813","[[    0   461     8     3 16881    18 24315  1612  3116    41 11495  2491
     61   132    33     3  3436     6   927  2368]]",5,[[305   1]]
2e37e681942e28b5b05639baaff4cd5129adb5fb,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
03e9ac1a2d90152cd041342a11293a1ebd33bcc3,0.0,No,[[  0 465   1]],NLG datasets,[[  445 24214 17953     7     1]]
89497e93980ab6d8c34a6d95ebf8c1e1d98ba43f,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
c08aab979dcdc8f4fe8ec1337c3c8290ab13414e,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
9bd080bb2a089410fd7ace82e91711136116af6c,0.0,Character-level model improves by 0.62% and 0.62% respectively.,"[[    0 20087    18  4563   825  1172     7    57     3 22787  5406    11
      3 22787  5406  6898     5     1]]",BiLSTM+CNN(grapheme-level) which turns out to be performing on par with BiLSTM+CNN(character-level) under the same configuration,"[[ 2106  7600  2305  1220   254 17235   599  9413    15   526    18  4563
     61    84  5050    91    12    36  5505    30   260    28  2106  7600
   2305  1220   254 17235   599 31886    18  4563    61   365     8   337
   5298     1]]"
45f7c03a686b68179cadb1413c5f3c1d373328bd,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses","[[ 2579   147   314  5898     3 26892  2984     6   379   147   220 11212
     28   423  1499     6    81  2847  7765   308  4481     6   180 25210
     18  3881   553  4949     6    11  1341  4301   106     9 18095    15
      7     1]]"
16535db1d73a9373ffe9d6eedaa2369cefd91ac4,0.0,"Wikipedea Corpus, Wikipedia",[[    0  2142  2168  3138    15     9 10052   302     6 16885     1]],PubMed+PMC (the data used for BioBERTv1.0) and/or CORD-19 (Covid-19 Open Research Dataset),"[[22057 20123  1220   345  3698    41   532   331   261    21  3318 12920
    382   208 12734    61    11    87   127   205 18400  4481    41  3881
   6961  4481  2384  2200  2747  2244    61     1]]"
2e37e681942e28b5b05639baaff4cd5129adb5fb,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
e3a2d8886f03e78ed5e138df870f48635875727e,18.1818,Twitter,[[   0 3046    1]],They developed a classifier to find ironic sentences in twitter data,"[[  328  1597     3     9   853  7903    12   253  3575   447 16513    16
  19010   331     1]]"
04d1b3b41fb62a7b896afe55e0e8bc5ffb8c6e39,0.0,3,[[  0 220   1]],three annotators,[[ 386   46 2264 6230    1]]
fe2666ace293b4bfac3182db6d0c6f03ea799277,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Acquire very large Vietnamese corpus and build a classifier with it, design a develop a big data warehouse and analytic framework, build a system to incrementally learn new corpora and interactively process feedback.","[[ 4292  1169    60   182   508 24532 11736   302    11   918     3     9
    853  7903    28    34     6   408     3     9  1344     3     9   600
    331 11625    11    46     9 14991  4732     6   918     3     9   358
     12 28351   120   669   126 11736   127     9    11  6076   120   433
   3160     5     1]]"
b3de9357c569fb1454be8f2ac5fcecaea295b967,40.0,"10,000",[[    0 13923     1]],"10,000 Arabic tweet dataset ",[[13923 19248 10657 17953     1]]
f318a2851d7061f05a5b32b94251f943480fbd15,9.8361,The authors draw from their findings that the emotional appeal of ISIS and non-violent religious,"[[    0    37  5921  3314    45    70  7469    24     8  3973  3958    13
     27 14408    11   529    18 11275   295  4761]]","both corpuses used words that aim to inspire readers while avoiding fear, actual words that lead to these effects are very different in the two contexts, our findings indicate that, using proper methods, automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda","[[  321 11736  1074     7   261  1234    24  2674    12  6512  3962   298
      3 16217  2971     6  1805  1234    24   991    12   175  1951    33
    182   315    16     8   192  2625     7     6    69  7469  6360    24
      6   338  2757  2254     6 10069  1693    13   508  5678    13  1499
   3471   331    54   370  3714  6574  6574   139  5273   343 25071     1]]"
5fa36dc8f7c4e65acb962fc484989d20b8fdaeec,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751,21.4286,"toxic comments, hate speech, profanity, insult, or abuse","[[    0 12068  2622     6  5591  5023     6  7108   152   485     6 21548
      6    42  5384     1]]","Targeted Insult (TIN): Posts which contain an insult/threat to an individual, group, or others , Untargeted (UNT): Posts containing non-targeted profanity and swearing.","[[12615    15    26 22615    17    41 25424    61    10  1844     7    84
   3480    46 21548    87   189    60   144    12    46   928     6   563
      6    42   717     3     6   597 24315    15    26    41 17161    61
     10  1844     7     3  6443   529    18 24315    15    26  7108   152
    485    11 23782    53     5     1]]"
307e8ab37b67202fe22aedd9a98d9d06aaa169c5,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
cd06d775f491b4a17c9d616a8729fd45aa2e79bf,50.0,sentiment class,[[   0 6493  853    1]],neutral sentiment,[[7163 6493    1]]
8bf7f1f93d0a2816234d36395ab40c481be9a0e0,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
45a5961a4e1d1c22874c4918e5c98bd3c0a670b3,0.0,2 types,[[   0  204 1308    1]],seven ,[[2391    1]]
41b2355766a4260f41b477419d44c3fd37f3547d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],sampling tweets from specific keywords create systematic and substancial racial biases in datasets,"[[17222 10657     7    45   806 12545   482 20036    11   769  5627  4703
      3    52     9  4703 14387    15     7    16 17953     7     1]]"
63488da6c7aff9e374561a24ba224e9ce7f65e40,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
07b70b2b799b9efa630e8737df8b1dd1284f032c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"a sample of  29,794 wikipedia articles and 2,794 arXiv papers ","[[    3     9  3106    13 14405  4440   591     3 29474  2984    11  3547
   4440   591  1584     4    23   208  5778     1]]"
fa527becb8e2551f4fd2ae840dbd4a68971349e0,100.0,LSTM,[[   0    3 7600 2305    1]],LSTM,[[   3 7600 2305    1]]
e82fa03f1638a8c59ceb62bb9a6b41b498950e1f,0.0,"Bi-LSTM, LSTM, LSTM, LSTM, LS","[[   0 2106   18 7600 2305    6    3 7600 2305    6    3 7600 2305    6
     3 7600 2305    6    3 7600]]","Two knowledge-based systems,
two traditional word expert supervised systems, six recent neural-based systems, and one BERT feature-based system.","[[ 2759  1103    18   390  1002     6   192  1435  1448  2205     3 23313
   1002     6  1296  1100 24228    18   390  1002     6    11    80   272
  24203  1451    18   390   358     5     1]]"
6a633811019e9323dc8549ad540550d27aa6d972,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
40c0f97c3547232d6aa039fcb330f142668dea4b,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
8060a773f6a136944f7b59758d08cc6f2a59693b,0.0,849090,[[   0    3 4608 2394 2394    1]],23085 hours of data,[[    3 13427  4433   716    13   331     1]]
223dc2b9ea34addc0f502003c2e1c1141f6b36a7,40.0,BIBREF7,[[    0     3  5972 25582   371   940     1]], reward learning algorithm BIBREF7,[[ 9676  1036 12628     3  5972 25582   371   940     1]]
e831041d50f3922265330fcbee5a980d0e2586dd,0.0,a normal reading paradigm,[[    0     3     9  1389  1183 20491     1]],read the sentences normally without any special instructions,[[  608     8 16513  4929   406   136   534  3909     1]]
352bc6de5c5068c6c19062bad1b8f644919b1145,0.0,2,[[  0 204   1]],535,[[ 305 2469    1]]
dcc1115aeaf87118736e86f3e3eb85bf5541281c,0.0,SST,[[   0  180 4209    1]],Random Forest,[[25942  6944     1]]
ef7212075e80bf35b7889dc8dd52fcbae0d1400a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"linked entities extracted from ELS's have issues because of low precision rates BIBREF11 and design challenges in training datasets BIBREF12 . These issues can be summarized into two parts: ambiguity and coarseness., the linked entities may also be too common to be considered an entity.","[[ 5229 12311 21527    45   262  7600    31     7    43   807   250    13
    731 11723  1917     3  5972 25582   371  2596    11   408  2428    16
    761 17953     7     3  5972 25582   371  2122     3     5   506   807
     54    36 21603    26   139   192  1467    10     3 24621   485    11
  27978   655     5     6     8  5229 12311   164    92    36   396  1017
     12    36  1702    46 10409     5     1]]"
4d28c99750095763c81bcd5544491a0ba51d9070,0.0,celebrities,[[    0 20076     1]],"Amitabh Bachchan, Ariana Grande, Barack Obama, Bill Gates, Donald Trump,
Ellen DeGeneres, J K Rowling, Jimmy Fallon, Justin Bieber, Kevin Durant, Kim Kardashian, Lady Gaga, LeBron James,Narendra Modi, Oprah Winfrey","[[  736    23 10309   107 16453  8694     6  1533 13662 15940     6 20653
   4534     6  3259 11118     7     6  7459  2523     6 20990   374 13714
     49    15     7     6   446   480 11768   697     6 16754  2589   106
      6 12446  2106    15  1152     6  8595  8633   288     6  6777 31279
      6  8571   350  4711     6   312 22780    29  2549     6   567   291
  12524  5073    23     6   411  5319   107  4871    89    60    63     1]]"
a4a1fcef760b133e9aa876ac28145ad98a609927,28.5714,number of clusters,[[   0  381   13 9068    7    1]],selection of word vectors,[[ 1801    13  1448 12938     7     1]]
a5505e25ee9ae84090e1442034ddbb3cedabcf04,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],F1 score result of 0.8099,[[ 377  536 2604  741   13 4097 2079 3264    1]]
21548433abd21346659505296fb0576e78287a74,0.0,Twitter,[[   0 3046    1]],The dataset from the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference.,"[[   37 17953    45     8 16874     9   434  3159   275  3504     9     7
      7   297 15571    13     8     3  3073  6858     3 16782 11253  1360
   4379     5     1]]"
7595260c5747aede0b32b7414e13899869209506,47.0588,IMDb corpus used for sentiment analysis is the IMDb corpus of movie reviews,"[[    0    27 11731   115 11736   302   261    21  6493  1693    19     8
     27 11731   115 11736   302    13  1974  2456]]",IMDb dataset of movie reviews,[[   27 11731   115 17953    13  1974  2456     1]]
46227b4265f1d300a5ed71bf40822829de662bc2,0.0,BIBREF1,[[    0     3  5972 25582   371   536     1]],"AMR Bank BIBREF10, CNN-Dailymail ( BIBREF11 BIBREF12 )","[[   71  9320  1925     3  5972 25582   371  1714     6 19602    18   308
      9  9203  1963    41     3  5972 25582   371  2596     3  5972 25582
    371  2122     3    61     1]]"
902b3123aec0f3a39319ffa9d05ab8e08a2eb567,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
ddb23a71113cbc092cbc158066d891cae261e2c6,0.0,YouTube videos,[[   0 5343 3075    1]],"main news channels, such as Yahoo News, The Guardian or The Washington Post","[[  711  1506  6047     6   224    38 15670  3529     6    37 17606    42
     37  2386  1844     1]]"
da55bd769721b878dd17f07f124a37a0a165db02,0.0,repetitive tasks,[[    0 24869  4145     1]]," What kind of topic (or sub-topic) a student has a problem with, At which examination mode (i.e., quiz, chapter level training or exercise, section level training or exercise, or final examination) the student is working right now,  the exact question number and exact problem formulation","[[  363   773    13  2859    41   127   769    18 19710    61     3     9
   1236    65     3     9   682    28     6   486    84  6498  2175    41
     23     5    15     5     6 20967     6  5800   593   761    42  2510
      6  1375   593   761    42  2510     6    42   804  6498    61     8
   1236    19   464   269   230     6     8  2883   822   381    11  2883
    682 20029     1]]"
9b97805a0c093df405391a85e4d3ab447671c86a,0.0,accuracy,[[   0 7452    1]],Exact Match (EM) and Macro-averaged F1 scores (F1) ,"[[ 1881  2708 12296    41  6037    61    11  1534  2771    18 28951    26
    377   536  7586    41   371  6982     1]]"
1dac4bc5af239024566fcb0f43bb9ff1c248ecec,100.0,No,[[  0 465   1]],No,[[465   1]]
203337c15bd1ee05763c748391d295a1f6415b9b,20.0,single attention model,[[   0  712 1388  825    1]],the model with multi-attention mechanism and a projected layer,"[[    8   825    28  1249    18 25615  8557    11     3     9 16037  3760
      1]]"
2fbb6322e485e7743ec3fb4bb02d44bf4b5ea8a6,0.0,WN16SEM dataset,[[    0     3 21170  2938   134  6037 17953     1]],English Wikipedia dump from June 2016,[[ 1566 16885 11986    45  1515  1421     1]]
1ff0fccf0dca95a6630380c84b0422bed854269a,0.0,LSTM-based model,[[   0    3 7600 2305   18  390  825    1]],by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews,"[[   57   761    46  1510 25288   358    30  2899   439 21306  3106    26
  16513    45  7271    40   102  2456     1]]"
7b4fb6da74e6bd1baea556788a02969134cf0800,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
da9c0637623885afaf023a319beee87898948fe9,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
b08f88d1facefceb87e134ba2c1fa90035018e83,100.0,No,[[  0 465   1]],No,[[465   1]]
9c44df7503720709eac933a15569e5761b378046,0.0,"English, Spanish, Japanese and Arabic",[[    0  1566     6  5093     6  4318    11 19248     1]],Unanswerable,[[ 597 3247 3321  179    1]]
3611a72f754de1e256fbd25b012197e1c24e8470,0.0,No,[[  0 465   1]],Unanswerable,[[ 597 3247 3321  179    1]]
bc16ce6e9c61ae13d46970ebe6c4728a47f8f425,14.2857,average dialog length is 8.7 min.,[[    0  1348 13463  2475    19  4848   940  3519     5     1]],4.5 turns per dialog (8533 turns / 1900 dialogs),"[[    3 12451  5050   399 13463    41  4433  4201  5050     3    87 19036
  13463     7    61     1]]"
44497509fdf5e87cff05cdcbe254fbd288d857ad,0.0,"Compared to the baseline, the merged words improve by about 1%.","[[    0     3 25236    12     8 20726     6     8     3 21726  1234  1172
     57    81     3  4704     5     1]]",Unanswerable,[[ 597 3247 3321  179    1]]
12c50dea84f9a8845795fa8b8c1679328bd66246,0.0,"WSJ10, WSJ10 Live Chat",[[   0    3 8439  683 1714    6    3 8439  683 1714 3306 9802    1]],"CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus","[[  205 17517 17953     3     6   460  1506 10739     7     6 14639 12559
    209 11736   302     1]]"
585626d18a20d304ae7df228c2128da542d248ff,0.0,accuracy,[[   0 7452    1]],"Coverage, Avg. MCC and avg. +ve F1 score","[[5620  545    6   71  208  122    5  283 2823   11    3    9  208  122
     5 1768  162  377  536 2604    1]]"
792f6d76d2befba2af07198584aac1b189583ae4,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],established task,[[2127 2491    1]]
6d4400f45bd97b812e946b8a682b018826e841f1,0.0,"LSTM, NVDM",[[    0     3  7600  2305     6     3 17058  7407     1]],"spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering","[[ 2140  4264    57   131   479    44     3     9  1232    13  1383     6
   7860    66 15293    28   294    18   858    18     7   855 10217   251
      6    27  2930 11884   900    77  9068    53     1]]"
2007bfb8f66e88a235c3a8d8c0a3b3dd88734706,32.2581,We use topic modeling to analyze the similarities and differences between extremist and non-extremist articles,"[[    0   101   169  2859 15309    12  8341     8 25758    11  5859   344
   5273   343    11   529    18 30650   343  2984]]",By using topic modeling and unsupervised emotion detection on ISIS materials and articles from Catholic women forum,"[[  938   338  2859 15309    11    73 23313 13868 10664    30    27 14408
   1397    11  2984    45  6502   887  5130     1]]"
02428a8fec9788f6dc3a86b5d5f3aa679935678d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced","[[ 1418    12 12700   853  4921 14877   237   116     8   866    13  1884
   1103    21   315  2287    19    73  3849   663    26     6    11   116
      8   853  3438    13     8 17953    19    73  3849   663    26     1]]"
cd32a38e0f33b137ab590e1677e8fb073724df7f,0.0,Chinese,[[   0 2830    1]],English ,[[1566    1]]
b0a18628289146472aa42f992d0db85c200ec64b,0.0,accuracy,[[   0 7452    1]],"precision, recall , F1 score",[[11723     6  7881     3     6   377   536  2604     1]]
f8f4e4a50d2b3fbd193327e79ea32d8d057e1414,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],crowdsourcing,[[ 4374 19035     1]]
8d074aabf4f51c8455618c5bf7689d3f62c4da1d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]]," ambiguous words, unknown words",[[    3 24621  1162  1234     6  7752  1234     1]]
f94b53db307685d572aefad52cd55f53d23769c2,0.0,by calculating variance from the outputs,[[    0    57     3 25956 27154    45     8  3911     7     1]], Fisher Information Ratio,[[14639  2784  6455    23    32     1]]
603fee7314fa65261812157ddfc2c544277fcf90,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],By 14 times.,[[938 968 648   5   1]]
ab9b0bde6113ffef8eb1c39919d21e5913a05081,0.0,"Result: 0 â€“ 82.0%, Result: 82.0%","[[    0     3 20119    10     3   632     3   104     3  4613     5  6932
      6     3 20119    10     3  4613     5  6932]]","Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. ","[[20578    53  3275     3   390    11  5879  7314  6315  1891     8   200
   1879   377 12100  7586     5    94    47  9526     5  2596    21   377
   4770 17953     3     6  1401     5  4225    21     8   166 30729    13
    638   567 10376 11590     6    11   604     5  2368    21     8   511
  30729    13   638   567 10376 11590     5     1]]"
ddf5e1f600b9ce2e8f63213982ef4209bab01fd8,0.0,"SQuAD, ODSQA",[[    0   180  5991  6762     6   411  3592 23008     1]],Spoken-SQuAD testing set,[[8927 2217   18  134 5991 6762 2505  356    1]]
c49ee6ac4dc812ff84d255886fd5aff794f53c39,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
384bf1f55c34b36cb03f916f50bbefade6c86a75,100.0,No,[[  0 465   1]],No,[[465   1]]
005cca3c8ab6c3a166e315547a2259020f318ffb,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],FIGREF10,[[11376  4386   371  1714     1]]
c00ce1e3be14610fb4e1f0614005911bb5ff0302,0.0,Random Forest classifier,[[    0 25942  6944   853  7903     1]],"relu, selu, tanh",[[8318   76    6  142   40   76    6    3   17  152  107    1]]
9eabb54c2408dac24f00f92cf1061258c7ea2e1a,0.0,a corpus of 256 sentences and 828 syllables containing,"[[    0     3     9 11736   302    13     3 19337 16513    11   505  2577
      3     7    63   195   179     7     3  6443]]","paragraph, lines, textspan element (paragraph segmentation, line segmentation, Information on physical page segmentation(for PDF only))","[[ 8986     6  2356     6 14877  2837  3282    41  6583  9413  5508   257
      6   689  5508   257     6  2784    30  1722   543  5508   257   599
   1161  3948   163    61    61     1]]"
eda4869c67fe8bbf83db632275f053e7e0241e8c,0.0,WikiText-TL-02,[[    0  2142  2168 13598    17    18 12733    18  4305     1]],Unanswerable,[[ 597 3247 3321  179    1]]
df0257ab04686ddf1c6c4d9b0529a7632330b98e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]]," On the other hand, phase 1 of Go-Explore finds an optimal trajectory with approximately half the interactions with the environment, Moreover, the trajectory length found by Go-Explore is always optimal (i.e. 30 steps) whereas both DQN++ and DRQN++ have an average length of 38 and 42 respectively., Especially interesting is that the performance of DRRN is substantially lower than that of the Go-Explore Seq2Seq model","[[  461     8   119   609     6  3944   209    13  1263    18 12882   322
     15 12902    46  6624 29912    28  3241   985     8  9944    28     8
   1164     6     3  7371     6     8 29912  2475   435    57  1263    18
  12882   322    15    19   373  6624    41    23     5    15     5   604
   2245    61     3 10339   321   309  2247   567 16702    11     3  3913
   2247   567 16702    43    46  1348  2475    13  6654    11  6426  6898
      5     6     3 14232  1477    19    24     8   821    13     3  3913
  14151    19 15284  1364   145    24    13     8  1263    18 12882   322
     15   679  1824   357   134    15  1824   825     1]]"
009ce6f2bea67e7df911b3f93443b23467c9f4a1,0.0,BiLSTM with max pooling,[[   0 2106 7600 2305   28 9858 2201   53    1]],"QANet , BIBREF14,  fine-tuned a BERT model","[[    3 23008  9688     3     6     3  5972 25582   371  2534     6  1399
     18    17   444    26     3     9   272 24203   825     1]]"
6424e442b34a576f904d9649d63acf1e4fdefdfc,0.0,Wiktionary,[[    0  2142 12696  1208     1]],Unanswerable,[[ 597 3247 3321  179    1]]
3a6e843c6c81244c14730295cfb8b865cd7ede46,0.0,ANOVA,[[    0    71 30103     1]],"BIBREF9 , BIBREF8 ","[[    3  5972 25582   371  1298     3     6     3  5972 25582   371   927
      1]]"
79a44a68bb57b375d8a57a0a7f522d33476d9f33,0.0,F1 score,[[   0  377  536 2604    1]],"Relation Generation (RG), Content Selection (CS), Content Ordering (CO)","[[28898 11946    41 12912   201  7185 22246    41  4778   201  7185  5197
     53    41  5911    61     1]]"
891c2001d6baaaf0da4e65b647402acac621a7d2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],They use the first principal component of a word's contextualized representation in a given layer as its static embedding.,"[[  328   169     8   166  3218  3876    13     3     9  1448    31     7
  28131  1601  6497    16     3     9   787  3760    38   165 14491 25078
     26    53     5     1]]"
d46c0ea1ba68c649cc64d2ebb6af20202a74a3c7,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],It may lead to poor rare word representations and word analogies.,"[[   94   164   991    12  2714  3400  1448  6497     7    11  1448 10552
    725     5     1]]"
aef607d2ac46024be17b1ddd0ed3f13378c563a6,14.8148,by the number of words that are under-translated,"[[    0    57     8   381    13  1234    24    33   365    18 11665 26860
      1]]","we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair","[[   62   987     3   324   936    46  2264  6230    12 12616  3783     8
    365    18 11665 26860  3785  1234     6    11    44   709   192    46
   2264  6230  3783   284  3785    18 13397    32 24874  3116     1]]"
aa6d956c2860f58fc9baea74c353c9d985b05605,0.0,"accuracy, recall, F-score",[[   0 7452    6 7881    6  377   18    7 9022    1]],ROUGE,[[  391 26260   427     1]]
b85fc420eb2f77f6f14f375cc1fcc5155eb5c0a8,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
c17b609b0b090d7e8f99de1445be04f8f66367d4,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Highest scores for ROUGE-1, ROUGE-2 and ROUGE-L on CNN/DailyMail test set are 43.85, 20.34 and 39.90 respectively; on the XSum test set 38.81, 16.50 and 31.27 and on the NYT test set 49.02, 31.02 and 45.55","[[ 1592   222  7586    21   391 26260   427  2292     6   391 26260   427
   4949    11   391 26260   427    18   434    30 19602    87   308     9
   9203  7098   794   356    33  8838     5  4433     6   460     5  3710
     11  6352     5  2394  6898   117    30     8     3     4   134   440
    794   356  6654     5  4959     6 10128  1752    11  2664     5  2555
     11    30     8  5825   382   794   356  9526     5  4305     6  2664
      5  4305    11  3479     5  3769     1]]"
71ba1b09bb03f5977d790d91702481cc406b3767,28.5714,accuracy of 82.0%,[[   0 7452   13    3 4613    5 6932    1]],75.1% and 75.6% accuracy,[[6374    5 4704   11 6374    5 6370 7452    1]]
425bd2ccfd95ead91d8f2b1b1c8ab9fc3446cb82,100.0,accuracy,[[   0 7452    1]],accuracy,[[7452    1]]
cc354c952b5aaed2d4d1e932175e008ff2d801dd,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Females are given higher sentiment intensity when predicting anger, joy or valence, but males are given higher sentiment intensity when predicting  fear.
African American names are given higher score on the tasks of anger, fear, and sadness intensity prediction,  but European American names are given higher scores on joy and valence task.","[[27144     7    33   787  1146  6493 13182   116     3 29856 11213     6
   3922    42     3  2165  1433     6    68  5069     7    33   787  1146
   6493 13182   116     3 29856  2971     5  3850   797  3056    33   787
   1146  2604    30     8  4145    13 11213     6  2971     6    11 24784
  13182 21332     6    68  1611   797  3056    33   787  1146  7586    30
   3922    11     3  2165  1433  2491     5     1]]"
1d197cbcac7b3f4015416f0152a6692e881ada6c,7.6923,they encode the relative distance between sentence words and the answer by position-aware attention and extract,"[[    0    79 23734     8  5237  2357   344  7142  1234    11     8  1525
     57  1102    18     9  3404  1388    11  5819]]",Using the OpenIE toolbox and applying heuristic rules to select the most relevant relation.,"[[   3 3626    8 2384 5091 1464 2689   11 6247    3   88  450 3040 2219
    12 1738    8  167 2193 4689    5    1]]"
6becff2967fe7c5256fe0b00231765be5b9db9f1,8.3333,auxiliary regularization terms,[[    0     3 31086  1646  1707  1353     1]],"a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution","[[    3     9  1646  1707  1657  1968    28  7163   753     6     8  2411
      3    35 12395    63    13   853  3438  1646  1707  1657     6     8
    480   434 12355   122  1433   344  2848    11 15439   853  3438     1]]"
cc850bc8245a7ae790e1f59014371d4f35cd46d7,0.0,manually selected by editors,[[    0 12616  2639    57 18008     1]],They use a multi-class classifier to determine the section it should be cited,"[[  328   169     3     9  1249    18  4057   853  7903    12  2082     8
   1375    34   225    36     3 11675     1]]"
b14217978ad9c3c9b6b1ce393b1b5c6e7f49ecab,0.0,"On a variety of tasks including semantic similarity, passage retrieval, and cross-en","[[    0   461     3     9  1196    13  4145   379 27632  1126   485     6
   5454 24515   138     6    11  2269    18    35]]","Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions","[[16107   209    10  2415   127     9   970 10435   342 11860 25072     3
  31636    23   106     6 16107   204    10 29153   746     1]]"
49764eee7fb523a6a28375cc699f5e0220b81766,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
d7644c674887ca9708eb12107acd964ae53b216d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)","[[ 7720    13 16366   120  7878    15    26  2570  9977     7    41   134
   2823   201  8941    13     8  7199     7    17 16366   120  7878    15
     26  2570  9977    41  7600  2823   201  7720    13   101  1639   120
   7878    15    26  2570  9977     7    41   518  2823   201  8941    13
      8  7199     7    17   101  1639   120  7878    15    26  2570  9977
     41   434   518  2823   201  5267  4401    13     8  7199     7    17
    101  1639   120  7878    15    26  2570  9977    41 20293  2823   201
  23836 28552    53   638 16995    41  2823   201  5140   480    18  9022
   7720    41 23405   201  3128     7   485    41    26    61     1]]"
d8627ba08b7342e473b8a2b560baa8cdbae3c7fd,100.0,No,[[  0 465   1]],No,[[465   1]]
22744c3bc68f120669fc69490f8e539b09e34b94,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
d3aa0449708cc861a51551b128d73e11d62207d2,0.0,"Freebase2M BIBREF2, BIBREF3","[[    0  1443 10925   357   329     3  5972 25582   371   357     3     6
      3  5972 25582   371   519     1]]","break the relation names into word sequences,  relation-level and word-level relation representations, bidirectional LSTMs (BiLSTMs),  residual learning method","[[ 1733     8  4689  3056   139  1448  5932     7     6  4689    18  4563
     11  1448    18  4563  4689  6497     7     6  2647 26352     3  7600
   2305     7    41   279    23  7600  2305     7   201 27687  1036  1573
      1]]"
d3093062aebff475b4deab90815004051e802aa6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information","[[  180 12623    28    73    23  5096     6   600  2375     6    11  6467
   5096   753     6   180 12623    28  1348  1448 25078    26    53     6
    180 12623    28  1348 13421  1448 25078    26    53     7     6 19602
      6     3    15 14907  1193 24817   138  1484  9709  3426     7     6
    180 12623    11  1659  1036  2250    28  1670   251     1]]"
2ccc26e11df4eb26fcccdd1f446dc749aff5d572,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
ad1be65c4f0655ac5c902d17f05454c0d4c4a15d,5.2632,MCScript provides a dataset for assessing the contribution of script knowledge to machine comprehension,"[[    0     3  3698 18255   795     3     9 17953    21     3 20861     8
   6275    13  4943  1103    12  1437 27160     1]]","Distribution of category labels, number of answerable-not answerable questions, number of text-based and script-based questions, average text, question, and answer length, number of questions per text","[[21968    13  3295 11241     6   381    13  1525   179    18  2264  1525
    179   746     6   381    13  1499    18   390    11  4943    18   390
    746     6  1348  1499     6   822     6    11  1525  2475     6   381
     13   746   399  1499     1]]"
31d695ba855d821d3e5cdb7bea638c7dbb7c87c7,0.0,"Hadamard product, Hadamard multiplication, BIBREF12","[[    0 10118   265   986   556     6 10118   265   986  1249 13555     6
      3  5972 25582   371  2122     1]]","two stacked GRU layers, attention for one model while for the another one it consists of attention and conflict combined, fully-connected layers","[[  192     3 24052   350  8503  7500     6  1388    21    80   825   298
     21     8   430    80    34     3  6848    13  1388    11  4129  3334
      6  1540    18 19386  7500     1]]"
dc1cec824507fc85ac1ba87882fe1e422ff6cffb,0.0,NABEL,[[   0  445 5359 3577    1]],3500 questions collected from the internet and books.,[[ 220 2560  746 4759   45    8 1396   11 1335    5    1]]
bf52c01bf82612d0c7bbf2e6a5bb2570c322936f,18.1818,high correlation results are observed for BIBREF1 and BIBREF2 respectively,"[[    0   306 18712   772    33  6970    21     3  5972 25582   371   536
     11     3  5972 25582   371   357  6898     1]]","Using Pearson corelation measure,  for example, ROUGE-1-P is 0.257 and ROUGE-3-F 0.878.","[[    3  3626 29300  2583  6105  3613     6    21   677     6   391 26260
    427  2292    18   345    19     3 18189  3436    11   391 26260   427
   3486    18   371     3 22384  3940     5     1]]"
2ea4347f1992b0b3958c4844681ff0fe4d0dd1dd,16.6667,"module embedding, CNN/RNN, Transformer","[[    0  6008 25078    26    53     6 19602    87 14151   567     6 31220
      1]]","Embedding Layer, Neural Network Layers, Loss Function, Metrics","[[    3 17467    15  7249 22697     6  1484  9709  3426 22697     7     6
   3144     7 21839     6  1212  3929     7     1]]"
96b07373756d7854bccc3c12e8d41454ab8741f5,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
20e38438471266ce021817c6364f6a46d01564f2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds","[[ 7573     7   284   761   677    28     3     9  1293    16  7385    12
   1514   599   536    18   102    61  3229     6    11    48  1293  4896
   1427  1112    38   761 14942     1]]"
25c1c4a91f5dedd4e06d14121af3b5921db125e9,100.0,No,[[  0 465   1]],No,[[465   1]]
9bb7ae50bff91571a945c1af025ed2e67714a788,0.0,"attention modeling the entire context, i.e., the series of posts that trigger an","[[    0  1388 15309     8  1297  2625     6     3    23     5    15     5
      6     8   939    13  3489    24  7294    46]]",hLSTM,[[   3  107 7600 2305    1]]
fd2c6c26fd0ab3c10aae4f2550c5391576a77491,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
044f922604b4b3f42ae381419fd5cd5624fa0637,0.0,"in some cases, attention is seen as capturing the relevance of multiple source words where their relevance","[[    0    16   128  1488     6  1388    19   894    38     3 18147     8
  20208    13  1317  1391  1234   213    70 20208]]","For certain POS tags, e.g. VERB, PRON.","[[  242   824     3 16034 12391     6     3    15     5   122     5     3
  16174   279     6   276 13044     5     1]]"
31b92c03d5b9be96abcc1d588d10651703aff716,0.0,3rd,[[  0 220  52  26   1]],0.7033,[[4097 2518 4201    1]]
6c96e910bd98c9fd58ba2050f99b9c9bac69840a,0.0,2,[[  0 204   1]],Unanswerable,[[ 597 3247 3321  179    1]]
49c32a2a64eb41381e5f12ccea4150cac9f3303d,0.0,accuracy,[[   0 7452    1]],Unanswerable,[[ 597 3247 3321  179    1]]
f875337f2ecd686cd7789e111174d0f14972638d,0.0,"BLEU, EMOT",[[    0     3  8775 12062     6     3  6037  6951     1]],"Affective Text, Fairy Tales, ISEAR","[[   71    89  4075   757  5027     6  4506    63 19098     7     6  6827
  19356     1]]"
8eefa116e3c3d3db751423cc4095d1c4153d3a5f,28.5714,CoNLL2003 shared task BIBREF15,"[[    0   638   567 10376 23948  2471  2491     3  5972 25582   371  1808
      1]]","The GENIA Corpus , CoNLL2003","[[   37     3  5042 26077 10052   302     3     6   638   567 10376 23948
      1]]"
5bc1dc6ebcb88fd0310b21d2a74939e35a4c1a11,34.7826,"English, German, French, Spanish, Japanese and Spanish",[[   0 1566    6 2968    6 2379    6 5093    6 4318   11 5093    1]],"English
French
Spanish
German
Greek
Bulgarian
Russian
Turkish
Arabic
Vietnamese
Thai
Chinese
Hindi
Swahili
Urdu
Finnish","[[ 1566  2379  5093  2968  6781 15536    29  4263 15423 19248 24532 12806
   2830 25763 17085   107   173    23  4575  1259 28124     1]]"
63bb2040fa107c5296351c2b5f0312336dad2863,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Word clusters are extracted using k-means on word embeddings,"[[ 4467  9068     7    33 21527   338     3   157    18   526  3247    30
   1448 25078    26    53     7     1]]"
3c3807f226ba72fc41f59f0338f12a49a0c35605,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
b6f7fadaa1bb828530c2d6780289f12740229d84,0.0,German-English,[[    0  2968    18 26749     1]],"English-German, English-French.",[[ 1566    18 24518     6  1566    18   371    60  5457     5     1]]
cf171fad0bea5ab985c53d11e48e7883c23cdc44,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Those tweets are mostly much noisier and shorter compared to the reviews in the movie corpus. In total, there are 1,716 tweets. 973 of them are negative and 743 of them are positive.","[[    3  3405 10657     7    33  3323   231  1746     7   972    11 10951
      3  2172    12     8  2456    16     8  1974 11736   302     5    86
    792     6   132    33  1914   940  2938 10657     7     5   668  4552
     13   135    33  2841    11   489  4906    13   135    33  1465     5
      1]]"
c21b87c97d1afac85ece2450ee76d01c946de668,0.0,BIBREF3,[[    0     3  5972 25582   371   519     1]],pointer networks with coverage mechanism (PG-net),[[ 500   49 5275   28 2591 8557   41 7861   18 1582   61    1]]
2ddb51b03163d309434ee403fef42d6b9aecc458,0.0,"a graph structure to map senones to phonemes, and a pronunciation","[[    0     3     9  8373  1809    12  2828     3     7    35   782     7
     12   951  2687     6    11     3     9 30637]]","LF-MMI Attention
Seq2Seq 
RNN-T 
Char E2E LF-MMI 
Phone E2E LF-MMI 
CTC + Gram-CTC","[[    3 18962    18  8257   196 20748   679  1824   357   134    15  1824
    391 17235    18   382  7435   262   357   427     3 18962    18  8257
    196  8924   262   357   427     3 18962    18  8257   196   205  3838
   1768 20278    18   254  3838     1]]"
132f752169adf6dc5ade3e4ca773c11044985da4,20.0,a dataset of 280 million sentences,[[    0     3     9 17953    13     3 17518   770 16513     1]],"Wang et al., CrowdFlower dataset ","[[18102     3    15    17   491     5     6 15343    26 15390    49 17953
      1]]"
c85b6f9bafc4c64fc538108ab40a0590a2f5768e,0.0,F1 score of 82.0%,[[   0  377  536 2604   13    3 4613    5 6932    1]],column Ens Test in Table TABREF19,[[ 6710   695     7  2300    16  4398     3  3221 25582   371  2294     1]]
cc608df2884e1e82679f663ed9d9d67a4b6c03f3,15.3846,accuracy,[[   0 7452    1]],"Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy.","[[16361    97     6  3487  5962    41   526  2528    63     6 13823     6
   1229 19703   201 11723     6  7881     6   377  4347  7452     5     1]]"
9a7ba5ed1779c664d2cac92494a43517d3e87c96,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],WSC collection,[[ 549 4112 1232    1]]
a6a48de63c1928238b37c2a01c924b852fe752f8,0.0,"AMR, Meaning Representation, Recurrent Networks, Deep Learning","[[    0    71  9320     6 25148   419 12640   257     6   419 14907  3426
      7     6  9509  6630     1]]","Lead-3 model,  Lead-1-AMR, BIBREF0 ","[[12208  3486   825     6 12208  2292    18   188  9320     6     3  5972
  25582   371   632     1]]"
2b3cac7af10d358d4081083962d03ea2798cf622,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
72e4e26d0dd79c590c28b10938952a9f9497ff1e,0.0,"LSTMs, LSTMs, LSTMs","[[   0    3 7600 2305    7    6    3 7600 2305    7    6    3 7600 2305
     7    1]]","generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models","[[    3 11600     3     9 14193    45  1383    62   169    46  1895  7556
     18  2685  1225  4648     6   796  1308    13  5932    12  5932  2250
      1]]"
6b9b9e5d154cb963f6d921093539490daa5ebbae,0.0,variants of the BERT model,[[    0  6826     7    13     8   272 24203   825     1]],"clipped $\mathit {PMI}$, $\mathit {NNEGPMI}$","[[ 5516  3138  1514     2  3357 10536     3     2  6218   196     2  3229
      6  1514     2  3357 10536     3     2   567  4171  8049  7075     2
   3229     1]]"
981fd79dd69581659cb1d4e2b29178e82681eb4d,0.0,DA gating-vector is more important in the NLG task because the proposed,"[[    0     3  4296     3   122  1014    18   162  5317    19    72   359
     16     8   445 24214  2491   250     8  4382]]","Introduce a ""Refinement Adjustment LSTM-based component"" to the decoder","[[30600     3     9    96  1649 13536   297 17820   297     3  7600  2305
     18   390  3876   121    12     8    20  4978    52     1]]"
869feb7f47606105005efdb6bea1c549824baea0,20.0,"TweetQA dataset contains 57,432 news articles and 4,714 questions and answers","[[    0 25335 23008 17953  2579     3  3436     6   591  2668  1506  2984
     11  6464   940  2534   746    11  4269     1]]","10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs","[[10372  3914   927  2984     6 12864  4440   591 10657     7     6    11
  10670  3072   940  4374 15551   822    18  3247  3321 14152     1]]"
2ad4d3d222f5237ed97923640bc8e199409cbe52,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
1c0ba6958da09411deded4a14dfea5be55687619,0.0,82,[[   0    3 4613    1]],Unanswerable,[[ 597 3247 3321  179    1]]
0f567251a6566f65170a1329eeeb5105932036b2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]]," BIBREF14, BIBREF15 ",[[    3  5972 25582   371  2534     6     3  5972 25582   371  1808     1]]
3b371ea554fa6639c76a364060258454e4b931d4,0.0,YouTube videos,[[   0 5343 3075    1]],NowThisNews Facebook page,[[ 852 3713 6861    7 1376  543    1]]
79a44a68bb57b375d8a57a0a7f522d33476d9f33,0.0,F1 score,[[   0  377  536 2604    1]]," Relation Generation (RG) , Content Selection (CS),  Content Ordering (CO)","[[28898 11946    41 12912    61     3     6  7185 22246    41  4778   201
   7185  5197    53    41  5911    61     1]]"
6236762b5631d9e395f81e1ebccc4bf3ab9b24ac,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
4625cfba3083346a96e573af5464bc26c34ec943,5.7143,LS improves by 0.5 points and 0.7 points in accuracy,"[[    0     3  7600  1172     7    57     3 12100   979    11     3 22426
    979    16  7452     1]]","For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.
For the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.","[[  242     8  2142  2168   434  8240    15 17953     6     8  4179   147
  20726   445  7323    19  1682  2596     3  8775 12062     6     3 18596
    377   439 13011    11  1300  4560   180 22410     5   242     8  2142
   2168   134  1982    40 17953     6     8  4179   147 20726   445  7323
     19  4848  4118     3  8775 12062     5     1]]"
c598028815066089cc1e131b96d6966d2610467a,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
b08f88d1facefceb87e134ba2c1fa90035018e83,100.0,No,[[  0 465   1]],No,[[465   1]]
3bf0306e9bd044f723e38170c13455877b2aeec3,0.0,a sensationalism scorer is trained on an article without labeling it as clickbait,"[[   0    3    9 9118 6835 2604   52   19 4252   30   46 1108  406 3783
    53   34   38 1214 9441   17]]",classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss,"[[  853  8587  9118   138    11   529    18     7    35     7   257   138
  12392     7   338     3     9    80    18 18270 19602    28     3     9
  14865  2269     3    35 12395    63  1453     1]]"
2439b6b92d73f660fe6af8d24b7bbecf2b3a3d72,23.5294,by comparing the candidate and the candidate mentions on the same document,"[[    0    57     3 14622     8  4775    11     8  4775  2652     7    30
      8   337  1708     1]]",By calculating Macro F1 metric at the document level.,"[[  938     3 25956  1534  2771   377   536     3  7959    44     8  1708
    593     5     1]]"
455d4ef8611f62b1361be4f6387b222858bb5e56,0.0,dialog policies,[[    0 13463  3101     1]],CrowdFlower,[[15343    26 15390    49     1]]
0682bf049f96fa603d50f0fdad0b79a5c55f6c97,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
ff6c9af28f0e2bb4fb6a69f124665f8ceb966fbc,0.0,sports clubs,[[   0 2100 8122    1]],"Galatasaray, FenerbahÃ§e","[[15210    17     9     7     9  2866     6  4163   687 17670  8970    15
      1]]"
5913930ce597513299e4b630df5e5153f3618038,33.3333,Adaptive version of sparse attention,[[    0     3 14808   757   988    13 14144     7    15  1388     1]],We introduce sparse attention into the Transformer architecture,[[  101  4277 14144     7    15  1388   139     8 31220  4648     1]]
477d9d3376af4d938bb01280fe48d9ae7c9cf7f7,0.0,accuracy,[[   0 7452    1]],"BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4), METEOR (MET), ROUGE-L (R-L)","[[    3  8775 12062  2292    41   279  6982     6     3  8775 12062  4949
     41   279  7318     6     3  8775 12062  3486    41   279  5268     6
      3  8775 12062  4278    41   279  7256     6  7934  3463  2990    41
  24506   201   391 26260   427    18   434    41   448    18   434    61
      1]]"
00050f7365e317dc0487e282a4c33804b58b1fb3,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
ea56148a8356a1918bedcf0a99ae667c27792cfe,21.0526,CoNLL 2014,[[    0   638   567 10376  1412     1]], FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) ,"[[  377  4770   794   331    41  4853   439 14145     7    61    11     8
    192  2433 30729     7    13     8   638   567 10376  1412  7105    26
  16107 17953 19684   439 14145     7    61     1]]"
9bd080bb2a089410fd7ace82e91711136116af6c,7.1429,Character-level model improves by 0.62% and 0.62% respectively.,"[[    0 20087    18  4563   825  1172     7    57     3 22787  5406    11
      3 22787  5406  6898     5     1]]","On OurNepali test dataset Grapheme-level representation model achieves average 0.16% improvement, on ILPRL test dataset it achieves maximum 1.62% improvement","[[  461   421   567    15  6459    23   794 17953     3 21094    15   526
     18  4563  6497   825  1984     7  1348     3 16029  6370  4179     6
     30    27  6892 12831   794 17953    34  1984     7  2411     3 15062
   5406  4179     1]]"
b66c9a4021b6c8529cac1a2b54dacd8ec79afa5f,28.5714,local context is the state of the union of all entities in a given global context,"[[    0   415  2625    19     8   538    13     8  7021    13    66 12311
     16     3     9   787  1252  2625     1]]","global (the whole document) and the local context (e.g., the section/topic) ","[[ 1252    41   532   829  1708    61    11     8   415  2625    41    15
      5   122     5     6     8  1375    87 19710    61     1]]"
db72a78a7102b5f0e75a4d9e1a06a3c2e7aabb21,7.1429,"long vowels, which are explicitly written, and short vowels, aka diacri","[[    0   307 22121  3573     6    84    33 21119  1545     6    11   710
  22121  3573     6     3  5667  1227     9  2685]]","Farasa BIBREF31, MADAMIRA BIBREF29, RDI (Rashwan et al., 2015), MIT (Belinkow and Glass, 2015), Microsoft ATKS BIBREF28","[[ 5186     9     7     9     3  5972 25582   371  3341     6   283 16759
    329 19426     3  5972 25582   371  3166     6     3 10255   196    41
    448  3198  3877     3    15    17   491     5     6  1230   201     3
  12604    41  2703  4907  2381    11  7642     6  1230   201  2803  8043
  13383     3  5972 25582   371  2577     1]]"
3941401a182a3d6234894a5c8a75d48c6116c45c,0.0,Twitter,[[   0 3046    1]],Tweets related to CyberAttack and tweets related to PoliticianDeath,"[[25335     7  1341    12 14183   188    17    17  4365    11 10657     7
   1341    12  6907 29562  2962     9   189     1]]"
d8627ba08b7342e473b8a2b560baa8cdbae3c7fd,100.0,No,[[  0 465   1]],No,[[465   1]]
b634ff1607ce5756655e61b9a6f18bc736f84c83,0.0,Twitter,[[   0 3046    1]],Energy with accuracy of 0.538,[[ 4654    28  7452    13     3 12100  3747     1]]
c87fcc98625e82fdb494ff0f5309319620d69040,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],hashtag features contain whether there is any hashtag in the tweet,[[25354   753  3480   823   132    19   136 25354    16     8 10657     1]]
05887a8466e0a2f0df4d6a5ffc5815acd7d9066a,0.0,HMM,[[   0  454 8257    1]],Target-1,[[12615  2292     1]]
f3c204723da53c7c8ef4dc1018ffbee545e81056,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
48bd71477d5f89333fa7ce5c4556e4d950fb16ed,6.6667,"long vowels, which are explicitly written, and short vowels, aka diacri","[[    0   307 22121  3573     6    84    33 21119  1545     6    11   710
  22121  3573     6     3  5667  1227     9  2685]]","affixes, leading and trailing characters in words and stems, and the presence of words in large gazetteers of named entities","[[    3  4127  2407    15     7     6  1374    11  5032    53  2850    16
   1234    11  6269     7     6    11     8  3053    13  1234    16   508
   7424  1954   277    13  2650 12311     1]]"
d509081673f5667060400eb325a8050fa5db7cc8,0.0,"38,432",[[   0 6654    6  591 2668    1]],2174 million tokens for English and 989 million tokens for Russian,"[[ 1401  4581   770 14145     7    21  1566    11   668  3914   770 14145
      7    21  4263     1]]"
b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54,0.0,"Bi-directional, multi-lingual",[[    0  2106    18 26352     6  1249    18 25207     1]],"BIBREF19, BIBREF20",[[    3  5972 25582   371  2294     6     3  5972 25582   371  1755     1]]
7b4fb6da74e6bd1baea556788a02969134cf0800,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
fff5c24dca92bc7d5435a2600e6764f039551787,0.0,StackExchange,[[    0     3 19814  5420 13073     1]],Unanswerable,[[ 597 3247 3321  179    1]]
26e2d4d0e482e6963a76760323b8e1c26b6eee91,0.0,"TIMIT-2013, TIBREF21, TIBREF22, ","[[    0     3  5494 12604    18 11138     6     3  5494 25582   371  2658
      6     3  5494 25582   371  2884     6     3]]"," this paper makes use of the official training and test sets, covering in total 630 speakers with 8 utterances each","[[   48  1040   656   169    13     8  2314   761    11   794  3369     6
   6013    16   792     3 26106  7215    28   505     3  5108   663     7
    284     1]]"
ed2eb4e54b641b7670ab5a7060c7b16c628699ab,0.0,"SQuAD, NLI and text classification",[[    0   180  5991  6762     6   445  8159    11  1499 13774     1]],SR,[[   3 6857    1]]
8e44c02c2d9fa56fb74ace35ee70a5add50b52ae,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
a3d9b101765048f4b61cbd3eaa2439582ebb5c77,23.5294,"English, Chinese",[[   0 1566    6 2830    1]],"English, Chinese, Korean, we translated the English and Chinese datasets into more languages, with Google Translate","[[ 1566     6  2830     6  9677     6    62 15459     8  1566    11  2830
  17953     7   139    72  8024     6    28  1163 30355    15     1]]"
500a8ec1c56502529d6e59ba6424331f797f31f0,0.0,"70,000",[[    0     3 28891     1]],700 ,[[12283     1]]
1ed49a8c07ef0ac15cfa6b7decbde6604decbd5b,0.0,Multi30K,[[   0 4908 1458  439    1]],the English-German dataset,[[    8  1566    18 24518 17953     1]]
d46c0ea1ba68c649cc64d2ebb6af20202a74a3c7,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
88e62ea7a4d1d2921624b8480b5c6b50cfa5ad42,22.2222,second order coâ€“occurrence matrix,[[    0   511   455   576   104 16526 16826     1]],"frequencies of the other words which occur with both of them (i.e., second order coâ€“occurrences)","[[23446    13     8   119  1234    84  4093    28   321    13   135    41
     23     5    15     5     6   511   455   576   104 16526     7    61
      1]]"
709feae853ec0362d4e883db8af41620da0677fe,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters","[[12520     7 10488    18    51 23552     3 26352  1388    54    36  3028
     38     3     9  1681    12  2828 13154    11   843    18 12097 14152
     12     8  6497    13  3785     6 12520     7 10488    18    51 23552
      3 26352  1388  1672     7    12   726  1388    12     8 12487  2850
     13   284  4655    11  4061     8   415   655  1675   344  2850    38
      3     9  2210 12520     7 10488  1293    21  1388     6 12520     7
  10488  1293   163     3  4610     7    30     8  2357   344  2850     1]]"
58ef2442450c392bfc55c4dc35f216542f5f2dbb,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
6c96e910bd98c9fd58ba2050f99b9c9bac69840a,0.0,2,[[  0 204   1]],Unanswerable,[[ 597 3247 3321  179    1]]
cb6a8c642575d3577d1840ca2f4cd2cc2c3397c5,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
c0bee6539eb6956a7347daa9d2419b367bd02064,100.0,No,[[  0 465   1]],No,[[465   1]]
891c2001d6baaaf0da4e65b647402acac621a7d2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]], by taking the first principal component (PC) of its contextualized representations in a given layer,"[[   57   838     8   166  3218  3876    41  4051    61    13   165 28131
   1601  6497     7    16     3     9   787  3760     1]]"
42084c41343e5a6ae58a22e5bfc5ce987b5173de,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
43eecc576348411b0634611c81589f618cd4fddf,0.0,"LSTMs, LSTMs with a minimum of 256 possible inputs","[[    0     3  7600  2305     7     6     3  7600  2305     7    28     3
      9  2559    13     3 19337   487  3785     7]]","SeqGAN, LeakGAN, MaliGAN, DialogGAN, DPGAN","[[  679  1824   517  5033     6   312  1639   517  5033     6  2148    23
    517  5033     6 25843   517  5033     6   309 24127   567     1]]"
6844683935d0d8f588fa06530f5068bf3e1ed0c0,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"$\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus","[[ 1514     2  3357 10536     3     2  6218   196     2   599   210     6
     75    61  3229  1550    12  2841    16 16925   116     8  1448    18
   1018  6327  3116  1514   599   210     6    75    61  3229   405    59
   2385    16     8   761 11736   302     1]]"
5a81732d52f64e81f1f83e8fd3514251227efbc7,0.0,"a Spanish dataset containing 9,473 annotations for 9,300 tweets","[[    0     3     9  5093 17953     3  6443  9902  4177   519 30729     7
     21  9902  5426 10657     7     1]]","BIBREF12 , BIBREF13","[[    3  5972 25582   371  2122     3     6     3  5972 25582   371  2368
      1]]"
d9980676a83295dda37c20cfd5d58e574d0a4859,0.0,LSTM BIBREF12,[[    0     3  7600  2305     3  5972 25582   371  2122     1]],"copy, copy-marked, copy-dummies",[[ 2405     6  2405    18 16376     6  2405    18    26 30885     1]]
9122de265577e8f6b5160cd7d28be9e22da752b2,34.7826,Detection accuracy of 0.82 vs 0.83 for scope resolution,"[[    0     3 31636    23   106  7452    13  4097  4613     3   208     7
   4097  4591    21  7401  3161     1]]",Figures FIGREF1 and FIGREF1 contain a summary of the papers addressing speculation detection and scope resolution,"[[ 7996     7 11376  4386   371   536    11 11376  4386   371   536  3480
      3     9  9251    13     8  5778     3 14198 22547 10664    11  7401
   3161     1]]"
d650101712e36594bd77b45930a990402a455222,0.0,"BIBREF13, BIBREF14","[[    0     3  5972 25582   371  2368     6     3  5972 25582   371  2534
      1]]","build a new one, collect INLINEFORM0 cases from China Judgments Online","[[  918     3     9   126    80     6  2868  3388 20006 24030   632  1488
     45  1473  3736    26   122  4128  1777     1]]"
81d193672090295e687bc4f4ac1b7a9c76ea35df,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach","[[  169     8  1448   357   162    75 12628     6   482   633    73 23313
    609    18  8810   753     6  3806  1708 12938     7    11  3305   135
     38  3785   139     8   380 12938  4096    41   134 12623    61  1295
      1]]"
f85ca6135b101736f5c16c5b5d40895280016023,0.0,BIBREF4 and BIBREF6,"[[    0     3  5972 25582   371   591    11     3  5972 25582   371   948
      1]]",baseline transformer BIBREF8,[[20726 19903     3  5972 25582   371   927     1]]
0ee73909ac638903da4a0e5565c8571fc794ab96,0.0,"Using a morphologically rich set of words, we assembled a set of","[[    0     3  3626     3     9     3  8886  4478   120  2354   356    13
   1234     6    62 17583     3     9   356    13]]","adequacy, precision and ranking values",[[    3     9   221  2436  4710     6 11723    11 11592  2620     1]]
72f7ef55e150e16dcf97fe443aff9971a32414ef,0.0,Unanswerable,[[   0  597 3247 3321  179    1]], +1.86,[[    3 18446     5  3840     1]]
1522ccedbb1f668958f24cca070f640274bc2549,0.0,concept-map-based MDS,[[    0  2077    18 11576    18   390   283  3592     1]],"Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2","[[11801    28   738  3586    10    41   427  7480   257  1212  3929     7
   1375    61 28464     6   419 16482     6   377   536    18     7  9022
      7     6   180  3929    17  1588     6  7934  3463  2990     6   391
  26260   427  4949     1]]"
60ce4868af45753c9e124e64e518c32376f12694,0.0,"12,594 unique hashtags and their associated tweets annotated in a multi-","[[    0 10440  3390   591   775 25354     7    11    70  1968 10657     7
     46  2264   920    16     3     9  1249    18]]",Stanford Sentiment Analysis Dataset BIBREF36,"[[19796  4892  2998   295 10582  2747  2244     3  5972 25582   371  3420
      1]]"
10d450960907091f13e0be55f40bcb96f44dd074,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94,0.0,"LSTM, LSTM-ID",[[   0    3 7600 2305    6    3 7600 2305   18 4309    1]],"Linear SVM, RBF SVM, and Random Forest","[[ 4919   291   180 12623     6   391 19780   180 12623     6    11 25942
   6944     1]]"
3cd185b7adc835e1c4449eff81222f5fc15c8500,27.2727,they use a combination of content-based features and human-like spammers' re,"[[    0    79   169     3     9  2711    13   738    18   390   753    11
    936    18  2376 13655  5567    31     3    60]]",Extract features from the LDA model and use them in a binary classification task,"[[18742   753    45     8   301  4296   825    11   169   135    16     3
      9 14865 13774  2491     1]]"
85590bb26fed01a802241bc537d85ba5ef1c6dc2,8.6957,"they are controlled by a set of annotators, they are given annotated annotation","[[    0    79    33  6478    57     3     9   356    13    46  2264  6230
      6    79    33   787    46  2264   920 30729]]", we use several of the MCQA baseline models first introduced in BIBREF0,"[[   62   169   633    13     8     3  3698 23008 20726  2250   166  3665
     16     3  5972 25582   371   632     1]]"
c6a0b9b5dabcefda0233320dd1548518a0ae758e,0.0,Word2Vec,[[   0 4467  357  553   15   75    1]],CJFA encoder,[[  205   683  4795 23734    52     1]]
60ce4868af45753c9e124e64e518c32376f12694,48.4848,"12,594 unique hashtags and their associated tweets annotated in a multi-","[[    0 10440  3390   591   775 25354     7    11    70  1968 10657     7
     46  2264   920    16     3     9  1249    18]]","1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset","[[ 1914   357  3651 21306  2639 10657     7    16     8 19796  4892  2998
    295 10582  2747  2244     3  5972 25582   371  3420     6    66 10440
   3390   591   775  1566 25354     7    11    70  1968 10657     7    45
      8   337 19796 17953     1]]"
286078813136943dfafb5155ee15d2429e7601d9,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"In case of Freebase knowledge base, LiLi model had better F1 score than the single model by 0.20 , 0.01, 0.159 for kwn, unk, and all test Rel type.  The values for WordNet are 0.25, 0.1, 0.2. 
","[[   86   495    13  1443 10925  1103  1247     6  1414   434    23   825
    141   394   377   536  2604   145     8   712   825    57  4097  1755
      3     6     3 11739  4347  4097 27904    21     3   157   210    29
      6     3  6513     6    11    66   794   419    40   686     5    37
   2620    21  4467  9688    33  4097  1828     6     3 16029     6     3
  18189     5     1]]"
f697d00a82750b14376fe20a5a2b249e98bebe9b,100.0,BiLSTM-CRF,[[   0 2106 7600 2305   18 4545  371    1]],Bi-LSTM-CRF,[[2106   18 7600 2305   18 4545  371    1]]
d4e5e3f37679ff68914b55334e822ea18e60a6cf,60.0,he and she,[[  0   3  88  11 255   1]],gendered word pairs like he and she,[[ 7285    15    26  1448 14152   114     3    88    11   255     1]]
a712718e6596ba946f29a99838d82f95b9ebb1ce,0.0,3%,[[   0    3 5170    1]],7.36% on accuracy and 9.69% on F1 score,"[[    3 27914  6370    30  7452    11  5835  3951  1454    30   377   536
   2604     1]]"
de53af4eddbc30c808d90b8a11a29217d377569e,0.0,"French hashtags, Emotions', S","[[    0     3     2   371    60  5457     2 25354     7     6     3     2
    427  7259     7    31     6     3     2   134]]","FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.","[[ 7547  6861     7     6 19602     6 25043     6   368  1060  5324     6
   2900  3835     6   454  2999  6029  1844   101    23    52    26  3529
      6    37 17606     6 31074  3426     6  6176    53  2892     6  1210
   6176    53 12316     6 12446  2106    15  1152     6 29005    32   221
    106     6  2526  2444    15 17396     6  6118     5     1]]"
adbf33c6144b2f5c40d0c6a328a92687a476f371,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
12f7fac818f0006cf33269c9eafd41bbb8979a48,50.0,biLSTM,[[   0 2647 7600 2305    1]],"Inception V3, biLSTM",[[  86 7239  584 6355 2647 7600 2305    1]]
e5c8e9e54e77960c8c26e8e238168a603fcdfcc6,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
9ca447c8959a693a3f7bdd0a2c516f4b86f95718,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],tweets are annotated with only Favor or Against for two targets - Galatasaray and FenerbahÃ§e,"[[10657     7    33    46  2264   920    28   163 16717    42     3 20749
     21   192  8874     3    18 15210    17     9     7     9  2866    11
   4163   687 17670  8970    15     1]]"
f2e8497aa16327aa297a7f9f7d156e485fe33945,0.0,manually reviewed,[[    0 12616  9112     1]],Experienced medical doctors used a linguistic annotation tool to annotate entities.,"[[ 7187    26  1035  6659   261     3     9     3 24703 30729  1464    12
     46  2264   342 12311     5     1]]"
c7486d039304ca9d50d0571236429f4f6fbcfcf7,0.0,non-english,[[   0  529   18 4606   40 1273    1]],Turkish,[[15423     1]]
3321d8d0e190d25958e5bfe0f3438b5c2ba80fd1,9.5238,"5,952 short factoid questions paired with a taxnomie developed by","[[    0  7836  3301   357   710   685    32    23    26   746     3 13804
     28     3     9  1104 32134 23140  1597    57]]",from 3rd to 9th grade science questions collected from 12 US states,"[[  45  220   52   26   12  668  189 2769 2056  746 4759   45  586  837
  2315    1]]"
0f7867f888109b9e000ef68965df4dde2511a55f,0.0,using a simple voting scheme and achieve new state-of-the-art results on the,"[[    0   338     3     9   650 10601  5336    11  1984   126   538    18
    858    18   532    18  1408   772    30     8]]","Among all the classes predicted by several models, for each test sentence, class with most votes are picked. In case of a tie, one of the most frequent classes are picked randomly.","[[    3  7264    66     8  2287 15439    57   633  2250     6    21   284
    794  7142     6   853    28   167 11839    33  4758     5    86   495
     13     3     9  6177     6    80    13     8   167  8325  2287    33
   4758 21306     5     1]]"
6024039bbd1118c5dab86c41cce1175d99f10a25,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0, NTCIR PatentMT Parallel Corpus BIBREF1","[[ 6578 19268  6564 20335    49   102    17 10052   302    41   188 20452
     61     3  5972 25582   371   632     6   445  3838  5705 20565  7323
  27535 10052   302     3  5972 25582   371   536     1]]"
111afb77cfbf4c98e0458606378fa63a0e965e36,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
de53af4eddbc30c808d90b8a11a29217d377569e,0.0,"French hashtags, Emotions', S","[[    0     3     2   371    60  5457     2 25354     7     6     3     2
    427  7259     7    31     6     3     2   134]]","FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney","[[ 7547  6861     7     6 19602     6 25043     6   368  1060  5324     6
   2900  3835     6   454  2999  6029  1844   101    23    52    26  3529
      6    37 17606     6 31074  3426     6  6176    53  2892     6  1210
   6176    53 12316     6 12446  2106    15  1152     6 29005    32   221
    106     6  2526  2444    15 17396     6  6118     1]]"
36ae003c7cb2a1bbfa90b89c671bc286bd3b3dfd,33.3333,character's identities are modelled using human-like attributes,"[[    0  1848    31     7 26203    33   825  1361   338   936    18  2376
  12978     1]]","attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics","[[12978    33  4187    57   936 13569    11    70  5709     7    13     8
   2850     6    11    33     3 29604    28   936    18  2376  6803     1]]"
38f58f13c7f23442d5952c8caf126073a477bac0,0.0,82.0%,[[   0    3 4613    5 6932    1]],EM Score of 51.10,[[    3  6037 17763    13 11696     5  1714     1]]
d571e0b0f402a3d36fb30d70cdcd2911df883bc7,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
a15bc19674d48cd9919ad1cf152bf49c88f4417d,100.0,DSTC2,[[   0    3 3592 3838  357    1]],DSTC2,[[   3 3592 3838  357    1]]
f4496316ddd35ee2f0ccc6475d73a66abf87b611,0.0,WN18M,[[    0     3 21170  2606   329     1]],a benchmark dataset created by ceccarelli2013learning from the CoNLL 2003 data,"[[    3     9 15705 17953   990    57   197    75  1720  7999 11138 20779
     45     8   638   567 10376  3888   331     1]]"
5c5aeee83ea3b34f5936404f5855ccb9869356c1,0.0,"Identifying Relationships between Heads, Identifying Relationships between Heads","[[    0     3 23393    53 28898  2009     7   344  3642     7     6     3
  23393    53 28898  2009     7   344  3642     7]]","four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German","[[  662  1437  7314  4145    10  2968     3 13114  1566     6  4318     3
  13114  1566     6  3871    29     3 13114  1566     6  1566     3 13114
   2968     1]]"
3b371ea554fa6639c76a364060258454e4b931d4,0.0,YouTube videos,[[   0 5343 3075    1]],NowThisNews Facebook page,[[ 852 3713 6861    7 1376  543    1]]
21548433abd21346659505296fb0576e78287a74,33.3333,Twitter,[[   0 3046    1]],Twitter dataset provided by the organizers,[[ 3046 17953   937    57     8 14250     7     1]]
32a3c248b928d4066ce00bbb0053534ee62596e7,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The task of predicting MSD tags: V, PST, V.PCTP, PASS.","[[   37  2491    13     3 29856  5266   308 12391    10   584     6   276
   4209     6   584     5  4051  7150     6     3 30317     5     1]]"
1d9aeeaa6efa1367c22be0718f5a5635a73844bd,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],information about the context and sequential nature of the text,[[  251    81     8  2625    11 29372  1405    13     8  1499     1]]
ed67359889cf61fa11ee291d6c378cccf83d599d,0.0,a bidirectional recurrent neural network called BiLSTM,"[[    0     3     9  2647 26352     3    60 14907 24228  1229   718  2106
   7600  2305     1]]", pre-trained GloVe word vectors ,[[  554    18    17 10761  9840   553    15  1448 12938     7     1]]
a130306c6662ff489df13fb3f8faa7cba8c52a21,40.0,pooling function,[[   0 2201   53 1681    1]],dynamic average pooling,[[4896 1348 2201   53    1]]
1ec152119cf756b16191b236c85522afeed11f59,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"They measure self-similarity, intra-sentence similarity and maximum explainable variance of the embeddings in the upper layers.","[[  328  3613  1044    18 26714   485     6  6344    18  5277  1433  1126
    485    11  2411  3209   179 27154    13     8 25078    26    53     7
     16     8  4548  7500     5     1]]"
0ba3ea93eef5660a79ea3c26c6a270eac32dfa4c,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
a6d37b5975050da0b1959232ae756fc09e5f87e8,0.0,BIBREF13,[[    0     3  5972 25582   371  2368     1]],"The encoder is essentially the same as tweet2vec, with the input as words instead of characters","[[   37 23734    52    19     3  8317     8   337    38 10657   357   162
     75     6    28     8  3785    38  1234  1446    13  2850     1]]"
a99fdd34422f4231442c220c97eafc26c76508dd,100.0,No,[[  0 465   1]],No,[[465   1]]
49c32a2a64eb41381e5f12ccea4150cac9f3303d,0.0,accuracy,[[   0 7452    1]],"F-score, Kappa",[[  377    18     7  9022     6 12232   102     9     1]]
ad08b215dca538930ef1f50b4e49cd25527028ad,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
5a65ad10ff954d0f27bb3ccd9027e3d8f7f6bb76,0.0,"LSTM, LSTM+attention, LSTM+attention, LSTM","[[    0     3  7600  2305     6     3  7600  2305  1220 25615     6     3
   7600  2305  1220 25615     6     3  7600  2305]]","Akbik et al. (2018), Link et al. (2012)","[[ 4823   115    23   157     3    15    17   491     5 28068     6  7505
      3    15    17   491     5 24705     1]]"
dac087e1328e65ca08f66d8b5307d6624bf3943f,100.0,No,[[  0 465   1]],No,[[465   1]]
506d21501d54a12d0c9fd3dbbf19067802439a04,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"terms common to hosts' descriptions of popular Airbnb properties, like 'subway', 'manhattan', or 'parking'","[[ 1353  1017    12  9855    31 15293    13  1012 30247  2605     6   114
      3    31  7304  1343    31     6     3    31   348   547    17   152
     31     6    42     3    31  6334    53    31     1]]"
0ee73909ac638903da4a0e5565c8571fc794ab96,7.1429,"Using a morphologically rich set of words, we assembled a set of","[[    0     3  3626     3     9     3  8886  4478   120  2354   356    13
   1234     6    62 17583     3     9   356    13]]","50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.","[[  943   936    46  2264  6230     3  8232     3     9  6504  3106    13
    910  7314     7    57  1980    15  2436  4710     6  9507  4392    11
   1879 11592    30     3     9  7670  2700  2643     5     1]]"
3611a72f754de1e256fbd25b012197e1c24e8470,100.0,No,[[  0 465   1]],No,[[465   1]]
9ca447c8959a693a3f7bdd0a2c516f4b86f95718,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
968b7c3553a668ba88da105eff067d57f393c63f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Viral tweets are the ones that are retweeted more than 1000 times,"[[ 1813  4900 10657     7    33     8  2102    24    33     3    60    17
   1123    15  1054    72   145  5580   648     1]]"
f8f4e4a50d2b3fbd193327e79ea32d8d057e1414,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Contributors record voice clips by reading from a bank of donated sentences.,"[[21489  5535  1368  2249 16234    57  1183    45     3     9  2137    13
  13207 16513     5     1]]"
43f074bacabd0a355b4e0f91a1afd538c0a6244f,5.5556,"by generating an expectation from the crowd, the model is trained with an expectation on micropost","[[    0    57     3 11600    46 18775    45     8  4374     6     8   825
     19  4252    28    46 18775    30  2179  5950]]","workers are first asked to find those microposts where the model predictions are deemed correct,  asked to find the keyword that best indicates the class of the microposts","[[ 2765    33   166  1380    12   253   273  2179  5950     7   213     8
    825 20099    33     3 10863  2024     6  1380    12   253     8 15693
     24   200  9379     8   853    13     8  2179  5950     7     1]]"
79f9468e011670993fd162543d1a4b3dd811ac5d,0.0,2.3 BLEU gains,[[    0     3 18561     3  8775 12062 11391     1]],"ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.","[[   71 23203    65  5153  4179   147    66 20726  2254   338 26236     7
     15   399  9247   485    11     3     7   109    89    18  8775 12062
      3  7959     5    37  2411  7211   399  9247   485  4179   668  3420
      6  2938    19  6886    21     3  6037   567  6892  9887   549  7323
  17953    11  4678     6  3628    21  2847  5911 17953     5     1]]"
e5c8e9e54e77960c8c26e8e238168a603fcdfcc6,0.0,No,[[  0 465   1]],From all reported results proposed method (NB+Lex) shows best accuracy on all 3 datasets - some models are not evaluated and not available in literature.,"[[ 1029    66  2196   772  4382  1573    41 14972  1220   434   994    61
   1267   200  7452    30    66   220 17953     7     3    18   128  2250
     33    59 14434    11    59   347    16  6678     5     1]]"
7595260c5747aede0b32b7414e13899869209506,15.3846,IMDb corpus used for sentiment analysis is the IMDb corpus of movie reviews,"[[    0    27 11731   115 11736   302   261    21  6493  1693    19     8
     27 11731   115 11736   302    13  1974  2456]]",IMDb,[[   27 11731   115     1]]
fb2b536dc8e442dffab408db992b971e86548158,0.0,agreement of 0.88 and 0.88,[[   0 2791   13 4097 4060   11 4097 4060    1]],Unanswerable,[[ 597 3247 3321  179    1]]
c4628d965983934d7a2a9797a2de6a411629d5bc,0.0,Yes,[[   0 2163    1]],There is nothing specific about the approach that depends on medical recommendations. The approach combines graph data and text data into a single embedding.,"[[  290    19  1327   806    81     8  1295    24  5619    30  1035  5719
      5    37  1295     3 15256  8373   331    11  1499   331   139     3
      9   712 25078    26    53     5     1]]"
dd53baf26dad3d74872f2d8956c9119a27269bd5,7.6923,manually reviewed,[[    0 12616  9112     1]],"1264 instances from simulated data, 1280 instances by adding two out-of-distribution symptoms and 944 instances manually delineated from the symptom checking portions of real-word dialogues","[[  586  4389 10316    45     3 31126   331     6   586  2079 10316    57
   2651   192    91    18   858    18    26   159  5135  1575  3976    11
    668  3628 10316 12616    20   747   920    45     8     3 18018  6450
  17622    13   490    18  6051  7478     7     1]]"
2ddb51b03163d309434ee403fef42d6b9aecc458,0.0,"a graph structure to map senones to phonemes, and a pronunciation","[[    0     3     9  8373  1809    12  2828     3     7    35   782     7
     12   951  2687     6    11     3     9 30637]]",Unanswerable,[[ 597 3247 3321  179    1]]
aa6d956c2860f58fc9baea74c353c9d985b05605,0.0,"accuracy, recall, F-score",[[   0 7452    6 7881    6  377   18    7 9022    1]],ROUGE BIBREF22 unigram score,"[[  391 26260   427     3  5972 25582   371  2884    73    23  5096  2604
      1]]"
08b57deb237f15061e4029b6718f1393fa26acce,0.0,political pundits of the Washington Post,[[   0 1827 4930   26 7085   13    8 2386 1844    1]],people in the US that use Amazon Mechanical Turk,[[  151    16     8   837    24   169  2536 24483 23694     1]]
ce2b921e4442a21555d65d8ce4ef7e3bde931dfc,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)","[[ 2379    41    89    52   201  4263    41    52    76   201 19248    41
    291   201  2830    41   172   107   201 25763    41   107    23   201
     11 24532    41  2099    61     1]]"
a74190189a6ced2a2d5b781e445e36f4e527e82a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],By evaluating the performance of the approach using accuracy and AUC,"[[  938     3 17768     8   821    13     8  1295   338  7452    11    71
   6463     1]]"
114934e1a1e818630ff33ac5c4cd4be6c6f75bb2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],NCEL consistently outperforms various baselines with a favorable generalization ability,"[[ 9187  3577  8182    91   883  2032     7   796 20726     7    28     3
      9 15229   879  1707  1418     1]]"
b91671715ad4fad56c67c28ce6f29e180fe08595,0.0,Twitter,[[   0 3046    1]],None,[[14794     1]]
71e4ba4e87e6596aeca187127c0d088df6570c57,0.0,"LSTM, MATREF2, LSTM, MATREF3","[[    0     3  7600  2305     6     3 18169  4386   371  4482     3  7600
   2305     6     3 18169  4386   371   519     1]]",Unanswerable,[[ 597 3247 3321  179    1]]
da55bd769721b878dd17f07f124a37a0a165db02,0.0,repetitive tasks,[[    0 24869  4145     1]],Unanswerable,[[ 597 3247 3321  179    1]]
e2e977d7222654ee8d983fd8ba63b930e9a5a691,0.0,Bi-directional,[[    0  2106    18 26352     1]],uni-directional RNN,[[   73    23    18 26352   391 17235     1]]
2740e3d7d33173664c1c5ab292c7ec75ff6e0802,0.0,"CAS, MAT, MAT, MAT, MAT, MAT, ","[[    0     3 18678     6     3 18169     6     3 18169     6     3 18169
      6     3 18169     6     3 18169     6     3]]","diacritized corpus that was used to train the RDI BIBREF7 diacritizer and the Farasa diacritizer BIBREF31, WikiNews test set BIBREF31,  large collection of fully diacritized classical texts (2.7M tokens) from a book publisher","[[ 1227     9 12563  1601 11736   302    24    47   261    12  2412     8
      3 10255   196     3  5972 25582   371   940  1227     9 12563  8585
     11     8  5186     9     7     9  1227     9 12563  8585     3  5972
  25582   371  3341     6  2142  2168  6861     7   794   356     3  5972
  25582   371  3341     6   508  1232    13  1540  1227     9 12563  1601
  11702 14877    41 21280   329 14145     7    61    45     3     9   484
  14859     1]]"
111afb77cfbf4c98e0458606378fa63a0e965e36,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
3fad42be0fb2052bb404b989cc7d58b440cd23a0,14.2857,"linear and semi-supervised learning frameworks, supervised learning framework, user-system communication framework","[[    0 13080    11  4772    18 23313  1036  4732     7     6     3 23313
   1036  4732     6  1139    18  3734  1901  4732]]",Unif and Stopword,[[ 597   99   11 9078 6051    1]]
ef3567ce7301b28e34377e7b62c4ec9b496f00bf,0.0,IMDb Movie Reviews,[[    0    27 11731   115 10743 16305     1]],Groningen Meaning Bank,[[ 8554    29    53    35 25148  1925     1]]
01f4a0a19467947a8f3bdd7ec9fac75b5222d710,0.0,BLEU and TER scores,[[    0     3  8775 12062    11     3  5946  7586     1]],"Unlabeled sentence-level F1, perplexity, grammatically judgment performance","[[  597  9339   400    26  7142    18  4563   377  4347   399  9247   485
      6     3 16582   144  6402  7661   821     1]]"
48088a842f7a433d3290eb45eb0d4c6ab1d8f13c,0.0,"LSTMs, LSTMs with cross-validation","[[    0     3  7600  2305     7     6     3  7600  2305     7    28  2269
     18 27769   257     1]]","NaÃ¯ve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)","[[ 1823     2   162  2474    15     7    41 14972   201  7736  3040   419
  22430    41 12564   201  4224 29011  5879    41   134 12623   201 25942
   6944     7    41  8556   201 10771  4741     3 16481    15    26  7552
      7    41  3443   382   201  1193 24817   138  1484  9709  3426     7
     41   254 17235   201   419 14907  1484  9709  3426     7    41 14151
    567    61     1]]"
88e5d37617e14d6976cc602a168332fc23644f19,0.0,WN15 and FB15k,[[    0     3 21170  1808    11     3 15586  1808   157     1]]," `Conversations Gone Awry' dataset, subreddit ChangeMyView","[[    3     2  4302  2660  1628   350   782    71   210   651    31 17953
      6   769  1271    26   155  5968  7008 15270     1]]"
c32adef59efcb9d1a5b10e1d7c999a825c9e6d9a,0.0,"English, Spanish and Italian",[[   0 1566    6 5093   11 4338    1]],Unanswerable,[[ 597 3247 3321  179    1]]
ca4daafdc23f4e23d933ebabe682e1fe0d4b95ed,0.0,by analyzing the lexicon of the spoken words,"[[    0    57     3 19175     8     3 30949   106    13     8 11518  1234
      1]]",built over all the data and therefore includes the vocabulary from both the training and testing sets,"[[ 1192   147    66     8   331    11  2459   963     8 19067    45   321
      8   761    11  2505  3369     1]]"
2dbf6fe095cd879a9bf40f110b7b72c8bdde9475,0.0,a hierarchical model based on a multi-model LSTM with ,"[[    0     3     9  1382  7064  1950   825     3   390    30     3     9
   1249    18 21770     3  7600  2305    28     3]]",the group-specific embedding representations are tied through a global embedding,"[[    8   563    18  9500 25078    26    53  6497     7    33 10422   190
      3     9  1252 25078    26    53     1]]"
a69a59b6c0ab27bcee1a780d6867df21e30aec08,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
2ceced87af4c8fdebf2dc959aa700a5c95bd518f,100.0,No,[[  0 465   1]],No,[[465   1]]
a87a009c242d57c51fc94fe312af5e02070f898b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],logistic regression models,[[28820 26625  2250     1]]
dd53baf26dad3d74872f2d8956c9119a27269bd5,0.0,manually reviewed,[[    0 12616  9112     1]],held out from the simulated data,[[ 1213    91    45     8     3 31126   331     1]]
73bddaaf601a4f944a3182ca0f4de85a19cdc1d2,0.0,WN15k,[[    0     3 21170  1808   157     1]],Daily Mail news articles,[[8496 9614 1506 2984    1]]
f56d07f73b31a9c72ea737b40103d7004ef6a079,0.0,JP Morgan BIBREF22 and XML BIBREF23,"[[    0   446   345 11147     3  5972 25582   371  2884    11     3     4
   6858     3  5972 25582   371  2773     1]]","The homographic dataset contains 2,250 contexts, 1,607 of which contain a pun. The heterographic dataset consists of 1,780 contexts with 1,271 containing a pun.","[[   37 13503 14797 17953  2579  3547 11434  2625     7     6  1914  3328
    940    13    84  3480     3     9  4930     5    37 26481 14797 17953
      3  6848    13  1914   940  2079  2625     7    28  1914  2555   536
      3  6443     3     9  4930     5     1]]"
0d9fcc715dee0ec85132b3f4a730d7687b6a06f4,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],the number of distinct word recognition outputs that an attacker can induce,"[[    8   381    13  6746  1448  5786  3911     7    24    46 22600    54
  21151     1]]"
a267d620af319b48e56c191aa4c433ea3870f6fb,18.1818,"labels in a car-buying data set that contain the words ""car"", ""family","[[    0 11241    16     3     9   443    18 14584    53   331   356    24
   3480     8  1234    96  1720  1686    96 15474]]",car ,[[443   1]]
ab54cd2dc83141bad3cb3628b3f0feee9169a556,26.6667,the best performing algorithm,[[    0     8   200  5505 12628     1]],A hybrid model consisting of best performing popularity-based approach with the best similarity-based approach,"[[  71 9279  825 5608   53   13  200 5505 9897   18  390 1295   28    8
   200 1126  485   18  390 1295    1]]"
af5730d82535464cedfa707a03415ac2e7a21295,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set, Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus), RTE5","[[16885    41   858  5986    18  6007  2850    61    45     8 12616     3
  22579  5396  3013 23008   761   356     6  9950   120   389  2264   920
   3325    18 13026  7800    41 23010   254    61    13     8  2384   797
    868 10052   302   201   391  3463   755     1]]"
18c5d366b1da8447b5404eab71f4cc658ba12e6f,0.0,LSTMs,[[   0    3 7600 2305    7    1]],"Stanford NER, spaCy 2.0, recurrent model with a CRF top layer","[[19796     3 18206     6  4174   254    63  6864     6     3    60 14907
    825    28     3     9   205  8556   420  3760     1]]"
e2b0cd30cf56a4b13f96426489367024310c3a05,0.0,The evaluation was done by a team of 3 people.,[[   0   37 5002   47  612   57    3    9  372   13  220  151    5    1]],Spearman's rank-order correlation,[[ 8974   291   348    31     7 11003    18  9397 18712     1]]
27de1d499348e17fec324d0ef00361a490659988,17.3913,"23,700 queries",[[    0 12992  9295 13154     1]]," 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains and 1,200 out-of-scope queries.","[[12992  9295 13154     6   379   204 22092    16    18 11911 13154  6013
   4261  9508     7     6    84    54    36     3 31801   139   335   879
   3303     7    11  1914  3632    91    18   858    18 11911 13154     5
      1]]"
005cca3c8ab6c3a166e315547a2259020f318ffb,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],The resulting taxonomy of the framework is shown in Figure FIGREF10,"[[   37     3  5490  1104    32  3114    63    13     8  4732    19  2008
     16  7996 11376  4386   371  1714     1]]"
375b281e7441547ba284068326dd834216e55c07,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"seeker interacts with a real conversational interface, intermediary (or the wizard) receives the seeker's message and performs different information seeking actions","[[ 2762    49  6815     7    28     3     9   490  3634   138  3459     6
  25960    63    41   127     8 25027    61   911     7     8  2762    49
     31     7  1569    11  1912     7   315   251  3945  2874     1]]"
6d1217b3d9cfb04be7fcd2238666fa02855ce9c5,0.0,"Bi-LSTM, LSTM with atomic element labeling","[[    0  2106    18  7600  2305     6     3  7600  2305    28     3 20844
   3282  3783    53     1]]","BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21","[[ 2106  7600  2305  5972 25582   371  2534     6  2106  7600  2305  1220
    254 17235  5972 25582   371  1755     6  2106  7600  2305  1220  4545
    371  5972 25582   371  4347  2106  7600  2305  1220   254 17235  1220
   4545   371  5972 25582   371  4482 19602   825  5972 25582   371   632
     11 19796   205  8556   825  5972 25582   371  2658     1]]"
8756b7b9ff5e87e4efdf6c2f73a0512f05b5ae3f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16","[[ 9840   553    15     6 16504 25078    26    53     7     3  5972 25582
    371  2534     6  3967 21892 25078    26    53     7     3  5972 25582
    371  2938     1]]"
91e326fde8b0a538bc34d419541b5990d8aae14b,0.0,"WSJ2016, WSJ2016, WSJ2016, WSJ2016","[[    0     3  8439   683 11505     6     3  8439   683 11505     6     3
   8439   683 11505     6     3  8439   683 11505]]",RWTH German-English dataset,[[  391   518  4611  2968    18 26749 17953     1]]
bdc91d1283a82226aeeb7a2f79dbbc57d3e84a1a,20.0,performance improved by +3.5% on the QNLI task and +3.5% on the ,"[[    0   821  3798    57  1768  5787  2712    30     8  1593 18207   196
   2491    11  1768  5787  2712    30     8     3]]",The average score improved by 1.4 points over the previous best result.,"[[   37  1348  2604  3798    57     3 14912   979   147     8  1767   200
    741     5     1]]"
abe2393415e533cb06311e74ed1c5674cff8571f,0.0,accuracy,[[   0 7452    1]],"BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency","[[    3  8775 12062     3     6   445 13582     3     6  7934  3463  2990
      3     6   391 26260   427    18   434     6   205 13162    52     3
      6  5002  4943     6  6569  5002     6   936  5002     6  2559  4777
   5002     6  1448  3505  1080    41   518  3316   201   685  3471  6854
     11    70  1308     6  6720  4392   807     6  1845  2020    13     8
   3911    21   999   169    16     3     9  1506  3193     1]]"
31d695ba855d821d3e5cdb7bea638c7dbb7c87c7,0.0,"Hadamard product, Hadamard multiplication, BIBREF12","[[    0 10118   265   986   556     6 10118   265   986  1249 13555     6
      3  5972 25582   371  2122     1]]","GRU-based encoder, interaction block, and classifier consisting of stacked fully-connected layers.","[[  350  8503    18   390 23734    52     6  6565  2463     6    11   853
   7903  5608    53    13     3 24052  1540    18 19386  7500     5     1]]"
f225a9f923e4cdd836dd8fe097848da06ec3e0cc,28.5714,"SQuAD BIBREF3, QQuAD Knowledge Base BIBREF1","[[    0   180  5991  6762     3  5972 25582   371  6355  1593  5991  6762
  16113  8430     3  5972 25582   371   536     1]]",SQuAD,[[ 180 5991 6762    1]]
a4d115220438c0ded06a91ad62337061389a6747,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Facebook status update messages,[[1376 2637 2270 4175    1]]
12159f04e0427fe33fa05af6ba8c950f1a5ce5ea,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding","[[  381    13  9068     7     6  6677   701    16  9068    53     6  1801
     13  1448 12938     7     6  2034   812    11  9340    13 25078    26
     53     1]]"
50e80cfa84200717921840fddcf3b051a9216ad8,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
ed6a15f0f7fa4594e51d5bde21cc0c6c1bedbfdc,0.0,four,[[  0 662   1]],"performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks","[[  821    13 20726   262 11160    32    18  7031  2032    49    11     3
     51   134    63    29   254    33  1126     6    28     3    51   134
     63    29   254   692  3300  4131    30   489    91    13   668  4145
      1]]"
4c18081ae3b676cc7831403d11bc070c10120f8e,0.0,Clustering algorithms are defined as clusters of a set number of words that are grouped,"[[    0 28552    53 16783    33  4802    38  9068     7    13     3     9
    356   381    13  1234    24    33     3 31801]]","CLUTO, Carrot2 Lingo",[[ 205 9138 5647    6 1184 2719  357  301   53   32    1]]
d64383e39357bd4177b49c02eb48e12ba7ffd4fb,0.0,"BiDAF, R-NET, and QANet","[[    0  2106  4296   371     6   391    18  9978     6    11     3 23008
   9688     1]]","Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer","[[17546    23  1018     3 17467    15  7249 22697     6  1193  6327     3
  17467    15  7249 22697     6   638   291     7    15 19159 22697     6
    419 13536    26 19159 22697     6 11801     3 19675  1266 12472 22697
      1]]"
c5b0ed5db65051eebd858beaf303809aa815e8e5,50.0,large BERT,[[    0   508   272 24203     1]],small BERT,[[  422   272 24203     1]]
01f4a0a19467947a8f3bdd7ec9fac75b5222d710,33.3333,BLEU and TER scores,[[    0     3  8775 12062    11     3  5946  7586     1]],INLINEFORM0 scores,[[ 3388 20006 24030   632  7586     1]]
74b4779de437c697fe702e51f23e2b0538b0f631,9.0909,by calculating score for phrasal composition using weighted-word ratio,"[[    0    57     3 25956  2604    21     3 28698     7   138  5761   338
   1293    15    26    18  6051  5688     1]]",Spearman's INLINEFORM0 between phrasal similarities derived from our compositional functions and the human annotators,"[[ 8974   291   348    31     7  3388 20006 24030   632   344     3 28698
      7   138 25758     3  9942    45    69  5761   138  3621    11     8
    936    46  2264  6230     1]]"
ee417fea65f9b1029455797671da0840c8c1abbe,100.0,No,[[  0 465   1]],No,[[465   1]]
51fe4d44887c5cc5fc98b65ca4cb5876f0a56dad,0.0,NNN,[[    0   445 17235     1]],"CNN, BERT",[[19602     6   272 24203     1]]
6aa2a1e2e3666f2b2a1f282d4cbdd1ca325eb9de,0.0,1,[[  0 209   1]],"Book, Electronics, Beauty and Music each have 6000, IMDB 84919, Yelp 231163, Cell Phone 194792 and Baby 160792 labeled data.","[[ 3086     6  9885     7     6 12587    11  3057   284    43     3 21987
      6     3  5166  9213   505  3647  2294     6  7271    40   102  1902
  20159  6355  7845  8924 23992  4508    11  6069 11321  4440   357  3783
     15    26   331     5     1]]"
861187338c5ad445b9acddba8f2c7688785667b1,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
8c48c726bb17a17d70ab29db4d65a93030dd5382,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"57,505 sentences",[[    3  3436     6  1752   755 16513     1]]
003d6f9722ddc2ee13e879fefafc315fb8e87cb9,0.0,a node in the network approach,[[   0    3    9  150  221   16    8 1229 1295    1]],Unanswerable,[[ 597 3247 3321  179    1]]
d27438b11bc70e706431dda0af2b1c0b0d209f96,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
8b9c12df9f89040f1485b3847a29f11b5c9262e0,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
92d1a6df3041667dc662376938bc65527a5a1c3c,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
f0b1d8c0a44dbe8d444a5dbe2d9c3d51e048a6f6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Metric difference between Aloha and best baseline score:
Hits@1/20: +0.061 (0.3642 vs 0.3032)
MRR: +0.0572(0.5114 vs 0.4542)
F1: -0.0484 (0.3901 vs 0.4385)
BLEU: +0.0474 (0.2867 vs 0.2393)","[[ 1212  3929  1750   344   901    32  1024    11   200 20726  2604    10
  11436     7  1741 17637   632    10  1768 11739  4241 17482     5  3420
   4165     3   208     7  4097 23335  7318   283 12224    10  1768 25079
   5865   599 12100 18959     3   208     7     3 22776  5062  7318   377
    536    10     3    18 11739  3707   591 17482     5  3288  4542     3
    208     7     3 22776  3747  9120     3  8775 12062    10  1768 11739
   4177   591 17482     5  2577  3708     3   208     7     3 18189  3288
   5268     1]]"
56a8826cbee49560592b2d4b47b18ada236a12b9,0.0,We use the meta-data embedded within those tweets to look for differences between tweets ,"[[    0   101   169     8 10531    18  6757 13612   441   273 10657     7
     12   320    21  5859   344 10657     7     3]]",an expert annotator determined if the tweet fell under a specific category,"[[   46  2205    46  2264  1016  4187     3    99     8 10657  4728   365
      3     9   806  3295     1]]"
06b5272774ec43ee5facfa7111033386f06cf448,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],sentence,[[7142    1]]
fabf6fdcfb4c4c7affaa1e4336658c1e6635b1bf,0.0,"BIBREF15, BIBREF16","[[    0     3  5972 25582   371  1808     6     3  5972 25582   371  2938
      1]]","Semeval 2014 BIBREF34 Twitter Sentiment Analysis Dataset ,  dataset was created by BIBREF8,  English dataset from BIBREF8,  dataset from The Sarcasm Detector","[[  679   526  2165  1412     3  5972 25582   371  3710  3046  4892  2998
    295 10582  2747  2244     3     6 17953    47   990    57     3  5972
  25582   371 11864  1566 17953    45     3  5972 25582   371 11864 17953
     45    37  9422  6769    51     3 31636   127     1]]"
dca86fbe1d57b44986055b282a03c15ef7882e51,0.0,We use the meta-data embedded within those tweets to look for differences between tweets ,"[[    0   101   169     8 10531    18  6757 13612   441   273 10657     7
     12   320    21  5859   344 10657     7     3]]",Ground truth is not established in the paper,[[13908  2827    19    59  2127    16     8  1040     1]]
9225b651e0fed28d4b6261a9f6b443b52597e401,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"when using BT, cases where the source is shorter than the target are rarer; cases when they have the same length are more frequent, automatic word alignments between artificial sources tend to be more monotonic than when using natural sources","[[  116   338     3  9021     6  1488   213     8  1391    19 10951   145
      8  2387    33  3400    52   117  1488   116    79    43     8   337
   2475    33    72  8325     6  6569  1448 14632     7   344  7353  2836
   2134    12    36    72 30378   447   145   116   338   793  2836     1]]"
37f8c034a14c7b4d0ab2e0ed1b827cc0eaa71ac6,0.0,"document classification, named entity recognition, part-of-speech tagging, dependency","[[    0  1708 13774     6  2650 10409  5786     6   294    18   858    18
      7   855 10217     3    17 15242     6 27804]]","accuracy, Labeled Attachment Scores (LAS)","[[ 7452     6 16229    15    26 28416   297 17763     7    41 20245    61
      1]]"
984fc3e726848f8f13dfe72b89e3770d00c3a1af,11.1111,"feature number, id of an entity, a syllable, ","[[    0  1451   381     6     3    23    26    13    46 10409     6     3
      9     3     7    63   195   179     6     3]]",KL-divergences of language models for the news article and the already added news references,"[[ 480  434   18 8481   49  729 2319   13 1612 2250   21    8 1506 1108
    11    8  641  974 1506 9811    1]]"
8dd8e5599fc56562f2acbc16dd8544689cddd938,14.2857,they are defined as singletons in the context of other words,[[   0   79   33 4802   38  712 8057   16    8 2625   13  119 1234    1]],Similar words were ranked by computing Cosine distance between the embedding vector ( INLINEFORM0 ) representation of the query equation and the context vector representation of the words ( INLINEFORM1 ). Similar equations were discovered using Euclidean distance computed between the context vector representations of the equations ( INLINEFORM2 ). We give additional example results in Appendix B.,"[[18347  1234   130     3  8232    57 10937   638     7   630  2357   344
      8 25078    26    53 12938    41  3388 20006 24030   632     3    61
   6497    13     8 11417 13850    11     8  2625 12938  6497    13     8
   1234    41  3388 20006 24030   536     3   137 18347 13850     7   130
   3883   338  4491 14758   221   152  2357 29216    26   344     8  2625
  12938  6497     7    13     8 13850     7    41  3388 20006 24030   357
      3   137   101   428  1151   677   772    16  2276   989  2407   272
      5     1]]"
5aa12b4063d6182a71870c98e4e1815ff3dc8a72,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
17a1eff7993c47c54eddc7344e7454fbe64191cd,0.0,BLEU scores,[[    0     3  8775 12062  7586     1]],semantic category-based approach,[[27632  3295    18   390  1295     1]]
6b53e1f46ae4ba9b75117fc6e593abded89366be,0.0,"classification, classification, classification based on sequences, classification based on sequences","[[    0 13774     6 13774     6 13774     3   390    30  5932     7     6
  13774     3   390    30  5932     7     1]]","NER model, CRF classifier trained with sklearn-crfsuite, classifier has been developed that consists of regular-expressions and dictionary look-up","[[    3 18206   825     6   205  8556   853  7903  4252    28     3     7
   9434   291    29    18    75    52    89     7  8431     6   853  7903
     65   118  1597    24     3  6848    13  1646    18 20940     7    11
  24297   320    18   413     1]]"
219af68afeaecabdfd279f439f10ba7c231736e4,0.0,Japanese INLINEFORM0 Vietnamese,[[    0  4318  3388 20006 24030   632 24532     1]],WIT3's corpus,[[  549  3177   519    31     7 11736   302     1]]
186b7978ee33b563a37139adff1da7d51a60f581,21.0526,closed test setting is a set of closed test sets which are set according to the parameters of,"[[   0 3168  794 1898   19    3    9  356   13 3168  794 3369   84   33
   356 1315   12    8 8755   13]]","closed test limits all the data for learning should not be beyond the given training set, while open test does not take this limitation","[[ 3168   794  6790    66     8   331    21  1036   225    59    36  1909
      8   787   761   356     6   298   539   794   405    59   240    48
  14130     1]]"
63c3550c6fb42f41a0c93133e9fca12ac00df9b3,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
b3307d5b68c57a074c483636affee41054be06d1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length, NLI models tend to predict entailment for sentence pairs with a high lexical overlap","[[22455    18  9926 20726  1912     7   394   145  1253   788    12   123
     15     7    30    70     3 30949   138  1160    11  7142  2475     6
    445  8159  2250  2134    12  9689     3    35  5756   297    21  7142
  14152    28     3     9   306     3 30949   138 21655     1]]"
218615a005f7f00606223005fef22c07057d9d77,0.0,samsung english corpus,[[    0     3     7   265     7   425 22269 11736   302     1]],Answer with content missing: (Data section) Penn Treebank (PTB),"[[11801    28   738  3586    10    41 20367  1375    61 11358  7552  4739
     41  6383   279    61     1]]"
e3a2d8886f03e78ed5e138df870f48635875727e,0.0,Twitter,[[   0 3046    1]],by crawling,[[   57 18639    53     1]]
509af1f11bd6f3db59284258e18fdfebe86cae47,31.5789,diversity is measured as the mean of all derived captions.,"[[    0  7322    19  8413    38     8  1243    13    66     3  9942 25012
      7     5     1]]",diversity score as the ratio of observed number versus optimal number,[[7322 2604   38    8 5688   13 6970  381    3 8911 6624  381    1]]
81588e0e207303c2867c896f3911a54a1ef7c874,92.3077,"Friends TV sitcom scripts, Facebook messenger chats","[[    0  9779  1424  2561   287  4943     7     6  1376 28110  3582     7
      1]]","Friends TV sitcom, Facebook messenger chats",[[ 9779  1424  2561   287     6  1376 28110  3582     7     1]]
8c852fc29bda014d28c3ee5b5a7e449ab9152d35,16.6667,"Bi-LSTM, BERT",[[    0  2106    18  7600  2305     6   272 24203     1]],"linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)","[[13080   180 12623     6  2647 26352  3230  7110    18 11679    18   329
     15  2528    63    41   279    23  7600  2305   201  1193 24817   138
   1484  9709  3426    41   254 17235    61     1]]"
9ecde59ffab3c57ec54591c3c7826a9188b2b270,0.0,"standard SQuAD, standard WSJ5, standard WSJ10","[[    0  1068   180  5991  6762     6  1068     3  8439   683 11116  1068
      3  8439   683  1714     1]]","fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\ year) \times 20$ citations","[[ 1400    69   682  4903    11   130  1790    16     8   203  1421    12
   7887    43    44   709  1514   599  8584     3    18  5707     2   215
     61     3     2   715     7   460  3229     3 13903     7     1]]"
1591068b747c94f45b948e12edafe74b5e721047,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],10K user-generated image (snap) and textual caption pairs,"[[  335   439  1139    18 29955  1023    41     7    29     9   102    61
     11  1499  3471 25012 14152     1]]"
dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Classification system use n-grams, bag-of-words, common words and hashtags as features and SVM, random forest, extra tree and NB classifiers.","[[ 4501  2420   358   169     3    29    18  5096     7     6  2182    18
    858    18  6051     7     6  1017  1234    11 25354     7    38   753
     11   180 12623     6  6504  5827     6   996  2195    11     3 14972
    853  7903     7     5     1]]"
32d99dcd8d46e2cda04a9a9fa0e6693d2349a7a9,11.2676,a modification to the objective function that takes the vector representations into account,"[[    0     3     9 12767    12     8  5997  1681    24  1217     8 12938
   6497     7   139   905     1]]","The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,","[[   37   583  1681    21   136    80    13     8  1234    13  2077  1448
     18 10739     7    19  8473    57     8  5302    13    46 20541  1657
     12     8   583  1681     5     3     5  1698 25078    26    53 12938
   9340    19   166  1968    28     3     9  2077     5   242     3     9
   1448 12770    12   136    80    13     8  1448    18 10739     7  9085
    175  6085     6     8  8473   583  1657  4971     7    46   993    21
      8   701    13    48  1448    31     7 25078    26    53 12938  9340
      3  9921    12     8  2077    24     8  1090  1448 16952    12     6
      1]]"
74fb77a624ea9f1821f58935a52cca3086bb0981,0.0,"3,200 messages",[[   0 6180 3632 4175    1]],Unanswerable,[[ 597 3247 3321  179    1]]
27dbbd63c86d6ca82f251d4f2f030ed3e88f58fa,0.0,"Bi-LSTM, LSTM-Alignment","[[   0 2106   18 7600 2305    6    3 7600 2305   18  188   40 3191  297
     1]]","RNN-based NMT model, Transformer-NMT","[[  391 17235    18   390   445  7323   825     6 31220    18   567  7323
      1]]"
e54257585cc75564341eb02bdc63ff8111992f82,0.0,"BIBREF11, BIBREF12, BIBREF13","[[    0     3  5972 25582   371  2596     6     3  5972 25582   371  2122
      6     3  5972 25582   371  2368     1]]","Answer with content missing: (LVL1, LVL2, LVL3) 
- Stanford CoreNLP
- Optical Character Recognition (OCR) system, ParsCIT 
- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion.","[[11801    28   738  3586    10    41 15086   434  4347     3 15086   434
   4482     3 15086   434  5268     3    18 19796  9020   567  6892     3
     18     3  9546  1489 20087 31110    41  5618   448    61   358     6
   2180     7   254  3177     3    18   856     3     9  9818     8  3785
   1499    45   593   204   554 15056    15    26  2691    12     8   826
     10  2233     6 13956     7     6  9838     6  5302     6  1341   161
      6  2458    11  7489     5     1]]"
b0dbe75047310fec4d4ce787be5c32935fc4e37b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"we also use two of its adversarial sets, namely AddSent and AddOneSent BIBREF6 , to evaluate the robustness to noise","[[   62    92   169   192    13   165 23210    23   138  3369     6     3
  17332  2334   134   295    11  2334 10723   134   295     3  5972 25582
    371   948     3     6    12  6825     8  6268   655    12  4661     1]]"
312e9cc11b9036a6324bdcb64eca6814053ffa17,0.0,agreement of 86.0% between the report and the author and the annotations of the patient,"[[    0  2791    13     3  3840     5  6932   344     8   934    11     8
   2291    11     8 30729     7    13     8  1868]]",Unanswerable,[[ 597 3247 3321  179    1]]
4a4ce942a7a6efd1fa1d6c91dedf7a89af64b729,30.303,"visual question data is collected from a collection of over 450,000 visual questions","[[   0 3176  822  331   19 4759   45    3    9 1232   13  147  314 9286
  3176  746    1]]",The number of redundant answers to collect from the crowd is predicted to efficiently capture the diversity of all answers from all visual questions.,"[[   37   381    13 28282  4269    12  2868    45     8  4374    19 15439
     12  8877  4105     8  7322    13    66  4269    45    66  3176   746
      5     1]]"
a0543b4afda15ea47c1e623c7f00d4aaca045be0,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
9a05a5f4351db75da371f7ac12eb0b03607c4b87,0.0,ACL NLP 20K,[[   0   71 8440  445 6892  460  439    1]],"Europarl, MultiUN",[[5578   52   40    6 4908 7443    1]]
d28d86524292506d4b24ae2d486725a6d57a3db3,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"33.33 average of ROUGE-1, ROUGE-2 and ROUGE-L ","[[ 5400     5  4201  1348    13   391 26260   427  2292     6   391 26260
    427  4949    11   391 26260   427    18   434     1]]"
1062a0506c3691a93bb914171c2701d2ae9621cb,0.0,"feature sets for news annotations, news annotations, and captions","[[    0  1451  3369    21  1506 30729     7     6  1506 30729     7     6
     11 25012     7     1]]","Sentiment, Morality, Style, Words embeddings","[[ 4892  2998   295     6 28466   485     6  7936     6  4467     7 25078
     26    53     7     1]]"
6becff2967fe7c5256fe0b00231765be5b9db9f1,9.0909,auxiliary regularization terms,[[    0     3 31086  1646  1707  1353     1]],"a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution","[[    3     9  1646  1707  1657  1968    28  7163   753     6     8  2411
      3    35 12395    63    13   853  3438     6   480   434 12355   122
   1433   344  2848    11 15439   853  3438     1]]"
3bf0306e9bd044f723e38170c13455877b2aeec3,0.0,a sensationalism scorer is trained on an article without labeling it as clickbait,"[[   0    3    9 9118 6835 2604   52   19 4252   30   46 1108  406 3783
    53   34   38 1214 9441   17]]",by classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss $L_{\text{sen}}$,"[[   57   853  8587  9118   138    11   529    18     7    35     7   257
    138 12392     7   338     3     9    80    18 18270 19602    28     3
      9 14865  2269     3    35 12395    63  1453  1514   434   834     2
   6327     2     7    35     2  3229     1]]"
4dc268e3d482e504ca80d2ab514e68fd9b1c3af1,0.0,30,[[  0 604   1]],"48,705",[[4678    6 2518  755    1]]
87bb3105e03ed6ac5abfde0a7ca9b8de8985663c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],They do not require the availability of a backward translation engine.,"[[ 328  103   59 1457    8 5576   13    3    9  223 2239 7314 1948    5
     1]]"
f9edd8f9c13b54d8b1253ed30e7decc1999602da,17.3913,"evaluation protocol, a set of standard evaluation protocols, and a set of standard evaluation protocols","[[    0  5002 10015     6     3     9   356    13  1068  5002 18870     6
     11     3     9   356    13  1068  5002 18870]]","three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set","[[  386 11082  5818     7    28   315  2302    13  7215    16     8  5002
    356     6   386 11082  5818     7    28   315   381    13  5873    16
      8  5002   356    33  4802     6   166    80     6 16467    28    44
    709  1003  5592  3975    33  1285    12     8  5002   356     6 16467
     28   898  3975    12     8   606    11     8   880    13 16467    12
      8  2458   356     6   511  5818     6 16467    28    44   709   505
   3975    33  1285    12     8  5002   356     6 16467    28   431    42
    489  3975    12     8   606    11     8   880    13 16467    12     8
   2458   356     1]]"
1170e4ee76fa202cabac9f621e8fbeb4a6c5f094,100.0,No,[[  0 465   1]],No,[[465   1]]
d571e0b0f402a3d36fb30d70cdcd2911df883bc7,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
0fc2b5bc2ead08a6fe0280fb3a47477c6df1587c,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
58c6737070ef559e9220a8d08adc481fdcd53a24,0.0,accuracy,[[   0 7452    1]],correct classification rate (CCR),[[ 2024 13774  1080    41   254  4545    61     1]]
6bbbb9933aab97ce2342200447c6322527427061,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],By multiplying crowd-annotated document-emotion matrix with emotion-word matrix. ,"[[  938 30333    53  4374    18   152  2264   920  1708    18    15  7259
  16826    28 13868    18  6051 16826     5     1]]"
af34051bf3e628c1e2a00b110bb84e5f018b419f,0.0,"MT encoder, MT decoder, End-to-End MT","[[    0     3  7323 23734    52     6     3  7323    20  4978    52     6
   3720    18   235    18  8532    26     3  7323]]","Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train","[[31664  5097 20726     6  1266    18 13023 20726     7     6  4908    18
  23615 20726     7     6  1404    18   235    18   348    63  1220  2026
     18 13023     6 29260  1220  2026    18  9719     1]]"
022c365a14fdec406c7a945a1a18e7e79df37f08,27.2727,attention module is pre-trained on a set of English-English topics.,"[[    0  1388  6008    19   554    18    17 10761    30     3     9   356
     13  1566    18 26749  4064     5     1]]",the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage.,"[[    8   825    19   554    18    17 10761    30   205  3838    18   390
     71  6857  2491    11     3  7323  2491    16     8   554    18 13023
   1726     5     1]]"
2f901dab6b757e12763b23ae8b37ae2e517a2271,0.0,English-Japanese,[[   0 1566   18  683 9750 1496   15    1]],Germanâ€“English,[[ 2968   104 26749     1]]
f17ca24b135f9fe6bb25dc5084b13e1637ec7744,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],explicit discourse relations,[[17623 22739  5836     1]]
22ccee453e37536ddb0c1c1d17b0dbac04c6c607,66.6667,"Arabic, English",[[    0 19248     6  1566     1]],English ,[[1566    1]]
4986f420884f917d1f60d3cea04dc8e64d3b5bf1,100.0,crosslingual latent variables,[[    0  2269 25207    50  4669 11445     1]],crosslingual latent variables,[[ 2269 25207    50  4669 11445     1]]
e44a6bf67ce3fde0c6608b150030e44d87eb25e3,44.4444,"abortion, medicine usage, marijuana",[[    0 20526     6  4404  4742     6 10434     1]],"abortion, gay rights, Obama, marijuana",[[20526     6 16998  2166     6  4534     6 10434     1]]
e0e379e546f1da9da874a2e90c79b41c60feb817,22.2222,they use unlabeled data for tagging and segmentation purposes,"[[    0    79   169    73  9339   400    26   331    21     3    17 15242
     11  5508   257  3659     1]]","During training, the model is trained alternately with one mini-batch of labeled data and INLINEFORM0 mini-batches of unlabeled data.","[[    3  2092   761     6     8   825    19  4252 13902   120    28    80
   3016    18   115 14547    13  3783    15    26   331    11  3388 20006
  24030   632  3016    18  3697  2951    13    73  9339   400    26   331
      5     1]]"
348886b4762db063711ef8b7a10952375fbdcb57,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
04bde1d2b445f971e97bb46ade2d0290981c7a32,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
81dbe9a9ddaa5d02b02e01a306d898015a56ffb6,0.0,latent context,[[   0   50 4669 2625    1]],the series of posts that trigger an intervention,[[   8  939   13 3489   24 7294   46 7897    1]]
a313e98994fc039a82aa2447c411dda92c65a470,0.0,they are matched by the same pair of words,[[    0    79    33     3 10304    57     8   337  3116    13  1234     1]],Unanswerable,[[ 597 3247 3321  179    1]]
bc01853512eb3c11528e33003ceb233d7c1d7038,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Adversarial misspellings are a real-world problem,"[[ 1980  2660     9 12042  3041 19510    53     7    33     3     9   490
     18  7276   682     1]]"
3e1829e96c968cbd8ad8e9ce850e3a92a76b26e4,0.0,"70,000",[[    0     3 28891     1]],Total dataset size: 171 account (522967 tweets),"[[ 9273 17953   812    10  1003   536   905    41  5373  3166  3708 10657
      7    61     1]]"
8f16dc7d7be0d284069841e456ebb2c69575b32b,0.0,BIBREF13,[[    0     3  5972 25582   371  2368     1]],"various versions of LiLi as baselines, Single, Sep, F-th, BG, w/o PTS","[[  796  5204    13  1414   434    23    38 20726     7     6  7871     6
  12219     6   377    18   189     6     3 19179     6     3   210    87
     32   276  4578     1]]"
c8541ff10c4e0c8e9eb37d9d7ea408d1914019a9,0.0,TABREF2,[[    0     3  3221 25582   371   357     1]],"DSL 2015, DSL 2017, JW300 parallel corpus , NCHLT text corpora","[[  309  5629  1230     6   309  5629  4791   446   518  5426  8449 11736
    302     3     6   445  8360  9012  1499 11736   127     9     1]]"
f9bf6bef946012dd42835bf0c547c0de9c1d229f,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
8d4ac4afbf5b14f412171729ceb5e822afcfa3f4,100.0,No,[[  0 465   1]],No,[[465   1]]
b5bc34e1e381dbf972d0b594fe8c66ff75305d71,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively","[[  242  1566 17953     7   379   638   567 10376 23948    11   461   235
  10358    15     7 20734     6    69  4382  1573    91   883  2032     7
    272 24203    18 27396  5972 25582   371  3747    57  1768 18189  1298
     11  1768 23758   948  6898     5     6  1450   821  4888     7    30
   2830 17953     7     6     3  9582   377   536  6867    57  1768 23758
    940    11  1768  4416  3420    30  5266  4763    11   461   235 10358
     15     7 15021     6  6898     1]]"
ca4daafdc23f4e23d933ebabe682e1fe0d4b95ed,0.0,by analyzing the lexicon of the spoken words,"[[    0    57     3 19175     8     3 30949   106    13     8 11518  1234
      1]]",Unanswerable,[[ 597 3247 3321  179    1]]
d9980676a83295dda37c20cfd5d58e574d0a4859,0.0,LSTM BIBREF12,[[    0     3  7600  2305     3  5972 25582   371  2122     1]],"copy, copy-marked, copy-dummies",[[ 2405     6  2405    18 16376     6  2405    18    26 30885     1]]
c09a92e25e6a81369fcc4ae6045491f2690ccc10,0.0,polarity vs. positive emotion lexicons,"[[    0     3  9618   485     3   208     7     5  1465 13868     3 30949
    106     7     1]]","1000 term groups based on the number of total annotations(200 term groups with 2 total annotations, 200 term groups with 3 total annotations, and so on up to term groups with 6 total annotations)","[[ 5580  1657  1637     3   390    30     8   381    13   792 30729     7
    599  3632  1657  1637    28   204   792 30729     7     6  2382  1657
   1637    28   220   792 30729     7     6    11    78    30    95    12
   1657  1637    28   431   792 30729     7    61     1]]"
a9a532399237b514c1227f2d6be8601474e669be,0.0,MR-CNS,[[    0     3  9320    18 10077   134     1]], UM Inventory ,[[    3  6122 31040     1]]
3e432d71512ffbd790a482c716e7079ee78ce732,0.0,BIBREF15,[[    0     3  5972 25582   371  1808     1]],"Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.","[[ 6150 17953    28  1139   746   117   356    13  2691     6 19010  3489
     11  1506  2984     6    66  1341    12  4747     5     1]]"
0fc2b5bc2ead08a6fe0280fb3a47477c6df1587c,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
9132d56e26844dc13b3355448d0f14b95bd2178a,8.3333,cwrs capture a surprisingly large amount of syntax,"[[    0     3    75   210    52     7  4105     3     9     3 12713   508
    866    13 28230     1]]","token-level chunk label embeddings,  chunk boundary information is passed into the task model via BIOUL encoding of the labels","[[14145    18  4563 16749  3783 25078    26    53     7     6 16749 20430
    251    19  2804   139     8  2491   825  1009 23112  4254     3    35
   9886    13     8 11241     1]]"
d00bbeda2a45495e6261548710afa6b21ea32870,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"first a model is trained on the training set and then this model is used to predict the labels of the silver data, This silver data is then simply added to our training set, after which the model is retrained","[[  166     3     9   825    19  4252    30     8   761   356    11   258
     48   825    19   261    12  9689     8 11241    13     8  4294   331
      6   100  4294   331    19   258   914   974    12    69   761   356
      6   227    84     8   825    19     3    60    17 10761     1]]"
777217e025132ddc173cf33747ee590628a8f62f,0.0,a random task and a random task to learn distributed representations of the vocabulary,"[[    0     3     9  6504  2491    11     3     9  6504  2491    12   669
   8308  6497     7    13     8 19067     1]]",Calculate test log-likelihood on the three considered datasets,"[[18555   342   794  4303    18  2376    40    23  4500    30     8   386
   1702 17953     7     1]]"
03c967763e51ef2537793db7902e2c9c17e43e95,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Conditional Copy (CC) model ,[[24239   138 20255    41  2823    61   825     1]]
6aaf12505add25dd133c7b0dafe8f4fe966d1f1d,0.0,"PT-L, GRU-L, PT-L, and PT-","[[   0    3 6383   18  434    6  350 8503   18  434    6    3 6383   18
   434    6   11    3 6383   18]]","Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM","[[12928   257   138     3  7600  2305     6  7435   254 17235     6  4564
     49  4892  4070    40    18  7600  2305     6   391   566   567     6
      3 21277  7845     6   180  8503     6 25710 17235     6     3 16375
      6  7769 18270 11202    18 28102     3  7600  2305     6    71 17698
     18  7600  2305     6 12716  1601     3  7600  2305     1]]"
fd8e23947095fe2230ffe1a478945829b09c8c95,0.0,are meaningful chains selected from the graph,[[    0    33  7892 16534  2639    45     8  8373     1]],utilize the machinery of language modeling using deep neural networks to learn Dolores embeddings.,"[[ 5849     8 13226    13  1612 15309   338  1659 24228  5275    12   669
    531   322    15     7 25078    26    53     7     5     1]]"
955de9f7412ba98a0c91998919fa048d339b1d48,0.0,standard shallow approaches,[[    0  1068 16906  6315     1]],SVM,[[  180 12623     1]]
8b9c12df9f89040f1485b3847a29f11b5c9262e0,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Yes,[[2163    1]]
e2db361ae9ad9dbaa9a85736c5593eb3a471983d,0.0,BIBREF2 BIBREF3 BIBREF4 BIBREF,"[[    0     3  5972 25582   371   357     3  5972 25582   371   519     3
   5972 25582   371   591     3  5972 25582   371]]","Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder.","[[   71   208   122     5  9840   553    15 25078    26    53     7     6
     71   208   122     5  1006    18  6327 25078    26    53     7     6
     71   208   122     5   272 24203 25078    26    53     7     6   272
  24203 11175   134    18   162  5317     6    86  1010   134   295     3
     18  9840   553    15    11 12489  4892    17  1433   695  4978    52
      5     1]]"
5c5aeee83ea3b34f5936404f5855ccb9869356c1,0.0,"Identifying Relationships between Heads, Identifying Relationships between Heads","[[    0     3 23393    53 28898  2009     7   344  3642     7     6     3
  23393    53 28898  2009     7   344  3642     7]]"," four machine translation tasks, IWSLT 2017 German $\rightarrow $ English BIBREF27, KFTT Japanese $\rightarrow $ English BIBREF28, WMT 2016 Romanian $\rightarrow $ English BIBREF29, WMT 2014 English $\rightarrow $ German BIBREF30","[[  662  1437  7314  4145     6    27  8439  9012  1233  2968  1514     2
   3535  6770  1514  1566     3  5972 25582   371  2555     6   480  6245
    382  4318  1514     2  3535  6770  1514  1566     3  5972 25582   371
   2577     6   549  7323  1421  3871    29  1514     2  3535  6770  1514
   1566     3  5972 25582   371  3166     6   549  7323  1412  1566  1514
      2  3535  6770  1514  2968     3  5972 25582   371  1458     1]]"
e79a5b6b6680bd2f63e9f4adbaae1d7795d81e38,0.0,non-english,[[   0  529   18 4606   40 1273    1]],Russian,[[4263    1]]
38e4aaeabf06a63a067b272f8950116733a7895c,32.0,a tagging scheme that uses a combination of word and phonological similarity,"[[    0     3     9     3    17 15242  5336    24  2284     3     9  2711
     13  1448    11     3  9621  4478  1126   485]]",A new tagging scheme that tags the words before and after the pun as well as the pun words.,"[[   71   126     3    17 15242  5336    24 12391     8  1234   274    11
    227     8  4930    38   168    38     8  4930  1234     5     1]]"
d3dbb5c22ef204d85707d2d24284cc77fa816b6c,0.0,"BiLSTM, LSTM-B, LSTM-C, LSTM","[[   0 2106 7600 2305    6    3 7600 2305   18  279    6    3 7600 2305
    18  254    6    3 7600 2305]]","SAN Baseline, BNA, DocQA, R.M-Reader, R.M-Reader+Verifier and DocQA+ELMo","[[    3 19976  8430   747     6   272  5999     6 17268 23008     6   391
      5   329    18 19915    49     6   391     5   329    18 19915    49
   1220  5000  7903    11 17268 23008  1220  3577   329    32     1]]"
41ac23e32bf208b69414f4b687c4f324c6132464,0.0,"English, Chinese",[[   0 1566    6 2830    1]],small portion of the large parallel corpus for English-German is used as a simulation,"[[  422  4149    13     8   508  8449 11736   302    21  1566    18 24518
     19   261    38     3     9 11108     1]]"
a5e49cdb91d9fd0ca625cc1ede236d3d4672403c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],adopt a multi-turn answer module for the span detector BIBREF1,"[[ 4693     3     9  1249    18  7535  1525  6008    21     8  8438 19199
      3  5972 25582   371   536     1]]"
b6f7fadaa1bb828530c2d6780289f12740229d84,0.0,German-English,[[    0  2968    18 26749     1]],"English-German, English-French",[[ 1566    18 24518     6  1566    18   371    60  5457     1]]
dc2a2c177cd5df6da5d03e6e74262bf424850ec9,6.6667,political biases are not included in the model,[[    0  1827 14387    15     7    33    59  1285    16     8   825     1]],"we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries.","[[   62    92   905    21  1827 14387    15     7 18340    12   315  1506
   2836     6     3 13215    12     8  3979  4382    16     3  5972 25582
    371   357    12  3783   315 14290     5  9126    62   504    24    62
     33     3   179    12   853  4921 25337     3   208     7   529    18
  17216  2317 21740  5275    41   232 21612   120  1506  2984    61    28
    306  7452    41  6727 26893    95    12     3  4240  6210     6   237
    116  7625    21     8  1827 14387    13  2836    41   232   761   163
     30   646    18 15500  3843    42   269    18 15500  3843  2984   137
    101  7743    24     8  3760    13  2652     7  2238 11770     7  1934
    251    21     8 13774     6    20  2264    53     3     9   315  4742
     13    48  6730   116  2178  1506 12770    12     8   192  1506  3303
      7     5   101    92   504    24   167  9192  1528   753     6    84
     33  5237    12     8  4109   189    11  4963    13  2015  1990  6615
      7    16   315  7500     6    33     8   337   640     8   192  1440
      5     1]]"
6a14379fee26a39631aebd0e14511ce3756e42ad,0.0,"Bi-directional BERT, Convolutional BERT, Convolutional BERT","[[    0  2106    18 26352   272 24203     6  1193 24817   138   272 24203
      6  1193 24817   138   272 24203     1]]","BERT-base, BERT-large, BERT-uncased, BERT-cased","[[  272 24203    18 10925     6   272 24203    18 15599     6   272 24203
     18   202  6701    26     6   272 24203    18  6701    26     1]]"
2858620e0498db2f2224bfbed5263432f0570832,0.0,the vertex-level aggregation component,[[    0     8   548 10354    18  4563     3 31761    23   106  3876     1]],Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets.,"[[ 6719    30   953   772   937  2839  6640    12    73 22955  9804   141
    709  1113     3    18  9858   703     7  1750    13  4097  4201   979
     30    66   386 17953     7     5     1]]"
54830abe73fef4e629a36866ceeeca10214bd2c8,20.5128,LDA and Gibbs sampling methods,[[    0   301  4296    11 17223   115     7 17222  2254     1]],"discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\alpha =0.01$, $\beta = 0.01$ and using Gibbs sampling as a parameter estimation","[[ 2928     8  5001    13     8  4064    11   253  1675   344   301  4296
   4064    11  1040   753    11  3806  2019 12391     6   669     3     9
    301  4296   825    28   910  4064   117  1514     2   138  6977  3274
  11739   536  3229     6  1514     2   346    17     9  3274  4097  4542
   3229    11   338 17223   115     7 17222    38     3     9 15577 22781
      1]]"
149da739b1c19a157880d9d4827f0b692006aa2c,0.0,BIBREF23,[[    0     3  5972 25582   371  2773     1]],"SVM, MLP, FastText, CNN, BERT, Google's DialogFlow, Rasa NLU","[[  180 12623     6   283  6892     6  6805 13598    17     6 19602     6
    272 24203     6  1163    31     7 25843 15390     6  9053     9   445
   9138     1]]"
b8d0e4e0e820753ffc107c1847fe1dfd48883989,0.0,Yes,[[   0 2163    1]],More than that in some cases (next to adjacent) ,"[[ 1537   145    24    16   128  1488    41    29 10398    12 12487    61
      1]]"
4907096cf16d506937e592c50ae63b642da49052,0.0,"vulgarity, profanity, sexiness, gender, sexual orientation, sexual orientation","[[    0 31648   485     6  7108   152   485     6     3     7   994  6096
      6  7285     6  6949 12602     6  6949 12602]]","detecting abusive language extremely laborious, it is difficult to build a large and reliable dataset","[[    3 29782 27031  1612  2033  5347  2936     6    34    19  1256    12
    918     3     9   508    11  3468 17953     1]]"
897ba53ef44f658c128125edd26abf605060fb13,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
d9b6c61fc6d29ad399d27b931b6cb7b1117b314a,25.0,a question generation model is used to generate the question.,[[   0    3    9  822 3381  825   19  261   12 3806    8  822    5    1]],The question generation model provides each candidate answer with a score by measuring semantic relevance between the question and the generated question based on the semantics of the candidate answer. ,"[[   37   822  3381   825   795   284  4775  1525    28     3     9  2604
     57 11297 27632 20208   344     8   822    11     8  6126   822     3
    390    30     8 27632     7    13     8  4775  1525     5     1]]"
e807d347742b2799bc347c0eff19b4c270449fee,20.0,Training and testing data provided by BioASQ organizers.,"[[    0  4017    11  2505   331   937    57  3318  3291  2247 14250     7
      5     1]]",BioASQ  dataset,[[ 3318  3291  2247 17953     1]]
2df4a045a9cd7b44874340b6fdf9308d3c55327a,0.0,Microsoft Azure (CBizSphere),[[    0  2803 21808    41   254   279    23   172 31722    61     1]],"They did not use any platform, instead they hired undergraduate students to do the annotation.","[[  328   410    59   169   136  1585     6  1446    79 10626 12260   481
     12   103     8 30729     5     1]]"
642e8cf1d39faa1cd985d16750cdc6696c52db2f,0.0,"INLINEFORM0 Czech, INLINEFORM1 German, INLINEFORM2 Romanian,","[[    0  3388 20006 24030   632 16870     6  3388 20006 24030   536  2968
      6  3388 20006 24030   357  3871    29     6]]",attentional encoder-decoder networks BIBREF0,"[[ 1388   138 23734    52    18   221  4978    52  5275     3  5972 25582
    371   632     1]]"
761de1610e934189850e8fda707dc5239dd58092,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17","[[16959    18   390  7314     3  4610    53    30     3     9  2022  1612
      3  5972 25582   371  1714     6     3    29  8291    13  9261  5056
     45  7414 25207   331     3  5972 25582   371  2534     3     6  1388
    138   391 17235    18   390   825    41 14151  7323    61     3  5972
  25582   371  4482 31220   825     3  5972 25582   371  2606     6  2647
     18 26352   825     3  5972 25582   371  2596     6  1249    18   235
     18 23829    41   329   357   329    61   825     3  5972 25582   371
  11864   223    18  7031  6105     3  5972 25582   371  2517     1]]"
d3dbb5c22ef204d85707d2d24284cc77fa816b6c,0.0,"BiLSTM, LSTM-B, LSTM-C, LSTM","[[   0 2106 7600 2305    6    3 7600 2305   18  279    6    3 7600 2305
    18  254    6    3 7600 2305]]","BNA, DocQA, R.M-Reader, R.M-Reader + Verifier, DocQA + ELMo, R.M-Reader+Verifier+ELMo","[[  272  5999     6 17268 23008     6   391     5   329    18 19915    49
      6   391     5   329    18 19915    49  1768   781  7903     6 17268
  23008  1768   262 11160    32     6   391     5   329    18 19915    49
   1220  5000  7903  1220  3577   329    32     1]]"
72755c2d79210857cfff60bfbcb55f83c71ada51,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"104 telephone calls, transcripts contain 168,195 Spanish word tokens,  translations contain 159,777 English word tokens","[[    3 15442  6596  3088     6 20146     7  3480     3 24274     6 22464
   5093  1448 14145     7     6  7314     7  3480     3 27904     6 26225
   1566  1448 14145     7     1]]"
bb3267c3f0a12d8014d51105de5d81686afe5f1b,0.0,"WN18, FB15k",[[    0     3 21170  2606     6     3 15586  1808   157     1]],"CoNLL-YAGO, TAC2010, ACE2004, AQUAINT, WW","[[  638   567 10376    18 17419  5577     6   332  5173 14926     6     3
  11539 21653     6    71 16892   188 13777     6 18548     1]]"
3a6e843c6c81244c14730295cfb8b865cd7ede46,0.0,ANOVA,[[    0    71 30103     1]],"BIBREF9 , BIBREF8","[[    3  5972 25582   371  1298     3     6     3  5972 25582   371   927
      1]]"
c81f215d457bdb913a5bade2b4283f19c4ee826c,0.0,WikiText-TL-201,[[    0  2142  2168 13598    17    18 12733    18 22772     1]],"Waseem and Hovey BIBREF5, Davidson et al. BIBREF9","[[ 2751    15    15    51    11  1546   162    63     3  5972 25582   371
  11116     3 23268     3    15    17   491     5     3  5972 25582   371
   1298     1]]"
392fb87564c4f45d0d8d491a9bb217c4fce87f03,0.0,Attention-based multi-attention,[[    0 20748    18   390  1249    18 25615     1]],"LastStateRNN, AvgRNN, AttentionRNN ","[[ 2506   134  4748 14151   567     6    71   208   122 14151   567     6
  20748 14151   567     1]]"
50cb50657572e315fd452a89f3e0be465094b66f,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
7ee5c45b127fb284a4a9e72bb9b980a602f7445a,0.0,LSTM with a corresponding LSTM feature set,"[[   0    3 7600 2305   28    3    9    3 9921    3 7600 2305 1451  356
     1]]",Unanswerable,[[ 597 3247 3321  179    1]]
56b034c303983b2e276ed6518d6b080f7b8abe6a,0.0,"Wuhan University, Xinhua University, Xinhua University","[[    0 17792  2618   636     6     3     4    77   107    76     9   636
      6     3     4    77   107    76     9   636]]","FSD BIBREF12 , Twitter, and Google datasets","[[  377  7331     3  5972 25582   371  2122     3     6  3046     6    11
   1163 17953     7     1]]"
b1bc9ae9d40e7065343c12f860a461c7c730a612,0.0,"WSJ11, WSJ12",[[   0    3 8439  683 2596    6    3 8439  683 2122    1]],"Existential (OneShape, MultiShapes), Spacial (TwoShapes, Multishapes), Quantification (Count, Ratio) datasets are generated from ShapeWorldICE","[[17061  7220    41 10723   134  9516    15     6  4908   134  9516    15
      7   201  5641  4703    41   382   210    32   134  9516    15     7
      6  4908     7  9516    15     7   201 12716  2420    41 10628     6
   6455    23    32    61 17953     7    33  6126    45 23890 17954  8906
      1]]"
596aede2b311deb8cb0a82d2e7de314ef6e83e4e,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
88ab7811662157680144ed3fdd00939e36552672,0.0,Action-Space,[[    0  6776    18 24722     1]],"KG-A2C-chained, KG-A2C-Explore","[[    3 18256    18   188   357   254    18 19836    15    26     6     3
  18256    18   188   357   254    18 12882   322    15     1]]"
0737954caf66f2b4c898b356d2a3c43748b9706b,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
a3efe43a72b76b8f5e5111b54393d00e6a5c97ab,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
59e58c6fc63cf5b54b632462465bfbd85b1bf3dd,0.0,No,[[  0 465   1]],"It does not use a seed list to gather tweets so the dataset does not skew to specific topics, dialect, targets.","[[   94   405    59   169     3     9  6677   570    12  7479 10657     7
     78     8 17953   405    59     3 11049   210    12   806  4064     6
  28461     6  8874     5     1]]"
8dda1ef371933811e2a25a286529c31623cca0c6,0.0,"10,000",[[    0 13923     1]],One experienced annotator tagged all tweets,[[  555  1906    46  2264  1016     3  6153    66 10657     7     1]]
fb1227b3681c69f60eb0539e16c5a8cd784177a7,15.7895,relative authority of all citations,[[    0  5237  5015    13    66     3 13903     7     1]],"Salience features positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in.
The relative authority of entity features:   comparative relevance of the news article to the different entities occurring in it.","[[ 5158    23  1433   753  1102   138   753     6     3 16526  7321    11
      8  3224     3 16034  1809    13     8 10409    11     8  7142    34
   6986    16     5    37  5237  5015    13 10409   753    10 18858    15
  20208    13     8  1506  1108    12     8   315 12311 16198    16    34
      5     1]]"
f225a9f923e4cdd836dd8fe097848da06ec3e0cc,28.5714,"SQuAD BIBREF3, QQuAD Knowledge Base BIBREF1","[[    0   180  5991  6762     3  5972 25582   371  6355  1593  5991  6762
  16113  8430     3  5972 25582   371   536     1]]",SQuAD,[[ 180 5991 6762    1]]
2c59528b6bc5b5dc28a7b69b33594b274908cca6,19.0476,character-level model with a minimum of four attainable changes,"[[    0  1848    18  4563   825    28     3     9  2559    13   662     3
  31720  1112     1]]","processes a sentence of words with misspelled characters, predicting the correct words at each step","[[ 2842     3     9  7142    13  1234    28  3041 14528  2850     6     3
  29856     8  2024  1234    44   284  1147     1]]"
e9cfbfdf30e48cffdeca58d4ac6fdd66a8b27d7a,8.6957,"By annotators who have access to a large corpus of documents, they can identify","[[    0   938    46  2264  6230   113    43   592    12     3     9   508
  11736   302    13  2691     6    79    54  2862]]",provide only a description of the document cluster's topic along with the propositions,"[[  370   163     3     9  4210    13     8  1708  9068    31     7  2859
    590    28     8 12491     7     1]]"
8dc707a0daf7bff61a97d9d854283e65c0c85064,100.0,No,[[  0 465   1]],No,[[465   1]]
da9c0637623885afaf023a319beee87898948fe9,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
de830c534c23f103288c198eb19174c76bfd38a1,0.0,"technical terms, computer science, quantitative finance, nonlinear sciences","[[    0  2268  1353     6  1218  2056     6 18906  4747     6   529   747
    291 13554     1]]",intelligence,[[6123    1]]
955de9f7412ba98a0c91998919fa048d339b1d48,0.0,standard shallow approaches,[[    0  1068 16906  6315     1]],SVM with linear kernel using bag-of-words features,"[[  180 12623    28 13080 20563   338  2182    18   858    18  6051     7
    753     1]]"
2c7494d47b2a69f182e83455fe4c75ae3b2893e9,100.0,No,[[  0 465   1]],No,[[465   1]]
53dfcd5d7d2a81855ec1728f0d8e6e24c5638f1e,0.0,F1 score,[[   0  377  536 2604    1]],"BLEU-1, Meteor , Rouge-L ","[[    3  8775 12062  2292     6  8146    15   127     3     6 23777    18
    434     1]]"
73a7acf33b26f5e9475ee975ba00d14fd06f170f,5.4054,"labeled as ""Dialog"" or ""Question""","[[    0  3783    15    26    38    96 23770  2152   121    42    96  5991
   3340   106   121     1]]","(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer","[[ 5637     8    97     8  1868    65   118  8154     8     3 18018     6
   6499  1087    24  7294     8     3 18018    41   235  4093    42  4131
     29   201 10153     8  5996    13  2261   655     6     3 10820     8
   7321     3 16526    13     8     3 18018     6    11     3 15757     8
   1128    13     3 18018     6   465 11801     1]]"
87bb3105e03ed6ac5abfde0a7ca9b8de8985663c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],They use a slightly modified copy of the target to create the pseudo-text instead of full BT to make their technique cheaper,"[[  328   169     3     9  3300  8473  2405    13     8  2387    12   482
      8 22726    18  6327  1446    13   423     3  9021    12   143    70
   3317  8139     1]]"
9eabb54c2408dac24f00f92cf1061258c7ea2e1a,8.6957,a corpus of 256 sentences and 828 syllables containing,"[[    0     3     9 11736   302    13     3 19337 16513    11   505  2577
      3     7    63   195   179     7     3  6443]]","paragraphs, lines, Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation","[[8986    7    6 2356    6 2784   30 1722  543 5508  257   41 1161 3948
     7  163  201 8986 5508  257    6   11  689 5508  257    1]]"
f1e90a553a4185a4b0299bd179f4f156df798bce,0.0,"BIBREF10, BIBREF10","[[    0     3  5972 25582   371  1714     6     3  5972 25582   371  1714
      1]]","CopyRNN (Meng et al., 2017), Multi-Task (Ye and Wang, 2018), and TG-Net (Chen et al., 2018b)","[[20255 14151   567    41   329  4606     3    15    17   491     5     6
   1233   201  4908    18   382     9     7   157    41   476    15    11
  18102     6   846   201    11     3 18943    18  9688    41  3541    35
      3    15    17   491     5     6   846   115    61     1]]"
7e62a53823aba08bc26b2812db016f5ce6159565,18.1818,Parallel English-Japanese,[[    0 27535  1566    18   683  9750  1496    15     1]],"IITB English-Hindi parallel corpus BIBREF22, ILCI English-Hindi parallel corpus","[[ 2466  9041  1566    18   566  8482  8449 11736   302     3  5972 25582
    371  2884     6     3  3502  3597  1566    18   566  8482  8449 11736
    302     1]]"
2a564b092916f2fabbfe893cf13de169945ef2e1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"there are 20,244 reviews divided into positive and negative with an average 39 words per review, each one having a star-rating score","[[  132    33 16047   357  3628  2456  8807   139  1465    11  2841    28
     46  1348  6352  1234   399  1132     6   284    80   578     3     9
   2213    18    52  1014  2604     1]]"
c17b609b0b090d7e8f99de1445be04f8f66367d4,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Best results on unigram:
CNN/Daily Mail: Rogue F1 43.85
NYT: Rogue Recall 49.02
XSum: Rogue F1 38.81","[[ 1648   772    30    73    23  5096    10 19602    87   308     9  9203
   9614    10   391 12220   377   536  8838     5  4433  5825   382    10
    391 12220   419 16482  9526     5  4305     3     4   134   440    10
    391 12220   377   536  6654     5  4959     1]]"
f4496316ddd35ee2f0ccc6475d73a66abf87b611,0.0,WN18M,[[    0     3 21170  2606   329     1]],dataset created by ceccarelli2013learning from the CoNLL 2003 data,"[[17953   990    57   197    75  1720  7999 11138 20779    45     8   638
    567 10376  3888   331     1]]"
808f0ad46ca4eb4ea5492f9e14ca043fe1e206cc,0.0,3,[[  0 220   1]],"45,821 characters",[[3479    6 4613  536 2850    1]]
9cf070d6671ee4a6353f79a165aa648309e01295,50.0,"1,200 sentences",[[    0  1914  3632 16513     1]],1500 sentences,[[15011 16513     1]]
f7a89b9cd2792f23f2cb43d50a01b8218a6fbb24,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"PER, LOC, ORG, MISC",[[   3 8742    6  301 5618    6 4674  517    6 8161 4112    1]]
3e88fb3d28593309a307eb97e875575644a01463,0.0,Random Forest classifier,[[    0 25942  6944   853  7903     1]],"LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets","[[    3 12564  1768  8055    18   858    18  6051     7     6 25335   357
    162    75     6     3 12564  1768   432 13977    41    17  1123    15
     17    18  4563   201     3 12564  1768   432 13977    41   524  6513
     18  4563   201  1699    75   382  1123    15    17    41    17  1123
     15    17    18  4563   201  2224    18  3229   157  3229 26719     6
    114     7     6    42     3    60    18    17  1123    15    17     7
      1]]"
34dc0838632d643f33c8dbfe7bd4b656586582a2,0.0,personalized text generation BIBREF2,[[    0  9354  1499  3381     3  5972 25582   371   357     1]],"name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)","[[  564    18   390 10455   222    18   567    15  9031  6693   825    41
  17235   201   695  4978    52    18  2962  4978    52 20726    28 11322
   1388    41  8532    75    18  2962    75    61     1]]"
92dfacbbfa732ecea006e251be415a6f89fb4ec6,13.3333,"Nguni, Xho",[[   0  445 8765   23    6    3    4  107   32    1]],"The Nguni languages are similar to each other, The same is true of the Sotho languages","[[  37  445 8765   23 8024   33 1126   12  284  119    6   37  337   19
  1176   13    8  264  189   32 8024    1]]"
1097768b89f8bd28d6ef6443c94feb04c1a1318e,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
bc4dca3e1e83f3b4bbb53a31557fc5d8971603b2,14.2857,"semantic analysis, syntactic analysis, and machine translation","[[    0 27632  1693     6  8953    17  2708   447  1693     6    11  1437
   7314     1]]","SimLex, Rare Word, Google Semantic, Semantic Textual Similarity, Word Content (WC) probing, Google Syntactic analogies, Depth, Top Constituent, part-of-speech (POS) tagging","[[ 6619   434   994     6 23043  4467     6  1163   679   348  1225     6
    679   348  1225  5027  3471 18347   485     6  4467  7185    41 10038
     61 12361    53     6  1163  8951    17  2708   447 10552   725     6
  25734   107     6  2224 22636   295     6   294    18   858    18     7
    855 10217    41 16034    61     3    17 15242     1]]"
3cf1edfa6d53a236cf4258afd87c87c0a477e243,100.0,English,[[   0 1566    1]],English,[[1566    1]]
30af1926559079f59b0df055da76a3a34df8336f,0.0,DeepMine,[[    0  9509 12858    15     1]],Android application,[[3054  917    1]]
3d7d865e905295d11f1e85af5fa89b210e3e9fdf,0.0,10,[[  0 335   1]],100 crowdworkers ,[[ 910 4374 1981  277    1]]
f7ed3b9ed469ed34f46acde86b8a066c52ecf430,0.0,FastText BIBREF6 and LSTM-Vec BIBRE,"[[    0  6805 13598    17     3  5972 25582   371   948    11     3  7600
   2305    18   553    15    75     3  5972 25582]]",weighted factorization of a word-context co-occurrence matrix ,"[[ 1293    15    26  2945  1707    13     3     9  1448    18  1018  6327
    576    18 16526 16826     1]]"
bd5bd1765362c2d972a762ca12675108754aa437,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],1 percent,[[ 209 1093    1]]
55bd59076a49b19d3283af41c5e3ccb875f3eb0c,0.0,BiLSTM with max-order function,[[   0 2106 7600 2305   28 9858   18 9397 1681    1]],CNN ,[[19602     1]]
f2e8497aa16327aa297a7f9f7d156e485fe33945,0.0,manually reviewed,[[    0 12616  9112     1]],WebAnno,[[ 1620 17608    32     1]]
b9d07757e2d2c4be41823dd1ea3b9c7f115b5f72,9.0909,bilingual dictionary of Chinese words,[[    0 30521 24297    13  2830  1234     1]],Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet ,"[[22239  2830   892  3187    16   633     3 24805     7  3010    11  2984
   1545    57 20076   383  5580  7645    18  3632  7645  4759    45     8
   1396     1]]"
051df74dc643498e95d16e58851701628fdfd43e,0.0,"They collected online conversations from a variety of sources including ReachOut.com, ReachOut.","[[    0   328  4759   367  9029    45     3     9  1196    13  2836   379
  23202 15767     5   287     6 23202 15767     5]]",data has been developed by crawling and pre-processing an OSG web forum,"[[  331    65   118  1597    57 18639    53    11   554    18 15056    53
     46  6328   517   765  5130     1]]"
ceb767e33fde4b927e730f893db5ece947ffb0d8,10.5263,"Surgical, Dental, and Sports",[[    0     3 31186     6 11826     6    11  5716     1]],"Demographics, Diagnosis History, Medication History, Procedure History, Symptoms, Labs, Procedures, Treatments, Hospital movements, and others","[[10007 16587     7     6  5267  6715     7   159  5528     6  1212 17530
   5528     6 25266  5528     6     3 21828     7     6  8100     7     6
  25266     7     6 13516     7     6  4457  9780     6    11   717     1]]"
642e8cf1d39faa1cd985d16750cdc6696c52db2f,0.0,"INLINEFORM0 Czech, INLINEFORM1 German, INLINEFORM2 Romanian,","[[    0  3388 20006 24030   632 16870     6  3388 20006 24030   536  2968
      6  3388 20006 24030   357  3871    29     6]]", the dl4mt-tutorial,"[[    8     3    26    40   591    51    17    18    17    76    17 11929
      1]]"
1d3e914d0890fc09311a70de0b20974bf7f0c9fe,0.0,QA,[[    0     3 23008     1]],"BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800","[[ 9580   755   254  3913    18    26   159 14608     6  9187  5972    18
     26   159 14608     6  9580   755   254  3913    18  6482     6  9580
    591 13717 11731     6  9580   357  7381     6   446   567  6892  4882
      6     3 20931  5999   427  3063     6     3  7727   725    18  6192
      1]]"
84bad9a821917cb96584cf5383c6d2a035358d7c,0.0,MCScript,[[    0     3  3698 18255     1]],"The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation.","[[   37   331    47  4759   338   220  3379    10  5530     3     9   939
     13  4487  2116    24   130  4468    12  2868  1017     7  5167    16
  11788   746     6   258  2497     8     3  5490   331  1232    13   746
      6 14877    11  4269  1009  4374 19035    30  2536 24483 23694    11
   1527   251    81   128  1316   442 15056    53  2245    11     8 17953
  16148     5     1]]"
2ee715c7c6289669f11a79743a6b2b696073805d,3.8462,LSTM with a corresponding LDVM model,"[[    0     3  7600  2305    28     3     9     3  9921     3  9815 12623
    825     1]]","For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. 

For Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.","[[  242  7491    18 16924   485  7907     6    79  1099   192 20726     7
     10     8   166    80   338   163  5394    23  1433    18   390   753
      6    11     8   511 20726 11642     3    99     8 10409  3475    16
      8  2233    13     8  1108     5   242  7491    18   134    15  4985
   3399   297     6    79  1099   192 20726     7    10     8   166  1432
      7     8  1375    28     8  2030     3 30949   138  1126   485    12
      8  1108     6    11     8   511    80  1432     7     8   167  8325
   1375     5     1]]"
9af3142630b350c93875441e1e1767312df76d17,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
01209a3bead7c87bcdc628be2a5a26b41abde9d1,0.0,WSJ15 English-Japanese,[[   0    3 8439  683 1808 1566   18  683 9750 1496   15    1]],"SNLI BIBREF22 and MultiNLI BIBREF23 datasets, Quora Question Pairs dataset BIBREF24, Stanford Sentiment Treebank (SST) BIBREF25","[[    3  8544  8159     3  5972 25582   371  2884    11  4908 18207   196
      3  5972 25582   371  2773 17953     7     6  2415   127     9 11860
  25072     7 17953     3  5972 25582   371  2266     6 19796  4892  2998
    295  7552  4739    41   134  4209    61     3  5972 25582   371  1828
      1]]"
fde700d5134a9ae8f7579bea1f1b75f34d7c1c4c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],The speech was collected from respondents using an android application.,[[   37  5023    47  4759    45 16467   338    46 17729   917     5     1]]
ce0e2a8675055a5468c4c54dbb099cfd743df8a7,0.0,a phenotype of a patient with a moderate to severe gastrointestinal condition,"[[    0     3     9     3 19017    32  6137    13     3     9  1868    28
      3     9  8107    12  5274     3 30282  1706]]","Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse","[[    3 21021     5  6219 14326     6     3 21021     5   301   425 14326
      6 21891   891  1074     6 30393 11574 20113 12991     7 14618   725
      6   374   297    23     9     6 23138     6  2958   138   374  5595
      6  4249    15     7   485     6     3 21513    23     9  3929 10461
     11  3325  8389   891  1074     1]]"
5aa12b4063d6182a71870c98e4e1815ff3dc8a72,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
c571deefe93f0a41b60f9886db119947648e967c,0.0,"BIBREF14, BIBREF16","[[    0     3  5972 25582   371  2534     3     6     3  5972 25582   371
   2938     1]]",MIMIC-III,[[ 8161 24896    18 13671     1]]
18942ab8c365955da3fd8fc901dfb1a3b65c1be1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],TripAdvisor,[[16993   188    26 24680     1]]
23e2971c962bb6486bc0a66ff04242170dd22a1d,21.0526,visual features,[[   0 3176  753    1]],It depends on the dataset. Experimental results over two datasets reveal that textual and visual features are complementary. ,"[[   94  5619    30     8 17953     5 30871   772   147   192 17953     7
   6731    24  1499  3471    11  3176   753    33 24345     5     1]]"
71b1af123fe292fd9950b8439db834212f0b0e32,0.0,manually reviewed,[[    0 12616  9112     1]],"1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process","[[ 1300  1698    46  2264  1016   398 12317    46 30278  2604   344     3
    632    11   431    41 23099    15    61     3 15716   149 27632  1427
   1126     8   192  1234    16     3     9   787  3116    33     5    71
   2604    13   431  9379   182   306  1126   485    41    23     5    15
      5     6   626 29443    63   201   298  5733  9379   150  1126   485
      5     6  1682  1698    46  2264  1016   398  2604     8  1297   356
     13  1914 10927 14152    16     8 17953     5     6     3   179    12
    169  3866  2836    41    15     5   122     5     3 12472  5414     6
      8     7     9   459     6  4467  9688    61     3    99   831     6
     59     3   179    12  4521    28   284   119   383     8 30729   433
      1]]"
ee2ad0ab64579cffb60853db6a8c0f971d7cf0ff,0.0,the author and the supervisor,[[    0     8  2291    11     8 14640     1]],Unanswerable,[[ 597 3247 3321  179    1]]
88e5d37617e14d6976cc602a168332fc23644f19,7.6923,WN15 and FB15k,[[    0     3 21170  1808    11     3 15586  1808   157     1]],"An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. ","[[  389  8148   988    13     8  1895     3    31  4302  2660  1628   350
    782    71   210   651    31 17953    11     8  5968  7008 15270 17953
      6     3     9   769  1271    26   155     3  2544   163 30729    19
    823     8  3634   831  1041    57     8  1624    26   155  1794    49
   6230     5     1]]"
f8c1b17d265a61502347c9a937269b38fc3fcab1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"On the datasets DE-EN, JA-EN, RO-EN, and EN-DE, the baseline achieves 29.79, 21.57, 32.70, and 26.02  BLEU score, respectively. The 1.5-entmax achieves  29.83, 22.13, 33.10, and 25.89 BLEU score, which is a difference of +0.04, +0.56, +0.40, and -0.13 BLEU score versus the baseline. The Î±-entmax achieves 29.90, 21.74, 32.89, and 26.93 BLEU score, which is a difference of +0.11, +0.17, +0.19, +0.91 BLEU score versus the baseline.","[[  461     8 17953     7  3396    18  5332     6     3 13853    18  5332
      6 10264    18  5332     6    11 13209    18  5596     6     8 20726
   1984     7  2838     5  4440     6  1401     5  3436     6  3538     5
   2518     6    11  2208     5  4305     3  8775 12062  2604     6  6898
      5    37  8613    18   295  9128  1984     7  2838     5  4591     6
   1630     5  2368     6  5400     5  1714     6    11   944     5  3914
      3  8775 12062  2604     6    84    19     3     9  1750    13  1768
  11739  8525  1768 12100 11071  1768 22776   632     6    11     3    18
  16029   519     3  8775 12062  2604     3  8911     8 20726     5    37
      3     2    18   295  9128  1984     7  2838     5  2394     6  1401
      5  4581     6  3538     5  3914     6    11  2208     5  4271     3
   8775 12062  2604     6    84    19     3     9  1750    13  1768 16029
   4347  1768 16029   940     6  1768 16029  1298     6  1768 23758   536
      3  8775 12062  2604     3  8911     8 20726     5     1]]"
9ec1f88ceec84a10dc070ba70e90a792fba8ce71,0.0,3rd,[[  0 220  52  26   1]],0.5115,[[    3 12100 15660     1]]
3fad42be0fb2052bb404b989cc7d58b440cd23a0,14.2857,"linear and semi-supervised learning frameworks, supervised learning framework, user-system communication framework","[[    0 13080    11  4772    18 23313  1036  4732     7     6     3 23313
   1036  4732     6  1139    18  3734  1901  4732]]",Unif and Stopword,[[ 597   99   11 9078 6051    1]]
72ce05546c81ada05885026470f4c8c218805055,100.0,English,[[   0 1566    1]],English,[[1566    1]]
a3efe43a72b76b8f5e5111b54393d00e6a5c97ab,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],around 332k questions,[[ 300  220 2668  157  746    1]]
6a31db1aca57a818f36bba9002561724655372a7,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"32,595 posts",[[3538    6  755 3301 3489    1]]
64af7f5c109ed10eda4fb1b70ecda21e6d5b96c8,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"First, propaganda is a social phenomenon and takes place as an act of communication BIBREF19, and so it is more than a simple information-theoretic message of zeros and onesâ€”it also incorporates an addresser and addressee(s), each in phatic contact (typically via broadcast media), ideally with a shared denotational code and contextual surround(s) BIBREF20.","[[ 1485     6 25071    19     3     9   569 15037    11  1217   286    38
     46  1810    13  1901     3  5972 25582   371  2294     6    11    78
     34    19    72   145     3     9   650   251    18   532   127  7578
   1569    13  5733     7    11  2102   318   155    92  6300     7    46
   1115    49    11  1115    15    15   599     7   201   284    16     3
   6977  1225   574    41 21888   120  1009  6878   783   201     3 20690
     28     3     9  2471    20  2264   257   138  1081    11 28131 14227
    599     7    61     3  5972 25582   371  1755     5     1]]"
cf171fad0bea5ab985c53d11e48e7883c23cdc44,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"one of the Twitter datasets is about Turkish mobile network operators, there are positive, neutral and negative labels and provide the total amount plus the distribution of labels","[[   80    13     8  3046 17953     7    19    81 15423  1156  1229  9490
      6   132    33  1465     6  7163    11  2841 11241    11   370     8
    792   866   303     8  3438    13 11241     1]]"
612c3675b6c55b60ae6d24265ed8e20f62cb117e,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
88bf368491f9613767f696f84b4bb1f5a7d7cb48,0.0,"translations performed by professional translators are usually in English, but not all translations are in","[[    0  7314     7  3032    57   771 22770     7    33  1086    16  1566
      6    68    59    66  7314     7    33    16]]",Yes,[[2163    1]]
62f27fe08ddb67f16857fab2a8a721926ecbb6fb,8.0,the accuracy,[[   0    8 7452    1]],Irony accuracy is judged only by human ; senriment preservation and content preservation are judged  both by human and using automatic metrics (ACC and BLEU).,"[[ 9046    63  7452    19  5191    26   163    57   936     3   117     3
      7    35  5397   295 19368    11   738 19368    33  5191    26   321
     57   936    11   338  6569 15905    41 14775    11     3  8775 12062
    137     1]]"
f85ca6135b101736f5c16c5b5d40895280016023,0.0,BIBREF4 and BIBREF6,"[[    0     3  5972 25582   371   591    11     3  5972 25582   371   948
      1]]",the baseline transformer BIBREF8,[[    8 20726 19903     3  5972 25582   371   927     1]]
ebf0d9f9260ed61cbfd79b962df3899d05f9ebfb,0.0,"two sets of test sentences, one for a novel'simplified' version of","[[    0   192  3369    13   794 16513     6    80    21     3     9  3714
      3    31     7 10296  3676    31   988    13]]",WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. ,"[[ 2142  2168   134  1982    40     3  3914     3 24978  7142  3116    11
   2142  2168   434  8240    15   204  3916   489  4241  7142 14152     5
      1]]"
3116453e35352a3a90ee5b12246dc7f2e60cfc59,8.3333,"LSTM BIBREF13, LSTM BIBREF18","[[    0     3  7600  2305     3  5972 25582   371  2368     6     3  7600
   2305     3  5972 25582   371  2606     1]]","support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self","[[  380 12938  1437   853  7903    41   134 12623   201 28820 26625   853
   7903    41 12564   201  1823   757  2474    15     7   853  7903    41
  14972   201  6504  5827    41  8556   201 19602     6     3  7600  2305
      3     6     3  7600  2305    18 12369     6     3  7600  2305    18
   7703     1]]"
d6e353e0231d09fd5dcba493544d53706f3fe1ab,0.0,The average accuracy of all the proposed methods is metric of the quality of the singing voice,"[[   0   37 1348 7452   13   66    8 4382 2254   19    3 7959   13    8
   463   13    8 8782 2249    1]]","Automatic: Normalized cross correlation (NCC)
Manual: Mean Opinion Score (MOS)","[[19148    10 16612  1601  2269 18712    41   567  2823    61  9950    10
  23045   411 22441 17763    41  5365   134    61     1]]"
0d34c0812f1e69ea33f76ca8c24c23b0415ebc8d,8.3333,"feature generation on a review basis, hand-crafted feature generation on a review basis,","[[   0 1451 3381   30    3    9 1132 1873    6  609   18 8810 1451 3381
    30    3    9 1132 1873    6]]","polarity scores, which are minimum, mean, and maximum polarity scores, from each review","[[   3 9618  485 7586    6   84   33 2559    6 1243    6   11 2411    3
  9618  485 7586    6   45  284 1132    1]]"
bdf93053b1b9b0a21f77ed370cf4d5a10df70e3e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"accuracy and F1-score of 89.6% and 89.2%, respectively","[[7452   11  377  536   18    7 9022   13    3 3914    5 6370   11    3
  3914    5 5406    6 6898    1]]"
fd0a3e9c210163a55d3ed791e95ae3875184b8f8,66.6667,WSJ 2013,[[   0    3 8439  683 2038    1]],WSJ,[[   3 8439  683    1]]
584af673429c7f8621c6bf83362a37048daa0e5d,5.7971,by combining image captions with text and generating a story from sequences,"[[    0    57     3 13275  1023 25012     7    28  1499    11     3 11600
      3     9   733    45  5932     7     1]]","The encoder takes the images in order, one at every timestep $t$ . At time $t=5$ , we obtain the context vector through $h_e^{(t)}$ (represented by $\mathbf {Z}$ ). This vector is used to initialize each decoder's hidden state while the first input to each decoder is its corresponding image embedding $e(I_i)$ . Each decoder generates a sequence of words $\lbrace p_1,...,p_{n}\rbrace $ for each image in the sequence. ","[[   37 23734    52  1217     8  1383    16   455     6    80    44   334
     97  7910  1514    17  3229     3     5   486    97  1514    17  2423
    755  3229     3     6    62  3442     8  2625 12938   190  1514   107
    834    15     2   599    17    61     2  3229    41 29845    57  1514
      2  3357   107   115    89     3     2   956     2  3229     3   137
    100 12938    19   261    12  2332  1737   284    20  4978    52    31
      7  5697   538   298     8   166  3785    12   284    20  4978    52
     19   165     3  9921  1023 25078    26    53  1514    15   599   196
    834    23    61  3229     3     5  1698    20  4978    52  3806     7
      3     9  5932    13  1234  1514     2    40  1939   565     3   102
    834  4347   233     6   102   834     2    29     2    52  1939   565
   1514    21   284  1023    16     8  5932     5     1]]"
b5608076d91450b0d295ad14c3e3a90d7e168d0e,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
5daeb8d4d6f3b8543ec6309a7a35523e160437eb,0.0,German,[[   0 2968    1]],English,[[1566    1]]
2c7494d47b2a69f182e83455fe4c75ae3b2893e9,100.0,No,[[  0 465   1]],No,[[465   1]]
935d6a6187e6a0c9c0da8e53a42697f853f5c248,0.0,industry,[[  0 681   1]],the aggregate of enterprises in a particular field,[[    8 12955    13 14876    16     3     9  1090  1057     1]]
c5b0ed5db65051eebd858beaf303809aa815e8e5,50.0,large BERT,[[    0   508   272 24203     1]],small BERT,[[  422   272 24203     1]]
9b76f428b7c8c9fc930aa88ee585a03478bff9b3,0.0,103,[[    0     3 17864     1]],53 documents,[[12210  2691     1]]
887c6727e9f25ade61b4853a869fe712fe0b703d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists,"[[   62 27354    69 24228   516   127    28   192  1502    10  5637  3388
  20006 24030   632    11  6499  3388 20006 24030   536  8085     1]]"
f2155dc4aeab86bf31a838c8ff388c85440fce6e,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
62ea141d0fb342dfb97c69b49d1c978665b93b3c,36.3636,"erroneous text segments, grammatical errors","[[    0     3    49    52   782  1162  1499 15107     6     3  5096  4992
    138  6854     1]]","grammatical, spelling and word order errors",[[    3  5096  4992   138     6 19590    11  1448   455  6854     1]]
4ac2c3c259024d7cd8e449600b499f93332dab60,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
d7b60abb0091246e29d1a9c28467de598e090c20,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
df6d327e176740da9edcc111a06374c54c8e809c,0.0,"Li and Roth BIBREF11, Roth BIBREF22","[[    0  1414    11 17269     3  5972 25582   371  2596     6 17269     3
   5972 25582   371  2884     1]]","bag-of-words model, CNN",[[ 2182    18   858    18  6051     7   825     6 19602     1]]
e35a7f9513ff1cc0f0520f1d4ad9168a47dc18bb,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"traditional phrase-based statistical machine translation (SMT), NMT system","[[ 1435  9261    18   390 11775  1437  7314    41   134  7323   201   445
   7323   358     1]]"
2916bbdb95ef31ab26527ba67961cf5ec94d6afe,0.0,"82,432",[[   0    3 4613    6  591 2668    1]],"The corpus comprises 8,275 sentences and 167,739 words in total.","[[   37 11736   302 13009  9478 25988 16513    11     3 27650     6   940
   3288  1234    16   792     5     1]]"
4eaf9787f51cd7cdc45eb85cf223d752328c6ee4,0.0,WSJ5 and jyoti2016,"[[    0     3  8439   683   755    11     3   354    63    32    17    23
  11505     1]]","the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary","[[    8 29331  5049    40   106   749 15358  4733 28767     3  5972 25582
    371  2122     6     8  1249 25207 30637 11736   302  4759    57    74
     23 11505  9413    15   526     3     6  4037 11830     7 21527    45
   2142 12696  1208     1]]"
b9ea841b817ba23281c95c7a769873b840dee8d5,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
e8f969ffd637b82d04d3be28c51f0f3ca6b3883e,0.0,"AMR, Meaning Representation, Recurrent Networks, Recurrent Networks, Random","[[    0    71  9320     6 25148   419 12640   257     6   419 14907  3426
      7     6   419 14907  3426     7     6 25942]]","Quantitative evaluation methods using ROUGE, Recall, Precision and F1.","[[12716   155  1528  5002  2254   338   391 26260   427     6   419 16482
      6 28464    11   377  5411     1]]"
a7d020120a45c39bee624f65443e09b895c10533,13.6364,in a conversational fashion by imitating human apprehensions and in,"[[    0    16     3     9  3634   138  2934    57   256   155  1014   936
      3     9   102 22459   106     7    11    16]]","newly acquired facts are retained in the KB and used in inference for future queries, and that the accumulated knowledge in addition to the updated KB including past inference performances are leveraged to guide future interaction and learning","[[ 6164  7347  6688    33 19346    16     8     3 17827    11   261    16
     16 11788    21   647 13154     6    11    24     8     3 22148  1103
     16   811    12     8  3250     3 17827   379   657    16 11788  7357
     33 11531    26    12  1539   647  6565    11  1036     1]]"
7fb27d8d5a8bb351f97236a1f6dcd8b2613b16f1,0.0,BIBREF2 is the current state of the art,"[[    0     3  5972 25582   371   357    19     8   750   538    13     8
    768     1]]",RoBERTa,[[ 2158 12920   382     9     1]]
37edc25e39515ffc2d92115d2fcd9e6ceb18898b,0.0,a five-point scale ranging from VeryNegative to VeryPos,"[[   0    3    9  874   18 2700 2643    3 6836   45 4242  567   15  122
  1528   12 4242  345   32    7]]","SVM INLINEFORM0, SVM INLINEFORM1, LR INLINEFORM2, MaxEnt","[[  180 12623  3388 20006 24030   632     6   180 12623  3388 20006 24030
   4347     3 12564  3388 20006 24030  4482  5370 16924     1]]"
27275fe9f6a9004639f9ac33c3a5767fea388a98,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Dimension size, window size, architecture, algorithm, epochs, hidden dimension size, learning rate, loss function, optimizer algorithm.","[[14689   812     6  2034   812     6  4648     6 12628     6     3    15
    102  6322     7     6  5697  9340   812     6  1036  1080     6  1453
   1681     6 13436    52 12628     5     1]]"
a253749e3b4c4f340778235f640ce694642a4555,0.0,CSA broadcasts,[[    0     3 24135  6878     7     1]],"ESTER1, ESTER2, ETAPE, REPERE","[[  262 20727  4347   262 20727  4482     3 25747  5668     6  4083  8742
    427     1]]"
a69a59b6c0ab27bcee1a780d6867df21e30aec08,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
7006c66a15477b917656f435d66f63760d33a304,20.0,abus outperforms by 0.61 points 0.82 in the test set and 0.,"[[   0    3    9 3465   91  883 2032    7   57 4097 4241  979 4097 4613
    16    8  794  356   11 4097]]",Average success rate is higher by 2.6 percent points.,[[23836  1269  1080    19  1146    57     3 22724  1093   979     5     1]]
2236386729105f5cf42f73cc055ce3acdea2d452,100.0,English,[[   0 1566    1]],English,[[1566    1]]
2268c9044e868ba0a16e92d2063ada87f68b5d03,0.0,Yes,[[   0 2163    1]],"They increased F1 Score by 0.029 in Sentence Level Classification, and by 0.044 in Fragment-Level classification","[[  328  1936   377   536 17763    57     3 11739  3166    16  4892    17
   1433  7166  4501  2420     6    11    57     3 11739  3628    16 10323
    122   297    18 25465 13774     1]]"
6424e442b34a576f904d9649d63acf1e4fdefdfc,0.0,Wiktionary,[[    0  2142 12696  1208     1]], Wall Street Journal (WSJ) portion of the Penn Treebank,"[[ 3556  1887  3559    41  8439   683    61  4149    13     8 11358  7552
   4739     1]]"
feafcc1c4026d7f55a2c8ce7850d7e12030b5c22,0.0,Yes,[[   0 2163    1]],"the objective of our model is sum of the two processes, jointly trained using ""teacher-forcing"" algorithm, we feed the ground-truth summary to each decoder and minimize the objective, At test time, each time step we choose the predicted word by $\hat{y} = argmax_{y^{\prime }} P(y^{\prime }|x)$ , use beam search to generate the draft summaries, and use greedy search to generate the refined summaries., the model can be trained end-to-end","[[    8  5997    13    69   825    19  4505    13     8   192  2842     6
  22801  4252   338    96 30215    18  1161    75    53   121 12628     6
     62  3305     8  1591    18  2666   189  9251    12   284    20  4978
     52    11 10558     8  5997     6   486   794    97     6   284    97
   1147    62   854     8 15439  1448    57  1514     2   547     2    63
      2  3274     3  8240  9128   834     2    63     2  8234    15     3
      2   276   599    63     2  8234    15     3     2  9175   226    61
   3229     3     6   169 11638   960    12  3806     8  6488  4505    51
   5414     6    11   169 30337    63   960    12  3806     8 16097  4505
     51  5414     5     6     8   825    54    36  4252   414    18   235
     18   989     1]]"
29923a824c98b3ba85ced964a0e6a2af35758abe,11.7647,BLEU scores,[[    0     3  8775 12062  7586     1]],"computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations","[[29216    26  7142    18  4563     3  8775 12062     6   101 12616     3
  26300  4062   213     8  1391 20146    47 12022    12     8  7314     6
   8413     8   399  9247   485    13     8  7314     7     6 29216    26
      8  5688    13  1566  2850    16     8  7314     7     6 11837  1126
    485  7586   344 20146     7    11  7314     7     1]]"
e587559f5ab6e42f7d981372ee34aebdc92b646e,3.6364,"Result: 62.0%, 62.0%, 63.0%","[[    0     3 20119    10     3  4056     5  6932     3     6     3  4056
      5  6932     6     3  3891     5  6932     1]]","On WSJ datasets author's best approach achieves 9.3 and 6.9 WER compared to best results of 7.5 and 4.1 on nov93 and nov92 subsets.
On Hub5'00 datasets author's best approach achieves WER of 7.8 and 16.2 compared to best result of 7.3 and 14.2 on Switchboard (SWB) and Callhome (CHM) subsets.","[[  461     3  8439   683 17953     7  2291    31     7   200  1295  1984
      7  5835   519    11  4357  1298   549  3316     3  2172    12   200
    772    13     3 15731    11     3 19708    30     3  5326  4271    11
      3  5326  4508   769  2244     7     5   461 10261   755    31  1206
  17953     7  2291    31     7   200  1295  1984     7   549  3316    13
   4306   927    11 10128   357     3  2172    12   200   741    13     3
  27914    11  9264   357    30 13218  1976    41 17838   279    61    11
   2571  5515    41  8360   329    61   769  2244     7     5     1]]"
8d074aabf4f51c8455618c5bf7689d3f62c4da1d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"lacks of complete review approaches, datasets and toolkits ","[[ 2136     7    13   743  1132  6315     6 17953     7    11  1464  9229
      7     1]]"
75df70ce7aa714ec4c6456d0c51f82a16227f2cb,33.3333,"Hindi, English, Tamil, Punjabi",[[    0 25763     6  1566     6 22503     6 27864    23     1]],"Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam","[[25763     6  1566     6  4540 18089     6  2255  8076    76     6   282
      7     9  2687    15     6 20008    23    11   283 22858    40   265
      1]]"
10d450960907091f13e0be55f40bcb96f44dd074,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
c00ce1e3be14610fb4e1f0614005911bb5ff0302,0.0,Random Forest classifier,[[    0 25942  6944   853  7903     1]],"Activation function is hyperparameter. Possible values: relu, selu, tanh.","[[    3 13035   257  1681    19  6676  6583  4401     5 29403  2620    10
   8318    76     6   142    40    76     6     3    17   152   107     5
      1]]"
fb5fb11e7d01b9f9efe3db3417b8faf4f8d6931f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Logistic regression, LSTM, End-to-end memory networks, Deep projective reader","[[ 7736  3040 26625     6     3  7600  2305     6  3720    18   235    18
    989  2594  5275     6  9509   516   757  5471     1]]"
f7ed3b9ed469ed34f46acde86b8a066c52ecf430,0.0,FastText BIBREF6 and LSTM-Vec BIBRE,"[[    0  6805 13598    17     3  5972 25582   371   948    11     3  7600
   2305    18   553    15    75     3  5972 25582]]",The LexVec BIBREF7,[[   37 17546   553    15    75     3  5972 25582   371   940     1]]
ea56148a8356a1918bedcf0a99ae667c27792cfe,33.3333,CoNLL 2014,[[    0   638   567 10376  1412     1]],"FCE ,  two alternative annotations of the CoNLL 2014 Shared Task dataset","[[  377  4770     3     6   192  2433 30729     7    13     8   638   567
  10376  1412  7105    26 16107 17953     1]]"
435570723b37ee1f5898c1a34ef86a0b2e8701bb,0.0,LSTMs,[[   0    3 7600 2305    7    1]], English-Spanish MT system ,[[ 1566    18 19675  1273     3  7323   358     1]]
53014cfb506f6fffb22577bf580ae6f4d5317ce5,0.0,WN18 and FB15k,[[    0     3 21170  2606    11     3 15586  1808   157     1]],"CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum","[[19602    87   308     9  9203  7098  1506  8285     6   368  1060  5324
    389  2264   920 10052   302     6     3     4   134   440     1]]"
6263b2cba18207474786b303852d2f0d7068d4b6,18.1818,"English, Chinese",[[   0 1566    6 2830    1]],"English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian","[[ 1566     6  2968     6  5093     6 31057     6 16073     6  4263     6
   9677    11 22831    29     1]]"
b5bc34e1e381dbf972d0b594fe8c66ff75305d71,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively","[[ 1566 17953     7   379   638   567 10376 23948    11   461   235 10358
     15     7 20734     6    69  4382  1573    91   883  2032     7   272
  24203    18 27396  5972 25582   371  3747    57  1768 18189  1298    11
   1768 23758   948  6898     6  2830 17953     7     6     3  9582   377
    536  6867    57  1768 23758   940    11  1768  4416  3420    30  5266
   4763    11   461   235 10358    15     7 15021     6  6898     1]]"
440faf8d0af8291d324977ad0f68c8d661fe365e,0.0,WN18M,[[    0     3 21170  2606   329     1]],Reuters-8 dataset without stop words,[[    3 18844  6039 17953   406  1190  1234     1]]
9de2f73a3db0c695e5e0f5a3d791fdc370b1df6e,0.0,No,[[  0 465   1]],They use text transcription.,[[  328   169  1499 20267     5     1]]
11e8bd4abf5f8bdabad3e8f0691e6d0ad6c326af,0.0,strong neural baseline,[[    0  1101 24228 20726     1]],"NO-MOVE: the only position considered is the starting point, RANDOM: As in BIBREF4, turn to a randomly selected heading, then execute a number of *WALK actions of an average route, JUMP: at each sentence, extract entities from the map and move between them in the order they appear","[[ 5693    18   329 26479    10     8   163  1102  1702    19     8  1684
    500     6     3 16375 27415    10   282    16     3  5972 25582   371
   8525   919    12     3     9 21306  2639  6904     6   258 12133     3
      9   381    13  1429 12054 22527  2874    13    46  1348  2981     6
    446 28468    10    44   284  7142     6  5819 12311    45     8  2828
     11   888   344   135    16     8   455    79  2385     1]]"
c497e8701060583d91bb64b9f9202d40047effc4,0.0,by examining tweets for relevant information.,[[    0    57     3 20424 10657     7    21  2193   251     5     1]]," we look into the archived snapshots of two major news websites (CNN, NBC), and then extract the tweet blocks that are embedded in the news articles","[[   62   320   139     8 13269    26 23052     7    13   192   779  1506
   3395    41   254 17235     6     3 15829   201    11   258  5819     8
  10657  6438    24    33 13612    16     8  1506  2984     1]]"
b970f48d30775d3468952795bc72976baab3438e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"identifying the questions we wish to explore, Can text analysis provide a new perspective on a â€œbig questionâ€ that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines","[[    3  9690     8   746    62  1663    12  2075     6  1072  1499  1693
    370     3     9   126  3503    30     3     9   105 12911   822   153
     24    65   118     3 21600  1046    21   203    58     6   571    54
     62  3209   125    62  7743    58     6   897    12  1979    12  1317
  15015     1]]"
9ee07edc371e014df686ced4fb0c3a7b9ce3d5dc,0.0,"BIBREF21, BIBREF22, BIBREF23","[[    0     3  5972 25582   371  2658     6     3  5972 25582   371  2884
      6     3  5972 25582   371  2773     1]]","WebQSP, SimpleQuestions",[[1620 2247 4274    6 9415 5991  222 2865    1]]
72755c2d79210857cfff60bfbcb55f83c71ada51,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"104 telephone calls, which pair 11 hours of audio",[[    3 15442  6596  3088     6    84  3116   850   716    13  2931     1]]
662870a90890c620a964720b2ca122a1139410ea,50.0,English,[[   0 1566    1]],"English, French, German ",[[1566    6 2379    6 2968    1]]
46227b4265f1d300a5ed71bf40822829de662bc2,0.0,BIBREF1,[[    0     3  5972 25582   371   536     1]],"AMR Bank, CNN-Dailymail",[[   71  9320  1925     6 19602    18   308     9  9203  1963     1]]
193ee49ae0f8827a6e67388a10da59e137e7769f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],A task for seq2seq model pra-training that recovers a masked document to its original form.,"[[   71  2491    21   142  1824   357     7    15  1824   825     3  5319
     18 13023    24  8303     7     3     9     3    51 23552  1708    12
    165   926   607     5     1]]"
f903396d943541a8cc65edefb04ca37814ed30dd,0.0,The Federalist Papers,[[   0   37 5034  343 6564    7    1]],Unanswerable,[[ 597 3247 3321  179    1]]
bdf93053b1b9b0a21f77ed370cf4d5a10df70e3e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"accuracy and F1-score of 89.6% and 89.2%, respectively","[[7452   11  377  536   18    7 9022   13    3 3914    5 6370   11    3
  3914    5 5406    6 6898    1]]"
1dac4bc5af239024566fcb0f43bb9ff1c248ecec,100.0,No,[[  0 465   1]],No,[[465   1]]
dc2a2c177cd5df6da5d03e6e74262bf424850ec9,6.6667,political biases are not included in the model,[[    0  1827 14387    15     7    33    59  1285    16     8   825     1]],By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains,"[[  938 12317    53     3     9  1827 14387  3783    12   284  1506  1108
     11   761   163    30   646    18 15500  3843    42   269    18 15500
   3843 14290    13   321  1028  6391    11 12946  3303     7     1]]"
ae7c93646aa5f3206cd759904965b4d484d12f83,22.2222,"Using a distance supervision strategy, we achieve a 5% improvement over the best state","[[    0     3  3626     3     9  2357 15520  1998     6    62  1984     3
      9     3  2712  4179   147     8   200   538]]",absolute improvement of 18.2% over the Pointer-Gen baseline,"[[ 6097  4179    13 12265  5406   147     8  4564    49    18 13714 20726
      1]]"
f161e6d5aecf8fae3a26374dcb3e4e1b40530c95,0.0,LSTMs,[[   0    3 7600 2305    7    1]]," simple lookup table embeddings learned from scratch, using high-performance contextual embeddings, which are ELMo BIBREF11, BERT BIBREF16 and ClinicalBERT BIBREF13","[[  650   320   413   953 25078    26    53     7  2525    45  8629     6
    338   306    18 18558 28131 25078    26    53     7     6    84    33
    262 11160    32     3  5972 25582   371  2596     6   272 24203     3
   5972 25582   371  2938    11 14067 12920   382     3  5972 25582   371
   2368     1]]"
92dfacbbfa732ecea006e251be415a6f89fb4ec6,30.7692,"Nguni, Xho",[[   0  445 8765   23    6    3    4  107   32    1]],"Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)","[[ 445 8765   23 8024   41  172   83    6    3  226  107   32    6    3
    29  115   40    6    3    7    7  210  201  264  189   32 8024   41
    29    7   32    6   78   17    6    3   17    7   29   61    1]]"
0ba3ea93eef5660a79ea3c26c6a270eac32dfa4c,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
5da26954fbcd3cf6a7dba9f8b3c9a4b0391f67d4,11.1111,they combine text sequences to generate a single RNN model,"[[    0    79  5148  1499  5932     7    12  3806     3     9   712   391
  17235   825     1]]",combines the information from these sources using a feed-forward neural model,"[[    3 15256     8   251    45   175  2836   338     3     9  3305    18
  26338 24228   825     1]]"
eda4869c67fe8bbf83db632275f053e7e0241e8c,0.0,WikiText-TL-02,[[    0  2142  2168 13598    17    18 12733    18  4305     1]]," Paraphrase Database (PPDB) ,  book corpus","[[ 4734 27111 20230    41  6158  9213    61     3     6   484 11736   302
      1]]"
3e432d71512ffbd790a482c716e7079ee78ce732,0.0,BIBREF15,[[    0     3  5972 25582   371  1808     1]],a self-collected financial intents dataset in Portuguese,"[[    3     9  1044    18 22153    15    26   981  9508     7 17953    16
  21076     1]]"
b3d01ac226ee979e188a4141877a6d2a5482de98,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"state-of-the-art models learn to exploit spurious statistical patterns in datasets, human annotatorsâ€”be they seasoned NLP researchers or non-expertsâ€”might easily be able to construct examples that expose model brittleness","[[  538    18   858    18   532    18  1408  2250   669    12  9248 18421
   2936 11775  4264    16 17953     7     6   936    46  2264  6230   318
    346    79     3 18720   445  6892  4768    42   529    18 20216     7
    318    51  2632  1153    36     3   179    12  6774  4062    24 14644
    825     3  2160  8692   655     1]]"
89497e93980ab6d8c34a6d95ebf8c1e1d98ba43f,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
113d791df6fcfc9cecfb7b1bebaf32cc2e4402ab,0.0,BLEU,[[    0     3  8775 12062     1]],Using file size on disk,[[   3 3626 1042  812   30 8987    1]]
a8f51b4e334a917702422782329d97304a2fe139,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],1000,[[5580    1]]
e97186c51d4af490dba6faaf833d269c8256426c,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
0682bf049f96fa603d50f0fdad0b79a5c55f6c97,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
f0b2289cb887740f9255909018f400f028b1ef26,40.0,sexual harassment,[[    0  6949 23556     1]],"indirect, physical, sexual",[[16335     6  1722     6  6949     1]]
1c0ba6958da09411deded4a14dfea5be55687619,0.0,82,[[   0    3 4613    1]],Unanswerable,[[ 597 3247 3321  179    1]]
477d9d3376af4d938bb01280fe48d9ae7c9cf7f7,0.0,accuracy,[[   0 7452    1]],"BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19","[[    3  8775 12062  2292    41   279  6982     6     3  8775 12062  4949
     41   279  7318     6     3  8775 12062  3486    41   279  5268     6
      3  8775 12062  4278    41   279  7256     3  5972 25582   371  2517
      6  7934  3463  2990    41 24506    61     3  5972 25582   371  2606
     11   391 26260   427    18   434    41   448    18   434    61     3
   5972 25582   371  2294     1]]"
d9412dda3279729e95fcb35cbed09e61577a896e,40.0,accuracy,[[   0 7452    1]],"precision, recall, F1 , accuracy ",[[11723     6  7881     6   377   536     3     6  7452     1]]
c9b8d3858c112859eabee54248b874331c48f71b,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
2eb9280d72cde9de3aabbed993009a98a5fe0990,0.0,MCScript is a dataset of 155k events spanning 57 languages,"[[    0     3  3698 18255    19     3     9 17953    13     3 20896   157
    984     3 25754     3  3436  8024     1]]","13,939",[[10670  1298  3288     1]]
d915b401bb96c9f104a0353bef9254672e6f5a47,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions,"[[    3    52   189    49 27354     8   825    30     8   331  1809    16
    455    12  1709 27801    13   237 21454   127    63 15293     1]]"
899ed05c460bf2aa0aa65101cad1986d4f622652,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"$3,209$ reviews about 553 different cars from 49 different car manufacturers","[[5583    6  357 4198 3229 2456   81  305 4867  315 2948   45 9526  315
   443 5360    1]]"
feb448860918ef5b905bb25d7b855ba389117c1f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]], $\textbf {All India Radio}$ news channel,"[[1514    2 6327  115   89    3    2 6838 1547 5061    2 3229 1506 4245
     1]]"
72ce05546c81ada05885026470f4c8c218805055,0.0,English,[[   0 1566    1]],Unanswerable,[[ 597 3247 3321  179    1]]
b0d66760829f111b8fad0bd81ca331ddd943ef41,17.1429,generating headlines based on an article without labeling them as sensational samples,"[[    0     3 11600 12392     7     3   390    30    46  1108   406  3783
     53   135    38  9118   138  5977     1]]","ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE","[[11398   746    81     3 11600  9118   138 12392     7     6    84    54
     36   856 15883     6  4863     8  9118  6835  2604    52     6 17679
      8  1564    13  4896     3 10810  4733  2254   344     3 12831    11
    283  3765     1]]"
1462eb312944926469e7cee067dfc7f1267a2a8c,0.0,82,[[   0    3 4613    1]],"OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities","[[  421   567    15  6459    23  2579   220   315  1308    13 12311     6
     27  6892 12831  2579   314   315  1308    13 12311     1]]"
fd8e23947095fe2230ffe1a478945829b09c8c95,0.0,are meaningful chains selected from the graph,[[    0    33  7892 16534  2639    45     8  8373     1]],No,[[465   1]]
23b2901264bda91045258b5d4120879ae292e950,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],+0.58,[[ 1768 12100   927     1]]
c87fcc98625e82fdb494ff0f5309319620d69040,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
22732cb9476e521452bf0538f3fdb94cf3867651,0.0,stylistic features are extracted from human eye-movement patterns that supports the conjecture that,"[[    0 22308   447   753    33 21527    45   936  1580    18  7168  1194
   4264    24  4951     8   975 11827  1462    24]]",Unanswerable,[[ 597 3247 3321  179    1]]
8e52637026bee9061f9558178eaec08279bf7ac6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],machine translation platform Apertium BIBREF5,"[[ 1437  7314  1585    71   883    17  2552     3  5972 25582   371   755
      1]]"
58355e2a782bf145b61ee2a3e0e426119985c179,52.6316,Twitter dataset for the ECML PKDD 2019 Conference,"[[    0  3046 17953    21     8     3  3073  6858     3 16782 11253  1360
   4379     1]]",The dataset from the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference. ,"[[   37 17953    45     8 16874     9   434  3159   275  3504     9     7
      7   297 15571    13     8     3  3073  6858     3 16782 11253  1360
   4379     5     1]]"
55507f066073b29c1736b684c09c045064053ba9,5.2632,"vulgarity, gender, proximity to target, proximity to target","[[    0 31648   485     6  7285     6 16595    12  2387     6 16595    12
   2387     1]]","Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.","[[ 5532 19404   169    13  1223  2586   564  3874     6   338   108  8770
     11 21253     7     6   190 16335  5023   114     3     7  4667     9
      7    51     6     3 19865  8293    12   717     6   564  8310   257
      6     3 27908 10133  2420     6   256 21511  3889    11  6949   120
   1341  2284     5     1]]"
8c852fc29bda014d28c3ee5b5a7e449ab9152d35,12.5,"Bi-LSTM, BERT",[[    0  2106    18  7600  2305     6   272 24203     1]],"linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) ","[[13080   180 12623  4252    30  1448    73    23  5096     7     6  2647
  26352  3230  7110    18 11679    18   329    15  2528    63    41   279
     23  7600  2305   201  1193 24817   138  1484  9709  3426    41   254
  17235    61     1]]"
2317ca8d475b01f6632537b95895608dc40c4415,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],sequence of $s$ tweets,[[ 5932    13  1514     7  3229 10657     7     1]]
493e971ee3f57a821ef1f67ef3cd47ade154e7c4,0.0,word2vec BIBREF16,[[    0  1448   357   162    75     3  5972 25582   371  2938     1]],"Bernoulli embeddings, continuous bag-of-words, Distributed Memory version of Paragraph Vector, Global Vectors, equation embeddings, equation unit embeddings","[[ 8942  1063   195    23 25078    26    53     7     6  7558  2182    18
    858    18  6051     7     6 21796  1054 19159   988    13  4734  9413
  29011     6  3699 29011     7     6 13850 25078    26    53     7     6
  13850  1745 25078    26    53     7     1]]"
bc31a3d2f7c608df8c019a64d64cb0ccc5669210,0.0,BiLSTM BIBREF2,[[    0  2106  7600  2305     3  5972 25582   371   357     1]],BERTbase,[[  272 24203 10925     1]]
6b9b9e5d154cb963f6d921093539490daa5ebbae,0.0,variants of the BERT model,[[    0  6826     7    13     8   272 24203   825     1]],clipped PMI; NNEGPMI,[[5516 3138 3246  196  117  445 4171 8049 7075    1]]
37a79be0148e1751ffb2daabe4c8ec6680036106,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],anti-nuclear-power,[[1181   18   29   76 2482  291   18 6740    1]]
b1ce129678e37070e69f01332f1a8587e18e06b0,0.0,BIBREF14,[[    0     3  5972 25582   371  2534     1]],Collected tweets and opening and closing stock prices of Microsoft.,"[[ 9919  7633 10657     7    11  2101    11  6733  1519  1596    13  2803
      5     1]]"
6cd8bad8a031ce6d802ded90f9754088e0c8d653,17.3913,"their best model outperforms by 0.61, 0.61, 0.61 ","[[   0   70  200  825   91  883 2032    7   57 4097 4241    3    6 4097
  4241    3    6 4097 4241    3]]",Best proposed model achieves F1 score of 84.9 compared to best previous result of 84.1.,"[[ 1648  4382   825  1984     7   377   536  2604    13   505 27336     3
   2172    12   200  1767   741    13   505 19708     5     1]]"
c49ee6ac4dc812ff84d255886fd5aff794f53c39,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
1329280df5ee9e902b2742bde4a97bc3e6573ff3,100.0,No,[[  0 465   1]],No,[[465   1]]
45e9533586199bde19313cd43b3d0ecadcaf7a33,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
1eef2d2c296fdd10b08bf7b4ff7792cccf177d3b,0.0,"feature extraction, classification, labeling, wording, and word embeddings","[[    0  1451 16629     6 13774     6  3783    53     6  1448    53     6
     11  1448 25078    26    53     7     1]]",Unanswerable,[[ 597 3247 3321  179    1]]
1b1a30e9e68a9ae76af467e60cefb180d135e285,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"353 conversations from 40 speakers (11 nurses, 16 patients, and 13 caregivers), we build templates and expression pools using linguistic analysis","[[  220  4867  9029    45  1283  7215    41  2596 14993     6   898  1221
      6    11  1179 17997     7   201    62   918  7405    11  3893 14652
    338     3 24703  1693     1]]"
3ddff6b707767c3dd54d7104fe88b628765cae58,0.0,"Bigram, trigram hidden Markov, decision trees, MEMMs, CRFs","[[   0 2734 2375    6 6467 5096 5697 1571 9789    6 1357 3124    6 7934
  8257    7    6  205 8556    7]]","Universal Dependencies v1.2 treebanks for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German,
Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish","[[12489 30718 11573     3   208 10917  2195  4739     7    21     8   826
    898  8024    10 15536    29     6 19789    29     6 16870     6 23124
      6  1566     6  2379     6  2968     6  9995    29     6  4338     6
  21894     6 25518     6 16073     6 21076     6 27425    29     6  5093
      6    11 16531     1]]"
15cdd9ea4bae8891c1652da2ed34c87bbbd0edb8,0.0,"jokes, quotes, self deprecation etc","[[    0 10802     7     6  7599     6  1044    20  2026    75   257   672
      1]]","tweets from the past two years from domains like `sports', `politics', `entertainment'","[[10657     7    45     8   657   192   203    45  3303     7   114     3
      2  6661     7    31     6     3     2  3003  7636    31     6     3
      2   295    49    17     9    77   297    31     1]]"
e1ab11885f72b4658263a60751d956ba661c1d61,0.0,"BIBREF4, BIBREF5, BIBREF6","[[    0     3  5972 25582   371  8525     3  5972 25582   371 11116     3
   5972 25582   371   948     1]]","Answer with content missing: (Subscript 1: ""We did not participate in subtask 5 (E-c)"") Authors participated in EI-Reg, EI-Oc, V-Reg and V-Oc subtasks.","[[11801    28   738  3586    10    41 25252 11815   209    10    96  1326
    410    59  3716    16   769 23615   305    41   427    18    75    61
   8512 10236     7 10627    16   262   196    18 17748     6   262   196
     18   667    75     6   584    18 17748    11   584    18   667    75
    769 23615     7     5     1]]"
0101ebfbaba75fd47868ad0c796ac44ebc19c566,0.0,They use two separate aggregation mechanisms: word-level aggregation and sentence,"[[    0   328   169   192  2450     3 31761    23   106 12009    10  1448
     18  4563     3 31761    23   106    11  7142]]",Unanswerable,[[ 597 3247 3321  179    1]]
5cc2daca2a84ddccba9cdd9449e51bb3f64b3dde,0.0,"LSTM, Deep Neural Network (RNN)","[[    0     3  7600  2305     6  9509  1484  9709  3426    41 14151   567
     61     1]]","For speech synthesis, they build a speech clustergen statistical speech synthesizer BIBREF9. For speech recognition, they use Kaldi BIBREF11. For Machine Translation, they use a Transformer architecture from BIBREF15.","[[  242  5023     3 17282     6    79   918     3     9  5023  9068   729
  11775  5023 13353    15     7  8585     3  5972 25582   371  8797   242
   5023  5786     6    79   169  5740    26    23     3  5972 25582   371
  10032   242  5879 24527     6    79   169     3     9 31220  4648    45
      3  5972 25582   371  1808     5     1]]"
3cf1edfa6d53a236cf4258afd87c87c0a477e243,0.0,English,[[   0 1566    1]],Unanswerable,[[ 597 3247 3321  179    1]]
bfc2dc913e7b78f3bd45e5449d71383d0aa4a890,9.0909,general knowledge learning engine (GKD),[[   0  879 1103 1036 1948   41  517  439  308   61    1]],"Knowledge Store (KS) , Knowledge Graph ( INLINEFORM0 ),  Relation-Entity Matrix ( INLINEFORM2 ), Task Experience Store ( INLINEFORM15 ), Incomplete Feature DB ( INLINEFORM29 )","[[16113  4493    41 13383    61     3     6 16113     3 21094    41  3388
  20006 24030   632     3   201 28898    18 16924   485  5708    52  2407
     41  3388 20006 24030   357     3   201 16107  7187  4493    41  3388
  20006 24030  1808     3   201    86 25288     3 16772     3  9213    41
   3388 20006 24030  3166     3    61     1]]"
cf874cd9023d901e10aa8664b813d32501e7e4d2,31.5789,Named Entity Recognition,[[    0  5570    26  4443   485 31110     1]],"Named Entity Recognition, including entities such as proteins, genes, diseases, treatments, drugs, etc. in the biomedical domain","[[ 5570    26  4443   485 31110     6   379 12311   224    38 13167     6
  13485     6  6716     6  5872     6  4845     6   672     5    16     8
   2392  2726  1950  3303     1]]"
a712718e6596ba946f29a99838d82f95b9ebb1ce,0.0,3%,[[   0    3 5170    1]],it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too,"[[   34    65     3 11739  2266  4179    16  7452     3 14622    12     3
   3577  5365  3462    11     3 10667   948  4179    16   377   536  2604
      3 14622    12     3  3577  5365  3462   396     1]]"
75ff6e425ce304a35f18c0230c0d13d3913a31a9,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
a32c792a0cef03218bf66322245677fc2d5e5a31,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
99e78c390932594bd833be0f5c890af5c605d808,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"QA PGNet, Multi-decoder QA PGNet with lookup table embedding","[[    3 23008     3  7861  9688     6  4908    18   221  4978    52     3
  23008     3  7861  9688    28   320   413   953 25078    26    53     1]]"
58ef2442450c392bfc55c4dc35f216542f5f2dbb,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
28067da818e3f61f8b5152c0d42a531bf0f987d4,0.0,length of the N-grams is calculated as the mean of the mean of the mean of,"[[    0  2475    13     8   445    18  5096     7    19 11338    38     8
   1243    13     8  1243    13     8  1243    13]]",Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4,"[[11801    28   738  3586    10    41 13212   295 29233     7    61  5148
  11723     7    21     3    29    18  5096  5022     3 22840     1]]"
8b0abc1907c2bf3e0256f8cf85e0ba66a839bd92,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
ae90c5567746fe25af2fcea0cc5f355751e05c71,0.0,WikiText-TL-08 and WikiText-TL-09,"[[    0  2142  2168 13598    17    18 12733    18  4018    11  2142  2168
  13598    17    18 12733    18  4198     1]]","US dataset, Italian dataset",[[  837 17953     6  4338 17953     1]]
22714f6cad2d5c54c28823e7285dc85e8d6bc109,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Reduction, Selection, Rank",[[  419  8291     6 22246     6     3 22557     1]]
d6ea7a30b0b61ae126b00b59d2a14fff2ef887bf,0.0,"LSTM, LSTM, LSTM+CNN, LSTM+","[[    0     3  7600  2305     6     3  7600  2305     6     3  7600  2305
   1220   254 17235     6     3  7600  2305  1220]]","Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis.","[[ 5154    53  4587     7    10     8 11445    41 17158  9689   127     7
     11  6353    61    33  8207   914 14865    42  9624 11498  1489   117
    338     3     9  1090 13774  5336   598     3 12053    84 10914    33
   5183     6   117  2011   208  3375    11    73 23313  1036    33     8
    167  1017  6315    12  1036    45   331   117     8  1745    13  1499
     24    62    33  3783    53    41   127    46  2264  1014     6    42
      3  9886   201   893  6569    42  3354     6    54  1664    36   315
    145    80    31     7   804  1745    13  1693     5     1]]"
4625cfba3083346a96e573af5464bc26c34ec943,0.0,LS improves by 0.5 points and 0.7 points in accuracy,"[[    0     3  7600  1172     7    57     3 12100   979    11     3 22426
    979    16  7452     1]]",6.37 BLEU,[[ 4357  4118     3  8775 12062     1]]
218bc82796eb8d91611996979a4a42500131a936,0.0,LSTMs,[[   0    3 7600 2305    7    1]],MLP,[[ 283 6892    1]]
bfbd6040cb95b179118557352e8e3899ef25c525,0.0,No,[[  0 465   1]],Unanswerable,[[ 597 3247 3321  179    1]]
f59f1f5b528a2eec5cfb1e49c87699e0c536cc45,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],3606 sentences,[[ 9181   948 16513     1]]
e2b0cd30cf56a4b13f96426489367024310c3a05,8.6957,The evaluation was done by a team of 3 people.,[[   0   37 5002   47  612   57    3    9  372   13  220  151    5    1]],As the metric to assess how well the model's output fits the gold ranking Spearman's $\rho $ was used,"[[  282     8     3  7959    12  6570   149   168     8   825    31     7
   3911  7307     8  2045 11592  8974   291   348    31     7  1514     2
     52   107    32  1514    47   261     1]]"
6ca938324dc7e1742a840d0a54dc13cc207394a1,0.0,"SST-1, SST-2, SST-3, SST-4","[[   0  180 4209 2292    6  180 4209 4949    6  180 4209 3486    6  180
  4209 4278    1]]","German newscrawl, English newscrawl, WMT'18 English-German (en-de) news, WMT'18 English-Turkish (en-tr) news task, WMT'18 English-German (en-de) news translation task and we validate our findings on the WMT'18 English-Turkish (en-tr) news task","[[ 2968  1506  2935   210    40     6  1566  1506  2935   210    40     6
    549  7323    31  2606  1566    18 24518    41    35    18   221    61
   1506     6   549  7323    31  2606  1566    18   382   450   157  1273
     41    35    18    17    52    61  1506  2491     6   549  7323    31
   2606  1566    18 24518    41    35    18   221    61  1506  7314  2491
     11    62 16742    69  7469    30     8   549  7323    31  2606  1566
     18   382   450   157  1273    41    35    18    17    52    61  1506
   2491     1]]"
9a8b9ea3176d30da2453cac6e9347737c729a538,0.0,82.0%,[[   0    3 4613    5 6932    1]],hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes,"[[ 9279     3 18206   825  5153     3     9   377   536  2604    13 20324
   3264   755  3229    30 13353    15  5120 13154    11 20324  4240   927
   3229    30  3739  3358     1]]"
975e60535724f4149c7488699a199ba2920a062c,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5260cb56b7d127772425583c5c28958c37cb9bea,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],two previous turns,[[ 192 1767 5050    1]]
2740e3d7d33173664c1c5ab292c7ec75ff6e0802,0.0,"CAS, MAT, MAT, MAT, MAT, MAT, ","[[    0     3 18678     6     3 18169     6     3 18169     6     3 18169
      6     3 18169     6     3 18169     6     3]]","the diacritized corpus that was used to train the RDI BIBREF7 diacritizer , WikiNews , a large collection of fully diacritized classical texts","[[    8  1227     9 12563  1601 11736   302    24    47   261    12  2412
      8     3 10255   196     3  5972 25582   371   940  1227     9 12563
   8585     3     6  2142  2168  6861     7     3     6     3     9   508
   1232    13  1540  1227     9 12563  1601 11702 14877     1]]"
92294820ac0d9421f086139e816354970f066d8a,4.6512,14%$ improvement over the previous baseline,"[[    0     3     2   536     2   591     2  1454  3229  4179   147     8
   1767 20726     1]]","Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1","[[ 1212  3929     7   504   394   772    30    66 15905     3  2172    12
  20726  3578 11805    76   536    30  1027  9492  5679    41   210   127
      7    15    57  4097  2596     3  2172    12 20726   137 11805    76
    536  2604    30   970   134  5900    17    19  3479     5  3539     3
   2172    12   200 20726  8838     5  4177     6   119 15905    30  1348
     57   209     1]]"
06cc8fcafc0880cf69a2514bb7341642b9833041,0.0,"7,007",[[   0 7973 1206  940    1]], INLINEFORM1 cases,[[ 3388 20006 24030   536  1488     1]]
662870a90890c620a964720b2ca122a1139410ea,25.0,English,[[   0 1566    1]],"French, English, Spanish, Italian, Portuguese, Hebrew, Arabic","[[ 2379     6  1566     6  5093     6  4338     6 21076     6 20428     6
  19248     1]]"
584af673429c7f8621c6bf83362a37048daa0e5d,25.641,by combining image captions with text and generating a story from sequences,"[[    0    57     3 13275  1023 25012     7    28  1499    11     3 11600
      3     9   733    45  5932     7     1]]",we provide the decoder with the context of the whole sequence and the content of the current image (i.e. global and local information) to generate the corresponding text that will contribute to the overall story,"[[  62  370    8   20 4978   52   28    8 2625   13    8  829 5932   11
     8  738   13    8  750 1023   41   23    5   15    5 1252   11  415
   251   61   12 3806    8    3 9921 1499   24   56 4139   12    8 1879
   733    1]]"
ca5a82b54cb707c9b947aa8445aac51ea218b23a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],It defined a sequence labeling task to extract custom entities from user input and label the next action (out of 13  custom actions defined).,"[[   94  4802     3     9  5932  3783    53  2491    12  5819  1653 12311
     45  1139  3785    11  3783     8   416  1041    41   670    13  1179
   1653  2874  4802   137     1]]"
74e866137b3452ec50fb6feaf5753c8637459e62,0.0,Rouge BIBREF1,[[    0 23777     3  5972 25582   371   536     1]], higher tiers of the pyramid,[[ 1146     3  3276     7    13     8 22734     1]]
50cb50657572e315fd452a89f3e0be465094b66f,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
b634ff1607ce5756655e61b9a6f18bc736f84c83,0.0,Twitter,[[   0 3046    1]],Energy,[[4654    1]]
a4d115220438c0ded06a91ad62337061389a6747,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Facebook status update messages,[[1376 2637 2270 4175    1]]
6bfba3ddca5101ed15256fca75fcdc95a53cece7,3.2787,"logical fallacy, emotion-based argumentation",[[    0     3  6207  1590  4710     6 13868    18   390  5464   257     1]],"1. Loaded language, 2. Name calling or labeling, 3. Repetition, 4. Exaggeration or minimization, 5. Doubt, 6. Appeal to fear/prejudice, 7. Flag-waving, 8. Causal oversimplification, 9. Slogans, 10. Appeal to authority, 11. Black-and-white fallacy, dictatorship, 12. Thought-terminating clichÃ©, 13. Whataboutism, 14. Reductio ad Hitlerum, 15. Red herring, 16. Bandwagon, 17. Obfuscation, intentional vagueness, confusion, 18. Straw man","[[ 1300  1815 15624  1612     6  1682  5570  3874    42  3783    53     6
   1877   419  4995  4749     6  2853  1881     9  6938   257    42  8984
   1707     6  3594 17159   115    17     6  4357 25024    12  2971    87
   2026 14312   867     6  4306 17016    18   210     9  3745     6  4848
  16371     7   138   147     7    23 27717     6  5835 18043  2565     7
      6  5477 25024    12  5015     6  7806  1589    18   232    18 13698
   1590  4710     6 25280  2009     6  8013  4229    17    18  6544  1014
  27319     6  8808   363  7932   159    51     6  9264   419  7472    23
     32     3     9    26 22640   440     6  9996  1624   160  1007     6
  10128  4483 15238   106     6 11030  4249    89   302    75   257     6
  24768 15986   655     6 12413     6 12265  5438   210   388     1]]"
b4f881331b975e6e4cab1868267211ed729d782d,0.0,30%,[[    0 10738     1]],"33,663",[[5400    6 3539  519    1]]
01e2d10178347d177519f792f86f25575106ddc7,0.0,WSJ and WSJX,[[   0    3 8439  683   11    3 8439  683    4    1]],"Switchboard Telephone Speech Corpus BIBREF21, LORELEI (Low Resource Languages for Emergent Incidents) Program","[[13218  1976 28747 26351 10052   302     3  5972 25582   371  2658     6
    301  2990 16479   196    41   434  2381 16154 10509     7    21 25579
    295  1542  4215     7    61  2350     1]]"
0602a974a879e6eae223cdf048410b5a0111665e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"K-means, LEM BIBREF13, DPEMM BIBREF14","[[  480    18   526  3247     6   301  6037     3  5972 25582   371  2368
      6   309  5668  8257     3  5972 25582   371  2534     1]]"
701571680724c05ca70c11bc267fb1160ea1460a,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
29f2954098f055fb19d9502572f085862d75bf61,0.0,a classifier that can translate natural language into real life language,"[[    0     3     9   853  7903    24    54 13959   793  1612   139   490
    280  1612     1]]","KNN
RF
SVM
MLP",[[  480 17235     3  8556   180 12623   283  6892     1]]
d201b9992809142fe59ae74508bc576f8ca538ff,0.0,Yes,[[   0 2163    1]],"The graph representation appears to be semi-supervised. It is included in the learning pipeline for the medical recommendation, where the attention model is learned. (There is some additional evidence that is unavailable in parsed text)","[[   37  8373  6497  3475    12    36  4772    18 23313     5    94    19
   1285    16     8  1036 12045    21     8  1035 10919     6   213     8
   1388   825    19  2525     5    41  7238    19   128  1151  2084    24
     19 24948    16   260  3843  1499    61     1]]"
c35806cf68220b2b9bb082b62f493393b9bdff86,4.1667,Best performance achieved was 0.83 F1 score,[[   0 1648  821 5153   47 4097 4591  377  536 2604    1]],"In SNLI, our best model achieves the new state-of-the-art accuracy of 87.0%,  we can see that our models outperform other models by large margin, achieving the new state of the art., Our models achieve the new state-of-the-art accuracy on SST-2 and competitive accuracy on SST-5","[[  86    3 8544 8159    6   69  200  825 1984    7    8  126  538   18
   858   18  532   18 1408 7452   13    3 4225    5 6932    6   62   54
   217   24   69 2250   91  883 2032  119 2250   57  508 6346    6    3
  9582    8  126  538   13    8  768    5    6  421 2250 1984    8  126
   538   18  858   18  532   18 1408 7452   30  180 4209 4949   11 3265
  7452   30  180 4209 4525    1]]"
58ee0cbf1d8e3711c617b1cd3d7aca8620e26187,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for ""Starry Night"" with a low average content score","[[   62   103    59    43    46   414    18   235    18   989 17953     6
      8  6126  1566 14193   164    59   161   168    28 18191   869  2025
     38  2008    16  7996 11376  4386   371  2122    21    96  7681   651
   5190   121    28     3     9   731  1348   738  2604     1]]"
ac7f6497be4bcca64e75f28934b207c9e8097576,19.0476,sentences that are based on a particular category or category of text,"[[    0 16513    24    33     3   390    30     3     9  1090  3295    42
   3295    13  1499     1]]",sentences that were selected from the Wikipedia corpus provided by culotta2006integrating,"[[16513    24   130  2639    45     8 16885 11736   302   937    57   123
   3171    17     9 21196 20030     1]]"
f875337f2ecd686cd7789e111174d0f14972638d,0.0,"BLEU, EMOT",[[    0     3  8775 12062     6     3  6037  6951     1]]," Affective Text dataset, Fairy Tales dataset, ISEAR dataset","[[   71    89  4075   757  5027 17953     6  4506    63 19098     7 17953
      6  6827 19356 17953     1]]"
154a721ccc1d425688942e22e75af711b423e086,0.0,MCScript,[[    0     3  3698 18255     1]],Amazon Mechanical Turk,[[ 2536 24483 23694     1]]
ffeb67a61ecd09542b1c53c3e4c3abd4da0496a8,48.2759,concept map is a representation of a summary concisely and clearly showing relationships between concepts and,"[[    0  2077  2828    19     3     9  6497    13     3     9  9251 22874
    120    11  3133  2924  3079   344  6085    11]]","concept map BIBREF5 , a labeled graph showing concepts as nodes and relationships between them as edges","[[ 2077  2828     3  5972 25582   371   755     3     6     3     9  3783
     15    26  8373  2924  6085    38   150  1395    11  3079   344   135
     38  9804     1]]"
de3b1145cb4111ea2d4e113f816b537d052d9814,0.0,BERT,[[    0   272 24203     1]],"Wang et al. , maximum entropy classifier with bag of words model","[[18102     3    15    17   491     5     3     6  2411     3    35 12395
     63   853  7903    28  2182    13  1234   825     1]]"
197b276d0610ebfacd57ab46b0b29f3033c96a40,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK, Pattern-based approach","[[ 7736  3040   419 22430     6  4908  3114    23   138  1823   757  2474
     15     7     6 25942  6944     6  1980     9 16481     6  4919   291
    180 12623     6   180 12623    28    71 20293 10047     6 20918    18
    390  1295     1]]"
dd20d93166c14f1e57644cd7fa7b5e5738025cd0,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],mainstream and disinformation news,[[12946    11  1028  6391  1506     1]]
e0122fc7b0143d5cbcda2120be87a012fb987627,40.0,"Compared to the state-of-the-art results, their model outperforms","[[    0     3 25236    12     8   538    18   858    18   532    18  1408
    772     6    70   825    91   883  2032     7]]","the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)","[[    8  1388   825     6  8530 13223     6    92    91   883  2032     7
      8   200  1895   585   772    41   518  2965     3 22787  2394    12
      3 22787  4060    61     1]]"
443d2448136364235389039cbead07e80922ec5c,0.0,"BIBREF16, BIBREF17","[[    0     3  5972 25582   371  2938     6     3  5972 25582   371  2517
      1]]","LSA, TextRank, LexRank",[[  301  4507     6  5027 22557     6 17546 22557     1]]
664b3eadc12c8dde309e8bbd59e9af961a433cde,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
b46c0015a122ee5fb95c2a45691cb97f80de1bb6,0.0,"LSTM with a corresponding source and target domains, LSTM with ","[[   0    3 7600 2305   28    3    9    3 9921 1391   11 2387 3303    7
     6    3 7600 2305   28    3]]","one-layer CNN structure from previous works BIBREF22 , BIBREF4","[[   80    18 18270 19602  1809    45  1767   930     3  5972 25582   371
   2884     3     6     3  5972 25582   371   591     1]]"
1dc2da5078a7e5ea82ccd1c90d81999a922bc9bf,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
2ad4d3d222f5237ed97923640bc8e199409cbe52,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],completion times and accuracies ,[[6929  648   11    3 6004 2414 6267    7    1]]
8df35c24af9efc3348d3b8d746df116480dfe661,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
6aa2a1e2e3666f2b2a1f282d4cbdd1ca325eb9de,0.0,1,[[  0 209   1]],719313,[[   3 4450 4271 2368    1]]
c2e475adeddcdc4d637ef0d4f5065b6a9b299827,0.0,accuracy,[[   0 7452    1]],"BLEU-4, NIST-4, ROUGE-4","[[    3  8775 12062  4278     6   445 13582  4278     6   391 26260   427
   4278     1]]"
cb196725edc9cdb2c54b72364f3bbf7c76471490,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5cb610d3d5d7d447b4cd5736d6a7d8262140af58,12.5,using a multilingual approach,[[    0   338     3     9  1249 25207  1295     1]],Multilingual training is performed by randomly alternating between languages for every new minibatch,"[[ 4908 25207   761    19  3032    57 21306     3 30859   344  8024    21
    334   126  3016   115 14547     1]]"
c21b87c97d1afac85ece2450ee76d01c946de668,0.0,BIBREF3,[[    0     3  5972 25582   371   519     1]], pointer networks with coverage mechanism (PG-net)BIBREF0,"[[  500    49  5275    28  2591  8557    41  7861    18  1582    61  5972
  25582   371   632     1]]"
37be0d479480211291e068d0d3823ad0c13321d3,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En","[[  116  2505    30  1566     6     8   377   536  2604    13     8   825
    761    30  2830    41   956   107    61    19   305 26195     6   377
    536  2604    19   163   314 19708    21     8   825   761    30  1027
    107    18  8532     1]]"
d3aa0449708cc861a51551b128d73e11d62207d2,0.0,"Freebase2M BIBREF2, BIBREF3","[[    0  1443 10925   357   329     3  5972 25582   371   357     3     6
      3  5972 25582   371   519     1]]","break the relation names into word sequences for question-relation matching, build both relation-level and word-level relation representations, use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations, residual learning method for sequence matching, a simple KBQA implementation composed of two-step relation detection","[[ 1733     8  4689  3056   139  1448  5932     7    21   822    18    60
   6105  8150     6   918   321  4689    18  4563    11  1448    18  4563
   4689  6497     7     6   169  1659  2647 26352     3  7600  2305     7
     41   279    23  7600  2305     7    61    12   669   315  1425    13
    822  6497     7     6 27687  1036  1573    21  5932  8150     6     3
      9   650     3 17827 23008  4432 10431    13   192    18  7910  4689
  10664     1]]"
f9aa055bf73185ba939dfb03454384810eb17ad1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],No data. Pretrained model is used.,[[  465   331     5  1266    17 10761   825    19   261     5     1]]
e438445cf823893c841b2bc26cdce32ccc3f5cbe,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
a29c071065d26e5ee3c3bcd877e7f215c59d1d33,0.0,"cosine similarity, Manhatten / Euclidean distance","[[    0   576     7   630  1126   485     6  1140   547   324     3    87
   4491 14758   221   152  2357     1]]", Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels,"[[ 8974   291   348    31     7 11003 18712   344     8   576     7   630
     18 26714   485    13     8  7142 25078    26    53     7    11     8
   2045 11241     1]]"
df5a4505edccc0ee11349ed6e7958cf6b84c9ed4,25.0,"corpus of news articles labelled as propaganda, trusted, hoax, or sati","[[    0 11736   302    13  1506  2984     3 29506    38 25071     6  7731
      6  3534     9   226     6    42     3  9275]]", news articles in free-text format,[[1506 2984   16  339   18 6327 1910    1]]
8748e8f64af57560d124c7b518b853bf2711c13e,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
31101dc9937f108e27e08a5f34be44f0090b8b6b,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
cd32a38e0f33b137ab590e1677e8fb073724df7f,0.0,Chinese,[[   0 2830    1]],English ,[[1566    1]]
b591853e938984e6069d738371500ebdec50d256,0.0,Sentiment Classification,[[   0 4892 2998  295 4501 2420    1]],IMDb movie review,[[   27 11731   115  1974  1132     1]]
63a1cbe66fd58ff0ead895a8bac1198c38c008aa,0.0,"BERT-Seq, MATREF12, MATREF13","[[    0   272 24203    18   134    15  1824     6     3 18169  4386   371
   2122     6     3 18169  4386   371  2368     1]]",Show&Tell and LRCN1u,[[ 3111   184   382  3820    11     3 12564 10077   536    76     1]]
7ab9c0b4ceca1c142ff068f85015a249b14282d0,0.0,Yes,[[   0 2163    1]],construct context-gloss pairs from glosses of all possible senses (in WordNet) of the target word,"[[ 6774  2625    18  9680     7     7 14152    45 20666    15     7    13
     66   487  1254     7    41    77  4467  9688    61    13     8  2387
   1448     1]]"
d859cc37799a508bbbe4270ed291ca6394afce2c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Using NMF based topic modeling and their coherence prominent topics are identified,"[[    3  3626   445 13286     3   390  2859 15309    11    70   576   760
   1433  8304  4064    33  4313     1]]"
ce6a3ca102a5ee62e86fc7def3b20b1f10d1eb25,0.0,Yes,[[   0 2163    1]],Unanswerable,[[ 597 3247 3321  179    1]]
9658b5ffb5c56e5a48a3fea0342ad8fc99741908,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
1dc2da5078a7e5ea82ccd1c90d81999a922bc9bf,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
e79a5b6b6680bd2f63e9f4adbaae1d7795d81e38,0.0,non-english,[[   0  529   18 4606   40 1273    1]],Russsian,[[ 2770     7     7 10488     1]]
92bb41cf7bd1f7886784796a8220ed5aa07bc49b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"architecture of the classifier, sentence length,  input domain",[[4648   13    8  853 7903    6 7142 2475    6 3785 3303    1]]
e807d347742b2799bc347c0eff19b4c270449fee,32.0,Training and testing data provided by BioASQ organizers.,"[[    0  4017    11  2505   331   937    57  3318  3291  2247 14250     7
      5     1]]","A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers.","[[   71 17953   937    57  3318  3291  2247  5608    53    13   746     6
   2045  1068  2691     6     3 20317  4995     7     6  6085    11  1523
     11  1523  4269     5     1]]"
784ce5a983c5f2cc95a2c60ce66f2a8a50f3636f,100.0,No,[[  0 465   1]],No,[[465   1]]
4c07c33dfaf4f3e6db55e377da6fa69825d0ba15,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],300,[[3147    1]]
4dcf67b5e7bd1422e7e70c657f6eacccd8de06d3,0.0,two,[[  0 192   1]],Unanswerable,[[ 597 3247 3321  179    1]]
bd6dc38a9ac8d329114172194b0820766458dacc,0.0,"BIBREF13, BIBREF18","[[    0     3  5972 25582   371  2368     6     3  5972 25582   371  2606
      1]]",the ERP data: BIBREF0,[[    8 22568   331    10     3  5972 25582   371   632     1]]
6a31db1aca57a818f36bba9002561724655372a7,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"32,595",[[3538    6  755 3301    1]]
29c014baf99fb9f40b5171aab3e2c7f12a748f79,0.0,"BIBREF5, BIBREF6","[[    0     3  5972 25582   371   755     3     6     3  5972 25582   371
    948     1]]","popularity-based, similarity-based, hybrid",[[9897   18  390    6 1126  485   18  390    6 9279    1]]
ba28ce9a2f7e8524243adf288cc3f11055e667bb,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
1bdc990c7e948724ab04e70867675a334fdd3051,0.0,Amazon reviews,[[   0 2536 2456    1]],from Food.com,[[  45 3139    5  287    1]]
440faf8d0af8291d324977ad0f68c8d661fe365e,0.0,WN18M,[[    0     3 21170  2606   329     1]],The Reuters-8 dataset (with stop words removed),[[   37     3 18844  6039 17953    41  4065  1190  1234  3641    61     1]]
787c4d4628eac00dbceb1c96020bff0090edca46,0.0,commentary on x-stance,[[    0 18204    30     3   226    18  8389     1]],"answer each question with either `yes', `rather yes', `rather no', or `no'., can supplement each answer with a comment of at most 500 characters","[[ 1525   284   822    28   893     3     2 10070    31     6     3     2
   1795   760  4273    31     6     3     2  1795   760   150    31     6
     42     3     2    29    32    31     5     6    54  8839   284  1525
     28     3     9  1670    13    44   167  2899  2850     1]]"
3f0ae9b772eeddfbfd239b7e3196dc6dfa21365f,0.0,"reward for generating an ironic sentence, which is inverted from the intended meaning of the","[[    0  9676    21     3 11600    46  3575   447  7142     6    84    19
     16 19825    45     8  3855  2530    13     8]]","irony accuracy, sentiment preservation",[[ 3575    63  7452     6  6493 19368     1]]
f428618ca9c017e0c9c2a23515dab30a7660f65f,25.8065,"NBC, K-NN, Random Forest",[[    0   445  7645     6   480    18 17235     6 25942  6944     1]],"Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN) and Random Forest (RF)","[[ 4908    18  3612  7975  1915  6873    52   106    41   329  6892   201
   1823   757  2474    15     7  4501  7903    41 15829   201  4224 29011
   5879    41   134 12623   201 10771  4741     3 16481    53  4501  7903
     41  3443   254   201   472  6322 10057 10771  4741   309 11719    41
    134 18405   201   480 10455   222  1484  9031   115  1211    41   439
     18 17235    61    11 25942  6944    41  8556    61     1]]"
a1dac888f63c9efaf159d9bdfde7c938636f07b1,0.0,dataset from the Greenland Institute for Environmental Research (GREENL) BIBREF,"[[    0 17953    45     8  1862    40   232  2548    21  9185  2200    41
   8727 23394   434    61     3  5972 25582   371]]",same datasets as BIBREF7,[[  337 17953     7    38     3  5972 25582   371   940     1]]
3c93894c4baf49deacc6ed2a14ef5e0f13b7d96f,0.0,"demographic, grammatical, and social",[[    0 14798     6     3  5096  4992   138     6    11   569     1]],"occupation, industry, profile information, language use, gender ",[[13792     6   681     6  3278   251     6  1612   169     6  7285     1]]
7ab9c0b4ceca1c142ff068f85015a249b14282d0,0.0,Yes,[[   0 2163    1]],"construct context-gloss pairs from all possible senses of the target word in WordNet, thus treating WSD task as a sentence-pair classification problem","[[ 6774  2625    18  9680     7     7 14152    45    66   487  1254     7
     13     8  2387  1448    16  4467  9688     6  2932 10902   549  7331
   2491    38     3     9  7142    18   102  2256 13774   682     1]]"
aef607d2ac46024be17b1ddd0ed3f13378c563a6,28.5714,by the number of words that are under-translated,"[[    0    57     8   381    13  1234    24    33   365    18 11665 26860
      1]]","They measured the under-translated words with low word importance score as calculated by Attribution.
method","[[  328  8413     8   365    18 11665 26860  1234    28   731  1448  3172
   2604    38 11338    57   486  5135  1575     5  1573     1]]"
5f60defb546f35d25a094ff34781cddd4119e400,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
52f8a3e3cd5d42126b5307adc740b71510a6bdf5,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Detection of an aspect in a review, Prediction of the customer general satisfaction, Prediction of the global trend of an aspect in a given review, Prediction of whether the rating of a given aspect is above or under a given value, Prediction of the exact rating of an aspect in a review, Prediction of the list of all the positive/negative aspects mentioned in the review, Comparison between aspects, Prediction of the strengths and weaknesses in a review","[[    3 31636    23   106    13    46  2663    16     3     9  1132     6
   1266 12472    13     8   884   879  5044     6  1266 12472    13     8
   1252  4166    13    46  2663    16     3     9   787  1132     6  1266
  12472    13   823     8  5773    13     3     9   787  2663    19   756
     42   365     3     9   787   701     6  1266 12472    13     8  2883
   5773    13    46  2663    16     3     9  1132     6  1266 12472    13
      8   570    13    66     8  1465    87 31600  3149  2799    16     8
   1132     6 30760   344  3149     6  1266 12472    13     8 14350    11
  21506    16     3     9  1132     1]]"
0f7867f888109b9e000ef68965df4dde2511a55f,4.878,using a simple voting scheme and achieve new state-of-the-art results on the,"[[    0   338     3     9   650 10601  5336    11  1984   126   538    18
    858    18   532    18  1408   772    30     8]]","we apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes, In case of a tie, we pick one of the most frequent classes randomly","[[   62  1581   633 19602    11   391 17235  2250  2569    16  4398     7
      3  3221 25582   371  2122    11     3  3221 25582   371  2534    11
   9689     8   853    28     8   167 11839     6    86   495    13     3
      9  6177     6    62  1432    80    13     8   167  8325  2287 21306
      1]]"
ff338921e34c15baf1eae0074938bf79ee65fdd2,0.0,BERT,[[    0   272 24203     1]],by answering always YES (in batch 2 and 3) ,"[[   57 18243   373     3   476  3205    41    77 11587   204    11     3
   5268     1]]"
badc9db40adbbf2ea7bac29f2e4e3b6b9175b1f9,7.4074,"Using a supervised LSTM to detect a set of puns, we","[[    0     3  3626     3     9     3 23313     3  7600  2305    12  8432
      3     9   356    13  4930     7     6    62]]","F1 score of 92.19 on homographic pun detection, 80.19 on homographic pun location, 89.76 on heterographic pun detection.","[[  377   536  2604    13     3  4508     5  2294    30 13503 14797  4930
  10664     6  2775     5  2294    30 13503 14797  4930  1128     6     3
   3914     5  3959    30 26481 14797  4930 10664     5     1]]"
f5e6f43454332e0521a778db0b769481e23e7682,0.0,LSTMs,[[   0    3 7600 2305    7    1]],firstly translates a source language into the pivot language which is later translated to the target language,"[[  166   120     3 29213     3     9  1391  1612   139     8 16959  1612
     84    19   865 15459    12     8  2387  1612     1]]"
b591853e938984e6069d738371500ebdec50d256,0.0,Sentiment Classification,[[   0 4892 2998  295 4501 2420    1]],the IMDb movie review dataset BIBREF17,"[[    8    27 11731   115  1974  1132 17953     3  5972 25582   371  2517
      1]]"
2cd37743bcc7ea3bd405ce6d91e79e5339d7642e,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
2a3e36c220e7b47c1b652511a4fdd7238a74a68f,100.0,244,[[  0 997 591   1]],244,[[997 591   1]]
d6401cece55a14d2a35ba797a0878dfe2deabedc,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],linguistic variability,[[    3 24703 27980     1]]
4ef11518b40cc55d86c485f14e24732123b0d907,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"FGM, FGVM, DeepFool, HotFlip, TYC","[[  377  7381     6     3 22807 12623     6  9509   371    32    32    40
      6  5396   371  7446     6     3 12016   254     1]]"
9da1e124d28b488b0d94998d32aa2fa8a5ebec51,0.0,84.0%,[[   0    3 4608    5 6932    1]],"Overall F1 score:
- He and Sun (2017) 58.23
- Peng and Dredze (2017) 58.99
- Xu et al. (2018) 59.11","[[ 9126   377   536  2604    10     3    18   216    11  3068     3 26224
      3  3449     5  2773     3    18  4511   122    11   309  1271   776
      3 26224     3  3449 14990     3    18     3     4    76     3    15
     17   491     5 28068     3  3390     5  2596     1]]"
a778b8204a415b295f73b93623d09599f242f202,27.2727,Random Kitchen Sink approach,[[    0 25942  5797 26560  1295     1]],Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.,"[[25942  5797 26560  1573  2284     3     9 20563  1681    12  2828   331
  12938     7    12     3     9   628   213 13080 13608    19   487     5
      1]]"
d087539e6a38c42f0a521ff2173ef42c0733878e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.","[[20487   257  2097  1178  5849   175     6   437    79  1457     8  1236
     11  3145  2250    12   698     8   337 19067    11  3911   628     5
    100 13343   120  6790    70  1055    12   856  1428   825  4342     5
      1]]"
dcea88698949da4a1bd00277c06df06c33f6a5ff,10.0,a good proxy for the general-purpose sequence to sequence tasks,"[[    0     3     9   207 19784    21     8   879    18 19681  5932    12
   5932  4145     1]]","The task is set up to mimic (albeit, in an oversimplified manner) the input-output symbol alignments and local syntactic properties that models must learn in many natural language tasks, such as translation, tagging and summarization.","[[   37  2491    19   356    95    12 22336    41 22154     6    16    46
    147     7 10296  3676  3107    61     8  3785    18   670  2562  6083
  14632     7    11   415  8953    17  2708   447  2605    24  2250   398
    669    16   186   793  1612  4145     6   224    38  7314     6     3
     17 15242    11  4505  1635  1707     5     1]]"
cf874cd9023d901e10aa8664b813d32501e7e4d2,100.0,Named Entity Recognition,[[    0  5570    26  4443   485 31110     1]],Named Entity Recognition,[[ 5570    26  4443   485 31110     1]]
4bc2784be43d599000cb71d31928908250d4cef3,0.0,TDNN-TDNN,[[    0     3 10494 17235    18 10494 17235     1]],An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content.,"[[  389  4924    13  6540   553  4569   308    84  3601     7   614  9587
     18   390  9068    53    28  1835  9587    18   390  9068    53    28
      8   617   155   106     3    32     3    89  9381 17644  9068     7
     12  1154    28 26847   738     5     1]]"
5fda8539a97828e188ba26aad5cda1b9dd642bc8,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],F1 score of 97.5 on MSR and 95.7 on AS,"[[  377   536  2604    13   668 15731    30   283  6857    11   668 27220
     30  6157     1]]"
22ccee453e37536ddb0c1c1d17b0dbac04c6c607,66.6667,"Arabic, English",[[    0 19248     6  1566     1]],English,[[1566    1]]
72ed5fed07ace5e3ffe9de6c313625705bc8f0c7,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],204 tokens,[[    3 26363 14145     7     1]]
9da1e124d28b488b0d94998d32aa2fa8a5ebec51,0.0,84.0%,[[   0    3 4608    5 6932    1]],"For Named entity the maximum precision was 66.67%, and the average 62.58%, same values for Recall was 55.97% and 50.33%, and for F1 57.14% and 55.64%. Where for Nominal Mention had maximum recall of 74.48% and average of 73.67%, Recall had values of 54.55% and 53.7%,  and F1 had values of  62.97% and 62.12%. Finally the Overall F1 score had maximum value of 59.11% and average of 58.77%","[[  242  5570    26 10409     8  2411 11723    47   431 28833  6170     6
     11     8  1348   431 15967  5953     6   337  2620    21   419 16482
     47  6897     5  4327  1454    11   305 19997  5170     6    11    21
    377   536   305 25059  5988    11   305 25134  5988     5  2840    21
    465  1109   138  3137  1575   141  2411  7881    13   489 23444  5953
     11  1348    13   489 23074  6170     6   419 16482   141  2620    13
    305 12451  2712    11 12210     5  6170     6    11   377   536   141
   2620    13   431 27297  6170    11     3  4056     5 26821     5  4213
      8  9126   377   536  2604   141  2411   701    13     3  3390     5
    536  4704    11  1348    13     3  3449     5  4013  1454     1]]"
511517efc96edcd3e91e7783821c9d6d5a6562af,0.0,"CoNLL-2010 Shared Task (BIBREF0), which had 3 different tasks","[[    0   638   567 10376    18 14926  7105    26 16107    41  5972 25582
    371   632   201    84   141   220   315  4145]]","BioScope Abstracts, SFU, and BioScope Full Papers","[[ 3318   134 10845    15 20114     7     6   180 19813     6    11  3318
    134 10845    15  4043  6564     7     1]]"
f71b52e00e0be80c926f153b9fe0a06dd93af11e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"average content score across the paintings is 3.7, average creativity score is 3.9, average style score is 3.9","[[ 1348   738  2604   640     8  9843    19     3 25168     6  1348  6933
   2604    19     3 28640     6  1348   869  2604    19     3 28640     1]]"
e35c2fa99d5c84d8cb5d83fca2b434dcd83f3851,7.1429,Twitter,[[   0 3046    1]],"For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy","[[  242     8   529    18    89 25481  3744     6    62     3  4610    30
      3     9   570    13  8003  3046  3744    45     3  5972 25582   371
   4347    62   169     3     9   570    28   430  3538  3046  3744    45
      3  5972 25582   371  2294    24    33  1702 20739     1]]"
