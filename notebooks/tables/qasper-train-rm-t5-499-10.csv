ids,f1_seg_0,preds_seg_0,preds_seg_0_tokens,target_text,labels
1a1293e24f4924064e6fb9998658f5a329879109,0.0,"LSTM, LSTM, LSTM+repeat, LSTM","[[   0    3 7600 2305    6    3 7600 2305    6    3 7600 2305 1220   60
   855  144    6    3 7600 2305]]","SEQ2SEQ, CVAE, Transformer, HRED, DialogWAE","[[  180 23346   357   134 23346     6 10430 14611     6 31220     6   454
  13729     6 25843 12054   427     1]]"
d28260b5565d9246831e8dbe594d4f6211b60237,9.0909,Plackett-Luce Model for SMT Reranking,"[[    0  8422    75 12922    18 11748    15  5154    21   180  7323   419
   6254    53     1]]","boost the training BLEU very greatly, the over-fitting problem of the Plackett-Luce models PL($k$) is alleviated with moderately large $k$","[[ 4888     8   761     3  8775 12062   182  6891     6     8   147    18
   5616  1222   682    13     8  8422    75 12922    18 11748    15  2250
      3  5329   599  3229   157  3229    61    19 18807    15    26    28
   8107   120   508  1514   157  3229     1]]"
daf624f7d1623ccd3facb1d93d4d9d616b3192f4,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
8b71ede8170162883f785040e8628a97fc6b5bcb,0.0,referential error rate,[[   0 2401 7220 3505 1080    1]],"it is necessary to evaluate the performance of the above mentioned part of the pipeline before proceeding further. The evaluation of the performance is summarised in Table TABREF11. It shows that organising the two models into the pipeline boosted the performance of the reference recognition model, leading to a higher F1 measure in the initial recognition of the text spans and their classification.","[[   34    19  1316    12  6825     8   821    13     8   756  2799   294
     13     8 12045   274 23613   856     5    37  5002    13     8   821
     19  4505  1635  3375    16  4398     3  3221 25582   371 10032    94
   1267    24     3 27172     8   192  2250   139     8 12045     3 30695
      8   821    13     8  2848  5786   825     6  1374    12     3     9
   1146   377   536  3613    16     8  2332  5786    13     8  1499  8438
      7    11    70 13774     5     1]]"
a08b5018943d4428f067c08077bfff1af3de9703,100.0,neutral class,[[   0 7163  853    1]],neutral class,[[7163  853    1]]
973f6284664675654cc9881745880a0e88f3280e,30.7692,"lexical richness, grammatical correctness, semantic coherence, etc","[[    0     3 30949   138  2354   655     6     3  5096  4992   138  2024
    655     6 27632   576   760  1433     6   672]]","6 indicators:
- lexical richness
- pronunciation and fluency
- syntactical correctness
- fulfillment of delivery
- coherence and cohesion
- communicative, descriptive, narrative skills","[[  431 15600    10     3    18     3 30949   138  2354   655     3    18
  30637    11  6720  4392     3    18  8953    17  2708  1950  2024   655
      3    18 24013    13  1929     3    18   576   760  1433    11   576
     88  1938     3    18  6639   447  1528     6 25444     6  8109  1098
      1]]"
99c50d51a428db09edaca0d07f4dab0503af1b94,33.3333,YouTube video transcripts,[[    0  5343   671 20146     7     1]],"youtube video transcripts on news covering different topics like technology, human rights, terrorism and politics","[[   25  9863   671 20146     7    30  1506  6013   315  4064   114   748
      6   936  2166     6     3 14389    11  6525     1]]"
f37026f518ab56c859f6b80b646d7f19a7b684fa,0.0,36%,[[   0  220 6370    1]],"our model requires 100k parameters , while BIBREF8 requires 250k parameters","[[   69   825  2311   910   157  8755     3     6   298     3  5972 25582
    371   927  2311  5986   157  8755     1]]"
d9c6493e1c3d8d429d4ca608f5acf29e4e7c4c9b,6.6667,DBCA is a novel method to quantitatively assess the compositionality of a given,"[[    0     3  9213  4490    19     3     9  3714  1573    12 18906   120
   6570     8  5761 10355    13     3     9   787]]",CFQ contains the most query patterns by an order of magnitude and also contains significantly more queries and questions than the other datasets,"[[    3  7380  2247  2579     8   167 11417  4264    57    46   455    13
  20722    11    92  2579  4019    72 13154    11   746   145     8   119
  17953     7     1]]"
0427ca83d6bf4ec113bc6fec484b2578714ae8ec,33.3333,"LSTM+attention, LSTM+attention","[[    0     3  7600  2305  1220 25615     6     3  7600  2305  1220 25615
      1]]","LSTM+attention, Transformer , Universal Transformer",[[    3  7600  2305  1220 25615     6 31220     3     6 12489 31220     1]]
ce807a42370bfca10fa322d6fa772e4a58a8dca1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Darkode,  Hack Forums, Blackhat and Nulled.","[[ 7597    32   221     6 12715  5841     7     6  1589   547    11  1174
     40  1361     5     1]]"
394cf73c0aac8ccb45ce1b133f4e765e8e175403,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
5450f27ccc0406d3bffd08772d8b59004c2716da,100.0,a new metric to reveal a model's robustness against exposure bias,"[[    0     3     9   126     3  7959    12  6731     3     9   825    31
      7  6268   655   581  4773 14387     1]]",a new metric to reveal a model's robustness against exposure bias,"[[    3     9   126     3  7959    12  6731     3     9   825    31     7
   6268   655   581  4773 14387     1]]"
01866fe392d9196dda1d0b472290edbd48a99f66,5.5556,"Using a lexicon of trafficking flags, we can identify the most common","[[    0     3  3626     3     9     3 30949   106    13  2117  1765  5692
      7     6    62    54  2862     8   167  1017]]","re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones","[[    3    60    18  9719     8 11202    18  5096   825    11  2270     8
      3    15    51 21892  2828 22654    30   126     3  1579   127    17
   6543     6   116  2117 11758  3615    12   126     3    15    51 21892
      7     6     8  2828    54  1309     8   126     3    15    51 21892
      7    12     8   625  2102     1]]"
77cf4379106463b6ebcb5eb8fa5bb25450fa5fb8,0.0,Unanswerable,[[   0  597 3247 3321  179    1]]," three types of questions, namely tumor size, proximal resection margin and distal resection margin","[[  386  1308    13   746     6     3 17332  8985   812     6     3 20042
     23  1982     3    60 14309  6346    11  1028  1947     3    60 14309
   6346     1]]"
dd8f72cb3c0961b5ca1413697a00529ba60571fe,18.1818,MS-MARCO contains multiple passages.,[[    0  5266    18 13845  5911  2579  1317  5454     7     5     1]],"there are several related passages for each question in the MS-MARCO dataset., MS-MARCO also annotates which passage is correct","[[  132    33   633  1341  5454     7    21   284   822    16     8  5266
     18 13845  5911 17953     5     6  5266    18 13845  5911    92    46
   2264  6203    84  5454    19  2024     1]]"
cafa6103e609acaf08274a2f6d8686475c6b8723,32.0,"Compared to the best results on SWDA and MRDA, our models outperform","[[    0     3 25236    12     8   200   772    30 12222  4296    11     3
   9320  4296     6    69  2250    91   883  2032]]",improves the DAR accuracy over Bi-LSTM-CRF by 2.1% and 0.8% on SwDA and MRDA respectively,"[[1172    7    8  309 4280 7452  147 2106   18 7600 2305   18 4545  371
    57 1682 4704   11 4097 5953   30 7320 4296   11    3 9320 4296 6898
     1]]"
ac706631f2b3fa39bf173cd62480072601e44f66,100.0,No,[[  0 465   1]],No,[[465   1]]
92cfac12d9583747bd9be8604275b4a9ddd8afe6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"we suggest to estimate the interpretability of $k$ th component as $ \operatorname{interp}_k W = \sum _{i,j=1}^N W_{i,k} W_{j,k} \left(W_i \cdot W_j \right). $","[[   62  3130    12  7037     8  7280  2020    13  1514   157  3229     3
    189  3876    38  1514     3     2    32   883  1016  4350     2  3870
    102     2   834   157   549  3274     3     2  4078     3   834     2
     23     6   354  2423   536     2   567   549   834     2    23     6
    157     2   549   834     2   354     6   157     2     3     2 17068
    599   518   834    23     3     2    75    26    32    17   549   834
    354     3     2  3535   137  1514     1]]"
4e4946c023211712c782637fcca523deb126e519,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
86e3136271a7b93991c8de5d310ab15a6ac5ab8c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"(1) Good (3 points): The response is grammatical, semantically relevant to the query, and more importantly informative and interesting, (2) Acceptable (2 points): The response is grammatical, semantically relevant to the query, but too trivial or generic, (3) Failed (1 point): The response has grammar mistakes or irrelevant to the query","[[ 5637  1804  6918   979    61    10    37  1773    19     3  5096  4992
    138     6 27632  1427  2193    12     8 11417     6    11    72  7521
  11152    11  1477     6  6499 20592   179  4743   979    61    10    37
   1773    19     3  5096  4992   138     6 27632  1427  2193    12     8
  11417     6    68   396 22377    40    42  8165     6 10153   377 10990
   4077   500    61    10    37  1773    65 19519  8176    42 26213    12
      8 11417     1]]"
5f6fbd57cce47f20a0fda27d954543c00c4344c2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The workers were also asked to annotate both user states and system states, we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories","[[   37  2765   130    92  1380    12    46  2264   342   321  1139  2315
     11   358  2315     6    62   261   128  2219    12  3269    46  2264
    342  7478  6775  1315    12  1139  2315     6   358  2315     6    11
   7478 27771     1]]"
642c4704a71fd01b922a0ef003f234dcc7b223cd,3.3898,"gender-omissions, human decisions when the schemat are underspecified, and","[[    0  7285    18    32  5451     7     6   936  3055   116     8 26622
     17 32134    33   365  7576  3676     6    11]]","irremediable annotation discrepancies, differences in choice of attributes to annotate, The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them, the two annotations encode distinct information, incorrectly applied UniMorph annotation, cross-lingual inconsistency in both resources","[[19598  5700   179 30729  5025    60   102 28704     6  5859    16  1160
     13 12978    12    46  2264   342     6    37  1438  1452   133   174
  14809    12 23734     8  2193     3  8886    32     7    63    29    17
   2708   447   251     5   886  8024   141     3     9   182   731   381
     13     3 31212  2807     6    11   150  7860  6407    42  1084    18
  19515    15     7   344   135     6     8   192 30729     7 23734  6746
    251     6 12153   120  2930  5645   329   127   102   107 30729     6
   2269    18 25207    16  8056   343  4392    16   321  1438     1]]"
8ca31caa34cc5b65dc1d01d0d1f36bf8c4928805,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
54a2c08aa55c3db9b30ae2922c96528d3f4fc733,0.0,LSTM,[[   0    3 7600 2305    1]],"ontology-based knowledge tree, heuristics-based, n-grams model","[[  30   17 1863   18  390 1103 2195    6    3   88  450 3040    7   18
   390    6    3   29   18 5096    7  825    1]]"
a999761aa976458bbc7b4f330764796446d030ff,100.0,cross-lingual NE recognition,[[    0  2269    18 25207     3  4171  5786     1]],cross-lingual NE recognition,[[ 2269    18 25207     3  4171  5786     1]]
b69897deb5fb80bf2adb44f9cbf6280d747271b3,100.0,BERT,[[    0   272 24203     1]],BERT,[[  272 24203     1]]
3a1bd3ec1a7ce9514da0cb2dfcaa454ba8c0ed14,78.5714,"five subtasks which involve standard classification, ordinal classification and distributional estimation","[[    0   874   769 23615     7    84  7789  1068 13774     6 14503   138
  13774    11  3438   138 22781     1]]"," five subtasks which involve standard classification, ordinal classification and distributional estimation. For a more detailed description see BIBREF0","[[  874   769 23615     7    84  7789  1068 13774     6 14503   138 13774
     11  3438   138 22781     5   242     3     9    72  3117  4210   217
      3  5972 25582   371   632     1]]"
fe578842021ccfc295209a28cf2275ca18f8d155,0.0,LSTMs,[[   0    3 7600 2305    7    1]],"RNNs, CNNs, Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel","[[  391 17235     7     6 19602     7     6  1823   757  2474    15     7
     28   325  4687 23128    53     6     3   157    18    75 26156    53
      6   180 12623    28 13080 20563     1]]"
19c9cfbc4f29104200393e848b7b9be41913a7ac,100.0,"2,714",[[   0 3547  940 2534    1]],"2,714 ",[[3547  940 2534    1]]
c95fd189985d996322193be71cf5be8858ac72b5,0.0,machine translation,[[   0 1437 7314    1]],"sentiment analysis, information extraction, document summarization, spoken dialogue, cross lingual (research), dialogue, systems, language generation","[[ 6493  1693     6   251 16629     6  1708  4505  1635  1707     6 11518
   7478     6  2269     3 25207    41    60 13173   201  7478     6  1002
      6  1612  3381     1]]"
9ab43f941c11a4b09a0e4aea61b4a5b4612e7933,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Only MTMSM specifically tried to tackle the multi-span questions. Their approach consisted of two parts: first train a dedicated categorical variable to predict the number of spans to extract and the second was to generalize the single-span head method of extracting a span,"[[ 3462   283  2305  4212  3346  1971    12  8000     8  1249    18     7
   2837   746     5  2940  1295 14280    26    13   192  1467    10   166
   2412     3     9  2425  9624 11498  1489  7660    12  9689     8   381
     13  8438     7    12  5819    11     8   511    47    12   879  1737
      8   712    18     7  2837   819  1573    13  5819    53     3     9
   8438     1]]"
fa218b297d9cdcae238cef71096752ce27ca8f4a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Our model achieves a 68.73% EM score and 77.39% F1 score,"[[  421   825  1984     7     3     9     3  3651     5   940  5170     3
   6037  2604    11   489 27914  7561   377   536  2604     1]]"
73738e42d488b32c9db89ac8adefc75403fa2653,0.0,0.99%,[[    0     3 23758  7561     1]], 69.10%/78.38%,[[   3 3951    5 1714 1454   87 3940    5  519 5953    1]]
de4e180f49ff187abc519d01eff14ebcd8149cad,13.7931,"satirical news inconsistencies, contradictions between main clause and prepositional","[[    0     3  9275    52  1950  1506    16  8056   343 11573     6 27252
      7   344   711 14442    11   554  4718   138]]","Inconsistency in Noun Phrase Structures,  Inconsistency Between Clauses, Inconsistency Between Named Entities and Noun Phrases, Word Level Feature Using TF-IDF","[[   86  8056   343  4392    16   465   202  3657 15447 21627     7     6
     86  8056   343  4392 13095 22021    15     7     6    86  8056   343
   4392 13095  5570    26  4443  2197    11   465   202  3657 15447     7
      6  4467  7166     3 16772     3  3626     3  9164    18  4309   371
      1]]"
fb5ce11bfd74e9d7c322444b006a27f2ff32a0cf,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],96-97.6% using the objects color or shape and 79% using shape alone,"[[   3 4314   18 4327    5 6370  338    8 4820  945   42 2346   11  489
  7561  338 2346 2238    1]]"
9f89bff89cea722debc991363f0826de945bc582,0.0,"WSJ14, WSJ15, WSJ15, WSJ15","[[   0    3 8439  683 2534    6    3 8439  683 1808    6    3 8439  683
  1808    6    3 8439  683 1808]]","MEN, MTurk287, MTurk771, RG, RW, SimLex999, SimVerb3500, WS353, WS353R, WS353S","[[    3 22952     6     3  7323   450   157   357  4225     6     3  7323
    450   157  4013  4347     3 12912     6   391   518     6  6619   434
    994 19446     6  6619  5000   115   519  2560     6     3  8439  2469
   6355     3  8439  2469   519   448     6     3  8439  2469   519   134
      1]]"
65ba7304838eb960e3b3de7c8a367d2c2cd64c54,100.0,No,[[  0 465   1]],No,[[465   1]]
f27502c3ece9ade265389d5ace90ca9ca42b46f3,9.5238,they evaluate the stories using their own judgements,[[    0    79  6825     8  1937   338    70   293 22555     7     1]],separate set of Turkers to rate the stories for overall quality and the three improvement areas,"[[ 2450   356    13 23694   277    12  1080     8  1937    21  1879   463
     11     8   386  4179   844     1]]"
601f96770726a0063faf9bacd5db01c4af5add1f,100.0,rule-based and dictionary-based methods,[[    0  3356    18   390    11 24297    18   390  2254     1]],rule-based and dictionary-based methods ,[[ 3356    18   390    11 24297    18   390  2254     1]]
43d8057ff0d3f0c745a7164aed7ed146674630e0,100.0,national dialects of English,[[    0  1157 28461     7    13  1566     1]],national dialects of English,[[ 1157 28461     7    13  1566     1]]
dfd9302615b27abf8cbef1a2f880a73dd5f0c753,0.0,"LSTMs, LSTMs, QAGS","[[    0     3  7600  2305     7     6     3  7600  2305     7     6     3
  23008  8256     1]]","bert-large-wwm, bert-base, bert-large","[[    3  7041    18 15599    18   210   210    51     6     3  7041    18
  10925     6     3  7041    18 15599     1]]"
57388bf2693d71eb966d42fa58ab66d7f595e55f,21.0526,"Using the morpheme segmentation knowledge of the two source languages, we propose ","[[   0    3 3626    8    3 8886   15  526 5508  257 1103   13    8  192
  1391 8024    6   62 4230    3]]",A BPE model is applied to the stem after morpheme segmentation.,"[[  71  272 5668  825   19 2930   12    8 6269  227    3 8886   15  526
  5508  257    5    1]]"
d427e3d41c4c9391192e249493be23926fc5d2e9,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
e91692136033bbc3f19743d0ee5784365746a820,100.0,using multiple pivot sentences,[[    0   338  1317 16959 16513     1]],using multiple pivot sentences,[[  338  1317 16959 16513     1]]
e477e494fe15a978ff9c0a5f1c88712cdaec0c5c,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
58f08d38bbcffb2dd9d660faa8026718d390d64b,32.0,sentiment scores are quantified using mean average precision,[[    0  6493  7586    33 13500  3676   338  1243  1348 11723     1]],"For each cluster, its overall sentiment score is quantified by the mean of the sentiment scores among all tweets","[[  242   284  9068     6   165  1879  6493  2604    19 13500  3676    57
      8  1243    13     8  6493  7586   859    66 10657     7     1]]"
a8e4522ce2ce7336e731286654d6ad0931927a4e,46.1538,"a reduction in the number of visual features that are missing from the visual modality, ","[[    0     3     9  4709    16     8   381    13  3176   753    24    33
   3586    45     8  3176  1794 10355     6     3]]",existing visual features aren't sufficient enough to expect benefits from the visual modality in NMT,"[[ 1895  3176   753    33    29    31    17  6684   631    12  1672  1393
     45     8  3176  1794 10355    16   445  7323     1]]"
bd1a3c651ca2b27f283d3f36df507ed4eb24c2b0,14.2857,No,[[  0 465   1]],"No, it is a probabilistic model trained by finding feature weights through gradient ascent","[[  465     6    34    19     3     9  9551  3040   825  4252    57  2342
   1451  1293     7   190 26462    38  3728     1]]"
a93d4aa89ac3abbd08d725f3765c4f1bed35c889,100.0,"English, Chinese",[[   0 1566    3    6 2830    1]],"English , Chinese ",[[1566    3    6 2830    1]]
7889ec45b996be0b8bf7360d08f84daf3644f115,2.1978,a state-of-the-art MT system BIBREF7,"[[    0     3     9   538    18   858    18   532    18  1408     3  7323
    358     3  5972 25582   371   940     1]]","pal-EtAl:2018:WMT proposed an APE model that uses three self-attention-based encoders, tebbifakhr-EtAl:2018:WMT, the NMT-subtask winner of WMT 2018 ($wmt18^{nmt}_{best}$), employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics., shin-lee:2018:WMT propose that each encoder has its own self-attention and feed-forward layer to process each input separately. , The APE PBSMT-subtask winner of WMT 2018 ($wmt18^{smt}_{best}$) BIBREF11 also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross-attention component for $src \rightarrow pe$ above the previous cross-attention for $mt \rightarrow pe$.","[[ 7692    18   427    17   188    40    10  9457    10   518  7323  4382
     46    71  5668   825    24  2284   386  1044    18 25615    18   390
  23734    52     7     6     3    17    15   115   115    99 18965    52
     18   427    17   188    40    10  9457    10   518  7323     6     8
    445  7323    18  7304 23615  4668    13   549  7323   846  8785   210
     51    17  2606     2    29    51    17     2   834     2  9606     2
   3229   201  5936  5932    18  4563  1453  3621    16   455    12  1792
   4773 14387   383   761    11    12    36  4700    28     8  6569  5002
  15905     5     6     3     7  2907    18   109    15    10  9457    10
    518  7323  4230    24   284 23734    52    65   165   293  1044    18
  25615    11  3305    18 26338  3760    12   433   284  3785 12000     5
      3     6    37    71  5668   276  4547  7323    18  7304 23615  4668
     13   549  7323   846  8785   210    51    17  2606     2     7    51
     17     2   834     2  9606     2  3229    61     3  5972 25582   371
   2596    92  2569   430 19903    18   390  1249    18  7928    71  5668
     84  2284   192 23734    52     7    11  9013     7    46  1151  2269
     18 25615  3876    21  1514     7    52    75     3     2  3535  6770
    158  3229   756     8  1767  2269    18 25615    21  1514    51    17
      3     2  3535  6770   158  3229     5     1]]"
bbaf7cbae88c085faa6bbe3319e4943362fe1ad4,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
bb570d4a1b814f508a07e74baac735bf6ca0f040,100.0,better sentence pair representations,[[   0  394 7142 3116 6497    7    1]],better sentence pair representations,[[ 394 7142 3116 6497    7    1]]
405964517f372629cda4326d8efadde0206b7751,0.0,accuracy,[[   0 7452    1]],they use ROC curves and cross-validation,[[   79   169     3 26893  8435     7    11  2269    18 27769   257     1]]
8d4f0815f8a23fe45c298c161fc7a27f3bb0d338,23.8095,The network is evaluated on two benchmark treebanks for both constituent and dependency parsing.,"[[    0    37  1229    19 14434    30   192 15705  2195  4739     7    21
    321 17429    11 27804   260     7    53     5]]","For different numbers of shared layers, the results are in Table TABREF14. We respectively disable the constituent and the dependency parser to obtain a separate learning setting for both parsers in our model. ","[[  242   315  2302    13  2471  7500     6     8   772    33    16  4398
      3  3221 25582   371  2534     5   101  6898 20597     8 17429    11
      8 27804   260     7    49    12  3442     3     9  2450  1036  1898
     21   321   260     7   277    16    69   825     5     1]]"
48cf360a7753a23342f53f116eeccc2014bcc8eb,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
a4ff1b91643e0c8a0d4cc1502d25ca85995cf428,0.0,"Survey Dataset, Galicia, Northwestern Spain",[[    0 11418  2747  2244     6  6084  1294     9     6 30198  6436     1]],two surveys by two groups - school students and meteorologists to draw on a map a polygon representing a given geographical descriptor,"[[  192 13484    57   192  1637     3    18   496   481    11 23269 17382
     12  3314    30     3     9  2828     3     9  4251  5307  9085     3
      9   787 20187    20 11815   127     1]]"
7997b9971f864a504014110a708f215c84815941,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text","[[25335     7 26847  1405     6   169    13  1812 19590    11  5427    76
    257     6  3041 19510    53     7     6     3     7  4612     6   126
   1234     6  8889     7     6    11  5349    18  9500 27258    11   703
   1999  2099  1628     6   710    41 19457  1643    61  1499     1]]"
32e78ca99ba8b8423d4b21c54cd5309cb92191fc,50.0,13 volunteers,[[   0 1179 6496    1]],14 volunteers,[[ 968 6496    1]]
8c981f8b992cb583e598f71741c322f522c6d2ad,0.0,"the rules are then modified to reflect the inflectional changes, and then the lemmat","[[    0     8  2219    33   258  8473    12  3548     8    16 26303  6318
   1112     6    11   258     8    90   635   144]]",from the Database of Modern Icelandic Inflection (DMII) BIBREF1,"[[   45     8 20230    13  5070 20910   447    86    89 12252    41  7407
    196   196    61     3  5972 25582   371   536     1]]"
44c7c1fbac80eaea736622913d65fe6453d72828,100.0,"34,432 user conversations",[[   0 6154    6  591 2668 1139 9029    1]],"34,432 user conversations",[[6154    6  591 2668 1139 9029    1]]
c69f4df4943a2ca4c10933683a02b179a5e76f64,0.0,fully-attentional feed-forward sequence model,[[    0  1540    18 25615   138  3305    18 26338  5932   825     1]],"PPL: SVT
Diversity: GVT
Embeddings Similarity: SVT
Human Evaluation: SVT","[[  276  5329    10   180 18645 29064    10   350 18645     3 17467    15
   7249     7 18347   485    10   180 18645  3892 22714    10   180 18645
      1]]"
62ba1fefc1eb826fe0cbac092d37a3e2098967e9,100.0,"random method, LSTM",[[   0 6504 1573    3    6    3 7600 2305    1]],"random method , LSTM ",[[6504 1573    3    6    3 7600 2305    1]]
b3857a590fd667ecc282f66d771e5b2773ce9632,50.0,string kernel is a classifier that uses a string string to classify a text,"[[    0  6108 20563    19     3     9   853  7903    24  2284     3     9
   6108  6108    12   853  4921     3     9  1499]]",String kernel is a technique that uses character n-grams to measure the similarity of strings,"[[22341 20563    19     3     9  3317    24  2284  1848     3    29    18
   5096     7    12  3613     8  1126   485    13 18705     1]]"
bb4de896c0fa4bf3c8c43137255a4895f52abeef,0.0,BERT,[[    0   272 24203     1]],a RNN-based seq2seq VC model called ATTS2S based on the Tacotron model,"[[    3     9   391 17235    18   390   142  1824   357     7    15  1824
      3  7431   825   718  8043  4578   357   134     3   390    30     8
   2067   509  6255   825     1]]"
98eb245c727c0bd050d7686d133fa7cd9d25a0fb,0.0,"visual attention, word embeddings",[[    0  3176  1388     6  1448 25078    26    53     7     1]],BLEU scores,[[    3  8775 12062  7586     1]]
62048ea0aab61abe21fb30d70c4a1bc5fb946137,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
57fdb0f6cd91b64a000630ecb711550941283091,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
38c74ab8292a94fc5a82999400ee9c06be19f791,33.3333,"70,000 documents",[[    0     3 28891  2691     1]],"It contains 106,350 documents",[[   94  2579     3 16431     6 16975  2691     1]]
ec62c4cdbeaafc875c695f2d4415bce285015763,33.3333,"BERTa, RoBERTa, Transformer, BERT","[[    0     3 12920   382     9     6  2158 12920   382     9     6 31220
      6     3 12920   382     1]]","BERT, RoBERTa, DistilBERT, GPT, GPT2, Transformer-XL, XLNet, XLM","[[  272 24203     6  2158 12920   382     9     6  2043 17153 12920   382
      6   350  6383     6   350  6383  4482 31220    18     4   434     6
      3     4   434  9688     6     3     4 11160     1]]"
52b113e66fd691ae18b9bb8a8d17e1ee7054bb81,0.0,"The author's home is located in the heart of the city, Brasilia","[[    0    37  2291    31     7   234    19  1069    16     8   842    13
      8   690     6  3497 10578     9     1]]",Vagalume website,[[2964 6191  440   15  475    1]]
a96a1a354cb3a2a434b085e4d9c8844d0b672ec4,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
fb76e994e2e3fa129f1e94f1b043b274af8fb84c,36.0,CWVAE is trained on the Atomic dataset and CWVAE is trained on,"[[    0     3 18105  8230   427    19  4252    30     8 20474   447 17953
     11     3 18105  8230   427    19  4252    30]]"," CWVAE is trained on an auxiliary dataset to learn the event background information by using the context-aware latent variable.  Then, in finetute stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target.","[[    3 18105  8230   427    19  4252    30    46     3 31086 17953    12
    669     8   605  2458   251    57   338     8  2625    18     9  3404
     50  4669  7660     5    37    29     6    16  1399    17  2810  1726
      6     3 18105  8230   427    19  4252    30     8  2491    18  9500
  17953    12  3374     8   605  2458   251    12   284   806  2663    13
    156    18   634    29    16  1010  7220  2387     5     1]]"
593e307d9a9d7361eba49484099c7a8147d3dade,16.6667,"networks that contain $23,239$ causal links and $19,096","[[    0  5275    24  3480  3771   519     2     6   357  3288  3229 31161
   2416    11  1970  1298     2     6   632  4314]]","networks where nodes represent causes and effects, and directed edges represent cause-effect relationships proposed by humans","[[ 5275   213   150  1395  4221  4110    11  1951     6    11  6640  9804
   4221  1137    18 23677  3079  4382    57  6917     1]]"
0b8d64d6cdcfc2ba66efa41a52e09241729a697c,100.0,No,[[  0 465   1]],No,[[465   1]]
a78a6fd6ca36413586836838e38f3fa9282646ee,100.0,No,[[  0 465   1]],No,[[465   1]]
53d6cbee3606dd106494e2e98aa93fdd95920375,21.0526,"Compared to previous state of the art, the improvements are insignificant.","[[    0     3 25236    12  1767   538    13     8   768     6     8  6867
     33    16 26251     5     1]]","test accuracy of 88.9%, which exceeds the previous best by 16.9%","[[  794  7452    13     3  4060     5  7561     6    84  8193     7     8
   1767   200    57 10128  7561     1]]"
1a7d2ade16149630c0028339a816fcafa8192408,0.0,"2,507",[[   0 3547 1752  940    1]],"7,507",[[7973 1752  940    1]]
5679fabeadf680e35a4f7b092d39e8638dca6b4d,0.0,Yes,[[   0 2163    1]],No,[[465   1]]
1571e16063b53409f2d1bd6ec143fccc5b29ebb9,18.1818,Random Forest test,[[    0 25942  6944   794     1]],"Majority Class baseline (MC) , Random selection baseline (RAN)","[[ 9236   485  4501 20726    41  3698    61     3     6 25942  1801 20726
     41 16375    61     1]]"
9cbea686732b5b85f77868ca47d2f93cf34516ed,17.3913,"the encoder RNN reads the input message, encodes it into a fixed context","[[    0     8 23734    52   391 17235   608     7     8  3785  1569     6
  23734     7    34   139     3     9  3599  2625]]","we extract the emotion information from the utterances in $\mathbf {X}$ by leveraging an external text analysis program, and use an RNN to encode it into an emotion context vector $\mathbf {e}$, which is combined with $\mathbf {c}_t$ to produce the distribution","[[   62  5819     8 13868   251    45     8     3  5108   663     7    16
   1514     2  3357   107   115    89     3     2     4     2  3229    57
      3 26072    46  3866  1499  1693   478     6    11   169    46   391
  17235    12 23734    34   139    46 13868  2625 12938  1514     2  3357
    107   115    89     3     2    15     2  3229     6    84    19  3334
     28  1514     2  3357   107   115    89     3     2    75     2   834
     17  3229    12  1759     8  3438     1]]"
cd37ad149d500e1c7d2de9de1f4bae8dcc443a72,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"task-specific architecture during pre-training (task-specific methods), aim at building a general pre-training architecture to fit all downstream tasks (task-agnostic methods)","[[ 2491    18  9500  4648   383   554    18 13023    41 23615    18  9500
   2254   201  2674    44   740     3     9   879   554    18 13023  4648
     12  1400    66 26804  4145    41 23615    18     9  6715     7  1225
   2254    61     1]]"
1d791713d1aa77358f11501f05c108045f53c8aa,100.0,1061,[[   0  335 4241    1]],1061,[[ 335 4241    1]]
9213159f874b3bdd9b4de956a88c703aac988411,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
311a7fa62721e82265f4e0689b4adc05f6b74215,12.5,downward reasoning,[[    0 22032 20893     1]],"Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific.","[[ 3234  2239 20893    19  4802    38   352    45    80   806  2077    12
      3     9    72   879    80     5  7309  2239 20893    19  4802    38
      8  6401     6   352    45     3     9   879  2077    12    80    24
     19    72   806     5     1]]"
70f9358dc01fd2db01a6b165e0b4e83e4a9141a7,14.2857,"DeepWalk, LSTM, LSTM, LSTM+L, LS","[[    0  9509 29873     6     3  7600  2305     6     3  7600  2305     6
      3  7600  2305  1220   434     6     3  7600]]","MMB, DeepWalk, LINE,  Node2vec, TADW, CENE, CANE, WANE, DMTE","[[  283  4633     6  9509 29873     6     3 20006     6   465   221   357
    162    75     6     3  3221 20293     6   205 25576     6  3087  4171
      6  7896  4171     6     3  7407  3463     1]]"
85d1831c28d3c19c84472589a252e28e9884500f,33.3333,"BERT-Base, QRE",[[    0   272 24203    18 14885    15     6  1593  4386     1]],"QANet BIBREF39, BERT-Base BIBREF26","[[    3 23008  9688     3  5972 25582   371  3288     6   272 24203    18
  14885    15     3  5972 25582   371  2688     1]]"
aa1f605619b2487cc914fc2594c8efe2598d8555,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
589be705a5cc73a23f30decba23ce58ec39d313b,80.0,The Dutch section of the WSJ corpus,[[    0    37 10098  1375    13     8     3  8439   683 11736   302     1]],the Dutch section of the OSCAR corpus,[[    8 10098  1375    13     8  6328   254  4280 11736   302     1]]
7d59374d9301a0c09ea5d023a22ceb6ce07fb490,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],by number of distinct n-grams,[[  57  381   13 6746    3   29   18 5096    7    1]]
a726046eec1e2efa5fe3926963863bf755e64682,100.0,"German, English, Chinese",[[   0 2968    6 1566    6 2830    1]],"German, English, Chinese",[[2968    6 1566    6 2830    1]]
fe6bb55b28f14ed8ac82c122681905397e31279d,0.0,a Gaussian process is an ideal solution,[[    0     3     9 12520     7 10488   433    19    46  1523  1127     1]],avoids the need for expensive cross-validation for hyperparameter selection,"[[ 1792     7     8   174    21  2881  2269    18 27769   257    21  6676
   6583  4401  1801     1]]"
6b8a3100895f2192e08973006474428319dc298e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],MNCut spectral clustering algorithm BIBREF58,"[[  283  8137    76    17     3  5628  4900  9068    53 12628     3  5972
  25582   371  3449     1]]"
36b5f0f62ee9be1ab50d1bb6170e98328d45997d,16.6667,word-based embeddings model with Bi-LSTM and text-based embedd,"[[    0  1448    18   390 25078    26    53     7   825    28  2106    18
   7600  2305    11  1499    18   390 25078    26]]","Word2Vec, Wang2Vec, and FastText","[[ 4467   357   553    15    75     6 18102   357   553    15    75     6
     11  6805 13598    17     1]]"
f91835f17c0086baec65ebd99d12326ae1ae87d2,0.0,using an attentional seq2seq model,[[   0  338   46 1388  138  142 1824  357    7   15 1824  825    1]],Stanford CoreNLP BIBREF11 ,[[19796  9020   567  6892     3  5972 25582   371  2596     1]]
e574f0f733fb98ecef3c64044004aa7a320439be,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],DISPLAYFORM0,[[    3 15438   345 29002 24030   632     1]]
689d1d0c4653a8fa87fd0e01fa7e12f75405cd38,100.0,biLSTM-networks,[[    0  2647  7600  2305    18  1582 13631     1]],biLSTM-networks,[[ 2647  7600  2305    18  1582 13631     1]]
940873658ee64e131cafcf9b3d26a45a98920cc2,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
1ed6acb88954f31b78d2821bb230b722374792ed,22.2222,Private dashboard is a shared task where two teams are assigned to handle the hate task.,"[[    0  7086 16740    19     3     9  2471  2491   213   192  2323    33
   7604    12  2174     8  5591  2491     5     1]]",Private dashboard is leaderboard where competitors can see results after competition is finished - on hidden part of test set (private test set).,"[[ 7086 16740    19  2488  1976   213  9216    54   217   772   227  2259
     19  2369     3    18    30  5697   294    13   794   356    41 26881
    794   356   137     1]]"
aacb0b97aed6fc6a8b471b8c2e5c4ddb60988bf5,100.0,one,[[ 0 80  1]],one,[[80  1]]
fffbd6cafef96eeeee2f9fa5d8ab2b325ec528e6,100.0,58,[[   0    3 3449    1]],58,[[   3 3449    1]]
b1457feb6cdbf4fb19c8e87e1cd43981bc991c4c,100.0,No,[[  0 465   1]],No,[[465   1]]
94ec0e205790ec663a5353f3c68c8d77701573c7,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
14eb2b89ba39e56c52954058b6b799a49d1b74bf,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],The changes are evaluated based on accuracy of intent and entity recognition on SNIPS dataset,"[[   37  1112    33 14434     3   390    30  7452    13  9508    11 10409
   5786    30     3  8544 21563 17953     1]]"
0db1ba66a7e75e91e93d78c31f877364c3724a65,25.0,"classification, sentiment classification",[[    0 13774     6  6493 13774     1]],"Sentiment Classification, Transferability of Shared Sentence Representation, Introducing Sequence Labeling as Auxiliary Task","[[ 4892  2998   295  4501  2420     6  9900  2020    13  7105    26  4892
     17  1433   419 12640   257     6     3 30586 26859  1433 16229    53
     38    71  3090 13565   651 16107     1]]"
d43e868cae91b3dc393c05c55da0754b0fb3a46a,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
818c89b11471a6ca4f13c838713864fdf282c2ca,100.0,LSTM-LM,[[    0     3  7600  2305    18 11160     1]],LSTM-LM ,[[    3  7600  2305    18 11160     1]]
7486c9d9e6c407c0c3bc012405d689dbee072327,100.0,German,[[   0 2968    1]],German,[[2968    1]]
2c4003f25e8d95a3768204f52a7a5f5e17cb2102,100.0,No,[[  0 465   1]],No,[[465   1]]
2fea3c955ff78220b2c31a8ad1322bc77f6706f8,0.0,Unanswerable,[[   0  597 3247 3321  179    1]]," our method enables to control the morphological realization of first and second-person pronouns, together with verbs and adjectives related to them","[[   69  1573     3  7161    12   610     8     3  8886  4478 23543    13
    166    11   511    18  6075   813 15358    29     7     6   544    28
   7375     7    11 31268     7  1341    12   135     1]]"
86abeff85f3db79cf87a8c993e5e5aa61226dc98,100.0,"negative, positive",[[   0 2841    6 1465    1]],"negative, positive",[[2841    6 1465    1]]
4059c6f395640a6acf20a0ed451d0ad8681bc59b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Answer with content missing: (Formula) Formula is the answer.,"[[11801    28   738  3586    10    41  3809  4115     9    61 13786    19
      8  1525     5     1]]"
a7adb63db5066d39fdf2882d8a7ffefbb6b622f0,0.0,Yes,[[   0 2163    1]],There is no baseline.,[[  290    19   150 20726     5     1]]
c82e945b43b2e61c8ea567727e239662309e9508,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort,"[[15849    53   344  3739   120  1465    11  2841 24666   441   284  1020
   2945  3303    11  7625    21 14039   331  4759    30     8  2387 23785
      1]]"
d71cb7f3aa585e256ca14eebdc358edfc3a9539c,0.0,"syllable-to-phone system, syllable-tophone","[[   0    3    7   63  195  179   18  235   18 6399  358    6    3    7
    63  195  179   18  235 6399]]","CELEX (Dutch and English) - SVM-HMM
Festival, E-Hitz and OpenLexique - Liang hyphenation
IIT-Guwahat - Entropy CRF","[[  205 16479     4    41 12998    17   524    11  1566    61     3    18
    180 12623    18   566  8257  3397     6   262    18   566  5615    11
   2384   434   994  1495     3    18  1414  1468     3 13397  3225   257
     27  3177    18  9105 17771   144     3    18   695 12395    63   205
   8556     1]]"
e76139c63da0f861c097466983fbe0c94d1d9810,22.2222,No,[[  0 465   1]],"No, supervised models perform better for this task.",[[  465     6     3 23313  2250  1912   394    21    48  2491     5     1]]
5d6cc65b73f428ea2a499bcf91995ef5441f63d4,14.2857,"Using the quality score obtained from QG, we can conclude that the generated output is better","[[    0     3  3626     8   463  2604  5105    45  1593   517     6    62
     54 12692    24     8  6126  3911    19   394]]",Through human evaluation where they are asked to evaluate the generated output on a likert scale.,"[[4582  936 5002  213   79   33 1380   12 6825    8 6126 3911   30    3
     9  114   52   17 2643    5    1]]"
0a521541b9e2b5c6d64fb08eb318778eba8ac9f7,100.0,"SNLI, MultiNLI and SICK",[[    0     3  8544  8159     6  4908 18207   196    11   180 25286     1]],"SNLI, MultiNLI and SICK",[[    3  8544  8159     6  4908 18207   196    11   180 25286     1]]
eacc1eb65daad055df934e0e878f417b73b2ecc1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem","[[ 3830     3     9  2711    13 24515   138    11  1499  3471     3    35
   5756   297  5644     5   304 10446     3     9  1988    16     8 17953
     38  3510     6  6273  2810    26     6    42  3550 10812    15    26
      6     3     9   358   398 21830  2193  2984    11 16513    45 16885
      5    37    29    34   398  2204   823   284    13   273 16513     6
     42   128  2711    13   135     6     3    35  5756     7    42  6273
   2810     7     8  1988     6    84    19    46     3    35  5756   297
    682     1]]"
50690b72dc61748e0159739a9a0243814d37f360,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
c635dc8013e63505084b9daaa9ddb021a2d24543,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
aaf50a6a9f449389ef212d25d0fae59c10b0df92,100.0,"news publications, wine reviews, and Reddit",[[    0  1506 10142     6  2013  2456     6    11  1624    26   155     1]],"news publications, wine reviews, and Reddit",[[ 1506 10142     6  2013  2456     6    11  1624    26   155     1]]
d27f23bcd80b12f6df8e03e65f9b150444925ecf,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
ea6764a362bac95fb99969e9f8c773a61afd8f39,100.0,82.0%,[[   0    3 4613    5 6932    1]],82.0%,[[   3 4613    5 6932    1]]
2af66730a85b29ff28dbfa58342e0ae6265d2963,100.0,"78,976",[[   0    3 3940    6 4327  948    1]],"78,976",[[   3 3940    6 4327  948    1]]
423bb905e404e88a168e7e807950e24ca166306c,0.0,"Freebase logical parsers BIBREF11, BIBREF16","[[    0  1443 10925     3  6207   260     7   277     3  5972 25582   371
   2596     6     3  5972 25582   371  2938     1]]","GraphParser without paraphrases, monolingual machine translation based model for paraphrase generation","[[    3 21094 13212     7    49   406  3856 27111     7     6  7414 25207
   1437  7314     3   390   825    21  3856 27111  3381     1]]"
27c1c678d3862c7676320ca493537b03a9f0c77a,0.0,WSJ2WS,[[   0    3 8439  683  357 8439    1]],KVRET,[[  480   553 27514     1]]
9c9e90ceaba33242342a5ae7568e89fe660270d5,12.5,"Compared to the state-of-the-art methods, HAKE performs better than","[[    0     3 25236    12     8   538    18   858    18   532    18  1408
   2254     6   454 17253  1912     7   394   145]]","0.021 higher MRR, a 2.4% higher H@1, and a 2.4% higher H@3 against RotatE, respectively, doesn't outperform the previous state-of-the-art as much as that of WN18RR and YAGO3-10, HAKE gains a 0.050 higher MRR, 6.0% higher H@1 and 4.6% higher H@3 than RotatE, respectively","[[    3 11739  2658  1146   283 12224     6     3     9  1682  5988  1146
    454  1741  4347    11     3     9  1682  5988  1146   454  1741   519
    581  8704   144   427     6  6898     6   744    31    17    91   883
   2032     8  1767   538    18   858    18   532    18  1408    38   231
     38    24    13     3 21170  2606 12224    11     3 17419  5577   519
   4536     6   454 17253 11391     3     9     3 11739  1752  1146   283
  12224     6  4357  6932  1146   454  1741   536    11  2853  6370  1146
    454  1741   519   145  8704   144   427     6  6898     1]]"
551a17fc1d5b5c3d18bdc4923363cbbda7eb2516,100.0,No,[[  0 465   1]],No,[[465   1]]
fd0ef5a7b6f62d07776bf672579a99c67e61a568,0.0,Unanswerable,[[   0  597 3247 3321  179    1]]," we measure our system's performance for datasets across various domains, evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs","[[   62  3613    69   358    31     7   821    21 17953     7   640   796
   3303     7     6  5002     7    33   612    57  3030 14743   113   734
      7     8  1103  1247    11   258  5191  1139 13154 20208    12     8
      3 23008 14152     1]]"
252677c93feb2cb0379009b680f0b4562b064270,28.5714,"82,432 scientific entities, including 5,714 coding languages","[[    0     3  4613     6   591  2668  4290 12311     6   379  7836   940
   2534     3  9886  8024     1]]","6,127 scientific entities, including 2,112 Process, 258 Method, 2,099 Material, and 1,658 Data entities","[[ 8580 22367  4290 12311     6   379  3547  2596   357 10272     6   204
   3449  7717     6  3547   632  3264  7374     6    11  1914   948  3449
   2747 12311     1]]"
2ebd7a59baad1f935fe83f90526557bfa9df4047,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
982d375378238d0adbc9a4c987d633ed16b7f98f,57.1429,"Twitter, Reddit, Reddit",[[   0 3046    6 1624   26  155    6 1624   26  155    1]],"Twitter, Reddit, Online Dialogues",[[ 3046     6  1624    26   155     6  1777  5267 10384     7     1]]
d3d4eef047aa01391e3e5d613a0f1f786ae7cfc7,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa","[[  165   821   341     3  5430     7  1187  2250  4252    30     8   926
   1566   761   356    16     8 26585   434  6048    18   382  6038  1898
      6   505 10917     3   208     7     5     3  4613     5  4729    21
   2158 12920   382     9     1]]"
4c026715ee365c709381c5da770bdc8297eed19f,0.0,hirability,[[   0    3 9288 2020    1]],candidates who have been liked or shortlisted are considered part of the hirable class,"[[ 4341   113    43   118  6528    42   710 19279    33  1702   294    13
      8     3  9288   179   853     1]]"
b2b0321b0aaf58c3aa9050906ade6ef35874c5c1,100.0,"$150,000$ tweets",[[    0  1970  9286  3229 10657     7     1]]," $150,000$ tweets",[[ 1970  9286  3229 10657     7     1]]
7af01e2580c332e2b5e8094908df4e43a29c8792,15.3846,AUC scores were averaged for all responses and their diversity was measured using textual analyses and,"[[    0    71  6463  7586   130  1348    26    21    66  7216    11    70
   7322    47  8413   338  1499  3471 15282    11]]",By computing number of unique responses and number of responses divided by the number of unique responses to that question for each of the questions,"[[  938 10937   381    13   775  7216    11   381    13  7216  8807    57
      8   381    13   775  7216    12    24   822    21   284    13     8
    746     1]]"
7d4fad6367f28c67ad22487094489680c45f5062,0.0,"LSTM, LSTM, LSTM, LSTM, LSTM","[[   0    3 7600 2305    6    3 7600 2305    6    3 7600 2305    6    3
  7600 2305    6    3 7600 2305]]","window_size, alpha, sample, dm, hs, vector_size","[[ 2034   834  7991     6   491  6977     6  3106     6     3    26    51
      6     3   107     7     6 12938   834  7991     1]]"
a5b67470a1c4779877f0d8b7724879bbb0a3b313,100.0,micro-averaged F1,[[    0  2179    18 28951    26   377   536     1]],micro-averaged F1,[[ 2179    18 28951    26   377   536     1]]
2e1ededb7c8460169cf3c38e6cde6de402c1e720,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"mean prediction accuracy 0.99582651
S&P 500 Accuracy 0.99582651","[[ 1243 21332  7452  4097  3264  3449  2688  5553   180   184   345  2899
   4292  3663  4710  4097  3264  3449  2688  5553     1]]"
62a3dc90ba427c5985789001a02825c9434ce67d,0.0,BIBREF0,[[    0     3  5972 25582   371   632     1]],"1,160 physician logs of Medical ICU admission requests, 42,506 Wikipedia articles, 6 research papers and 2 critical care medicine textbooks","[[ 1914 19129 10027  4303     7    13  3721    27  5211  7209  6166     6
   6426     6  1752   948 16885  2984     6   431   585  5778    11   204
   2404   124  4404 19533     7     1]]"
d3bcfcea00dec99fa26283cdd74ba565bc907632,50.0,"77,287 images",[[   0    3 4013    6  357 4225 1383    1]],"133,287 images",[[    3 22974     6   357  4225  1383     1]]
65e2f97f2fe8eb5c2fa41cb95c02b577e8d6e5ee,0.0,accuracy,[[   0 7452    1]],number of dialogs that resulted in launching a skill divided by total number of dialogs,"[[  381    13 13463     7    24   741    15    26    16     3 14138     3
      9  4359  8807    57   792   381    13 13463     7     1]]"
1ef5fc4473105f1c72b4d35cf93d312736833d3d,100.0,No,[[  0 465   1]],No,[[465   1]]
f981781021d4bacbaf3d076c895dc42d7fa108ba,100.0,No,[[  0 465   1]],No,[[465   1]]
bd26a6d5d8b68d62e1b6eaf974796f3c34a839c4,0.0,proximity between documents is evaluated as a function of the parameters of the PV-DM train model,"[[    0 16595   344  2691    19 14434    38     3     9  1681    13     8
   8755    13     8 16303    18  7407  2412   825]]","String length, Words co-occurrences, Stems co-occurrences, MeSH similarity","[[22341  2475     6  4467     7   576    18 16526     7     6  3557    51
      7   576    18 16526     7     6  1212  9122  1126   485     1]]"
4056ee2fd7a0a0f444275e627bb881134a1c2a10,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the $4^\text{th}$ and $6^\text{th}$ columns of the file format) BIBREF13 . ,"[[  101   169     8     3  8886  4478     3    17 15242 17953     7   937
     57     8 12489 30718 11573    41 10161    61  2195  4739     7    41
    532   975  2138    35   257    13     8  7514     2  6327     2   189
      2  3229    11 10746     2  6327     2   189     2  3229 15752    13
      8  1042  1910    61     3  5972 25582   371  2368     3     5     1]]"
94edac71eea1e78add678fb5ed2d08526b51016b,0.0,linear programming relaxations and greedy search,[[    0 13080  6020 12633     7    11 30337    63   960     1]],"Parallel Scan Inference, Vectorized Parsing, Semiring Matrix Operations","[[27535   180  1608    86 11788     6 29011  1601  2180     7    53     6
  22217  1007  5708    52  2407 14111     1]]"
37016cc987d33be5ab877013ef26ec7239b48bd9,0.0,Yes,[[   0 2163    1]],"To achieve this purpose, we introduce a trainable class weight $\mathbf {w}$ to reweigh source domain examples by class when performing DIRL, with $\mathbf {w}_i > 0$","[[  304  1984    48  1730     6    62  4277     3     9  2412   179   853
   1293  1514     2  3357   107   115    89     3     2   210     2  3229
     12     3    60  1123  9031  1391  3303  4062    57   853   116  5505
  10826 12831     6    28  1514     2  3357   107   115    89     3     2
    210     2   834    23  2490     3   632  3229     1]]"
89b9e298993dbedd3637189c3f37c0c4791041a1,100.0,"embedding of the claim, Web evidence",[[    0 25078    26    53    13     8  1988     6  1620  2084     1]],"embedding of the claim, Web evidence",[[25078    26    53    13     8  1988     6  1620  2084     1]]
b84bce289c6c81d0a7507ae183b94982533576b3,11.1111,a PBMT system is trained using a pre-translation framework.,"[[    0     3     9     3 13970  7323   358    19  4252   338     3     9
    554    18  7031  6105  4732     5     1]]",systems were optimized on the tst2014 using Minimum error rate training BIBREF20,"[[ 1002   130 18149    30     8     3    17     7    17 10218   338 22619
   3505  1080   761     3  5972 25582   371  1755     1]]"
d0f831c97d345a5b8149a9d51bf321f844518434,0.0,"negative, positive",[[   0 2841    6 1465    1]],binary label of stress or not stress,[[14865  3783    13  2189    42    59  2189     1]]
f6202100cfb83286dc51f57c68cffdbf5cf50a3f,16.6667,"LSTM, Attention, Convolution",[[    0     3  7600  2305     6 20748     6  1193 24817     1]],"Step-Wise Decoder Fusion, Multimodal Attention Modulation, Visual-Semantic (VS) Regularizer","[[ 5021    18   518   159    15  4451 13487 23230     6  4908 20226 20748
   5073  7830     6 10893    18   134    15   348  1225    41 14945    61
  17116  8585     1]]"
3752bbc5367973ab5b839ded08c57f51336b5c3d,0.0,"Training2Relations, NLM-Relations, PD-SPL,","[[    0  4017   357  1649  6105     7     6   445 11160    18  1649  6105
      7     6     3  6251    18   134  5329     6]]","Training-22, NLM-180",[[ 4017 16149     6   445 11160    18 20829     1]]
bf25a202ac713a34e09bf599b3601058d9cace46,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Randomwalk, Walktrap, Louvain clustering",[[25942 12539     6 10801 16081     6 11884   900    77  9068    53     1]]
aecd09a817c38cf7606e2888d0df7f14e5a74b95,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Ordinal position, Length of sentence, The Ratio of Nouns, The Ratio of Numerical entities, Cue Words, Cosine position, Relative Length, TF-ISF, POS features, Document sentences, Document words, Topical category, Ratio of Verbs, Ratio of Adjectives, and Ratio of Adverbs","[[21983   138  1102     6   312  1725   189    13  7142     6    37  6455
     23    32    13   465   202     7     6    37  6455    23    32    13
   1174   935  1950 12311     6  1839    15  4467     7     6   638     7
    630  1102     6   419    40  1528   312  1725   189     6     3  9164
     18   196  7016     6     3 16034   753     6 11167 16513     6 11167
   1234     6 18059   138  3295     6  6455    23    32    13 15609     7
      6  6455    23    32    13  1980 11827  8763     6    11  6455    23
     32    13  1980 11868     7     1]]"
a3f108f60143d13fe38d911b1cc3b17bdffde3bd,0.0,BLEU 0,[[    0     3  8775 12062     3   632     1]],"Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively.","[[  749 12151   825  1984     7  4097  3840     6     3 23758  2266     6
   4097  4450   377   536  2604    30     3  6857     6   454  6048     6
      3 25430 17953     7  6898     5     1]]"
0ec4143a4f1a8f597b435f83c0451145be2ab95b,10.5263,LSTM with a lexicon and a softmax layer for each language,"[[    0     3  7600  2305    28     3     9     3 30949   106    11     3
      9  1835  9128  3760    21   284  1612     1]]","Frequency masking, Time masking, Additive noise, Speed and volume perturbation","[[ 5532   835 11298  8181    53     6  2900  8181    53     6  2334    23
   3268  4661     6  9913    11  2908 29404   257     1]]"
b65b1c366c8bcf544f1be5710ae1efc6d2b1e2f1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],The Lemming model in BIBREF17,[[   37   312   635    53   825    16     3  5972 25582   371  2517     1]]
38b527783330468bf6c4829f7d998e6f17c615f0,100.0,No,[[  0 465   1]],No,[[465   1]]
8ea664a72e6d6eca73c1b3e1f75a72a677474ab1,100.0,No,[[  0 465   1]],No,[[465   1]]
f46a907360d75ad566620e7f6bf7746497b6e4a9,0.0,"Word2Vec, Word2Vec, Word2Vec","[[   0 4467  357  553   15   75    3    6 4467  357  553   15   75    3
     6 4467  357  553   15   75]]","Kyubyong Park, Edouard Grave et al BIBREF11","[[11478    76   969  2444  1061     6  4857  1063   986 15199    15     3
     15    17   491     3  5972 25582   371  2596     1]]"
090fd9ce9a21438cdec1ea51ed216941d52eb3b6,40.0,Hierarchical Disentangled Disentangled Disentangled Disentangled Disentangled Dis,"[[    0  3204  7064  1950  2678   295 19834  2678   295 19834  2678   295
  19834  2678   295 19834  2678   295 19834  2678]]",Hierarchical Disentangled Self-Attention,[[ 3204  7064  1950  2678   295 19834  8662    18   188    17  9174     1]]
6f8386ad64dce3a20bc75165c5c7591df8f419cf,11.1111,mBERT representations can be split into a language-specific component and a,"[[    0     3    51 12920   382  6497     7    54    36  5679   139     3
      9  1612    18  9500  3876    11     3     9]]",We thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space.,"[[  101  2932   653    12  2036     8  1612    18  9500   251    45     8
   6497     7    57  1530    53     8  6497     7    13 16513    16   284
   1612    78    24    70  1348  7797    44     8  5233    13     8 12938
    628     5     1]]"
5328cc2588b2bf7b91f4e0f342e8cbfc6dc8ac00,66.6667,RNN-T,[[    0   391 17235    18   382     1]],LSTM-based RNN-T,[[    3  7600  2305    18   390   391 17235    18   382     1]]
993b896771c31f3478f28112a7335e7be9d03f21,0.0,RNNs,[[    0   391 17235     7     1]],"A network, whose learned functions satisfy a certain equation. The  network contains RNN cells with either nested internal memories or dependencies that extend temporally beyond the immediately previous hidden state.","[[   71  1229     6     3  2544  2525  3621 11132     3     9   824 13850
      5    37  1229  2579   391 17235  2640    28   893     3  1496  1054
   3224  5655    42  6002 11573    24  4285 10301   127  1427  1909     8
   2017  1767  5697   538     5     1]]"
cbbcafffda7107358fa5bf02409a01e17ee56bfd,0.0,No,[[  0 465   1]],It is observed some variability - but not significant. Bert does not seem to gain much more syntax information than with type level information.,"[[   94    19  6970   128 27980     3    18    68    59  1516     5 20612
    405    59  1727    12  2485   231    72 28230   251   145    28   686
    593   251     5     1]]"
c2432884287dca4af355698a543bc0db67a8c091,22.2222,"The expansion model takes advantage of general language understanding to suggest contextually relevant new words, without nec","[[    0    37  5919   825  1217  2337    13   879  1612  1705    12  3130
  28131   120  2193   126  1234     6   406  9705]]",number of relevant output words as a function of the headline’s category label,"[[  381    13  2193  3911  1234    38     3     9  1681    13     8 12392
     22     7  3295  3783     1]]"
b3a09d2e3156c51bd5fdc110a2a00a67bb8c0e42,0.0,"document-level LSTM, LSTM-based LSTM, LSTM","[[   0 1708   18 4563    3 7600 2305    6    3 7600 2305   18  390    3
  7600 2305    6    3 7600 2305]]","Answer with content missing: (Names of many identifiers missing) TextCat, ChromeCLD, LangDetect, langid.py, whatlang, whatthelang, YALI, LDIG, Polyglot 3000, Lextek Language Identifier and Open Xerox Language Identifier.","[[11801    28   738  3586    10    41 23954     7    13   186     3  8826
     52     7  3586    61  5027 18610     6 10780   254  9815     6  7073
  31636     6 12142    23    26     5   102    63     6   125  4612     6
    125   532  4612     6     3   476 24933     6     3  9815  8834     6
   7945  9680    17   220  2313     6 17546 15150 10509     3 21153  7903
     11  2384     3     4    15 12907 10509     3 21153  7903     5     1]]"
0c3924214572579ddbc1b4a87c7f7842ef20ff1b,100.0,Cuneiform,[[   0  205  444   23 2032    1]],Cuneiform,[[ 205  444   23 2032    1]]
d82ec1003a3db7370994c7522590f7e5151b1f33,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],We collected the historical 52 week stock prices prior to this event and calculated the daily stock price change. The distribution of the daily price change of the previous 52 weeks is Figure FIGREF13 with a mean INLINEFORM1 and standard deviation INLINEFORM2 . ,"[[  101  4759     8  4332  9065   471  1519  1596  1884    12    48   605
     11 11338     8  1444  1519   594   483     5    37  3438    13     8
   1444   594   483    13     8  1767  9065  1274    19  7996 11376  4386
    371  2368    28     3     9  1243  3388 20006 24030   536    11  1068
  25291  3388 20006 24030   357     3     5     1]]"
46f175e1322d648ab2c0258a9609fe6f43d3b44e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]], inclusion of longer parts of the conversation,[[11980    13  1200  1467    13     8  3634     1]]
957bda6b421ef7d2839c3cec083404ac77721f14,24.0,"capitalisation, spelling errors, etc",[[    0  1784  2121     6 19590  6854     6   672     1]],"LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors,  Spelling errors, Repeated characters, Capitalisation, Length,  Emoticon (Presence/Count ) 
 and Sentiment Ratio","[[  301  4296    73    23  5096     7    41 10572     7  1433    87 10628
    201     3 16034  6455    23    32     6  1713 23954    26  4443   485
   3137 10872     6  1713   308   159 19221  7878   127     7     6  8974
  12013  6854     6 20469    15    26  2850     6  5826  2121     6   312
   1725   189     6  3967  9798   106    41 10572     7  1433    87 10628
      3    61    11  4892  2998   295  6455    23    32     1]]"
0619fc797730a3e59ac146a5a4575c81517cc618,0.0,Bi-LSTM,[[   0 2106   18 7600 2305    1]],"We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN., we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. ","[[  101  4048    69   825   821    28     8  6315    13     3  5972 25582
    371   632     3  5972 25582   371   755    30  5097   134 10052   302
      5     3  5972 25582   371   632  2196     8   772    13 24210   695
  12395    63    41 21298 16924   201     3 14972     6   180 12623    30
   5097   134 10052   302   578   207   821    16  1767    97     5    37
    825    13     3  5972 25582   371   755    19     3     9   538    18
    858    18   532    18  1408    78   623    57   338     3     9  7435
   4112 17235     5     6    62  4048   772    28     8   825    13     3
   5972 25582   371  2534    24   261     3     9  8784    13  1317  1247
    853  7903     7    41 18433    61   224    38     3 14972     6 25942
   6944    41  8556   201   180 12623    11  7736  3040   419 22430    41
  12564   137    37     3 18433   825    19  3334    28  2182    18   858
     18  6051     7    41   279    32   518   201  1451    65   107    53
     41   371   566    61    11     3 30949   106     7     5    37   825
     13     3  5972 25582   371  2534    19     3     9   538    18   858
     18   532    18  1408    30 22439    11   454  4545 17953     7     5
      1]]"
9e1bf306658ef2972159643fdaf149c569db524b,66.6667,the Yucatan language family,[[   0    8 6214 2138  152 1612  384    1]],the Otomanguean language family,[[   8  411  235  348 5398  152 1612  384    1]]
ef872807cb0c9974d18bbb886a7836e793727c3d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.,"[[   37  1234    24    54  6360     8  6803    13     8 10678  1234    38
  28131 12545    11  3806    34    45     8  3269 21527 28131 12545     5
      1]]"
c4c9c7900a0480743acc7599efb359bc81cf3a4d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The increase is linearly from lowest on average 2.0 , medium around 3.5, and the largest is 6.0","[[   37   993    19 13080   120    45  7402    30  1348  6864     3     6
   2768   300     3  9285     6    11     8  2015    19     3 22642     1]]"
6389d5a152151fb05aae00b53b521c117d7b5e54,0.0,"a search-based GAN architecture with a search and supervised learning combined process,","[[    0     3     9   960    18   390   350  5033  4648    28     3     9
    960    11     3 23313  1036  3334   433     6]]","Semantic Enhancement GANs: DC-GANs, MC-GAN
Resolution Enhancement GANs: StackGANs, AttnGAN, HDGAN
Diversity Enhancement GANs: AC-GAN, TAC-GAN etc.
Motion Enhancement GAGs: T2S, T2V, StoryGAN","[[  679   348  1225 27190  1194   350  5033     7    10  5795    18   517
   5033     7     6     3  3698    18   517  5033 19957 27190  1194   350
   5033     7    10     3 19814   517  5033     7     6   486    17    29
    517  5033     6  3726   517  5033 29064 27190  1194   350  5033     7
     10  5686    18   517  5033     6   332  5173    18   517  5033   672
      5 18833 27190  1194   350  8418     7    10   332   357   134     6
    332   357   553     6  8483   517  5033     1]]"
3685bf2409b23c47bfd681989fb4a763bcab6be2,100.0,300 Dimensional Glove,[[    0  3147 14689   138  9840   162     1]],300 Dimensional Glove,[[ 3147 14689   138  9840   162     1]]
7bf3a7d19f17cf01f2c9fa16401ef04a3bef65d8,6.0606,The two datasets are merged to form a single dataset,"[[    0    37   192 17953     7    33     3 21726    12   607     3     9
    712 17953     1]]","we sort the speech segments by length, we take segments in pairs, zero-padding the shorter segment so both have the same length, These pairs are then mixed together","[[   62  1843     8  5023 15107    57  2475     6    62   240 15107    16
  14152     6  5733    18   102     9  7249     8 10951  5508    78   321
     43     8   337  2475     6   506 14152    33   258  4838   544     1]]"
c5980fe1a0c53bce1502cc674c8a2ed8c311f936,100.0,"3,206",[[    0  6180 24643     1]],"3,206",[[ 6180 24643     1]]
48c3e61b2ed7b3f97706e2a522172bf9b51ec467,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],correctness of all the question answer pairs are verified by at least two annotators,"[[ 2024   655    13    66     8   822  1525 14152    33 17087    57    44
    709   192    46  2264  6230     1]]"
9623884915b125d26e13e8eeebe9a0f79d56954b,100.0,documents are segmented into paragraphs and processed at the paragraph level,"[[   0 2691   33 5508   15   26  139 8986    7   11 8534   44    8 8986
   593    1]]",documents are segmented into paragraphs and processed at the paragraph level,"[[2691   33 5508   15   26  139 8986    7   11 8534   44    8 8986  593
     1]]"
cf0085c1d7bd9bc9932424e4aba4e6812d27f727,0.0,"Freebase BIBREF0, DBpedia BIBREF1 and YA","[[    0  1443 10925     3  5972 25582   371   632     6     3  9213 24477
      3  5972 25582   371   536    11     3 17419]]","FB24K, DBP24K, Game30K","[[    3 15586  2266   439     6   309 11165  2266   439     6  4435  1458
    439     1]]"
a53683d1a0647c80a4398ff8f4a03e11c0929be2,9.5238,listening comprehension task,[[    0  5351 27160  2491     1]],"We propose a listening comprehension model for the task defined above, the Attention-based Multi-hop Recurrent Neural Network (AMRNN) framework, and show that this model is able to perform reasonably well for the task. In the proposed approach, the audio of the stories is first transcribed into text by ASR, and the proposed model is developed to process the transcriptions for selecting the correct answer out of 4 choices given the question. ","[[  101  4230     3     9  5351 27160   825    21     8  2491  4802   756
      6     8 20748    18   390  4908    18 10776   419 14907  1484  9709
   3426    41   188  9320 17235    61  4732     6    11   504    24    48
    825    19     3   179    12  1912 13145   168    21     8  2491     5
     86     8  4382  1295     6     8  2931    13     8  1937    19   166
      3 11665 22573   139  1499    57    71  6857     6    11     8  4382
    825    19  1597    12   433     8 20267     7    21  9581     8  2024
   1525    91    13   314  3703   787     8   822     5     1]]"
637aa32a34b20b4b0f1b5dfa08ef4e0e5ed33d52,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
e96adf8466e67bd19f345578d5a6dc68fd0279a1,100.0,unsupervised,[[    0    73 23313     1]],unsupervised ,[[   73 23313     1]]
2a1069ae3629ae8ecc19d2305f23445c0231dc39,100.0,No,[[  0 465   1]],No,[[465   1]]
5c70fdd3d6b67031768d3e28336942e49bf9a500,10.0,the interaction is consumed by the model.,[[    0     8  6565    19 16647    57     8   825     5     1]],"displays three different versions of a story written by three distinct models for a human to compare, human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages","[[ 8397   386   315  5204    13     3     9   733  1545    57   386  6746
   2250    21     3     9   936    12  4048     6   936    54  1738     8
    825    12  6815    28    41  3013  7220   120   227   578  3934    34
   1009  2269    18 21770   201    11    54 11194    44    66  6518     1]]"
bd9930a613dd36646e2fc016b6eb21ab34c77621,100.0,"1,006 fake reviews and 994 real reviews",[[   0 1914 1206  948 9901 2456   11  668 4240  490 2456    1]],"1,006 fake reviews and 994 real reviews",[[1914 1206  948 9901 2456   11  668 4240  490 2456    1]]
7e161d9facd100544fa339b06f656eb2fc64ed28,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
0bd864f83626a0c60f5e96b73fb269607afc7c09,8.6957,fusing of the conversational-context information,[[   0    3   89 9381   13    8 3634  138   18 1018 6327  251    1]],BERT generates sentence embeddings that represent words in context. These sentence embeddings are merged into a single conversational-context vector that is used to calculate a gated embedding and is later combined with the output of the decoder h to provide the gated activations for the next hidden layer.,"[[  272 24203  3806     7  7142 25078    26    53     7    24  4221  1234
     16  2625     5   506  7142 25078    26    53     7    33     3 21726
    139     3     9   712  3634   138    18  1018  6327 12938    24    19
    261    12 11837     3     9 10530    26 25078    26    53    11    19
    865  3334    28     8  3911    13     8    20  4978    52     3   107
     12   370     8 10530    26  5817  1628    21     8   416  5697  3760
      5     1]]"
01123a39574bdc4684aafa59c52d956b532d2e53,17.3913,their model outperforms by 0.61 points in OD detection and 0.61 points in,"[[    0    70   825    91   883  2032     7    57  4097  4241   979    16
      3  7039 10664    11  4097  4241   979    16]]","AE-HCN outperforms by 17%, AE-HCN-CNN outperforms by 20% on average","[[    3 14611    18  8095   567    91   883  2032     7    57   209  6170
      6     3 14611    18  8095   567    18   254 17235    91   883  2032
      7    57  7580    30  1348     1]]"
0b39c20db6e60ce07bf5465bd3c08fedc0587780,0.0,Unanswerable,[[   0  597 3247 3321  179    1]]," previous emoji embedding methods fail to handle the situation when the semantics or sentiments of the learned emoji embeddings contradict the information from the corresponding contexts BIBREF5 , or when the emojis convey multiple senses of semantics and sentiments ","[[ 1767     3    15    51 21892 25078    26    53  2254  5124    12  2174
      8  1419   116     8 27632     7    42  6493     7    13     8  2525
      3    15    51 21892 25078    26    53     7 21454     8   251    45
      8     3  9921  2625     7     3  5972 25582   371   755     3     6
     42   116     8     3    15    51 21892     7 11770  1317  1254     7
     13 27632     7    11  6493     7     1]]"
fc29bb14f251f18862c100e0d3cd1396e8f2c3a1,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
29d917cc38a56a179395d0f3a2416fca41a01659,19.3548,"first, we automatically convert the claim into a query, which we execute against a search","[[    0   166     6    62  3269  5755     8  1988   139     3     9 11417
      6    84    62 12133   581     3     9   960]]"," Generate a query out of the claim and querying a search engine, rank the words by means of TF-IDF, use IBM's AlchemyAPI to identify named entities, generate queries of 5–10 tokens, which execute against a search engine, and collect the snippets and the URLs in the results, skipping any result that points to a domain that is considered unreliable.","[[ 6939  2206     3     9 11417    91    13     8  1988    11 11417    53
      3     9   960  1948     6 11003     8  1234    57   598    13     3
   9164    18  4309   371     6   169 11045    31     7   901  6482    63
   2965   196    12  2862  2650 12311     6  3806 13154    13   305   104
   1714 14145     7     6    84 12133   581     3     9   960  1948     6
     11  2868     8     3 20317  4995     7    11     8  8889     7    16
      8   772     6  5210  5341   136   741    24   979    12     3     9
   3303    24    19  1702    73    60  8860     5     1]]"
f7817b949605fb04b1e4fec9dd9ca8804fb92ae9,100.0,"Because, unlike other languages, English does not mark grammatical genders","[[   0 2070    6 9770  119 8024    6 1566  405   59 3946    3 5096 4992
   138 7285    7    1]]","Because, unlike other languages, English does not mark grammatical genders","[[2070    6 9770  119 8024    6 1566  405   59 3946    3 5096 4992  138
  7285    7    1]]"
e4d16050f0b457c93e590261732a20401def9cde,16.0,Levenshtein distance based system,[[   0  312 1926    7  107   17 2455 2357    3  390  358    1]],"Levenshtein distance metric BIBREF8, diacritical swapping, Levenshtein distance is used in a weighted sum to cosine distance between word vectors, ELMo-augmented LSTM","[[  312  1926     7   107    17  2455  2357     3  7959     3  5972 25582
    371 11864  1227     9 25863 15959  2462     6   312  1926     7   107
     17  2455  2357    19   261    16     3     9  1293    15    26  4505
     12   576     7   630  2357   344  1448 12938     7     6   262 11160
     32    18 28984     3  7600  2305     1]]"
9c68d6d5451395199ca08757157fbfea27f00f69,100.0,OpenIE4 and MiniIE,[[   0 2384 5091  591   11 4533 5091    1]],OpenIE4 and MiniIE,[[2384 5091  591   11 4533 5091    1]]
3518d8eb84f6228407cfabaf509fd63d60351203,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
a30958c7123d1ad4723dcfd19d8346ccedb136d5,100.0,No,[[  0 465   1]],No,[[465   1]]
bf7cb53f4105f2e6a413d1adef5349ff1e673500,100.0,WikiTableQuestions,[[    0  2142  2168 20354  5991   222  2865     1]],WikiTableQuestions,[[ 2142  2168 20354  5991   222  2865     1]]
d653d994ef914d76c7d4011c0eb7873610ad795f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"By using  keywords `breast' AND `cancer' in tweet collecting process. 
","[[  938   338 12545     3     2  1999     9     7    17    31  3430     3
      2  1608  2110    31    16 10657 10858   433     5     1]]"
7358a1ce2eae380af423d4feeaa67d2bd23ae9dd,10.0,by using a mixture of traditional and experimental methods.,"[[    0    57   338     3     9  4989    13  1435    11 11082  2254     5
      1]]","The embeddings are learned several times using the training set, then the average is taken.","[[   37 25078    26    53     7    33  2525   633   648   338     8   761
    356     6   258     8  1348    19  1026     5     1]]"
dc69256bdfe76fa30ce4404b697f1bedfd6125fe,100.0,"Hindi, English and German (German task won)",[[    0 25763     6  1566    11  2968    41 24518  2491   751    61     1]],"Hindi, English and German (German task won)",[[25763     6  1566    11  2968    41 24518  2491   751    61     1]]
6a9eb407be6a459dc976ffeae17bdd8f71c8791c,9.5238,"reward $$$ as the first parameter, $$$, $$,","[[    0  9676  1514     2  3229  3229    38     8   166 15577     6  1514
      2  3229  3229     6  1514     2  3229     6]]","reward 1 for successfully completing the task, with a discount by the number of turns, and reward 0 when fail","[[9676  209   21 4234    3 8828    8 2491    6   28    3    9 3898   57
     8  381   13 5050    6   11 9676    3  632  116 5124    1]]"
184e1f28f96babf468f2bb4e1734f69646590cda,12.9032,using a knowledge graph,[[   0  338    3    9 1103 8373    1]],the knowledge graph is used to prune this space by ranking actions based on their presence in the current knowledge graph and the relations between the objects in the graph as in BIBREF7,"[[    8  1103  8373    19   261    12 27185    48   628    57 11592  2874
      3   390    30    70  3053    16     8   750  1103  8373    11     8
   5836   344     8  4820    16     8  8373    38    16     3  5972 25582
    371   940     1]]"
7befb7a8354fca9d2a94e3fd4364625c98067ebb,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
21656039994cab07f79e89553cbecc31ba9853d4,100.0,document-level variants of the SQuAD dataset,"[[    0  1708    18  4563  6826     7    13     8   180  5991  6762 17953
      1]]",document-level variants of the SQuAD dataset ,[[ 1708    18  4563  6826     7    13     8   180  5991  6762 17953     1]]
86cd1228374721db67c0653f2052b1ada6009641,100.0,YouTube videos,[[   0 5343 3075    1]],YouTube videos,[[5343 3075    1]]
e854edcc5e9111922e6e120ae17d062427c27ec1,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
84a4a1f4695eba599d447e030c94f51e5f2f03bb,16.6667,"Using this method, we achieve better picture-based accuracy than the previous best method.","[[   0    3 3626   48 1573    6   62 1984  394 1554   18  390 7452  145
     8 1767  200 1573    5    1]]",our method still can improve the state-of-the-art accuracy BIBREF7 from 60.32% to 60.34%,"[[   69  1573   341    54  1172     8   538    18   858    18   532    18
   1408  7452     3  5972 25582   371   940    45   431 19997  5406    12
    431 19997  5988     1]]"
63723c6b398100bba5dc21754451f503cb91c9b8,0.0,POS-based model with a POS-based model,"[[    0     3 16034    18   390   825    28     3     9     3 16034    18
    390   825     1]]","POS and DP task: CONLL 2018
NER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF
NLI task: mBERT or XLM (not clear from text)","[[    3 16034    11     3  7410  2491    10  8472 10376   846     3 18206
   2491    10    41    29    32  3616   161    61 16366 20726     7   205
   8556    11  2106  7600  2305    18  4545   371   445  8159  2491    10
      3    51 12920   382    42     3     4 11160    41  2264   964    45
   1499    61     1]]"
f8f13576115992b0abb897ced185a4f9d35c5de9,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
81686454f215e28987c7ad00ddce5ffe84b37195,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
384d571e4017628ebb72f3debb2846efaf0cb0cb,16.0,Aristo has recently surpassed 90% on multiple choice questions from the 8th Grade New York Regen,"[[    0    71 17149    65  1310     3 26452 12669    30  1317  1160   746
     45     8   505   189 13027   368  1060 12412]]","Aristo Corpus
Regents 4th
Regents 8th
Regents `12th
ARC-Easy
ARC-challenge ","[[   71 17149 10052   302 12412    17     7   314   189 12412    17     7
    505   189 12412    17     7     3     2  2122   189     3 18971    18
    427     9     7    63     3 18971    18 12654    40    35   397     1]]"
e7329c403af26b7e6eef8b60ba6fefbe40ccf8ce,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The model outperforms at every point in the
implicit-tuples PR curve reaching almost 0.8 in recall","[[   37   825    91   883  2032     7    44   334   500    16     8 21773
     18    17   413   965  6045  8435  7232   966     3 22384    16  7881
      1]]"
046ff04d1018447b22e00acb125125cae5a23fb7,0.0,TEXT-Simulator dataset,[[    0     3  3463     4   382    18   134   603    83  1016 17953     1]],"small_parallel_enja, Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF5","[[  422   834  1893 13701    40   834    35  1191     6  6578 19268  6564
  20335    49   102    17 10052   302    41   188 20452    61     3  5972
  25582   371   755     1]]"
2ea382c676e418edd5327998e076a8c445d007a5,100.0,No,[[  0 465   1]],No,[[465   1]]
1a6e2bd41ee43df83fef2a1c1941e6f95a619ae8,100.0,"entity recognition, semantic role labeling and co-reference resolution","[[    0 10409  5786     6 27632  1075  3783    53    11   576    18    60
  11788  3161     1]]"," entity recognition, semantic role labeling and co-reference resolution","[[10409  5786     6 27632  1075  3783    53    11   576    18    60 11788
   3161     1]]"
282aa4e160abfa7569de7d99b8d45cabee486ba4,22.2222,By calculating the sum of all aspect terms and opinion-indicating words in a given sentence,"[[    0   938     3 25956     8  4505    13    66  2663  1353    11  3474
     18 15716  1234    16     3     9   787  7142]]","the weighted sum of the new opinion representations, according to their associations with the current aspect representation","[[    8  1293    15    26  4505    13     8   126  3474  6497     7     6
   1315    12    70 10906    28     8   750  2663  6497     1]]"
7fe48939ce341212c1d801095517dc552b98e7b3,12.5,feature-wise gating at character level,[[    0  1451    18 10684     3   122  1014    44  1848   593     1]],gating mechanism acts upon each dimension of the word and character-level vectors,"[[    3   122  1014  8557  6775  1286   284  9340    13     8  1448    11
   1848    18  4563 12938     7     1]]"
280f863cfd63b711980ca6c7f1409c0306473de7,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
2ec640e6b4f1ebc158d13ee6589778b4c08a04c8,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
69e678666d11731c9bfa99953e2cd5a5d11a4d4f,100.0,SParC BIBREF2 and CoSQL BIBREF6,"[[    0  6760   291   254     3  5972 25582   371   357    11   638   134
   2247   434     3  5972 25582   371   948     1]]",SParC BIBREF2 and CoSQL BIBREF6,"[[ 6760   291   254     3  5972 25582   371   357    11   638   134  2247
    434     3  5972 25582   371   948     1]]"
4040f5c9f365f9bc80b56dce944ada85bb8b4ab4,100.0,No,[[  0 465   1]],No,[[465   1]]
c034f38a570d40360c3551a6469486044585c63c,46.1538,Perplexity of proposed method is 0.88 vs 0.88 of baselines,"[[    0  1915  9247   485    13  4382  1573    19  4097  4060     3   208
      7  4097  4060    13 20726     7     1]]",Perplexity of proposed MEED model is 19.795 vs 19.913 of next best result on test set.,"[[1915 9247  485   13 4382 7934 2326  825   19 9997  940 3301    3  208
     7 9997 4729  519   13  416  200  741   30  794  356    5    1]]"
f9751e0ca03f49663a5fc82b33527bc8be1ed0aa,19.0476,simulations of real-time data feeds,[[    0 11108     7    13   490    18   715   331  3305     7     1]],"simplified set of input data, in a variety of different formats that occur frequently in a healthcare setting","[[24687   356    13  3785   331     6    16     3     9  1196    13   315
  10874    24  4093  4344    16     3     9  4640  1898     1]]"
aa4b38f601cc87bf93849245d5f65124da3dc112,0.0,"a neural model for generating sentences, a neural model for editing sentences, and ","[[    0     3     9 24228   825    21     3 11600 16513     6     3     9
  24228   825    21  8414 16513     6    11     3]]",Title-to-Story system,[[11029    18   235    18   134 10972   358     1]]
2c3b2c3bab6d18cb0895462e3cfd91cd0dee7f7d,100.0,"BiDAF, BERT",[[    0  2106  4296   371     6   272 24203     1]],"BiDAF, BERT ",[[ 2106  4296   371     6   272 24203     1]]
8427988488b5ecdbe4b57b3813b3f981b07f53a5,100.0,Variety prediction task,[[    0 31884 21332  2491     1]],Variety prediction task,[[31884 21332  2491     1]]
8a7bd9579d2783bfa81e055a7a6ebc3935da9d20,0.0,LSTM-Net,[[   0    3 7600 2305   18 9688    1]],"WAS, LipCH-Net-seq, CSSMCM-w/o video","[[ 7896   134     6 10840  8360    18  9688    18     7    15  1824     6
  18104  3698   329    18   210    87    32   671     1]]"
86be8241737dd8f7b656a3af2cd17c8d54bf1553,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
80de3baf97a55ea33e0fe0cafa6f6221ba347d0a,100.0,No,[[  0 465   1]],No,[[465   1]]
a130aa735de3b65c71f27018f20d3c068bafb826,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],16k images and 740k corresponding region descriptions,[[  898   157  1383    11   489  2445   157     3  9921  1719 15293     1]]
d27e3a099954e917b6491e81b2e907478d7f1233,100.0,No,[[  0 465   1]],No,[[465   1]]
4c1847f0f3e6f9cc6ac3dfbac9e135d34641a854,100.0,JavaScript,[[    0 10318 18255     1]],JavaScript,[[10318 18255     1]]
44c7c1fbac80eaea736622913d65fe6453d72828,50.0,"34,432 user conversations",[[   0 6154    6  591 2668 1139 9029    1]],"34,432 ",[[6154    6  591 2668    1]]
2948015c2a5cd6a7f2ad99b4622f7e4278ceb0d4,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
3288a50701a80303fd71c8c5ede81cbee14fa2c7,100.0,No,[[  0 465   1]],No,[[465   1]]
a1917232441890a89b9a268ad8f987184fa50f7a,94.7368,trinomials are more likely to appear in exactly one order,"[[   0 6467 3114   23 5405   33   72  952   12 2385   16 1776   80  455
     1]]",Trinomials are likely to appear in exactly one order,[[2702 3114   23 5405   33  952   12 2385   16 1776   80  455    1]]
02348ab62957cb82067c589769c14d798b1ceec7,42.1053,"BERT language representation model BIBREF6, ROUGE, Language model, and ","[[    0   272 24203  1612  6497   825     3  5972 25582   371 11071   391
  26260   427     6 10509   825     6    11     3]]","BiGRUs with attention, ROUGE, Language model, and next sentence prediction ","[[ 2106   517  8503     7    28  1388     6   391 26260   427     6 10509
    825     6    11   416  7142 21332     1]]"
ecfb2e75eb9a8eba8f640a039484874fa0d2fceb,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
acda028a21a465c984036dcbb124b7f03c490b41,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"MADL BIBREF0 extends the dual learning BIBREF1, BIBREF2 framework by introducing multiple primal and dual models.","[[ 4800 10013     3  5972 25582   371   632  4285     7     8  7013  1036
      3  5972 25582   371  4347     3  5972 25582   371   357  4732    57
      3 13505  1317  3778    40    11  7013  2250     5     1]]"
a85c2510f25c7152940b5ac4333a80e0f91ade6e,10.0,"general insights into the cohesion of political groups in the European Parliament, explore whether coalition","[[    0   879  7639   139 32128     8   576    88  1938    13  1827  1637
     16     8  1611 12876     6  2075   823 17952]]","Greens-EFA, S&D, and EPP exhibit the highest cohesion, non-aligned members NI have the lowest cohesion, followed by EFDD and ENL, two methods disagree is the level of cohesion of GUE-NGL","[[ 1862     7    18  9976   188     6   180   184   308     6    11   262
   6158  6981     8  2030   576    88  1938     6   529    18   138  6962
     26   724     3  6197    43     8  7402   576    88  1938     6  2348
     57     3  9976 11253    11   262 18207     6   192  2254 15788    19
      8   593    13   576    88  1938    13   350  5078    18 12531   434
      1]]"
e8029ec69b0b273954b4249873a5070c2a0edb8a,12.5,visual grounding is important for the learning of the multilingual representations,"[[    0  3176  1591    53    19   359    21     8  1036    13     8  1249
  25207  6497     7     1]]",performance is significantly degraded without pixel data,[[  821    19  4019    20  6801    26   406     3 14251   331     1]]
4704cbb35762d0172f5ac6c26b67550921567a65,22.7273,SVM improves F1 score by 0.86 and 0.985 on the BLEU,"[[    0   180 12623  1172     7   377   536  2604    57  4097  3840    11
      3 23758  4433    30     8     3  8775 12062]]","In task 1 best transfer learning strategy improves F1 score by 4.4% and accuracy score by 3.3%, in task 2 best transfer learning strategy improves F1 score by 2.9% and accuracy score by 1.7%","[[  86 2491  209  200 2025 1036 1998 1172    7  377  536 2604   57 2853
  5988   11 7452 2604   57 1877 5170    6   16 2491  204  200 2025 1036
  1998 1172    7  377  536 2604   57 1682 7561   11 7452 2604   57 1300
  6170    1]]"
2df910c9806f0c379d7bb1bc2be2610438e487dc,5.1282,"Twitter dataset, a dataset on political events, political events and terrorism","[[    0  3046 17953     6     3     9 17953    30  1827   984     6  1827
    984    11     3 14389     1]]","BIBREF32, BIBREF23, BIBREF33, discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. ","[[    3  5972 25582   371  2668     6     3  5972 25582   371  2773     6
      3  5972 25582   371  4201     6  7574    16   662   315  8024    10
   1566     6 21076     6  5093    11  2379     6 16198    16   874  6266
    147     8   296    10  1013    11  1117  1371     6  3782  1740     6
   2808    11  5193  3826     5     1]]"
cdf65116a7c50edddcb115e9afd86b2b6accb8ad,0.0,"word similarity, word analogy, entity/relation level mediate structure extraction","[[    0  1448  1126   485     6  1448 10552    63     6 10409    87    60
   6105   593   783    17    15  1809 16629     1]]","verb/preposition-based relation, nominal attribute, descriptive phrase and hyponymy relation.","[[ 7375    87  2026  4718    18   390  4689     6 19680 15816     6 25444
   9261    11     3 13397 19140    63  4689     5     1]]"
27f575e90487ef68298cfb6452683bb977e39e43,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
92412a449c28b9121a4a4f4acca996563f107131,0.0,Visual Question Answering (VQA) tasks,[[    0 10893 11860 11801    53    41   553 23008    61  4145     1]],"pre-trained word embeddings BIBREF11, BIBREF12, recurrent BIBREF13, transformer-based sentence encoders BIBREF14, distinct convolutional neural networks, standard fusion strategies,  two main attention mechanisms BIBREF18, BIBREF19","[[  554    18    17 10761  1448 25078    26    53     7     3  5972 25582
    371  2596     6     3  5972 25582   371  2122     6     3    60 14907
      3  5972 25582   371  2368     6 19903    18   390  7142 23734    52
      7     3  5972 25582   371  2534     6  6746   975 24817   138 24228
   5275     6  1068     3  7316  3266     6   192   711  1388 12009     3
   5972 25582   371  2606     6     3  5972 25582   371  2294     1]]"
071bcb4b054215054f17db64bfd21f17fd9e1a80,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
2fffff59e57b8dbcaefb437a6b3434fc137f813b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining,"[[ 3303    18 24315    15    26  1514     2  3229  2775   439 16513    11
      3 17518     3  3443    13  6080  1499 21527    45   765  1688   261
     57     3  5972 25582   371   948     3     9 17149 11505    10 13275
      1]]"
31b631a8634f6180b20a72477040046d1e085494,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
5c0b8c1b649df1b07d9af3aa9154ac340ec8b81c,100.0,No,[[  0 465   1]],No,[[465   1]]
7ff48fe5b7bd6b56553caacc891ce3d7e0070440,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
2d3bf170c1647c5a95abae50ee3ef3b404230ce4,100.0,standard parametrized attention and a non-attention baseline,"[[    0  1068 30706   776    26  1388    11     3     9   529    18 25615
  20726     1]]",standard parametrized attention and a non-attention baseline,"[[ 1068 30706   776    26  1388    11     3     9   529    18 25615 20726
      1]]"
2419b38624201d678c530eba877c0c016cccd49f,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
6c80bc3ed6df228c8ca6e02c0a8a1c2889498688,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
1405824a6845082eae0458c94c4affd7456ad0f7,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
30c6d34b878630736f819fd898319ac4e71ee50b,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
639c145f0bcb1dd12d08108bc7a02f9ec181552e,6.383,"phase transition, lexical trade-off between two competing pressures, ambiguity and","[[    0  3944  3508     6     3 30949   138  1668    18  1647   344   192
  12166  1666     7     6     3 24621   485    11]]","Phase I: $\langle cc \rangle $ increases smoothly for $\wp < 0.4$, indicating that for this domain there is a small correlation between word neighborhoods. Full vocabularies are attained also for $\wp < 0.4$, Phase II: a drastic transition appears at the critical domain $\wp ^* \in (0.4,0.6)$, in which $\langle cc \rangle $ shifts abruptly towards 1. An abrupt change in $V(t_f)$ versus $\wp $ is also found (Fig. FIGREF16) for $\wp ^*$, Phase III: single-word languages dominate for $\wp > 0.6$. The maximum value of $\langle cc \rangle $ indicate that word neighborhoods are completely correlated","[[12559    27    10  1514     2    40 13247     3    75    75     3     2
     52 13247  1514  5386 14000    21  1514     2   210   102     3     2
      3 22776  3229     6     3 15716    24    21    48  3303   132    19
      3     9   422 18712   344  1448 17383     5  4043     3  6117     9
   6724  5414    33    44 10733    92    21  1514     2   210   102     3
      2     3 22776  3229     6 12559  2466    10     3     9 27982  3508
   3475    44     8  2404  3303  1514     2   210   102     3     2  1935
      3     2    77    41 22776     6 22787    61  3229     6    16    84
   1514     2    40 13247     3    75    75     3     2    52 13247  1514
   4108     7 25119   120  1587  1300   389 25119   483    16  1514   553
    599    17   834    89    61  3229     3  8911  1514     2   210   102
   1514    19    92   435    41 12286     5 11376  4386   371  2938    61
     21  1514     2   210   102     3     2  1935  3229     6 12559  6289
     10   712    18  6051  8024 19314    21  1514     2   210   102  2490
      3 22787  3229     5    37  2411   701    13  1514     2    40 13247
      3    75    75     3     2    52 13247  1514  6360    24  1448 17383
     33  1551     3 29604     1]]"
b0fd686183b056ea3f63a7ab494620df1d598c24,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"train the parser on six other languages in the Google universal dependency treebanks version 2.0 (de, en, es, fr, it, pt, sv, excluding whichever is the target language), and we use gold coarse POS tags","[[ 2412     8   260     7    49    30  1296   119  8024    16     8  1163
   7687 27804  2195  4739     7   988  6864    41   221     6     3    35
      6     3    15     7     6  2515     6    34     6     3   102    17
      6     3     7   208     6     3 21763     3 25281    19     8  2387
   1612   201    11    62   169  2045 27978     3 16034 12391     1]]"
565d668947ffa6d52dad019af79289420505889b,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
6b2fbc1c083491a774233f9edf8f76bd879418df,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
69a7a6675c59a4c5fb70006523b9fe0f01ca415c,100.0,"link prediction, triplet classification",[[    0  1309 21332     3     6 12063    17 13774     1]],"link prediction , triplet classification",[[ 1309 21332     3     6 12063    17 13774     1]]
dd155f01f6f4a14f9d25afc97504aefdc6d29c13,85.7143,"quality measures using perplexity and query latency, and performance measured using latency and energy","[[    0   463  3629   338   399  9247   485    11 11417  1480 11298     6
     11   821  8413   338  1480 11298    11   827]]","Quality measures using perplexity and recall, and performance measured using latency and energy usage. ","[[ 6495  3629   338   399  9247   485    11  7881     6    11   821  8413
    338  1480 11298    11   827  4742     5     1]]"
4fa2faa08eeabc09d78d89aaf0ea86bb36328172,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5937ebbf04f62d41b48cbc6b5c38fc309e5c2328,5.6738,a dialogue act is a linguistic feature that is used to distinguish a user',"[[    0     3     9  7478  1810    19     3     9     3 24703  1451    24
     19   261    12 15849     3     9  1139    31]]","Quotation (⌃q) dialogue acts, on the other hand, are mostly used with `Anger' and `Frustration', Action Directive (ad) dialogue act utterances, which are usually orders, frequently occur with `Anger' or `Frustration' although many with `Happy' emotion in case of the MELD dataset, Acknowledgements (b) are mostly with positive or neutral, Appreciation (ba) and Rhetorical (bh) backchannels often occur with a greater number in `Surprise', `Joy' and/or with `Excited' (in case of IEMOCAP), Questions (qh, qw, qy and qy⌃d) are mostly asked with emotions `Surprise', `Excited', `Frustration' or `Disgust' (in case of MELD) and many are neutral, No-answers (nn) are mostly `Sad' or `Frustrated' as compared to yes-answers (ny)., Forward-functions such as Apology (fa) are mostly with `Sadness' whereas Thanking (ft) and Conventional-closing or -opening (fc or fp) are usually with `Joy' or `Excited'","[[ 2415    32  6821    41     2  1824    61  7478  6775     6    30     8
    119   609     6    33  3323   261    28     3     2  8365    49    31
     11     3     2   371  9277  2661    31     6  6776 28965    41     9
     26    61  7478  1810     3  5108   663     7     6    84    33  1086
   5022     6  4344  4093    28     3     2  8365    49    31    42     3
      2   371  9277  2661    31  2199   186    28     3     2 31141    31
  13868    16   495    13     8  7934  9815 17953     6  4292 20542 13553
   4128    41   115    61    33  3323    28  1465    42  7163     6  2276
   7886   257    41   115     9    61    11   391    88  3600  1489    41
    115   107    61   223 19778     7   557  4093    28     3     9  2123
    381    16     3     2   134   450   102  7854    31     6     3     2
    683    32    63    31    11    87   127    28     3     2  5420 11675
     31    41    77   495    13     3  5091  5365 16986   201 14218    41
   1824   107     6     3  1824   210     6     3  1824    63    11     3
   1824    63     2    26    61    33  3323  1380    28  7848     3     2
    134   450   102  7854    31     6     3     2  5420 11675    31     6
      3     2   371  9277  2661    31    42     3     2   308   159 17198
     31    41    77   495    13  7934  9815    61    11   186    33  7163
      6   465    18  3247  3321     7    41    29    29    61    33  3323
      3     2   134     9    26    31    42     3     2   371  9277  4094
     31    38     3  2172    12  4273    18  3247  3321     7    41    29
     63   137     6 25633    18 23993     7   224    38  5085  1863    41
     89     9    61    33  3323    28     3     2   134     9    26   655
     31     3 10339  1562    53    41    89    17    61    11 11347   138
     18  3903     7    53    42     3    18  8751    53    41    89    75
     42     3    89   102    61    33  1086    28     3     2   683    32
     63    31    42     3     2  5420 11675    31     1]]"
d7d41a1b8bbb1baece89b28962d23ee4457b9c3a,100.0,"Mandarin, English",[[    0 31057     6  1566     1]],"Mandarin, English",[[31057     6  1566     1]]
5bb96b255dab3e47a8a68b1ffd7142d0e21ebe2a,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
65e72ad72a9cbfc379f126b10b0ce80cfe44579b,0.0,"BiLSTMs BIBREF22, BIBREF23, BIBRE","[[    0  2106  7600  2305     7     3  5972 25582   371  2884     6     3
   5972 25582   371  2773     6     3  5972 25582]]","NAT w/ Fertility, NAT-IR, NAT-REG, LV NAR, CTC Loss, CMLM","[[    3 25271     3   210    87  7566    17 14277     6     3 25271    18
   5705     6     3 25271    18  4386   517     6     3 15086   445  4280
      6   205  3838  3144     7     6     3  5518 11160     1]]"
2fd8688c8f475ab43edaf5d189567f8799b018e1,100.0,No,[[  0 465   1]],No,[[465   1]]
da068b20988883bc324e55c073fb9c1a5c39be33,20.8333,"by adding the information that is not present in the source, we induce the system to commit to","[[    0    57  2651     8   251    24    19    59   915    16     8  1391
      6    62 21151     8   358    12 10042    12]]"," correct information substantially improves it - we see an increase of up to 2.3 BLEU over the baseline, Finally, the “She said” prefixes substantially increase the number of feminine-marked verbs, bringing the proportion much closer to that of the reference","[[ 2024   251 15284  1172     7    34     3    18    62   217    46   993
     13    95    12     3 18561     3  8775 12062   147     8 20726     6
   4213     6     8   105 12736   243   153   554 12304    15     7 15284
    993     8   381    13 21546    18 16376  7375     7     6     3  3770
      8  7385   231  4645    12    24    13     8  2848     1]]"
aa800b424db77e634e82680f804894bfa37f2a34,100.0,No,[[  0 465   1]],No,[[465   1]]
96ee62407b1ca2a6538c218781e73e8fbf45094a,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
f748cb05becc60e7d47d34f4c5f94189bc184d33,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Bulgarian, Czech, French, German, Korean, Polish, Portuguese, Russian, Thai, Vietnamese, South African English, These features are typically obtained by training a deep neural network jointly on several languages for which labelled data is available., The final shared layer often has a lower dimensionality than the input layer, and is therefore referred to as a `bottleneck'.","[[15536    29     6 16870     6  2379     6  2968     6  9677     6 16073
      6 21076     6  4263     6 12806     6 24532     6  1013  3850  1566
      6   506   753    33  3115  5105    57   761     3     9  1659 24228
   1229 22801    30   633  8024    21    84     3 29506   331    19   347
      5     6    37   804  2471  3760   557    65     3     9  1364     3
  11619   485   145     8  3785  3760     6    11    19  2459     3  4822
     12    38     3     9     3     2  4045    17   109 14694    31     5
      1]]"
334972ba967444f98865dea4c2bc0eb9416f2ff7,0.0,4590 hours of online content targeting teens,[[    0  3479  2394   716    13   367   738 13972 14752     1]], from 469 posts to 17 million,[[  45  314 3951 3489   12 1003  770    1]]
17988d65e46ff7d756076e9191890aec177b081e,100.0,No,[[  0 465   1]],No,[[465   1]]
c87b2dd5c439d5e68841a705dd81323ec0d64c97,26.087,"supervised topics models, LDA, regression/classification models, LDA, LDA","[[    0     3 23313  4064  2250     6   301  4296     6 26625    87  4057
   2420  2250     6   301  4296     6   301  4296]]","Bosch 2006 (mv), LDA + LogReg (mv), LDA + Raykar, LDA + Rodrigues, Blei 2003 (mv), sLDA (mv)","[[22698  3581    41    51   208   201   301  4296  1768  7736 17748    41
     51   208   201   301  4296  1768  8279  4031     6   301  4296  1768
   8222    52 15795     7     6 11805    23  3888    41    51   208   201
      3     7  9815   188    41    51   208    61     1]]"
450a359d117bcfa2de4ffd987f787945f25b3b25,34.0426,"first, we introduce the corpus-level embedding which can be used to compute the","[[    0   166     6    62  4277     8 11736   302    18  4563 25078    26
     53    84    54    36   261    12 29216     8]]","First, the embedding matrix INLINEFORM4 for all corpora is initialized, during the training phase, INLINEFORM9 can be used to bias the input feature, Next, we apply the language specific softmax to compute logits INLINEFORM4 and optimize them with the CTC objective","[[ 1485     6     8 25078    26    53 16826  3388 20006 24030   591    21
     66 11736   127     9    19  2332  1601     6   383     8   761  3944
      6  3388 20006 24030  1298    54    36   261    12 14387     8  3785
   1451     6  3021     6    62  1581     8  1612   806  1835  9128    12
  29216  4303  7085  3388 20006 24030   591    11 13436   135    28     8
    205  3838  5997     1]]"
e39d90b8d959697d9780eddce3a343e60543be65,0.0,LSTM vs LSTM,[[   0    3 7600 2305    3  208    7    3 7600 2305    1]],"widely used method for classifying misleading content is to use distant annotations, for example to classify a tweet based on the domain of a URL that is shared by the tweet, or a hashtag that is contained in the tweet, Natural Language Processing (NLP) models can be used to automatically label text content","[[ 5456   261  1573    21   853  8587 23828   738    19    12   169 10382
  30729     7     6    21   677    12   853  4921     3     9 10657     3
    390    30     8  3303    13     3     9  8889    24    19  2471    57
      8 10657     6    42     3     9 25354    24    19  6966    16     8
  10657     6  6869 10509 19125    41   567  6892    61  2250    54    36
    261    12  3269  3783  1499   738     1]]"
385dc96604e077611fbd877c7f39d3c17cd63bf2,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
34af2c512ec38483754e94e1ea814aa76552d60a,0.0,"Reasoner benchmark, Relation extraction benchmark",[[    0 21272    49 15705     6 28898 16629 15705     1]],Answer with content missing: (formula) The accuracy is defined as the ratio # of correct chains predicted to # of evaluation samples,"[[11801    28   738  3586    10    41  2032    83     9    61    37  7452
     19  4802    38     8  5688  1713    13  2024 16534 15439    12  1713
     13  5002  5977     1]]"
126e8112e26ebf8c19ca7ff3dd06691732118e90,17.6471,"a corpus of 96,287 documents, including 58,287 revision","[[    0     3     9 11736   302    13     3  4314     6   357  4225  2691
      6   379     3  3449     6   357  4225 14724]]",There are 6 simulated datasets collected which is initialised with a corpus of size 550 and simulated by generating new documents from Wikipedia extracts and replacing existing documents,"[[  290    33   431     3 31126 17953     7  4759    84    19  2332  3375
     28     3     9 11736   302    13   812     3 17147    11     3 31126
     57     3 11600   126  2691    45 16885  5819     7    11 11906  1895
   2691     1]]"
6548db45fc28e8a8b51f114635bad14a13eaec5b,0.0,a classifier which takes the input data into account and then uses it to classify the,"[[   0    3    9  853 7903   84 1217    8 3785  331  139  905   11  258
  2284   34   12  853 4921    8]]","weGAN, deGAN",[[  62  517 5033    6   20  517 5033    1]]
c8f11561fc4da90bcdd72f76414421e1527c0287,0.0,"text,audio> pairs",[[    0     3     2  6327     6 28696  3155 14152     1]],LJSpeech,[[  301 23787   855 10217     1]]
c000a43aff3cb0ad1cee5379f9388531b5521e9a,8.3333,"the LMs are pre-trained on a large, unlabeled corp","[[    0     8     3 11160     7    33   554    18    17 10761    30     3
      9   508     6    73  9339   400    26 11736]]","They pre-train forward and backward LMs separately, remove top layer softmax, and concatenate to obtain the bidirectional LMs.","[[  328   554    18  9719  1039    11   223  2239     3 11160     7 12000
      6  2036   420  3760  1835  9128     6    11   975  2138    35   342
     12  3442     8  2647 26352     3 11160     7     5     1]]"
928828544e38fe26c53d81d1b9c70a9fb1cc3feb,100.0,"29,500 documents",[[    0 14405  2560  2691     1]],"29,500 documents",[[14405  2560  2691     1]]
281cb27cfa0eea12180fd82ae33035945476609e,100.0,relations,[[   0 5836    1]],relations,[[5836    1]]
e4024db40f4b8c1ce593f53b28718e52d5007cd2,15.3846,They use a comparison of naturalness and standard deviation to compare the two main categories,"[[    0   328   169     3     9  4993    13   793   655    11  1068 25291
     12  4048     8   192   711  5897     1]]",using mean opinion score (MOS) naturalness judgments produced by a crowd-sourced pool of raters,"[[  338  1243  3474  2604    41  5365   134    61   793   655  7661     7
   2546    57     3     9  4374    18 15551  2201    13  1080    52     7
      1]]"
293e9a0b30670f4f0a380e574a416665a8c55bc2,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
6ea63327ffbab2fc734dd5c2414e59d3acc56ea5,10.5263,between the HMMs and the LSTMs is 0.82 BLEU.,"[[    0   344     8   454  8257     7    11     8     3  7600  2305     7
     19  4097  4613     3  8775 12062     5     1]]","With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower.","[[  438  1126   381    13  8755     6     8  4303 17902    19    81     3
  16029  1364    21     3  7600  2305     7   640 17953     7     5   366
      8   381    13  8755    16     3  7600  2305     7    19  1936     6
     70  4303 17902    19    95    12     3 22426  1364     5     1]]"
785eb3c7c5a5c27db14006ac357299ed1216313a,100.0,LASSO optimization problem,[[    0     3 20245  6582 11295   682     1]],LASSO optimization problem,[[    3 20245  6582 11295   682     1]]
05e3b831e4c02bbd64a6e35f6c52f0922a41539a,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
11e376f98df42f487298ec747c32d485c845b5cd,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
345f65eaff1610deecb02ff785198aa531648e75,10.0,feature extraction from patients' EHRs,[[    0  1451 16629    45  1221    31   262 11120     7     1]], automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15,"[[ 3269  5105   190     8  2859 16629    11  6493  1693 12045  3665    16
     69  1884   161     3  5972 25582   371  1808     1]]"
3241f90a03853fa85d287007d2d51e7843ee3d9b,0.0,a standard speaker verification corpus of around 280 000 words,"[[    0     3     9  1068  5873 17549 11736   302    13   300     3 17518
   6078  1234     1]]",non-native speech from the BULATS test ,[[  529    18    29  1528  5023    45     8   272  4254 23377   794     1]]
ce6f6cd55ada011233a9dab4d99a94d7944d5388,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
a2be2bd84e5ae85de2ab9968147b3d49c84dfb7f,10.0,"genres include rap, tv, vlog, vlog, ","[[   0 5349    7  560    3 5846    6    3   17  208    6    3  208 2152
     6    3  208 2152    6    3]]","genre, entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement","[[ 5349     6  4527     6  2772     6  8782     6   577     6  1974     6
      3   208  2152     6   619  6878     6  5023     6  6616     6     3
     60 13903    11 21592     1]]"
bdd8368debcb1bdad14c454aaf96695ac5186b09,8.8889,PTSD Peer Support Programs (PSP) use twitter posts as a baseline to,"[[    0     3 29609  1276    49  4224  2350     7    41  4176   345    61
    169 19010  3489    38     3     9 20726    12]]","Given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively, the estimated intensity  is established as mean  squared error.","[[ 9246    62    43   662 13182     6   465     3 29609     6  5586 14729
      3 29609     6  9258  2206 14729     3 29609    11  1592 14729     3
  29609    28     3     9  2604    13  8014  1914   204    11   220  6898
      6     8  5861 13182    19  2127    38  1243  2812    26  3505     5
      1]]"
be08ef81c3cfaaaf35c7414397a1871611f1a7fd,0.0,"BiLSTM, LSTM-D, LSTM-Vec, ","[[   0 2106 7600 2305    6    3 7600 2305   18  308    6    3 7600 2305
    18  553   15   75    6    3]]","WMD, VSM, PV-DTW, PV-TED","[[  549 11731     6   584  4212     6 16303    18 12111   518     6 16303
     18 11430     1]]"
980568848cc8e7c43f767da616cf1e176f406b05,100.0,27,[[   0 2307    1]],27 ,[[2307    1]]
fb427239c8d44f524a6c1bf1ce5c3383d5c33e52,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],model has around 836M parameters,[[ 825   65  300  505 3420  329 8755    1]]
f01aa192d97fa3cc36b6e316355dc5da0e9b97dc,0.0,GraLap,[[   0    3 4744 3612  102    1]],"(i) Uniform, (ii) SVR+W, (iii) SVR+O, (iv) C4.5SSL, (v) GLM","[[   41    23    61  5645  2032     6    41    23    23    61   180 13556
   1220   518     6    41    23    23    23    61   180 13556  1220   667
      6    41    23   208    61   205 12451   134  5629     6    41   208
     61   350 11160     1]]"
ab9453fa2b927c97b60b06aeda944ac5c1bfef1e,100.0,Sequence Copy Task and WMT'17,[[    0 26859  1433 20255 16107    11   549  7323    31  2517     1]],Sequence Copy Task and WMT'17,[[26859  1433 20255 16107    11   549  7323    31  2517     1]]
858c51842fc3c1f3e6d2d7d853c94f6de27afade,100.0,Logistic regression,[[    0  7736  3040 26625     1]],Logistic regression,[[ 7736  3040 26625     1]]
9d9b11f86a96c6d3dd862453bf240d6e018e75af,0.0,Transformer generation model,[[    0 31220  3381   825     1]],The training dataset is augmented by swapping all gendered words by their other gender counterparts,"[[   37   761 17953    19     3 28984    57 15959  2462    66  7285    15
     26  1234    57    70   119  7285 15473     7     1]]"
323e100a6c92d3fe503f7a93b96d821408f92109,100.0,"BIBREF13, BIBREF18","[[    0     3  5972 25582   371  2368     3     6     3  5972 25582   371
   2606     1]]","BIBREF13 , BIBREF18","[[    3  5972 25582   371  2368     3     6     3  5972 25582   371  2606
      1]]"
b24767fe7e6620369063e646fd3048dc645a8348,100.0,overlapping dialogue acts,[[    0     3 31212  7478  6775     1]],overlapping dialogue acts,[[    3 31212  7478  6775     1]]
265c9b733e4dfffb76acfbade4c0c9b14d3ccde1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male), data was aligned at the phone-level, 121fps with a 135 field of view, single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames)","[[    3 15977  1601     3     9  3422     7  1225    11 23486   331    45
      3  3449  3115  2421   502     6  9742   305  5947   203   625    41
   3341  3955     6  2307  5069   201   331    47  7901    15    26    44
      8   951    18  4563     6     3 22011    89   102     7    28     3
      9   209  2469  1057    13   903     6   712 23486  2835     3  6848
     13   314  2122 20747  5146    45   284    13     8     3  3891  5924
   2356    41  3891   226   591  2122  5902 10023    61     1]]"
1c85a25ec9d0c4f6622539f48346e23ff666cd5f,100.0,5 questions per image,[[   0  305  746  399 1023    1]],5 questions per image,[[ 305  746  399 1023    1]]
4e8233826f9e04f5763b307988298e73f841af74,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
3c3cb51093b5fd163e87a773a857496a4ae71f03,0.0,LSTM with a corresponding q-score,"[[   0    3 7600 2305   28    3    9    3 9921    3 1824   18    7 9022
     1]]","First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word","[[ 1485     6 14670     8  5508    15    26  7142    12     3     9  5932
     13  4775  1448 25078    26    53     7     5    37    29     6     8
  10389   825  1217     8  1448 25078    26    53  5932    38  3785     6
  10389   147   284   928  4775  1448     1]]"
0cf5132ac7904b7b81e17938d5815f70926a5180,0.0,"AUC-SVM, LSTM",[[    0    71  6463    18   134 12623     6     3  7600  2305     1]],fastText and SVM BIBREF16,[[ 1006 13598    17    11   180 12623     3  5972 25582   371  2938     1]]
c5ac07528cf99d353413c9d9ea61a1a699dd783e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"unigrams, bigrams, and trigrams, including sequences of punctuation, Word2Vec word embeddings","[[   73    23  5096     7     6   600  2375     7     6    11  6467  5096
      7     6   379  5932     7    13  5427    76   257     6  4467   357
    553    15    75  1448 25078    26    53     7     1]]"
4698298d506bef02f02c80465867f2cd12d29182,36.3636,BIBREF15 for VQA and BIBREF16,"[[    0     3  5972 25582   371  1808    21   584 23008    11     3  5972
  25582   371  2938     1]]","BIBREF35 for VQA dataset, BIBREF5, BIBREF36","[[    3  5972 25582   371  2469    21   584 23008 17953     6     3  5972
  25582   371 11116     3  5972 25582   371  3420     1]]"
3334f50fe1796ce0df9dd58540e9c08be5856c23,10.5263,"PTSD Peer Support Network (PSN) BIBREF12, LIWC ","[[    0     3 29609  1276    49  4224  3426    41  4176   567    61     3
   5972 25582   371  2122     6  8729 10038     3]]",to calculate the possible scores of each survey question using PTSD Linguistic Dictionary ,"[[   12 11837     8   487  7586    13   284  3719   822   338     3 29609
   6741  1744  3040 28767     1]]"
a37ef83ab6bcc6faff3c70a481f26174ccd40489,100.0,four different annotators,[[   0  662  315   46 2264 6230    1]], four different annotators,[[ 662  315   46 2264 6230    1]]
1124804c3702499b78cf0678bab5867e81284b6c,6.8966,"random access, highlighting of facts, highlighting of wrongdoing","[[    0  6504   592     6     3 19465    13  6688     6     3 19465    13
   1786    26    32    53     1]]","Non-contextual properties of a word, Word usage in an OP or PC (two groups), How a word connects an OP and PC, General OP/PC properties","[[5388   18 1018 6327 3471 2605   13    3    9 1448    6 4467 4742   16
    46    3 4652   42 2104   41 8264 1637  201  571    3    9 1448 1979
     7   46    3 4652   11 2104    6 2146    3 4652   87 4051 2605    1]]"
f37ed011e7eb259360170de027c1e8557371f002,0.0,"humorous, romantic, and click-baity",[[    0 28845     6  7966     6    11  1214    18   115     9   485     1]],"Humor in headlines (TitleStylist vs Multitask baseline):
Relevance: +6.53% (5.87 vs 5.51)
Attraction: +3.72% (8.93 vs 8.61)
Fluency: 1,98% (9.29 vs 9.11)","[[12410   127    16 12392     7    41   382   155   109   134    17    63
   3350     3   208     7  4908 23615 20726    61    10 31484   565    10
   1768 17255  5170  9209     5  4225     3   208     7     3 15938  6982
    486 10559    10  1768 25168  5406 13642     5  4271     3   208     7
   4848  4241    61  9507  4392    10  1914  3916  1454 14156     5  3166
      3   208     7  5835  2596    61     1]]"
d4d771bcb59bab4f3eb9026cda7d182eb582027d,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
bc9c31b3ce8126d1d148b1025c66f270581fde10,30.7692,WN18 and YAGO3-10,[[    0     3 21170  2606    11     3 17419  5577   519  4536     1]]," Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs ","[[13070  2009    11  9638  1103  8373     7     6     3 17419  5577   519
   4536    11     3 21170  2606 18256     7  1103  8373     7     1]]"
fa572f1f3f3ce6e1f9f4c9530456329ffc2677ca,100.0,No,[[  0 465   1]],No,[[465   1]]
0e9c08b635c1ebfd36472550d619095541bb5af1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"when the gate has high value, more information flows from the word-level representation; otherwise, char-level will take the dominating place,  for unfamiliar noun entities, the gates tend to bias towards char-level representation in order to care richer morphological structure","[[  116     8 10530    65   306   701     6    72   251 14428    45     8
   1448    18  4563  6497   117  2904     6     3  4059    18  4563    56
    240     8 10138  1014   286     6    21 24108   150   202 12311     6
      8 18975  2134    12 14387  1587     3  4059    18  4563  6497    16
    455    12   124  2354    49     3  8886  4478  1809     1]]"
cfbccb51f0f8f8f125b40168ed66384e2a09762b,0.0,LSTM classifier,[[   0    3 7600 2305  853 7903    1]],They perform t-SNE clustering to analyze discourse embeddings,"[[  328  1912     3    17    18   134  4171  9068    53    12  8341 22739
  25078    26    53     7     1]]"
21663d2744a28e0d3087fbff913c036686abbb9a,100.0,Their model does not differ from BERT.,[[    0  2940   825   405    59  7641    45   272 24203     5     1]],Their model does not differ from BERT.,[[ 2940   825   405    59  7641    45   272 24203     5     1]]
496304f63006205ee63da376e02ef1b3010c4aa1,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
42d66726b5bf8de5b0265e09d76f5ab00c0e851a,0.0,BLEU,[[    0     3  8775 12062     1]],"Single-Turn, Multi-Turn",[[7871   18  382  450   29    6 4908   18  382  450   29    1]]
c2cbc2637761a2c2cf50f5f8caa248814277430e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF) and gradient boosting (XGB)","[[ 4224 29011  5879    41   134 12623   201  7736  3040 26625    41 22084
      5 17748   201 25942  6944    41  8556    61    11 26462     3 24220
     41     4  3443    61     1]]"
54fe8f05595f2d1d4a4fd77f4562eac519711fa6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Systems do not perform well both in Facebook and Twitter texts,[[ 5479   103    59  1912   168   321    16  1376    11  3046 14877     1]]
dd9883f4adf7be072d314d7ed13fe4518c5500e0,0.0,"relation representation, directed edge representation",[[   0 4689 6497    6 6640 3023 6497    1]],"Task processing: converting data exploration tasks to algebraic operations on the embedding space, Query processing: executing semantic query on the embedding space and return results","[[16107  3026    10     3 21049   331  9740  4145    12 27502   447  2673
     30     8 25078    26    53   628     6     3 27569  3026    10     3
  26685 27632 11417    30     8 25078    26    53   628    11  1205   772
      1]]"
b540cd4fe9dc4394f64d5b76b0eaa4d9e30fb728,11.7647,accuracy,[[   0 7452    1]],"BLUE utilizes different metrics for each of the tasks: Pearson correlation coefficient, F-1 scores, micro-averaging, and accuracy","[[    3  8775  5078  5849     7   315 15905    21   284    13     8  4145
     10 29300 18712 27742     6   377  2292  7586     6  2179    18     9
  23980     6    11  7452     1]]"
0cd0755ac458c3bafbc70e4268c1e37b87b9721b,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
959490ba72bd02f742db1e7b19525d4b6c419772,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
71a7153e12879defa186bfb6dbafe79c74265e10,100.0,Chinese general corpus,[[    0  2830   879 11736   302     1]],Chinese general corpus,[[ 2830   879 11736   302     1]]
2122bd05c03dde098aa17e36773e1ac7b6011969,21.0526,evaluating on the basis of broad coverage of the natural language or on the basis of broad coverage,"[[    0     3 17768    30     8  1873    13  4358  2591    13     8   793
   1612    42    30     8  1873    13  4358  2591]]",Fill-in-the-blank natural language questions,"[[12607    18    77    18   532    18  4605    29   157   793  1612   746
      1]]"
ae60079da9d3d039965368acbb23c6283bc3da94,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
49ea25af6f75e2e96318bad5ecf784ce84e4f76b,0.0,WN18,[[    0     3 21170  2606     1]],the Pasokh dataset BIBREF42 ,[[    8  6156  1825   107 17953     3  5972 25582   371  4165     1]]
f41c401a4c6e1be768f8e68f774af3661c890ffd,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
bee74e96f2445900e7220bc27795bfe23accd0a7,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
16af38f7c4774637cf8e04d4b239d6d72f0b0a3a,40.0,"70,000 documents",[[    0     3 28891  2691     1]],over 104k documents,[[  147     3 15442   157  2691     1]]
3af9156b95a4c2d67cc54b80b92cc7b918fea2a9,0.0,predicting the word given its context,[[    0     3 29856     8  1448   787   165  2625     1]],"identify the boundaries of timexes and assign them to one of the following classes: date, time, duration, set,  Then we evaluated these results using more detailed measures for timexes","[[ 2862     8 11814    13    97   226    15     7    11 12317   135    12
     80    13     8   826  2287    10   833     6    97     6  8659     6
    356     6    37    29    62 14434   175   772   338    72  3117  3629
     21    97   226    15     7     1]]"
068dbcc117c93fa84c002d3424bafb071575f431,0.0,QA pairs per predicate,[[    0     3 23008 14152   399   554  4370   342     1]],"Inter-annotator agreement, comparison against expert annotation, agreement with PropBank Data annotations.","[[ 3037    18   152  2264  1016  2791     6  4993   581  2205 30729     6
   2791    28 13543 21347  2747 30729     7     5     1]]"
bde6fa2057fa21b38a91eeb2bb6a3ae7fb3a2c62,100.0,51.5,[[    0   305 16593     1]],51.5,[[  305 16593     1]]
30b5e5293001f65d2fb9e4d1fdf4dc230e8cf320,0.0,classification task,[[    0 13774  2491     1]],To classify a text as belonging to one of the ten possible classes.,"[[  304   853  4921     3     9  1499    38 12770    12    80    13     8
      3   324   487  2287     5     1]]"
8266642303fbc6a1138b4e23ee1d859a6f584fbb,100.0,"BIBREF3, BIBREF4, BIBREF9","[[    0     3  5972 25582   371  6355     3  5972 25582   371  8525     3
   5972 25582   371  1298     1]]","BIBREF3, BIBREF4, BIBREF9","[[    3  5972 25582   371  6355     3  5972 25582   371  8525     3  5972
  25582   371  1298     1]]"
1ccfd288f746c35006f5847297ab52020729f523,60.0,"stress, anxiety, and social.",[[   0 2189    6 6261    6   11  569    3    5    1]],"abuse, social, anxiety, PTSD, and financial",[[ 5384     6   569     6  6261     6     3 29609     6    11   981     1]]
989271972b3176d0a5dabd1cc0e4bdb671269c96,66.6667,Arabic WikiNews site https://arabwikinews.org/wiki,"[[    0 19248  2142  2168  6861     7   353  4893  1303     9  7093 17193
  15808     5  1677    87 17193     1]]",from Arabic WikiNews site https://ar.wikinews.org/wiki,"[[   45 19248  2142  2168  6861     7   353  4893  1303   291     5 17193
  15808     5  1677    87 17193     1]]"
0a92352839b549d07ac3f4cb997b8dc83f64ba6f,100.0,2 accuracy points,[[   0  204 7452  979    1]],2 accuracy points,[[ 204 7452  979    1]]
728c2fb445173fe117154a2a5482079caa42fe24,71.4286,"long-range syntactical links, though less frequent than adjacent syntactical relationships","[[    0   307    18  5517  8953    17  2708  1950  2416     6   713   705
   8325   145 12487  8953    17  2708  1950  3079]]","long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach","[[  307    18  5517  8953    17  2708  1950  2416     6   713   705  8325
    145 12487  8953    17  2708  1950  3079     6   429    36  1028 12327
     45     3     9   650  1448 19181     9    75  4392  1295     1]]"
cb370692fe0beef90cdaa9c8e43a0aab6f0e117a,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
334f90bb715d8950ead1be0742d46a3b889744e7,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"No feature is given, only discussion that semantic features are use in practice and yet to be discovered how to embed that knowledge into statistical decision theory framework.","[[  465  1451    19   787     6   163  3071    24 27632   753    33   169
     16  1032    11   780    12    36  3883   149    12 25078    24  1103
    139 11775  1357  4516  4732     5     1]]"
d32b6ac003cfe6277f8c2eebc7540605a60a3904,0.0,SVMRank,[[    0   180 12623 22557     1]],"Rank by the number of times a citation is mentioned in the document,  Rank by the number of times the citation is cited in the literature (citation impact). , Rank using Google Scholar Related Articles., Rank by the TF*IDF weighted cosine similarity. , ank using a learning-to-rank model trained on text similarity rankings","[[    3 22557    57     8   381    13   648     3     9     3 13903    19
   2799    16     8  1708     6     3 22557    57     8   381    13   648
      8     3 13903    19     3 11675    16     8  6678    41 13903  1113
    137     3     6     3 22557   338  1163 23229 16559  7491     7     5
      6     3 22557    57     8     3  9164  1935  4309   371  1293    15
     26   576     7   630  1126   485     5     3     6     3  5979   338
      3     9  1036    18   235    18  6254   825  4252    30  1499  1126
    485 16890     1]]"
4d4739682d540878a94d8227412e9e1ec1bb3d39,37.5,i2b2 de-identification challenge dataset BIBREF1,"[[    0     3    23   357   115   357    20    18  4215  2420  1921 17953
      3  5972 25582   371   536     1]]","2014 i2b2 de-identification challenge data set BIBREF2, nursing notes corpus BIBREF3","[[ 1412     3    23   357   115   357    20    18  4215  2420  1921   331
    356     3  5972 25582   371  4482  8205  3358 11736   302     3  5972
  25582   371   519     1]]"
9508e9ec675b6512854e830fa89fa6a747b520c5,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
a65e5c97ade6e697ec10bcf3c3190dc6604a0cd5,66.6667,"E2E NLG challenge dataset, E2E NLG challenge dataset","[[    0   262   357   427   445 24214  1921 17953     6   262   357   427
    445 24214  1921 17953     1]]","E2E NLG challenge Dataset, The Wikipedia Company Dataset","[[  262   357   427   445 24214  1921  2747  2244     6    37 16885  1958
   2747  2244     1]]"
50716cc7f589b9b9f3aca806214228b063e9695b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"- Font & Keyboard
- Speech-to-Text
- Text-to-Speech
- Text Prediction
- Spell Checker
- Grammar Checker
- Text Search
- Machine Translation
- Voice to Text Search
- Voice to Speech Search","[[    3    18 18685     3   184  4420  1976     3    18 26351    18   235
     18 13598    17     3    18  5027    18   235    18   134   855 10217
      3    18  5027  1266 12472     3    18  8974   195  1972    49     3
     18 30751  1972    49     3    18  5027  4769     3    18  5879 24527
      3    18 12347    12  5027  4769     3    18 12347    12 26351  4769
      1]]"
d83304c70fe66ae72e78aa1d183e9f18b7484cd6,8.0,Unanswerable,[[   0  597 3247 3321  179    1]],"True, Likely (i.e. Answerable), or Unsure (i.e. Unanswerable), why they are unsure from two choices (“Not stated in the article” or “Other”), The “summary” text boxes","[[10998     6  2792   120    41    23     5    15     5 11801   179   201
     42   597  4334    41    23     5    15     5   597  3247  3321   179
    201   572    79    33     3 20305    45   192  3703  8186 10358  4568
     16     8  1108   153    42   105   667   189    49  7058     6    37
    105  4078    51  1208   153  1499  5598     1]]"
4ce3a6632e7d86d29a42bd1fcf325114b3c11d46,100.0,No,[[  0 465   1]],No,[[465   1]]
d6e2b276390bdc957dfa7e878de80cee1f41fbca,0.0,"BERT14, BERT12, BERT13, BERT14, BERT15","[[    0   272 24203  2534     6   272 24203  2122     6   272 24203  2368
      6   272 24203  2534     6   272 24203  1808]]",Only Bert base and Bert large are compared to proposed approach.,"[[ 3462 20612  1247    11 20612   508    33     3  2172    12  4382  1295
      5     1]]"
f08a66665f01c91cb9dfe082e9d1015ecf3df71d,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
a24a7a460fd5e60d71a7e787401c68caa4702df6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"AraVec for Arabic, FastText for French, and Word2vec Google News for English.","[[ 1533     9   553    15    75    21 19248     6  6805 13598    17    21
   2379     6    11  4467   357   162    75  1163  3529    21  1566     5
      1]]"
117aa7811ed60e84d40cd8f9cb3ca78781935a98,100.0,No,[[  0 465   1]],No,[[465   1]]
07580f78b04554eea9bb6d3a1fc7ca0d37d5c612,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"There is no reason to think that this approach wouldn't also be successful for other technical domains. Technical terms are replaced with tokens, therefore so as long as there is a corresponding process for identifying and replacing technical terms in the new domain this approach could be viable.","[[  290    19   150  1053    12   317    24    48  1295  3290    31    17
     92    36  1574    21   119  2268  3303     7     5 10728  1353    33
   5821    28 14145     7     6  2459    78    38   307    38   132    19
      3     9     3  9921   433    21     3  9690    11 11906  2268  1353
     16     8   126  3303    48  1295   228    36 15109     5     1]]"
1ed006dde28f6946ad2f8bd204f61eda0059a515,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
f3e96c5487d87557a661a65395b0162033dc05b3,100.0,Zulu,[[   0 1811   40   76    1]],Zulu,[[1811   40   76    1]]
35eb8464e934a2769debe14148667c61bf1da243,19.0476,structured content analysis is similar to structured learning in that it uses discrete information rather than a,"[[    0 14039   738  1693    19  1126    12 14039  1036    16    24    34
   2284 19217    15   251  1066   145     3     9]]","structured content analysis (also called “closed coding”) is used to turn qualitative or unstructured data of all kinds into structured and/or quantitative data, Projects usually involve teams of “coders” (also called “annotators”, “labelers”, or “reviewers”), with human labor required to “code”, “annotate”, or “label” a corpus of items.","[[14039   738  1693    41 13208   718   105 16221    26     3  9886  7058
     19   261    12   919 19647    42    73 16180    26   331    13    66
   4217   139 14039    11    87   127 18906   331     6  2786     7  1086
   7789  2323    13   105  4978    52     7   153    41 13208   718   105
    152  2264  6230  1241   105    40 10333   277  1241    42   105    60
   4576   277  7058     6    28   936  5347   831    12   105  4978  1241
    105 24889  4748  1241    42   105    40 10333   153     3     9 11736
    302    13  1173     5     1]]"
d9c26c1bfb3830c9f3dbcccf4c8ecbcd3cb54404,100.0,English-Japanese,[[   0 1566   18  683 9750 1496   15    1]],English-Japanese,[[1566   18  683 9750 1496   15    1]]
e7c0cdc05b48889905cc03215d1993ab94fb6eaa,100.0,No,[[  0 465   1]],No,[[465   1]]
acd05f31e25856b9986daa1651843b8dc92c2d99,0.0,"10,000",[[    0 13923     1]]," 9,892 stories of sexual harassment incidents",[[ 9902  3914   357  1937    13  6949 23556 15935     1]]
496e81769a8d9992dae187ed60639ff2eec531f3,22.2222,"English, Chinese",[[   0 1566    6 2830    1]]," WSD is predominantly evaluated on English, we are also interested in evaluating our approach on Chinese","[[  549  7331    19 24448 14434    30  1566     6    62    33    92  1638
     16     3 17768    69  1295    30  2830     1]]"
b6a4ab009e6f213f011320155a7ce96e713c11cf,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
61aac406b648865f007a400dcd69f28e44efc636,0.0,Yes,[[   0 2163    1]],computationally inexpensive means to understand what happened at the stopping point,"[[25850   120 13938   598    12   734   125  2817    44     8 10847   500
      1]]"
a91abc7983fffa6b2e1e46133f559cec3d7d9438,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
7a7e279170e7a2f3bc953c37ee393de8ea7bd82f,87.5,cloze-style reading comprehension and query reading comprehension,"[[    0     3  3903   776    18  4084  1183 27160    11 11417  1183 27160
      1]]",cloze-style reading comprehension and user query reading comprehension questions,"[[    3  3903   776    18  4084  1183 27160    11  1139 11417  1183 27160
    746     1]]"
e2cfaa2ec89b944bbc46e5edf7753b3018dbdc8f,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
bd3ccb63fd8ce5575338d7332e96def7a3fabad6,16.6667,HERMIT dataset is available from the NLU Open Library.,"[[    0     3 17444 12604 17953    19   347    45     8   445  9138  2384
   5355     3     5     1]]","ROMULUS dataset, NLU-Benchmark dataset","[[    3 13103  4254  3063 17953     6   445  9138    18  2703  5457  3920
  17953     1]]"
10210d5c31dc937e765051ee066b971b6f04d3af,0.0,100 000 question samples,[[   0  910 6078  822 5977    1]], 16k questions,[[898 157 746   1]]
519db0922376ce1e87fcdedaa626d665d9f3e8ce,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
df2839dbd68ed9d5d186e6c148fa42fce60de64f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"1448 sentences more than the dataset from Bhat et al., 2017","[[  968  3707 16513    72   145     8 17953    45   272   547     3    15
     17   491     5     6  1233     1]]"
4a4616e1a9807f32cca9b92ab05e65b05c2a1bf5,6.0606,"3600 samples, 880 samples from the U.S. Department of Health and Human Services","[[   0  220 6007 5977    6  505 2079 5977   45    8  412    5  134    5
  1775   13 1685   11 3892 1799]]",Test set 1 contained 57 drug labels and 8208 sentences and test set 2 contained 66 drug labels and 4224 sentences,"[[ 2300   356   209  6966     3  3436  2672 11241    11   505 23946 16513
     11   794   356   204  6966     3  3539  2672 11241    11   314 24622
  16513     1]]"
b3ff166bd480048e099d09ba4a96e2e32b42422b,100.0,No,[[  0 465   1]],No,[[465   1]]
33ccbc401b224a48fba4b167e86019ffad1787fb,0.0,"198,287",[[    0     3 24151     6   357  4225     1]],from 50K to 4.8M,[[   45   943   439    12     3 27441   329     1]]
2a859e80d8647923181cb2d8f9a2c67b1c3f4608,4.3478,"a grade of 100%, 100%, 100%,","[[   0    3    9 2769   13    3    2 2915 1454    6    3    2 2915 1454
     6    3    2 2915 1454    6]]","Be explicit in what you evaluate., Faithfulness evaluation should not involve human-judgement on the quality of interpretation., Faithfulness evaluation should not involve human-provided gold labels., Do not trust “inherent interpretability” claims., Faithfulness evaluation of IUI systems should not rely on user performance.","[[  493 17623    16   125    25  6825     5     6 15364 18154  5002   225
     59  7789   936    18   354 13164   297    30     8   463    13  8868
      5     6 15364 18154  5002   225    59  7789   936    18 29189    26
   2045 11241     5     6   531    59  2019   105    77   760   295  7280
   2020   153  3213     5     6 15364 18154  5002    13    27  7563  1002
    225    59     3  4610    30  1139   821     5     1]]"
d3092f78bdbe7e741932e3ddf997e8db42fa044c,0.0,BIBREF3,[[    0     3  5972 25582   371   519     1]],root mean square error between the actual and the predicted price of Bitcoin for every minute,"[[ 5465  1243  2812  3505   344     8  1805    11     8 15439   594    13
   9310    21   334  1962     1]]"
f5e571207d9f4701b4d01199ef7d0bfcfa2c0316,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Each class has different patterns in adjectives, adverbs and verbs for sarcastic and non-sarcastic classes","[[ 1698   853    65   315  4264    16 31268     7     6     3     9    26
  11868     7    11  7375     7    21     3     7  4667 10057    11   529
     18     7  4667 10057  2287     1]]"
579a0603ec56fc2b4aa8566810041dbb0cd7b5e7,0.0,Oracle,[[    0 14617     1]],perform experiments to utilize ASR $n$-best hypotheses during evaluation,"[[ 1912 12341    12  5849    71  6857  1514    29  3229    18  9606 10950
  19712     7   383  5002     1]]"
5cc937c2dcb8fd4683cb2298d047f27a05e16d43,0.0,accuracy,[[   0 7452    1]], continuous relaxation to top-k-argmax,[[ 7558 12633    12   420    18   157    18  8240  9128     1]]
7955dbd79ded8ef4ae9fc28b2edf516320c1cb55,85.7143,"size, demographics, areas of research, impact, and correlation of citations with demographic","[[    0   812     6 14798     7     6   844    13   585     6  1113     6
     11 18712    13     3 13903     7    28 14798]]","size, demographics, areas of research, impact, and correlation of citations with demographic attributes (age and gender)","[[  812     6 14798     7     6   844    13   585     6  1113     6    11
  18712    13     3 13903     7    28 14798 12978    41   545    11  7285
     61     1]]"
c96a6b30d71c6669592504e4ee8001e9d1eb1fba,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
57e783f00f594e08e43a31939aedb235c9d5a102,0.0,BLEU and TER scores,[[    0     3  8775 12062    11     3  5946  7586     1]],AUC-ROC,[[   71  6463    18 26893     1]]
42c02c554ab4ceaf30a8ca770be4f271887554c2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Non-English code is a large-scale phenomena., Transliteration is common in identifiers for all languages., Languages clusters into three distinct groups based on how speakers use identifiers/comments/transliteration., Non-latin script users write comments in their L1 script but write identifiers in English., Right-to-left (RTL) language scripts, such as Arabic, have no observed prevalence on GitHub identifiers","[[ 5388    18 26749  1081    19     3     9   508    18  6649 24666     5
      6  4946  9842   257    19  1017    16     3  8826    52     7    21
     66  8024     5     6 10509     7  9068     7   139   386  6746  1637
      3   390    30   149  7215   169     3  8826    52     7    87   287
   4128    87  7031  9842   257     5     6  5388    18 14098  4943  1105
   1431  2622    16    70   301   536  4943    68  1431     3  8826    52
      7    16  1566     5     6  5068    18   235    18 17068    41  5934
    434    61  1612  4943     7     6   224    38 19248     6    43   150
   6970 24753    30     3 30516     3  8826    52     7     1]]"
c3a9732599849ba4a9f07170ce1e50867cf7d7bf,9.0909,"LSTM, LSTM, LSTM, LSTM ","[[   0    3 7600 2305    3    6    3 7600 2305    3    6    3 7600 2305
     3    6    3 7600 2305    3]]","2) Naïve Bayes with SVM (NBSVM), 3) Extreme Gradient Boosting (XGBoost), 4) FastText algorithm with Bidirectional LSTM (FastText-BiLSTM)","[[ 9266  1823     2   162  2474    15     7    28   180 12623    41 14972
    134 12623   201     3  5268 20355    15 10771  4741     3 16481    53
     41     4   517 16481   201     3  7256  6805 13598    17 12628    28
   2106 26352     3  7600  2305    41   371     9     7    17 13598    17
     18   279    23  7600  2305    61     1]]"
bbdb2942dc6de3d384e3a1b705af996a5341031b,0.0,monolingual ELMo,[[    0  7414 25207   262 11160    32     1]],A bi-LSTM with max-pooling on top of it,"[[   71  2647    18  7600  2305    28  9858    18 13194    53    30   420
     13    34     1]]"
0f60864503ecfd5b048258e21d548ab5e5e81772,100.0,No,[[  0 465   1]],No,[[465   1]]
8f882f414d7ea12077930451ae77c6e5f093adbc,0.0,We combine the proposed neural network models with a set of parallel corpora.,"[[    0   101  5148     8  4382 24228  1229  2250    28     3     9   356
     13  8449 11736   127     9     5     1]]","ncorporating NNGLM and NNJM both independently and jointly into, baseline system","[[    3    29 14723   127  1014   445 12531 11160    11     3 17235   683
    329   321 13971    11 22801   139     6 20726   358     1]]"
e854edcc5e9111922e6e120ae17d062427c27ec1,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
e75f5bd7cc7107f10412d61e3202a74b082b0934,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],the Transformer with 10 encoder layers and 2 decoder layers is $2.32$ times as fast as the 6-layer Transformer,"[[    8 31220    28   335 23734    52  7500    11   204    20  4978    52
   7500    19  3771     5  2668  3229   648    38  1006    38     8 12405
  18270 31220     1]]"
e1132564b0dd916a522e7690bc7719d2bba3fe68,8.0,Relation network is used to infer causality at segment level,"[[    0 28898  1229    19   261    12    16  1010 31161   485    44  5508
    593     1]]",we integrate object-pairs with global representation and make a pair-wise inference to detect the relationship among the segments,"[[   62  9162  3735    18   102  2256     7    28  1252  6497    11   143
      3     9  3116    18 10684    16 11788    12  8432     8  1675   859
      8 15107     1]]"
a69af5937cab861977989efd72ad1677484b5c8c,22.2222,"trained annotators BIBREF4, crowdsourcing BIBREF5","[[    0  4252    46  2264  6230     3  5972 25582   371  8525  4374 19035
      3  5972 25582   371   755     1]]",the annotation machinery of BIBREF5,[[    8 30729 13226    13     3  5972 25582   371   755     1]]
771b373d09e6eb50a74fffbf72d059ad44e73ab0,0.0,"they represent sentences in a more complex way, namely, they represent sentences in a","[[    0    79  4221 16513    16     3     9    72  1561   194     6     3
  17332     6    79  4221 16513    16     3     9]]", we were looking for original and uncommon sentence change suggestions,[[   62   130   479    21   926    11 21141  7142   483  5782     1]]
6270d5247f788c4627be57de6cf30112560c863f,0.0,No,[[  0 465   1]],They experimented with sentiment analysis and natural language inference task,"[[  328  5016    15    26    28  6493  1693    11   793  1612    16 11788
   2491     1]]"
4cc5ba404d6a47363f119d9db7266157d3bb246b,0.0,"a simple, single-hop QA system BIBREF0","[[    0     3     9   650     6   712    18 10776     3 23008   358     3
   5972 25582   371   632     1]]",$\textsc {BERT}_{\textsc {BASE}}$ ensemble from BIBREF3,"[[ 1514     2  6327     7    75     3     2 12920   382     2   834     2
   6327     7    75     3     2   279 17892     2  3229  8784    45     3
   5972 25582   371   519     1]]"
400efd1bd8517cc51f217b34cbf19c75d94b1874,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5845d1db7f819dbadb72e7df69d49c3f424b5730,100.0,Hindi,[[    0 25763     1]],Hindi,[[25763     1]]
eac042734f76e787cb98ba3d0c13a916a49bdfb3,100.0,GENIA corpus,[[    0     3  5042 26077 11736   302     1]],GENIA corpus,[[    3  5042 26077 11736   302     1]]
9e04730907ad728d62049f49ac828acb4e0a1a2a,0.0,82.0%,[[   0    3 4613    5 6932    1]],"On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%","[[  461  4769   134    29    23  6811    17     7 17953     3 14775   489
  26346  4704     6   445  7075   431 27297  5988     6    30     3 19814
  23847  7631 17953     3 14775   305 11039  5988     6   445  7075  9526
      5   632  5953     6    30  3318  2726  1950 17953     3 14775  8838
   4200  1454     6   445  7075   220 20677  5953     1]]"
4a8bceb3b6d45f14c4749115d6aa83912f0b0a6e,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
ad0a7fe75db5553652cd25555c6980f497e08113,20.6897,"if the logical form is consistent with the utterance, the model will learn that","[[   0    3   99    8    3 6207  607   19 4700   28    8    3 5108  663
     6    8  825   56  669   24]]",By treating logical forms as a latent variable and training a discriminative log-linear model over logical form y given x.,"[[  938 10902     3  6207  2807    38     3     9    50  4669  7660    11
    761     3     9  9192  1528  4303    18   747   291   825   147     3
   6207   607     3    63   787     3   226     5     1]]"
2c88b46c7e3a632cfa10b7574276d84ecec7a0af,0.0,Bi-LSTM with respect to Roman Urdu,[[   0 2106   18 7600 2305   28 1445   12 3385 4575 1259    1]],the model proposed in BIBREF3,[[    8   825  4382    16     3  5972 25582   371   519     1]]
39f8db10d949c6b477fa4b51e7c184016505884f,0.0,Using mostly raw data,[[   0    3 3626 3323 5902  331    1]],by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity,"[[   57  9248    53 22739  5836    12 17554   342     3  9618   485    45
   6677   554 11346  1422    12   804  6493     3  9618   485     1]]"
946676f1a836ea2d6fe98cb4cfc26b9f4f81984d,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
f6496b8d09911cdf3a9b72aec0b0be6232a6dba1,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
fd6c194632230e392088fc1f574c8626c6a2fa96,5.2632,the data is cleaned and filtered by a team of experts,"[[    0     8   331    19 12631    11     3 23161    57     3     9   372
     13  2273     1]]","many news articles begin with reporter names, media agencies, dates or other contents irrelevant to the content, to ensure that the summary is concise and the article contains enough salient information, we only keep articles with 10-150 words in the top three sentences and 150-1200 words in the rest, and that contain at least 6 sentences in total, we try to remove articles whose top three sentences may not form a relevant summary","[[  186  1506  2984  1731    28 17021  3056     6   783  4299     6  5128
     42   119 10223 26213    12     8   738     6    12   766    24     8
   9251    19 22874    11     8  1108  2579   631  5394  4741   251     6
     62   163   453  2984    28 25446   632  1234    16     8   420   386
  16513    11  4261  5947  1206  1234    16     8   880     6    11    24
   3480    44   709   431 16513    16   792     6    62   653    12  2036
   2984     3  2544   420   386 16513   164    59   607     3     9  2193
   9251     1]]"
a883bb41449794e0a63b716d9766faea034eb359,100.0,images and text,[[   0 1383   11 1499    1]],images and text,[[1383   11 1499    1]]
1fb73176394ef59adfaa8fc7827395525f9a5af7,100.0,AmazonQA and ConciergeQA datasets,[[    0  2536 23008    11  1193  9186   397 23008 17953     7     1]],AmazonQA and ConciergeQA datasets,[[ 2536 23008    11  1193  9186   397 23008 17953     7     1]]
f463db61de40ae86cf5ddd445783bb34f5f8ab67,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Perceptron model using the local features.,[[1915 6873   52  106  825  338    8  415  753    5    1]]
21df76462c76d6e2d52fb7dce573ee5336627cb5,18.1818,"agreement was very low, we carried out group discussions to find possible reasons","[[   0 2791   47  182  731    6   62 4006   91  563 7574   12  253  487
  2081    1]]",participants in group one very rarely gave different answers to questions one and two (18 of 500 instances or 3.6%),"[[ 3008    16   563    80   182  8207  1891   315  4269    12   746    80
     11   192  9323    13  2899 10316    42     3 23074  6210     1]]"
1d739bb8e5d887fdfd1f4b6e39c57695c042fa25,0.0,log regression model with character-level n-gram features,"[[    0  4303 26625   825    28  1848    18  4563     3    29    18  5096
    753     1]]",three parallel LSTM BIBREF21 layers,[[  386  8449     3  7600  2305     3  5972 25582   371  2658  7500     1]]
f92ee3c5fce819db540bded3cfcc191e21799cb1,0.0,a speech recognition tool called a t-snapper,"[[   0    3    9 5023 5786 1464  718    3    9    3   17   18    7   29
     9 8153    1]]",cannot be disclosed due to licensing restrictions,[[ 1178    36 19972   788    12 16604  9076     1]]
a458c649a793588911cef4c421f95117d0b9c472,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"At the moment defender can draw on methods from image area to text for improving the robustness of DNNs, e.g. adversarial training BIBREF107 , adding extra layer BIBREF113 , optimizing cross-entropy function BIBREF114 , BIBREF115 or weakening the transferability of adversarial examples.","[[  486     8   798     3 13720    54  3314    30  2254    45  1023   616
     12  1499    21  4863     8  6268   655    13   309 17235     7     6
      3    15     5   122     5 23210    23   138   761     3  5972 25582
    371 18057     3     6  2651   996  3760     3  5972 25582   371 20522
      3     6 19769    53  2269    18    35 12395    63  1681     3  5972
  25582   371 18959     3     6     3  5972 25582   371 15660    42  5676
     35    53     8  2025  2020    13 23210    23   138  4062     5     1]]"
b984612ceac5b4cf5efd841af2afddd244ee497a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],approximately equal parameterization,[[ 3241  4081 15577  1707     1]]
37d829cd42db9ae3d56ab30953a7cf9eda050841,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
77db56fee07b01015a74413ca31f19bea7203f0b,33.3333,"F$_1$, accuracy",[[   0  377 3229  834  536 3229    6 7452    1]],"F$_1$, precision, and recall",[[  377  3229   834   536  3229     6 11723     6    11  7881     1]]
2870fbce43a3cf6daf982f720137c008b30c60dc,0.0,a bi-directional Transformer encoder pre-trained on the language modeling task BI,"[[    0     3     9  2647    18 26352 31220 23734    52   554    18    17
  10761    30     8  1612 15309  2491     3  5972]]","nouns, verbs, pronouns, subjects, objects, negation words, special BERT tokens","[[  150   202     7     6  7375     7     6   813 15358    29     7     6
   7404     6  4820     6 14261   257  1234     6   534   272 24203 14145
      7     1]]"
1e4450e23ec81fdd59821055f998fd9db0398b16,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
bcd6befa65cab3ffa6334c8ecedd065a4161028b,14.2857,a form of humour that occurs spontaneously in human minds,"[[    0     3     9   607    13     3  4884  1211    24  6986 23496   120
     16   936 10884     1]]","a form of wordplay jokes in which one sign (e.g. a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect","[[    3     9   607    13  1448  4895 10802     7    16    84    80  1320
     41    15     5   122     5     3     9  1448    42     3     9  9261
     61  6490   192    42    72  2530     7    57  9248    53  4251     7
     15  2258     6     3 10207 19140    63     6    42     3  9621  4478
   1126   485    12   430  1320     6    21    46  3855 28845    42 23051
    138  1504     1]]"
ee9b95d773e060dced08705db8d79a0a6ef353da,30.0,are used as a means of improving the prediction of incident severity,"[[    0    33   261    38     3     9   598    13  4863     8 21332    13
   5415 20363     1]]",they are used as additional features in a supervised classification task,"[[   79    33   261    38  1151   753    16     3     9     3 23313 13774
   2491     1]]"
6d54bad91b6ccd1108d1ddbff1d217c6806e0842,8.6957,"they use a random LSTM to extract the word from the sentence, and then use","[[   0   79  169    3    9 6504    3 7600 2305   12 5819    8 1448   45
     8 7142    6   11  258  169]]",only the first word sense (usually the most common) is taken into account,"[[  163     8   166  1448  1254    41 21525     8   167  1017    61    19
   1026   139   905     1]]"
77e57d19a0d48f46de8cbf857f5e5284bca0df2b,50.0,8790 utterances,[[   0    3 4225 2394    3 5108  663    7    1]],30M utterances,[[ 604  329    3 5108  663    7    1]]
c806e891324af5d10a72c3b4b9b91177ae6446fb,0.0,"a CLINK-based method BIBREF7, a C-SI","[[    0     3     9   205 27472    18   390  1573     3  5972 25582   371
    940     6     3     9   205    18   134   196]]","TextCNN, TextRNN, SASE, DPCNN, and BERT, $LS \cup KLD \cup CONN$ and $KLD \cup LS \cup LS_{inter}$ are the best systems with the highest recall and F1-score respectively","[[ 5027   254 17235     6  5027 14151   567     6   180 17892     6   309
   4051 17235     6    11   272 24203     6  1514  7600     3     2  4658
    480  9815     3     2  4658  8472   567  3229    11  1514   439  9815
      3     2  4658     3  7600     3     2  4658     3  7600   834     2
   3870     2  3229    33     8   200  1002    28     8  2030  7881    11
    377   536    18     7  9022  6898     1]]"
ff28d34d1aaa57e7ad553dba09fc924dc21dd728,75.0,high correlation results range from 0.672 to 0.672,"[[    0   306 18712   772   620    45     3 22787  5865    12     3 22787
   5865     1]]",High correlation results range from 0.472 to 0.936,"[[ 1592 18712   772   620    45     3 22776  5865    12     3 23758  3420
      1]]"
792d7b579cbf7bfad8fe125b0d66c2059a174cf9,0.0,HEOT,[[   0    3 6021 6951    1]],Ternary Trans-CNN,[[ 9934    29  1208  4946    18   254 17235     1]]
7e34501255b89d64b9598b409d73f96489aafe45,95.2381,"dataset on Amazon Mechanical Turk involving human perception, action and communication","[[    0 17953    30  2536 24483 23694     3  6475   936  8136     6  1041
     11  1901     1]]"," dataset on Mechanical Turk involving human perception, action and communication","[[17953    30 24483 23694     3  6475   936  8136     6  1041    11  1901
      1]]"
23d0637f8ae72ae343556ab135eedc7f4cb58032,11.7647,"Using multiple regression models, we found that acquiring syllables related to","[[    0     3  3626  1317 26625  2250     6    62   435    24     3 19031
      3     7    63   195   179     7  1341    12]]","unsupervised morphological analyzer capable of using lattices improved the accuracy of phoneme recognition and word segmentation, Consequently, this result suggests that this word segmentation method considers the multiple hypothesis of speech recognition as a whole and reduces uncertainty such as variability in recognition","[[   73 23313     3  8886  4478  8341    52  3919    13   338    50    17
   1225    15     7  3798     8  7452    13   951   526  5786    11  1448
   5508   257     6     3 21900     6    48   741  6490    24    48  1448
   5508   257  1573  1099     7     8  1317 22455    13  5023  5786    38
      3     9   829    11  1428     7 14068   224    38 27980    16  5786
      1]]"
14634943d96ea036725898ab2e652c2948bd33eb,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Authors report their best models have following accuracy: English CELEX (98.5%), Dutch CELEX (99.47%), Festival (99.990%), OpenLexique (100%), IIT-Guwahat (95.4%), E-Hitz (99.83%)","[[10236     7   934    70   200  2250    43   826  7452    10  1566   205
  16479     4    41  3916     5  2712   201 10098   205 16479     4    41
   3264     5  4177  6210     6  3397 14156 21316  6932   201  2384   434
    994  1495 11704  6932   201    27  3177    18  9105 17771   144    41
   3301     5  5988   201   262    18   566  5615    41  3264     5  4591
   6210     1]]"
b3ec918827cd22b16212265fcdd5b3eadee654ae,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
a276d5931b989e0a33f2a0bc581456cca25658d9,0.0,"RNN, RNN-GRU, RNN-S, RNN-S,","[[    0   391 17235     6   391 17235    18   517  8503     6   391 17235
     18   134     6   391 17235    18   134     6]]","3-gram and 4-gram conditional language model, Convolution, LSTM models BIBREF27 with and without attention BIBREF28, Transformer, GPT-2","[[ 5354  5096    11  7769  5096  1706   138  1612   825     6  1193 24817
      6     3  7600  2305  2250     3  5972 25582   371  2555    28    11
    406  1388     3  5972 25582   371  2577     6 31220     6   350  6383
   4949     1]]"
63c0128935446e26eacc7418edbd9f50cba74455,26.087,"3,200 sentences, 826,714 relation extraction sentences, and 3,714 question-","[[    0  6180  3632 16513     6   505  2688     6   940  2534  4689 16629
  16513     6    11  6180   940  2534   822    18]]","440 sentences, 2247 triples extracted from those sentences, and 11262 judgements on those triples.","[[    3 22335 16513     6  1630  4177 12063     7 21527    45   273 16513
      6    11   850  2688   357 22555     7    30   273 12063     7     5
      1]]"
737397f66751624bcf4ef891a10b29cfc46b0520,0.0,Wikipedea Corpus and Wikipedia Corpus,"[[    0  2142  2168  3138    15     9 10052   302    11 16885 10052   302
      1]]","Google N-grams
COHA
Moral Foundations Dictionary (MFD)
","[[ 1163   445    18  5096     7  2847  5478 28466  2941     7 28767    41
  13286   308    61     1]]"
34fab25d9ceb9c5942daf4ebdab6c5dd4ff9d3db,0.0,"CNLL-2013, CNLL-2013, CNLL-2013","[[    0     3 10077 10376    18 11138     6     3 10077 10376    18 11138
      6     3 10077 10376    18 11138     1]]","weibo-100k, Ontonotes, LCQMC and XNLI","[[   62    23   115    32    18  2915   157     6   461   235  7977     7
      6     3  6480  2247  3698    11     3     4 18207   196     1]]"
313087c69caeab2f58e7abd62664d3bd93618e4e,5.4054,We evaluate our proposed metric on the basis of the following parameters: - The BL,"[[   0  101 6825   69 4382    3 7959   30    8 1873   13    8  826 8755
    10    3   18   37    3 8775]]","manually labeled and tell exactly if one sentence should be extracted (assuming our annotations are in agreement), to further verify that FAR correlates with human preference,","[[12616  3783    15    26    11   817  1776     3    99    80  7142   225
     36 21527    41 15433    69 30729     7    33    16  2791   201    12
    856 10446    24   377  4280 30575     7    28   936 11633     6     1]]"
a9b10e3db5902c6142e7d6a83253ad2a6cee77fc,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
0300cf768996849cab7463d929afcb0b09c9cf2a,100.0,extraction-then-synthesis framework,[[    0 16629    18   189    35    18 17282  4732     1]], extraction-then-synthesis framework,[[16629    18   189    35    18 17282  4732     1]]
a95188a0f35d3cb3ca70ae1527d57ac61710afa3,100.0,"60,000",[[    0     3 24620     1]],"60,000 ",[[    3 24620     1]]
205163715f345af1b5523da6f808e6dbf5f5dd47,100.0,"44,896 articles",[[   0 8537    6 3914  948 2984    1]],"44,896 articles",[[8537    6 3914  948 2984    1]]
2ec97cf890b537e393c2ce4c2b3bd05dfe46f683,40.0,by the number of detail matches in the explanation and the prediction predictions,"[[    0    57     8   381    13  2736  6407    16     8  7295    11     8
  21332 20099     1]]",They look at the performance accuracy of explanation and the prediction performance,"[[  328   320    44     8   821  7452    13  7295    11     8 21332   821
      1]]"
5f6fac08c97c85d5f4f4d56d8b0691292696f8e6,100.0,No,[[  0 465   1]],No,[[465   1]]
310e61b9dd4d75bc1bebbcb1dae578f55807cd04,0.0,WN18 and FB15k,[[    0     3 21170  2606    11     3 15586  1808   157     1]],"LDC corpus, NIST 2003(MT03), NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06), NIST 2008(MT08)","[[  301  6338 11736   302     6   445 13582  3888   599  7323  4928   201
    445 13582  4406   599  7323  6348   201   445 13582  3105   599  7323
   3076   201   445 13582  3581   599  7323  5176   201   445 13582  2628
    599  7323  4018    61     1]]"
65ebed1971dca992c3751ed985fbe294cbe140d7,11.1111,a new method for detecting the features of student talk that lead to high-quality discussions,"[[    0     3     9   126  1573    21     3 29782     8   753    13  1236
   1350    24   991    12   306    18  4497  7574]]",a reliability study for the proposed scheme ,[[    3     9 10581   810    21     8  4382  5336     1]]
f53be1266be1fea5598a671080226c9c983b69e3,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
58ad7e8f7190e2a4f1588cae9a7842c56b37694d,50.0,"4,432 messages",[[   0 6464  591 2668 4175    1]],"27,534 messages ",[[14141   755  3710  4175     1]]
863d5c6305e5bb4b14882b85b6216fa11bcbf053,0.0,"LSTM, LSTM+LD, LSTM+LD, LSTM","[[   0    3 7600 2305    6    3 7600 2305 1220 9815    6    3 7600 2305
  1220 9815    6    3 7600 2305]]","MOCC, OCCAV, COAV, AVeer, GLAD, DistAV, Unmasking, Caravel, GenIM, ImpGI, SPATIUM and NNCD","[[ 7109  2823     6   411  2823  6968     6  2847  6968     6     3  6968
     15    49     6     3 13011  6762     6  2678    17  6968     6   597
   2754  1765     6  1184     9  4911     6  5945  5166     6 14472  7214
      6  6760 21580  6122    11     3 17235  6931     1]]"
e051d68a7932f700e6c3f48da57d3e2519936c6d,22.2222,Bidirectional LSTM-CRF,[[    0  2106 26352     3  7600  2305    18  4545   371     1]],Bidirectional LSTM based NER model of Flair,"[[ 2106 26352     3  7600  2305     3   390     3 18206   825    13  5766
   2256     1]]"
73e715e485942859e1db75bfb5f35f1d5eb79d2e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Using the answer labels in the training set, we can find appropriate articles that include the information requested in the question.","[[    3  3626     8  1525 11241    16     8   761   356     6    62    54
    253  2016  2984    24   560     8   251  6709    16     8   822     5
      1]]"
07d7652ad4a0ec92e6b44847a17c378b0d9f57f5,14.2857,"BLEU results were comparable, showing that their NMT system only outperforms phrase","[[    0     3  8775 12062   772   130 13289     6  2924    24    70   445
   7323   358   163    91   883  2032     7  9261]]",10.37 BLEU,[[ 5477  4118     3  8775 12062     1]]
90159e143487505ddc026f879ecd864b7f4f479e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Little overlap except common basic Latin alphabet and that Hindi and Marathi languages use same script.,"[[ 5258 21655  3578  1017  1857  6271 20688    11    24 25763    11  1571
      9  7436  8024   169   337  4943     5     1]]"
4b128f9e94d242a8e926bdcb240ece279d725729,100.0,"DBQA, KBRE",[[    0     3  9213 23008     6   480 25582     1]],"DBQA, KBRE",[[    3  9213 23008     6   480 25582     1]]
7e54c7751dbd50d9d14b9f8b13dc94947a46e42f,100.0,ensemble model,[[   0 8784  825    1]],ensemble model,[[8784  825    1]]
cf15c4652e23829d8fb4cf2a25e64408c18734c1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"the image can play the role of a pivot “language"" to bridge the two languages without paralleled corpus","[[    8  1023    54   577     8  1075    13     3     9 16959   105 24925
    121    12  4716     8   192  8024   406  8449    15    26 11736   302
      1]]"
8847f2c676193189a0f9c0fe3b86b05b5657b76a,50.0,20K annotations,[[    0   460   439 30729     7     1]],1593 annotations,[[  627  4271 30729     7     1]]
7ce213657f7ee792148988c5a3578b24cd2f9c62,5.1282,emoji embeddings help to understand the semantics behind the em,"[[    0     3    15    51 21892 25078    26    53     7   199    12   734
      8 27632     7  1187     8     3    15    51]]",The different attention distributions suggest that the proposed senti-emoji embedding is capable of recognizing words with strong sentiments that are closely related to the true sentiment even with the presence of words with conflicting sentiments,"[[   37   315  1388  3438     7  3130    24     8  4382  1622    23    18
     15    51 21892 25078    26    53    19  3919    13     3 22873  1234
     28  1101  6493     7    24    33  4799  1341    12     8  1176  6493
    237    28     8  3053    13  1234    28  4129    53  6493     7     1]]"
e829f008d62312357e0354a9ed3b0827c91c9401,0.0,Non-aggressive,[[    0  5388    18     9   122 10292   757     1]],"Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, TF-IDF Emoticon features","[[  262  7259 19078     3 16772     6  2733    13 26351     6 18266    75
     17    76   257     6  4892  2998   295 10582     6  3967  8292     6
      3  9164    18  4309   371  3967  9798   106   753     1]]"
f1831b2e96ff8ef65b8fde8b4c2ee3e04b7ac4bf,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"NMI between cluster assignments and ground truth tones for all sylables is:
Mandarin: 0.641
Cantonese: 0.464","[[  445  7075   344  9068 14023    11  1591  2827    12  1496    21    66
      3     7    63    40   179     7    19    10 31057    10     3 22787
   4853  1072  6948     7    15    10     3 22776  4389     1]]"
9378b41f7e888e78d667e9763883dd64ddb48728,100.0,No,[[  0 465   1]],No,[[465   1]]
f268b70b08bd0436de5310e390ca5f38f7636612,0.0,"Bi-LSTMs, LSTMs, LSTMs, LSTM","[[   0 2106   18 7600 2305    7    6    3 7600 2305    7    6    3 7600
  2305    7    6    3 7600 2305]]",GIZA++ BIBREF3 or fast_align BIBREF4 ,"[[    3  7214 19873 16702     3  5972 25582   371   519    42  1006   834
    138  3191     3  5972 25582   371   591     1]]"
65461516098ed63c45a567648e8e47c38ea7e58a,0.0,manually reviewed,[[    0 12616  9112     1]], concatenating to the embedding vector,[[  975  2138    35  1014    12     8 25078    26    53 12938     1]]
cd8de03eac49fd79b9d4c07b1b41a165197e1adb,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],The image feature vectors are mapped into BERT embedding dimensions and treated like a text sequence afterwards.,"[[   37  1023  1451 12938     7    33     3 28400   139   272 24203 25078
     26    53  8393    11  4260   114     3     9  1499  5932 15627     5
      1]]"
b367b823c5db4543ac421d0057b02f62ea16bf9f,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
7bc993b32484d6ae3c86d0b351a68e59fd2757a5,100.0,Spanish,[[   0 5093    1]],Spanish,[[5093    1]]
41173179efa6186eef17c96f7cbd8acb29105b0e,0.0,five different biomedicine text-mining tasks with ten corpora,"[[    0   874   315  2392 29368  1499    18  1109    53  4145    28     3
    324 11736   127     9     1]]","Inference task
The aim of the inference task is to predict whether the premise sentence entails or contradicts the hypothesis sentence, Document multilabel classification
The multilabel classification task predicts multiple labels from the texts., Relation extraction
The aim of the relation extraction task is to predict relations and their types between the two entities mentioned in the sentences., Named entity recognition
The aim of the named entity recognition task is to predict mention spans given in the text , Sentence similarity
The sentence similarity task is to predict similarity scores based on sentence pairs","[[   86 11788  2491    37  2674    13     8    16 11788  2491    19    12
   9689   823     8     3 17398  7142     3    35  5756     7    42 21454
      7     8 22455  7142     6 11167  1249    40 10333 13774    37  1249
     40 10333 13774  2491  9689     7  1317 11241    45     8 14877     5
      6 28898 16629    37  2674    13     8  4689 16629  2491    19    12
   9689  5836    11    70  1308   344     8   192 12311  2799    16     8
  16513     5     6  5570    26 10409  5786    37  2674    13     8  2650
  10409  5786  2491    19    12  9689  2652  8438     7   787    16     8
   1499     3     6  4892    17  1433  1126   485    37  7142  1126   485
   2491    19    12  9689  1126   485  7586     3   390    30  7142 14152
      1]]"
5908d7fb6c48f975c5dfc5b19bb0765581df2b25,0.0,HEOT,[[   0    3 6021 6951    1]],Resulting dataset was 7934 messages for train and 700 messages for test.,"[[    3 20119    53 17953    47     3  4440  3710  4175    21  2412    11
  12283  4175    21   794     5     1]]"
bb5697cf352dd608edf119ca9b82a6b7e51c8d21,0.0,Yes,[[   0 2163    1]],"Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained.","[[13936    45    70   161     6    62   854    12 17261     8   926  1708
     57 21306     3  8499  1516  4149    13  1234     6    11  4221     8
   1708   338   163     8 25078    26    53     7    13     8  1234     3
   7361     5     1]]"
61c9f97ee1ac5a4b8654aa152f05f22e153e7e6e,100.0,Wikipedia toxic comments,[[    0 16885 12068  2622     1]], Wikipedia toxic comments,[[16885 12068  2622     1]]
d2e409031f4512375dd5cecec639c7373025f277,0.0,LSTM-based co-attention model BIBREF22,"[[    0     3  7600  2305    18   390   576    18 25615   825     3  5972
  25582   371  2884     1]]","(Layer Normalized Skip-Thoughts, ST-LN) BIBREF31, Cap2All, Cap2Cap, Cap2Img","[[   41  3612  7975 16612  1601 25378    18  8991  4607    17     7     6
   5097    18   434   567    61     3  5972 25582   371  3341     6  4000
    357  6838     6  4000   357 19566     6  4000   357   196    51   122
      1]]"
439af1232a012fc4d94ef2ffe305dd405bee3888,0.0,a noise-handling baseline for a global confusion matrix between the clean and the noisy,"[[    0     3     9  4661    18  2894   697 20726    21     3     9  1252
  12413 16826   344     8  1349    11     8 26847]]","Base , Base+Noise, Cleaning , Dynamic-CM ,  Global-CM,  Global-ID-CM, Brown-CM ,  K-Means-CM","[[ 8430     3     6  8430  1220  4168   159    15     6 11115     3     6
  13967  3113    18  5518     3     6  3699    18  5518     6  3699    18
   4309    18  5518     6  3899    18  5518     3     6   480    18   329
     15  3247    18  5518     1]]"
1c68d18b4b65c4d75dc199d2043079490f6310f8,57.1429,Entity identification with offset mapping and entity identification with a vector vector vector mapping,"[[    0  4443   485 10356    28 13746 14670    11 10409 10356    28     3
      9 12938 12938 12938 14670     1]]",Entity identification with offset mapping and concept indexing,[[ 4443   485 10356    28 13746 14670    11  2077  5538    53     1]]
96992460cfc5f0b8d065ee427067147293746b7a,100.0,"F1, accuracy",[[   0  377 4347 7452    1]],"F1, accuracy",[[ 377 4347 7452    1]]
bb8a0035b767688a98602c33f4714f8ac8ae60db,0.0,"Metric: BLEU: i-vector, bp,","[[    0  1212  3929    10     3  8775 12062    10     3    23    18   162
   5317     3     6     3   115   102     3     6]]","ROUGE-1, ROUGE-2 and ROUGE-L, F-measure ROUGE on XSUM and CNN/DailyMail, and use limited-length recall-measure ROUGE on NYT and DUC","[[  391 26260   427  2292     6   391 26260   427  4949    11   391 26260
    427    18   434     6   377    18 31038   391 26260   427    30     3
      4   134  6122    11 19602    87   308     9  9203  7098     6    11
    169  1643    18 19457  7881    18 31038   391 26260   427    30  5825
    382    11   309  6463     1]]"
dbdf13cb4faa1785bdee90734f6c16380459520b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"A combination of Minimum spanning trees, K-Nearest Neighbors and Markov Stability BIBREF15, BIBREF16, BIBREF17, BIBREF18","[[   71  2711    13 22619     3 25754  3124     6   480    18   567  2741
    222  1484  9031  6693     7    11  1571  9789   472  2020     3  5972
  25582   371  1808     6     3  5972 25582   371  2938     6     3  5972
  25582   371  2517     6     3  5972 25582   371  2606     1]]"
afe34e553c3c784dbf02add675b15c27638cdd45,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
ff307b10e56f75de6a32e68e25a69899478a13e4,0.0,Bi-LSTMs,[[   0 2106   18 7600 2305    7    1]],"logistic regression (LR), recurrent neural network (RNN) BIBREF35, convolutional neural network (CNN) BIBREF36 and Google BERT BIBREF37","[[28820 26625    41 12564   201     3    60 14907 24228  1229    41 14151
    567    61     3  5972 25582   371  2469     6   975 24817   138 24228
   1229    41   254 17235    61     3  5972 25582   371  3420    11  1163
    272 24203     3  5972 25582   371  4118     1]]"
48cc41c372d44b69a477998be449f8b81384786b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"we achieve better results than GCN+PADG but without any use of domain-specific hand-crafted features,  RegSum achieves a similar ROUGE-2 score","[[   62  1984   394   772   145   350 10077  1220  3965 13535    68   406
    136   169    13  3303    18  9500   609    18  8810   753     6  7777
    134   440  1984     7     3     9  1126   391 26260   427  4949  2604
      1]]"
29983f4bc8a5513a198755e474361deee93d4ab6,100.0,five-minute reuse and one-day return,[[    0   874    18  6890 15677    11    80    18  1135  1205     1]],five-minute reuse and one-day return,[[  874    18  6890 15677    11    80    18  1135  1205     1]]
284ea817fd79bc10b7a82c88d353e8f8a9d7e93c,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
2d62a75af409835e4c123a615b06235a352a67fe,0.0,"a non-invasive, clinically safe, and increasingly inexpensive method to visualize the vocal tract","[[    0     3     9   529    18 15267     6  3739   120  1346     6    11
   5684 13938  1573    12 25086     8  6721 13499]]","feedforward neural networks, convolutional neural networks",[[ 3305 26338 24228  5275     6   975 24817   138 24228  5275     1]]
1cbca15405632a2e9d0a7061855642d661e3b3a7,29.6296,"Compared to baseline, our system obtained a 0.28% improvement over the best baseline of","[[    0     3 25236    12 20726     6    69   358  5105     3     9     3
  18189  5953  4179   147     8   200 20726    13]]",Their GTRS approach got an improvement of 3.89% compared to SVM and 27.91% compared to Pawlak.,"[[ 2940 10188  5249  1295   530    46  4179    13     3 26195  7561     3
   2172    12   180 12623    11  2307     5  1298  4704     3  2172    12
  22559    40  1639     5     1]]"
20be7a776dfda0d3c9dc10270699061cb9bc8297,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
dd5c9a370652f6550b4fd13e2ac317eaf90973a8,50.0,0.9898 correlation,[[    0  4097  3916  3916 18712     1]],0.9098 correlation,[[ 4097  2394  3916 18712     1]]
0ca02893bda50007f7a76e7c8804101718fbb01c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]]," non-canonical text such as spelling mistakes, typographic errors, colloquialisms, abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs, mentions), non standard syntactic constructions and spelling variations, grammatically incorrect text, mixture of two or more languages","[[  529    18  1608   106  1950  1499   224    38 19590  8176     6 23042
  14797  6854     6  8029    32  1169  6835     7     6   703  1999  2099
   1628     6     3     7  4612     6  1396     3  5670  5307     6     3
     15    51 21892     7     6 13612 26686    41  4415    38 25354     7
      6  8889     7     6  2652     7   201   529  1068  8953    17  2708
    447  1449     7    11 19590 10914     6     3 16582   144  6402 12153
   1499     6  4989    13   192    42    72  8024     1]]"
3a9d391d25cde8af3334ac62d478b36b30079d74,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
3da9a861dfa25ed486cff0ef657d398fdebf8a93,7.6923,"Noun Match, Noun Match, Noun Match","[[    0   465   202 12296     6   465   202 12296     6   465   202 12296
      1]]","Noun WordNet Semantic Text Exchange Model (NWN-STEM), General WordNet Semantic Text Exchange Model (GWN-STEM), Word2Vec Semantic Text Exchange Model (W2V-STEM)","[[  465   202  4467  9688   679   348  1225  5027  8231  5154    41   567
  21170    18   134 20050   201  2146  4467  9688   679   348  1225  5027
   8231  5154    41   517 21170    18   134 20050   201  4467   357   553
     15    75   679   348  1225  5027  8231  5154    41   518   357   553
     18   134 20050    61     1]]"
930c51b9f3936d936ee745716536a4b40f531c7f,0.0,WN16 and FB15k,[[    0     3 21170  2938    11     3 15586  1808   157     1]],"Quora, MRPC",[[2415  127    9    6    3 9320 4051    1]]
5b6aec1b88c9832075cd343f59158078a91f3597,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Proposed SG model vs SINDHI FASTTEXT:
Average cosine similarity score: 0.650 vs 0.388
Average semantic relatedness similarity score between countries and their capitals: 0.663 vs 0.391","[[  749 12151     3  9945   825     3   208     7   180 13885  7094   377
  12510  3463     4   382    10 23836   576     7   630  1126   485  2604
     10  4097 15348     3   208     7     3 19997  4060 23836 27632  1341
    655  1126   485  2604   344  1440    11    70  1784     7    10     3
  22787  3891     3   208     7     3 19997  4729     1]]"
a891039441e008f1fd0a227dbed003f76c140737,100.0,machine comprehension,[[    0  1437 27160     1]],machine comprehension,[[ 1437 27160     1]]
62afbf8b1090e56fdd2a2fa2bdb687c3995477f6,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
4debd7926941f1a02266b1a7be2df8ba6e79311a,100.0,No,[[  0 465   1]],No,[[465   1]]
e75685ef5f58027be44f42f30cb3988b509b2768,0.0,IEMOCAP,[[    0     3  5091  5365 16986     1]],"set of related tasks are learned (e.g., emotional activation), primary task (e.g., emotional valence)","[[ 356   13 1341 4145   33 2525   41   15    5  122    5    6 3973 5817
   257  201 2329 2491   41   15    5  122    5    6 3973    3 2165 1433
    61    1]]"
1d74fd1d38a5532d20ffae4abbadaeda225b6932,14.2857,"F1 score of 82.0%, F1 score of 82.0%","[[   0  377  536 2604   13    3 4613    5 6932    6  377  536 2604   13
     3 4613    5 6932    1]]","F1 score and Recall are 68.66, 80.08 with Traditional NERs as reference and 59.56, 69.76 with Wikipedia titles as reference.","[[  377   536  2604    11   419 16482    33     3  3651     5  3539     6
   2775     5  4018    28 16505     3 18206     7    38  2848    11     3
   3390     5  4834     6     3  3951     5  3959    28 16885  8342    38
   2848     5     1]]"
b161febf86cdd58bd247a934120410068b24b7d1,42.8571,"agreement, answer, agreement, humor",[[    0  2791     6  1525     6  2791     6 13934     1]],"agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, question, other","[[ 2791     6  1525     6 11746     6 28155     6     3    15  9456   257
      6 13934     6  2841  6363     6   822     6   119     1]]"
28b2a20779a78a34fb228333dc4b93fd572fda15,100.0,supervised learning,[[    0     3 23313  1036     1]],supervised learning,[[    3 23313  1036     1]]
36feaac9d9dee5ae09aaebc2019b014e57f61fbf,100.0,By the number of parameters.,[[   0  938    8  381   13 8755    5    1]],By the number of parameters.,[[ 938    8  381   13 8755    5    1]]
0d755ff58a7e22eb4d02fca45d4a7a3920f4e725,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
b39f2249a1489a2cef74155496511cc5d1b2a73d,0.0,95.10%,[[    0   668 20519  6932     1]],"Answer with content missing: (Table 1)
Previous state-of-the art on same dataset: ResNet50 89% (6 languages), SVM-HMM 70% (4 languages)","[[11801    28   738  3586    10    41 20354  8925 10232   538    18   858
     18   532   768    30   337 17953    10  7127  9688  1752   505  7561
  11372  8024   201   180 12623    18   566  8257 14719  8457  8024    61
      1]]"
9282cf80265a914a13053ab23b77d1a8ed71db1b,100.0,"English, Russian",[[   0 1566    6 4263    1]],"English, Russian",[[1566    6 4263    1]]
41fd359b8c1402b31b6f5efd660143d1414783a0,100.0,"close to random,",[[   0  885   12 6504    6    1]]," close to random,",[[ 885   12 6504    6    1]]
53712f0ce764633dbb034e550bb6604f15c0cacd,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
7426a6e800d6c11795941616fc4a243e75716a10,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Which events authors choose to include in their history, which they leave out, and the way the events chosen relate to the march","[[ 4073   984  5921   854    12   560    16    70   892     6    84    79
   1175    91     6    11     8   194     8   984  3934  9098    12     8
  10556     1]]"
be9cadaebfa0ff1a3c5a5ed56ff3aae76cf5e0a4,9.0909,accuracy,[[   0 7452    1]],"average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All)","[[ 1348  7452   147   284   712    18 24925   825    41   188   208   122
    201    11  7452  5105   116   761    30     8   975  2138    35   257
     13    66  8024    68     8  2387    80    41  6838    61     1]]"
b807dd3d42251615b881632caa5e331e2203d269,50.0,ATE-based APC task on Chinese reviews datasets,"[[    0     3  6048    18   390    71  4051  2491    30  2830  2456 17953
      7     1]]",GANN obtained the state-of-the-art APC performance on the Chinese review datasets,"[[  350 21478  5105     8   538    18   858    18   532    18  1408    71
   4051   821    30     8  2830  1132 17953     7     1]]"
534f69c8c90467d5aa4e38d7c25c53dbc94f4b24,100.0,Amazon Mechanical Turk (AMT),[[    0  2536 24483 23694    41   188  7323    61     1]],Amazon Mechanical Turk (AMT),[[ 2536 24483 23694    41   188  7323    61     1]]
5a0841cc0628e872fe473874694f4ab9411a1d10,6.25,"Compared to the best results of all three methods, CNN outperforms by ","[[    0     3 25236    12     8   200   772    13    66   386  2254     6
    205 17235    91   883  2032     7    57     3]]","on SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI","[[   30  4769   134    29    23  6811    17     7 17953    57     3 29045
   5406    16     3 14775     6    57  4357  4240  1454    16   445  7075
    117    30  3318  2726  1950 17953    57     3 27220  6170    16     3
  14775     6     3 28640  4704    16   445  7075     1]]"
88e9e5ad0e4c369b15d81a4e18f7d12ff8fa9f1b,100.0,No,[[  0 465   1]],No,[[465   1]]
9a596bd3a1b504601d49c2bec92d1592d7635042,19.0476,ROC curves achieve a BLEU score of 82.61 and a,"[[    0     3 26893  8435     7  1984     3     9     3  8775 12062  2604
     13     3  4613     5  4241    11     3     9]]",Answer with content missing: (Table II) Proposed model has F1 score of  0.7220.,"[[11801    28   738  3586    10    41 20354  2466    61   749 12151   825
     65   377   536  2604    13  4097  5865  1755     5     1]]"
2ed02be0c183fca7031ccb8be3fd7bc109f3694b,0.0,"Compared to the current state-of-the-art, their improved performance improves by","[[    0     3 25236    12     8   750   538    18   858    18   532    18
   1408     6    70  3798   821  1172     7    57]]","1.08 points in ROUGE-L over our base pointer-generator model , 0.6 points in ROUGE-1","[[ 1300  4018   979    16   391 26260   427    18   434   147    69  1247
    500    49    18   729    49  1016   825     3     6     3 22787   979
     16   391 26260   427  2292     1]]"
6b9310b577c6232e3614a1612cbbbb17067b3886,11.1111,carefully writing the vernacular paragraph,[[   0 4321  913    8  548   29    9 4866 8986    1]]," if a vernacular paragraph contains more poetic images used in classical literature, its generated poem usually achieves higher score, poems generated from descriptive paragraphs achieve higher scores than from logical or philosophical paragraphs","[[    3    99     3     9   548    29     9  4866  8986  2579    72 25614
   1383   261    16 11702  6678     6   165  6126 14193  1086  1984     7
   1146  2604     6 18460  6126    45 25444  8986     7  1984  1146  7586
    145    45     3  6207    42 22466  8986     7     1]]"
c4a0c7b6f1a00f3233a5fe16240a98d9975701c0,100.0,No,[[  0 465   1]],No,[[465   1]]
dc4096b8bab0afcbbd4fbb015da2bea5d38251cd,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"we compressed English BERT using magnitude weight pruning BIBREF8 and observed the results on transfer learning to the General Language Understanding Evaluation (GLUE) benchmark BIBREF9, a diverse set of natural language understanding tasks including sentiment analysis, NLI, and textual similarity evaluation. ","[[   62 25423  1566   272 24203   338 20722  1293 31858     3  5972 25582
    371   927    11  6970     8   772    30  2025  1036    12     8  2146
  10509 17725 22714    41 13011  5078    61 15705     3  5972 25582   371
   1298     6     3     9  2399   356    13   793  1612  1705  4145   379
   6493  1693     6   445  8159     6    11  1499  3471  1126   485  5002
      5     1]]"
fbe5e513745d723aad711ceb91ce0c3c2ceb669e,100.0,None,[[    0 14794     1]],None,[[14794     1]]
bdc6664cec2b94b0b3769bc70a60914795f39574,0.0,QALD scores,[[    0     3 23008  9815  7586     1]],"average INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 values","[[ 1348  3388 20006 24030   632     3     6  3388 20006 24030   536     3
      6    11  3388 20006 24030   357  2620     1]]"
0117aa1266a37b0d2ef429f1b0653b9dde3677fe,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
4d4b9ff2da51b9e0255e5fab0b41dfe49a0d9012,100.0,No,[[  0 465   1]],No,[[465   1]]
487dc65bf8a8ecbf052cf05641caf1b90a502853,0.0,The New York Times Annotated Corpus BIBREF1,"[[    0    37   368  1060  5324   389  2264   920 10052   302     3  5972
  25582   371   536     1]]",three years of online news articles from June 2016 to June 2019,[[ 386  203   13  367 1506 2984   45 1515 1421   12 1515 1360    1]]
e84ba95c9a188fda4563f45e53fbc8728d8b5dab,100.0,one model per topic.,[[   0   80  825  399 2859    5    1]],One model per topic.,[[ 555  825  399 2859    5    1]]
29571867fe00346418b1ec36c3b7685f035e22ce,72.7273,"relation prediction, relation extraction, BIBREF14","[[    0  4689 21332     6  4689 16629     6     3  5972 25582   371  2534
      1]]","relation prediction, relation extraction, Open IE",[[ 4689 21332     6  4689 16629     6  2384     3  5091     1]]
6472f9d0a385be81e0970be91795b1b97aa5a9cf,0.0,No,[[  0 465   1]],"Answer with content missing: (list missing) 
Scheduled sampling: In our experiments, we found that models trained with scheduled sampling performed better (about 0.004 BLEU-4 on validation set) than the ones trained using teacher-forcing for the AVSD dataset. Hence, we use scheduled sampling for all the results we report in this paper.

Yes.","[[11801    28   738  3586    10    41  3350  3586    61 14890    26 17222
     10    86    69 12341     6    62   435    24  2250  4252    28  5018
  17222  3032   394    41  7932     3 10667   591     3  8775 12062  4278
     30 16148   356    61   145     8  2102  4252   338  3145    18  1161
     75    53    21     8     3  6968  7331 17953     5     3 13151     6
     62   169  5018 17222    21    66     8   772    62   934    16    48
   1040     5  2163     5     1]]"
0cfaca6f3f33ebdb338c5f991f6a7a33ff33844d,100.0,DeepDive BIBREF1,[[    0  9509 21313    15     3  5972 25582   371   536     1]],DeepDive BIBREF1,[[ 9509 21313    15     3  5972 25582   371   536     1]]
38e11663b03ac585863742044fd15a0e875ae9ab,100.0,peoples' sentiments expressed over social media,[[   0  151    7   31 6493    7 7103  147  569  783    1]], peoples' sentiments expressed over social media,[[ 151    7   31 6493    7 7103  147  569  783    1]]
2a7c40a72b6380e76511e722b4b02b3a1e5078fd,100.0,No,[[  0 465   1]],No,[[465   1]]
848ab388703c24faad79d83d254e4fd88ab27e2a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"'= ( , { ll k(h:, g:) if hV, gV

1 otherwise } )

where $_{h:}$ and $_{g:}$ denote the embedding representations of $h$ and $g$ , respectively.","[[    3    31  2423    41     3     6     3     2     3   195     3   157
    599   107    10     6     3   122    10    61     3    99     3   107
    553     6     3   122   553   209  2904     3     2     3    61   213
   1514   834     2   107    10     2  3229    11  1514   834     2   122
     10     2  3229    20  7977     8 25078    26    53  6497     7    13
   1514   107  3229    11  1514   122  3229     3     6  6898     5     1]]"
8df89988adff57279db10992846728ec4f500eaa,0.0,"Parallel Scanner, Parallel Scanner, Parallel Scanner, Parallel Scanner","[[    0 27535   180  1608   687     6 27535   180  1608   687     6 27535
    180  1608   687     6 27535   180  1608   687]]","Typical implementations of dynamic programming algorithms are serial in the length of the sequence, Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized, Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient","[[    3 31434  4432     7    13  4896  6020 16783    33 10501    16     8
   2475    13     8  5932     6   638 31148   138 11641    19   237    72
     13    46   962    21   260     7    53 16783     6    84  1178    36
     38  1153  8449  1601     6  4877    21   119  4772  1007     7     6
    224    38  4303    11  9858     6   175  2673    33   893  2684    42
    182  2594    16 16995     1]]"
53c8416f2983e07a7fa33bcb4c4281bbf49c8164,20.6897,LMs that write paragraph-length text with the coherence of human writing,"[[    0     3 11160     7    24  1431  8986    18 19457  1499    28     8
    576   760  1433    13   936   913     1]]",Outputs from models that are better in the sense of cross-entropy or perplexity are harder to distinguish from authentic text.,"[[ 3387  2562     7    45  2250    24    33   394    16     8  1254    13
   2269    18    35 12395    63    42   399  9247   485    33  7501    12
  15849    45  6876  1499     5     1]]"
83f24e4bbf9de82d560cdde64b91d6d672def6bf,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
524abe0ab77db168d5b2f0b68dba0982ac5c1d8e,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
663b36f99ad2422f4d3a8c6398ebf55ceab7770d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],words extracted from YouTube video comments and descriptions for all YouTube videos shared in the user's timeline,"[[ 1234 21527    45  5343   671  2622    11 15293    21    66  5343  3075
   2471    16     8  1139    31     7 13618     1]]"
6f8881e60fdaca7c1b35a5acc7125994bb1206a3,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
c77d6061d260f627f2a29a63718243bab5a6ed5a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],the training dataset is large while the target dataset is usually much smaller,"[[    8   761 17953    19   508   298     8  2387 17953    19  1086   231
   2755     1]]"
8ea4bd4c1d8a466da386d16e4844ea932c44a412,23.5294,text-code parallel corpus,[[    0  1499    18  4978  8449 11736   302     1]],A parallel corpus where the source is an English expression of code and the target is Python code.,"[[   71  8449 11736   302   213     8  1391    19    46  1566  3893    13
   1081    11     8  2387    19 20737  1081     5     1]]"
5908d7fb6c48f975c5dfc5b19bb0765581df2b25,0.0,HEOT,[[   0    3 6021 6951    1]],3189 rows of text messages,[[  220 25312 17918    13  1499  4175     1]]
8670989ca39214eda6c1d1d272457a3f3a92818b,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
2d1c0618b6106a57777b8d6bbf897712d9db7abc,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
3d7ab856a5cade7ab374fc2f2713a4d0a30bbd56,22.2222,multilingual word representations,[[    0  1249 25207  1448  6497     7     1]], a multilingual word representation which aims to learn a linear mapping from a source to a target embedding space,"[[    3     9  1249 25207  1448  6497    84     3  8345    12   669     3
      9 13080 14670    45     3     9  1391    12     3     9  2387 25078
     26    53   628     1]]"
90b2154ec3723f770c74d255ddfcf7972fe136a2,0.0,"humorous, romantic, and click-baity",[[    0 28845     6  7966     6    11  1214    18   115     9   485     1]],human evaluation task about the style strength,[[ 936 5002 2491   81    8  869 2793    1]]
36a9230fadf997d3b0c5fc8af8d89bd48bf04f12,6.0606,BiLSTM with max pooling,[[   0 2106 7600 2305   28 9858 2201   53    1]],"Answer with content missing: (Skip-thought vectors-Natural Language Inference paragraphs) The encoder for the current sentence and the decoders for the previous (STP) and next sentence (STN) are typically parameterized as separate RNNs
- RNN","[[11801    28   738  3586    10    41   134  2168   102    18 11841    17
  12938     7    18   567 13149    40 10509    86 11788  8986     7    61
     37 23734    52    21     8   750  7142    11     8    20  4978    52
      7    21     8  1767    41   134  7150    61    11   416  7142    41
    134 11053    61    33  3115 15577  1601    38  2450   391 17235     7
      3    18   391 17235     1]]"
247e1fe052230458ce11b98e3637acf0b86795cd,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
226ae469a65611f041de3ae545be0e386dba7d19,20.0,headlines in the newsgroups 'WorldPost' and 'Crime',"[[    0 12392     7    16     8  1506 10739     7     3    31 17954 22507
     31    11     3    31   254  5397    15    31]]",Wikipedea Corpus and BooksCorpus,[[ 2142  2168  3138    15     9 10052   302    11 10022 13026  7800     1]]
cdf7e60150a166d41baed9dad539e3b93b544624,0.0,"LSTM, LSTM+Vec, LSTM+Vec","[[   0    3 7600 2305    6    3 7600 2305 1220  553   15   75    6    3
  7600 2305 1220  553   15   75]]","WeedsPrec BIBREF8, invCL BIBREF11, SLQS model, cosine similarity","[[  101    15    26     7 10572    75     3  5972 25582   371 11864    16
    208  8440     3  5972 25582   371  2596     6     3  5629  2247   134
    825     6   576     7   630  1126   485     1]]"
1165fb0b400ec1c521c1aef7a4e590f76fee1279,0.0,By using a mixture of statistical and linguistic approaches.,"[[    0   938   338     3     9  4989    13 11775    11     3 24703  6315
      5     1]]",The data from collected travel surveys is used to model travel behavior.,"[[   37   331    45  4759  1111 13484    19   261    12   825  1111  3889
      5     1]]"
cda4612b4bda3538d19f4b43dde7bc30c1eda4e5,0.0,"Statistic regression, Rank-based model",[[    0  7135  3040 26625     6     3 22557    18   390   825     1]],"TextRank, Word2vec BIBREF19, GloVe BIBREF20","[[ 5027 22557     6  4467   357   162    75     3  5972 25582   371  2294
      6  9840   553    15     3  5972 25582   371  1755     1]]"
86f24ecc89e743bb1534ac160d08859493afafe9,0.0,encoding the syntactic information is an important step in the learning process,"[[   0    3   35 9886    8 8953   17 2708  447  251   19   46  359 1147
    16    8 1036  433    1]]","dependency head and dependency relation label, denoted as Dep and Rel for short, Tree-based Position Feature (TPF) as Dependency Path (DepPath), Shortest Dependency Path (SDP) as Relation Path (RelPath)","[[27804   819    11 27804  4689  3783     6    20  7977    26    38   374
    102    11   419    40    21   710     6  7552    18   390 14258     3
  16772    41  7150   371    61    38 30718  4392 12465    41  2962   102
    345     9   189   201  7110   222 30718  4392 12465    41   134  7410
     61    38 28898 12465    41  1649    40   345     9   189    61     1]]"
cfffc94518d64cb3c8789395707e4336676e0345,16.0,"classification, regression, neural methods",[[    0 13774     6 26625     6 24228  2254     1]]," Support Vector Regression (SVR) and Support Vector Classification (SVC), deep learning regression models of BIBREF2 to convert them to classification models","[[ 4224 29011   419 22430    41   134 13556    61    11  4224 29011  4501
   2420    41   134  7431   201  1659  1036 26625  2250    13     3  5972
  25582   371   357    12  5755   135    12 13774  2250     1]]"
7aae4533dbf097992f23fb2e0574ec5c891ca236,0.0,CNLP,[[    0     3 10077  6892     1]],"BTEC corpus, the CSTAR03 and IWSLT04 held out sets, the NIST2008 Open Machine Translation Campaign","[[  272 23933 11736   302     6     8   205 19481  4928    11    27  8439
   9012  6348  1213    91  3369     6     8   445 13582 16128  2384  5879
  24527 18409     1]]"
fed230cef7c130f6040fb04304a33bbc17ca3a36,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"for each multiple-choice question $(q,A) \in Q_\mathit {tr}$ and each choice $a \in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S, take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \in A$ and over all questions in $Q_\mathit {tr}$","[[   21   284  1317    18  3995   867   822  1514   599  1824     6   188
     61     3     2    77  1593   834     2  3357 10536     3     2    17
     52     2  3229    11   284  1160  1514     9     3     2    77    71
   3229     3     6    62   169    66   529    18  7618  6051 14145     7
     16  1514  1824  3229    11  1514     9  3229    38    46  1289 10057
  25001 11417   581   180     6   240     8   420  2382  8046     6   661
   2384     3  5091     3   208  8525    11 12955     8     3  5490     3
     17   413   965   147    66  1514     9     3     2    77    71  3229
     11   147    66   746    16  1514  2247   834     2  3357 10536     3
      2    17    52     2  3229     1]]"
0cf6d52d7eafd43ff961377572bccefc29caf612,100.0,AMT,[[   0   71 7323    1]],AMT,[[  71 7323    1]]
12d77ac09c659d2e04b5e3955a283101c3ad1058,0.0,"TEXT-SEM, TSUN, WSJM, WS","[[   0    3 3463    4  382   18  134 6037    6    3 4578 7443    6    3
  8439  683  329    6    3 8439]]","Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, Health Care Reform (HCR)","[[19796     3    18  3046  4892  2998   295 10052   302    41  4209   134
  10052   302   201 22439     3    18  3046  4892  2998   295 10052   302
      6  1685  2686 18490    41   566  4545    61     1]]"
6aee16c4f319a190c2a451c1c099b66162299a28,0.0,manually evaluated,[[    0 12616 14434     1]],"(1) grammatical correctness, (2) contextual coherence, (3) emotional appropriateness","[[ 5637     3  5096  4992   138  2024   655     6  6499 28131   576   760
   1433     6 10153  3973  2016   655     1]]"
5816ebf15e31bdf70e1de8234132e146d64e31eb,0.0,univariate measures of correlation between text features and party affiliation allow to relate the predictions to,"[[    0    73    23  9504   342  3629    13 18712   344  1499   753    11
   1088 24405   995    12  9098     8 20099    12]]", multinomial logistic regression,[[ 1249  3114    23   138 28820 26625     1]]
8e857e44e4233193c7b2d538e520d37be3ae1552,0.0,represent the state using natural language,[[   0 4221    8  538  338  793 1612    1]],"a basic scenario, a health gathering scenario, a scenario in which the agent must take cover from fireballs, a scenario in which the agent must defend itself from charging enemies, and a super scenario, where a mixture of the above scenarios","[[    3     9  1857  8616     6     3     9   533  7241  8616     6     3
      9  8616    16    84     8  3102   398   240  1189    45  1472  3184
      7     6     3     9  8616    16    84     8  3102   398 11220  1402
     45 10871 14645     6    11     3     9  1355  8616     6   213     3
      9  4989    13     8   756 13911     1]]"
62a6382157d5f9c1dce6e6c24ac5994442053002,100.0,"accuracy, normalized mutual information",[[   0 7452    6 1389 1601 8543  251    1]],"accuracy, normalized mutual information",[[7452    6 1389 1601 8543  251    1]]
e292676c8c75dd3711efd0e008423c11077938b1,100.0,LSTM and BERT,[[    0     3  7600  2305    11   272 24203     1]],LSTM and BERT ,[[    3  7600  2305    11   272 24203     1]]
a88f8cae1f59cdc4f1f645e496d6d2ac4d9fba1b,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5dc1aca619323ea0d4717d1f825606b2b7c21f01,87.5,"Northeast U.S., West U.S. and South U.S.","[[    0 21732   412     5   134     5     6  1244   412     5   134     5
     11  1013   412     5   134     5     1]]","Northeast U.S, South U.S., West U.S. and Midwest U.S.","[[21732   412     5   134     6  1013   412     5   134     5     6  1244
    412     5   134     5    11 22576   412     5   134     5     1]]"
cda4612b4bda3538d19f4b43dde7bc30c1eda4e5,5.5556,"Statistic regression, Rank-based model",[[    0  7135  3040 26625     6     3 22557    18   390   825     1]],"automated attribute-value extraction, score the attributes using the Bayes model, evaluate their importance with several different frequency metrics, aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model, OntoRank algorithm","[[10069 15816    18 12097 16629     6  2604     8 12978   338     8  2474
     15     7   825     6  6825    70  3172    28   633   315  7321 15905
      6 12955     8  1293     7    45   315  2836   139    80  4700  4541
    485  2604   338     3     9 29153   180 12623   825     6   461   235
  22557 12628     1]]"
e737cfe0f6cfc6d3ac6bec32231d9c893bfc3fc9,0.0,Unanswerable,[[   0  597 3247 3321  179    1]]," filter deletes all KB triples where the correct answer (e.g., Apple) is a case-insensitive substring of the subject entity name (e.g., Apple Watch), person name filter uses cloze-style questions to elicit name associations inherent in BERT, and deletes KB triples that correlate with them","[[ 4191  9268     7    66     3 17827 12063     7   213     8  2024  1525
     41    15     5   122     5     6  2184    61    19     3     9   495
     18    77 22118   769 16099    13     8  1426 10409   564    41    15
      5   122     5     6  2184  4195   201   568   564  4191  2284     3
   3903   776    18  4084   746    12     3    15 11192   564 10906 18340
     16   272 24203     6    11  9268     7     3 17827 12063     7    24
  30575    28   135     1]]"
41830ebb8369a24d490e504b7cdeeeaa9b09fd9c,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
7aab78e90ba1336950a2b0534cc0cb214b96b4fd,23.5294,the target-side affixes,[[   0    8 2387   18 1583    3 4127 2407   15    7    1]],"an additional morphology table including target-side affixes., We inject the decoder with morphological properties of the target language.","[[   46  1151     3  8886  1863   953   379  2387    18  1583     3  4127
   2407    15     7     5     6   101 15823     8    20  4978    52    28
      3  8886  4478  2605    13     8  2387  1612     5     1]]"
5efed109940bf74ed0a9d4a5e97a535502b23d27,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
7f452eb145d486c15ac4d1107fc914e48ebba60f,100.0,"the Common Voice website, iPhone app",[[    0     8  7155 12347   475     6  3146  1120     1]],"the Common Voice website,  iPhone app",[[    8  7155 12347   475     6  3146  1120     1]]
4c71ed7d30ee44cf85ffbd7756b985e32e8e07da,0.0,"statistical machine translation, part-of-speech tagging, chunking, and","[[    0 11775  1437  7314     6   294    18   858    18     7   855 10217
      3    17 15242     6 16749    53     6    11]]","document categorization, regression tasks",[[ 1708  9624   122   127  1707     6 26625  4145     1]]
7cdce4222cea6955b656c1a3df1129bb8119e2d0,55.1724,decision trees to predict individual hidden state dimensions,[[   0 1357 3124   12 9689  928 5697  538 8393    1]],"decision trees to predict individual hidden state dimensions, apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters","[[ 1357  3124    12  9689   928  5697   538  8393     6  1581     3   157
     18   526  3247  9068    53    12     8     3  7600  2305   538 12938
      7     6    11   945    18  4978     8   761   331    28     8  9068
      7     1]]"
3efc0981e7f959d916aa8bb32ab1c347b8474ff8,10.3896,emoji features are replaced by describing text using the Python emoji package,"[[    0     3    15    51 21892   753    33  5821    57     3 16012  1499
    338     8 20737     3    15    51 21892  2642]]","Our lexical features include 1-, 2-, and 3-grams in both word and character levels., number of characters and the number of words, POS tags, 300-dimensional pre-trained word embeddings from GloVe, latent semantic indexing, tweet representation by applying the Brown clustering algorithm, positive words (e.g., love), negative words (e.g., awful), positive emoji icon and negative emoji icon, boolean features that check whether or not a negation word is in a tweet","[[  421     3 30949   138   753   560  8218     6  8401     6    11  5354
   5096     7    16   321  1448    11  1848  1425     5     6   381    13
   2850    11     8   381    13  1234     6     3 16034 12391     6  3147
     18 11619   554    18    17 10761  1448 25078    26    53     7    45
   9840   553    15     6    50  4669 27632  5538    53     6 10657  6497
     57  6247     8  3899  9068    53 12628     6  1465  1234    41    15
      5   122     5     6   333   201  2841  1234    41    15     5   122
      5     6 16242   201  1465     3    15    51 21892  6705    11  2841
      3    15    51 21892  6705     6     3 12840   109   152   753    24
    691   823    42    59     3     9 14261   257  1448    19    16     3
      9 10657     1]]"
ed4fb6bce855ca932548689e45fde21f26a71035,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
3d99bc8ab2f36d4742e408f211bec154bc6696f7,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
22c36082b00f677e054f0f0395ed685808965a02,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
bfce2afe7a4b71f9127d4f9ef479a0bfb16eaf76,8.0,"F1 score, F1 score, and F1 score",[[   0  377  536 2604    6  377  536 2604    6   11  377  536 2604    1]],rating questions on a scale of 1-5 based on fluency of language used and relevance of the question to the context,"[[ 5773   746    30     3     9  2643    13   209  4525     3   390    30
   6720  4392    13  1612   261    11 20208    13     8   822    12     8
   2625     1]]"
b6858c505936d981747962eae755a81489f62858,18.1818,"Bi-LSTMs BIBREF22, LSTM-Net BIBRE","[[    0  2106    18  7600  2305     7     3  5972 25582   371  2884     6
      3  7600  2305    18  9688     3  5972 25582]]","BiLSTMs + CRF architecture BIBREF36, sententce-state LSTM BIBREF21","[[ 2106  7600  2305     7  1768   205  8556  4648     3  5972 25582   371
   3420     6  1622   295   565    18  5540     3  7600  2305     3  5972
  25582   371  2658     1]]"
0d0959dba3f7c15ee4f5cdee51682656c4abbd8f,82.3529,"Sememes are minimum semantic units of word meanings, and the meaning of each word sense","[[    0   679   526  2687    33  2559 27632  3173    13  1448  2530     7
      6    11     8  2530    13   284  1448  1254]]","Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed of several sememes","[[  679   526  2687    33  2559 27632  3173    13  1448  2530     7     6
     11     8  2530    13   284  1448  1254    19  3115 10431    13   633
    142   526  2687     1]]"
2a058f8f6bd6f8e80e8452e1dba9f8db5e3c7de8,8.0,"a polar coordinate system is created from a set of entities, which are mapped","[[    0     3     9     3  9618 11639   358    19   990    45     3     9
    356    13 12311     6    84    33     3 28400]]","radial coordinate and the angular coordinates correspond to the modulus part and the phase part, respectively","[[    3  5883   138 11639    11     8     3  1468  4885 11639     7 10423
     12     8  7246   302   294    11     8  3944   294     6  6898     1]]"
31b20a4bab09450267dfa42884227103743e3426,9.0909,"BIBREF13, BIBREF16","[[    0     3  5972 25582   371  2368     6     3  5972 25582   371  2938
      1]]","entity types or concepts BIBREF13, relations paths BIBREF17,  textual descriptions BIBREF11, BIBREF12, logical rules BIBREF23, deep neural network models BIBREF24","[[10409  1308    42  6085     3  5972 25582   371  2368     6  5836 13704
      3  5972 25582   371  2517     6  1499  3471 15293     3  5972 25582
    371  2596     6     3  5972 25582   371  2122     6     3  6207  2219
      3  5972 25582   371  2773     6  1659 24228  1229  2250     3  5972
  25582   371  2266     1]]"
4fdc707fae5747fceae68199851e3c3186ab8307,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
edc43e1b75c0970b7003deeabfe3ad247cb1ed83,100.0,Akkadian.,[[   0 4823 1258 8603    5    1]],Akkadian.,[[4823 1258 8603    5    1]]
79d999bdf8a343ce5b2739db3833661a1deab742,13.3333,No BLEU errors,[[    0   465     3  8775 12062  6854     1]],"No extraction, No annotation, Wrong range, Wrong tag, Wrong range and tag","[[  465 16629     6   465 30729     6   549    52  2444   620     6   549
     52  2444  7860     6   549    52  2444   620    11  7860     1]]"
aa979aed5a454b6705d0085ba2777859feb6fc62,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
c20b012ad31da46642c553ce462bc0aad56912db,0.0,"SST-2, SST-3, SST-4, SST-4, SST-4","[[   0  180 4209 4949    6  180 4209 3486    6  180 4209 4278    6  180
  4209 4278    6  180 4209 4278]]"," movie sentence polarity dataset from BIBREF19, laptop and restaurant datasets collected from SemEval-201, we collected 2,000 reviews for each domain from the same review source","[[ 1974  7142     3  9618   485 17953    45     3  5972 25582   371  2294
      6  4544    11  2062 17953     7  4759    45   679    51   427  2165
     18 22772     6    62  4759     3  8630  2456    21   284  3303    45
      8   337  1132  1391     1]]"
9219eef636ddb020b9d394868959325562410f83,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"BIBREF7, BIBREF39, BIBREF37, LitisMind, Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN","[[    3  5972 25582   371   940     6     3  5972 25582   371  3288     6
      3  5972 25582   371  4118     6 15507   159 12858    26     6 24210
      3    35 12395    63     6   180 12623     6     3  7600  2305     6
   2106    18  7600  2305     6    11 19602     1]]"
79cfd1b82c72d18e2279792c66a042c0e9dfa6b7,100.0,DyNet,[[    0 12991  9688     1]],DyNet,[[12991  9688     1]]
81dc39ee6cdacf90d5f0f62134bf390a29146c65,8.6957,typologically similar languages,[[    0 23042  6207   120  1126  8024     1]],contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks,"[[28131 25078    26    53     7   103    59  4221  1126 27632 24666 19467
     11  2459    79    33    59  1461   178   179    21  5733    18 11159
   2269    18 25207  4145     1]]"
af82043e7d046c2fb1ed86ef9b48c35492e6a48c,100.0,No,[[  0 465   1]],No,[[465   1]]
5be94c7c54593144ba2ac79729d7545f27c79d37,100.0,not researched as much as English,[[    0    59 24172    38   231    38  1566     1]],not researched as much as English,[[   59 24172    38   231    38  1566     1]]
81303f605da57ddd836b7c121490b0ebb47c60e7,0.0,"BIBREF3, BIBREF4, BIBREF9","[[    0     3  5972 25582   371  6355     3  5972 25582   371  8525     3
   5972 25582   371  1298     1]]","Sexist/Racist (SR) data set, HATE dataset, HAR","[[  180 12135    87   448     9    75   343    41  6857    61   331   356
      6   454  6048 17953     6     3 25430     1]]"
db9021ddd4593f6fadf172710468e2fdcea99674,0.0,text-code parallel branch,[[   0 1499   18 4978 8449 6421    1]],Unanswerable,[[ 597 3247 3321  179    1]]
be3e020ba84bc53dfb90b8acaf549004b66e31e2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"precision, recall, and F-measure on boundaries (BP, BR, BF), and tokens (WP, WR, WF),  exact-match (X) metric","[[11723     6  7881     6    11   377    18 31038    30 11814    41 11165
      6     3  6934     6     3 19780   201    11 14145     7    41   518
    345     6     3 15472     6   549   371   201  2883    18 19515    41
      4    61     3  7959     1]]"
a381ba83a08148ce0324b48b8ff35128e66f580a,0.0,weak learners used in ensemble methods,[[    0  5676 15257   261    16  8784  2254     1]],"High-order CNN, Tree-LSTM, DRNN, DCNN, CNN-MC, NBoW and SVM ","[[ 1592    18  9397 19602     6  7552    18  7600  2305     6     3  3913
  17235     6  5795 17235     6 19602    18  3698     6     3 14972    32
    518    11   180 12623     1]]"
0a050658d09f3c6e21e9ab828dc18e59b147cf7c,100.0,No,[[  0 465   1]],No,[[465   1]]
11c77ee117cb4de825016b6ccff59ff021f84a38,0.0,their model outperforms by 0.61 in the cross-domain evaluation,"[[    0    70   825    91   883  2032     7    57  4097  4241    16     8
   2269    18 22999  5002     1]]","$2.2\%$ absolute accuracy improvement on the laptops test set, $3.6\%$ accuracy improvement on the restaurants test set","[[ 1514 15300     2  1454  3229  6097  7452  4179    30     8  4544     7
    794   356     6  1514 23074     2  1454  3229  7452  4179    30     8
   3661   794   356     1]]"
5c95808cd3ee9585f05ef573b0d4a52e86d04c60,100.0,CL Journal and EMNLP conference,[[    0 11175  3559    11     3  6037   567  6892  2542     1]],CL Journal and EMNLP conference,[[11175  3559    11     3  6037   567  6892  2542     1]]
7ee29d657ccb8eb9d5ec64d4afc3ca8b5f3bcc9f,85.7143,Best performance achieved is 0.86 F1 score,[[   0 1648  821 5153   19 4097 3840  377  536 2604    1]],Best performance achieved is 0.72 F1 score,[[1648  821 5153   19 4097 5865  377  536 2604    1]]
599d9ca21bbe2dbe95b08cf44dfc7537bde06f98,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
8d8300d88283c73424c8f301ad9fdd733845eb47,0.0,crowdsourcing,[[    0  4374 19035     1]],confusion matrices of labels between annotators,[[12413     3 20705    15     7    13 11241   344    46  2264  6230     1]]
7c45c6e5db6cfca2d6de8751e28403b35420ae38,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],input byte embedding matrix has dimensionality 256,"[[ 3785     3 17770 25078    26    53 16826    65     3 11619   485     3
  19337     1]]"
9fe4a2a5b9e5cf29310ab428922cc8e7b2fc1d11,0.0,"MultiRC BIBREF4, MCTest BIBREF5, BERT BI","[[    0  4908  4902     3  5972 25582   371  8525   283  6227   222     3
   5972 25582   371 11116   272 24203     3  5972]]","FTLM++, BERT-large, XLNet","[[    3  6245 11160 16702     6   272 24203    18 15599     6     3     4
    434  9688     1]]"
a6419207d2299f25e2688517d1580b7ba07c8e4b,100.0,No,[[  0 465   1]],No,[[465   1]]
92240eeab107a4f636705b88f00cefc4f0782846,100.0,No,[[  0 465   1]],No,[[465   1]]
63a77d2640df8315bf0bc3925fdd7e27132b1244,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
879bec20c0fdfda952444018e9435f91e34d8788,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
6a099dfe354a79936b59d651ba0887d9f586eaaf,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
321429282557e79061fe2fe02a9467f3d0118cdd,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"phrase-based word embedding, Abstract Syntax Tree(AST)","[[ 9261    18   390  1448 25078    26    53     6 20114  8951 14727  7552
    599 12510    61     1]]"
b2fab9ffbcf1d6ec6d18a05aeb6e3ab9a4dbf2ae,20.0,We use a set of supervised learning tasks to train our models for a set of,"[[    0   101   169     3     9   356    13     3 23313  1036  4145    12
   2412    69  2250    21     3     9   356    13]]","Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL).","[[ 5309     6    69  2491  2311  2250    12     3     2 17012  1452    31
   1066   145 14987    18 17012    53   135    28   251     5   100  4061
      7     3 27396    38     3     9 29372  1357    18  5239   682   183
     35   179    12 28050  1036    41 12831   137     1]]"
b0fbd4b0f02b877a0d3df1d8ccc47d90dd49147c,28.5714,"constituent parser, dependency parser",[[    0 17429   260     7    49     6 27804   260     7    49     1]],"token representation, self-attention encoder,, Constituent Parsing Decoder,  Dependency Parsing Decoder","[[14145  6497     6  1044    18 25615 23734    52     6     6 22636   295
   2180     7    53  4451 13487     6 30718  4392  2180     7    53  4451
  13487     1]]"
b6f5860fc4a9a763ddc5edaf6d8df0eb52125c9e,92.3077,"English, Chinese, Japanese, French, Japanese and Arabic","[[    0  1566     6  2830     6  4318     6  2379     6  4318    11 19248
      1]]","English, Chinese, French, Japanese and Arabic",[[ 1566     6  2830     6  2379     6  4318    11 19248     1]]
0106bd9d54e2f343cc5f30bb09a5dbdd171e964b,100.0,twitter,[[    0 19010     1]],twitter ,[[19010     1]]
71f2b368228a748fd348f1abf540236568a61b07,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],unshuffled version of the French OSCAR corpus,"[[   73 14279 10105    26   988    13     8  2379  6328   254  4280 11736
    302     1]]"
46570c8faaeefecc8232cfc2faab0005faaba35f,0.0,"Twitter, Reddit, Reddit, Reddit, Reddit, Red","[[   0 3046    6 1624   26  155    6 1624   26  155    6 1624   26  155
     6 1624   26  155    6 1624]]","SemEval 2018 Task 3, BIBREF20, BIBREF4, SARC 2.0, SARC 2.0 pol, Sarcasm Corpus V1 (SC-V1), Sarcasm Corpus V2 (SC-V2)","[[  679    51   427  2165   846 16107  6180     3  5972 25582   371  1755
      6     3  5972 25582   371  8525   180 18971  6864     6   180 18971
   6864     3  3233     6  9422  6769    51 10052   302   584   536    41
   4112    18   553  6982     6  9422  6769    51 10052   302   584   357
     41  4112    18   553  7318     1]]"
f95097cf4a0dc036fd8b80c007cd8d7a157b7816,0.0,Using the top few sentences as the target summary,[[    0     3  3626     8   420   360 16513    38     8  2387  9251     1]],"$\textsc {Lead-X}$, $\textsc {PTGen}$, $\textsc {DRM}$, $\textsc {TConvS2S}$,  $\textsc {BottomUp}$, ABS, DRGD, SEQ$^3$, BottleSum, GPT-2","[[ 1514     2  6327     7    75     3     2  2796     9    26    18     4
      2  3229     6  1514     2  6327     7    75     3     2  6383 13714
      2  3229     6  1514     2  6327     7    75     3     2  3913   329
      2  3229     6  1514     2  6327     7    75     3     2  3838   106
    208   134   357   134     2  3229     6  1514     2  6327     7    75
      3     2 26465   235    51 11161     2  3229     6 20798     6     3
   3913 18405     6   180 23346  3229     2   519  3229     6 25921   134
    440     6   350  6383  4949     1]]"
54fa5196d0e6d5e84955548f4ef51bfd9b707a32,100.0,English to French and English to German,[[   0 1566   12 2379   11 1566   12 2968    1]],English to French and English to German,[[1566   12 2379   11 1566   12 2968    1]]
56e58bdf0df76ad1599021801f6d4c7b77953e29,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"influence of each word on the score of the correct relation, that impact should and does still correlate with human judgments","[[ 2860    13   284  1448    30     8  2604    13     8  2024  4689     6
     24  1113   225    11   405   341 30575    28   936  7661     7     1]]"
0154d8be772193bfd70194110f125813057413a4,9.0909,"encodings that are similar to BERT and ERNIE, encodings","[[    0     3    35  9886     7    24    33  1126    12   272 24203    11
      3 25992  5091     6     3    35  9886     7]]","mean-pooling their outputs (AVG), concatenating the entity and its name with a slash symbol (CONCAT)","[[ 1243    18 13194    53    70  3911     7    41   188 17217   201   975
   2138    35  1014     8 10409    11   165   564    28     3     9     3
      7  8058  6083    41 17752 18911    61     1]]"
3b745f086fb5849e7ce7ce2c02ccbde7cfdedda5,43.75,overall accuracy improves by 0.86 on sentiment classification tasks and 0.86 on intent classification tasks,"[[    0  1879  7452  1172     7    57  4097  3840    30  6493 13774  4145
     11  4097  3840    30  9508 13774  4145     1]]",In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average,"[[   86     8  6493 13774  2491    57     3  6370    12     3  5953    11
     16     8  9508 13774  2491    57     3 23758  5988    30  1348     1]]"
a9c5252173d3df1c06c770c180a77520de68531b,100.0,"CoNLL 2003, CoNLL 2000",[[    0   638   567 10376  3888     6   638   567 10376  2766     1]],"CoNLL 2003, CoNLL 2000",[[  638   567 10376  3888     6   638   567 10376  2766     1]]
fc54736e67f748f804e8f66b3aaaea7f5e55b209,0.0,We compare our tagger with the bilstm-aux tagger BIBRE,"[[    0   101  4048    69  7860  1304    28     8     3  3727     7    17
     51    18  1724  7860  1304     3  5972 25582]]",conduct experiments using artificially constructed unnormalized text by corrupting words in the normal dev set,"[[ 3498 12341   338  7353   120  8520    73 12110  1601  1499    57 17261
     53  1234    16     8  1389    20   208   356     1]]"
4640793d82aa7db30ad7b88c0bf0a1030e636558,0.0,"CoNLL 2003, CoNLL 2000",[[    0   638   567 10376  3888     6   638   567 10376  2766     1]],"Chiu and Nichols (2016), Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), Hashimoto et al. (2016), Søgaard and Goldberg (2016) ","[[ 2695    76    11  2504 14297     7 26228     6 13667   109     3    15
     17   491     5 26228     6  1534    11  1546   208    63 26228     6
  21078     3    15    17   491     5     3 26224     6  1626  5605 11188
      3    15    17   491     5 26228     6   180     2   122     9   986
     11  2540  2235 26228     1]]"
013a8525dbf7a9e1e69acc1cff18bb7b8261cbad,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
7fdeef2b1c8f6bd5d7c3a44e533d8aae2bbc155f,84.2105,Twitter features about ObamaCare' in USA collected during march 2010,"[[    0  3046   753    81     3     2   667   115   265     9  6936    15
     31    16  2312  4759   383 10556  2735     1]]",tweets about `ObamaCare' in USA collected during march 2010,"[[10657     7    81     3     2   667   115   265     9  6936    15    31
     16  2312  4759   383 10556  2735     1]]"
8f838ec579f2609b01227da3d8c77860ac1b39d2,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
3c0eaa2e24c1442d988814318de5f25729696ef5,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
72122e0bc5da1d07c0dadb3401aab2acd748424d,100.0,20K,[[  0 460 439   1]],20K,[[460 439   1]]
12c6ca435f4fcd4ad5ea5c0d76d6ebb9d0be9177,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
c554a453b6b99d8b59e4ef1511b1b506ff6e5aa4,1.3245,using supervised annotators,[[    0   338     3 23313    46  2264  6230     1]],"Questions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified: we found selecting queries where the first word is in a manually constructed set of indicator words and are of sufficient length, to be effective. Questions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing. Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as “not answerable"" if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question's answer is “yes"" or “no""","[[14218    33     3  9094    45 19581  1601     6 12955    26 13154    12
      8  1163   960  1948     5  7227  2593    24    33   952    12    36
   4273    87    29    32   746    33     3    88   450   343  6402  4313
     10    62   435  9581 13154   213     8   166  1448    19    16     3
      9 12616  8520   356    13 11169  1234    11    33    13  6684  2475
      6    12    36  1231     5 14218    33   163  2697     3    99     3
      9 16885   543    19  3666    38    80    13     8   166   874   772
      6    16    84   495     8   822    11 16885   543    33   787    12
      3     9   936    46  2264  1016    21   856  3026     5   389  2264
   6230  3783   822    87  8372 14152    16     3     9   386    18  7910
    433     5  1485     6    79  2204     3    99     8   822    19   207
      6  2530    34    19     3   287 22459  2296     6    73 24621  1162
      6    11     3 22686   685  3471   251     5   100  7661    19   263
    274     8    46  2264  1016   217     7     8 16885   543     5  3021
      6    21   207   746     6    46  2264  6230   253     3     9  5454
    441     8  1708    24  2579   631   251    12  1525     8   822     5
    389  2264  6230    54  3946   746    38   105  2264  1525   179   121
      3    99     8 16885  1108   405    59  3480     8  6709   251     5
   4213     6    46  2264  6230  3946   823     8   822    31     7  1525
     19   105 10070   121    42   105    29    32   121     1]]"
e2a637f1d93e1ea9f29c96ff0fc6bc017209065b,100.0,hand crafted by users,[[   0  609    3 8810   57 1105    1]],hand crafted by users,[[ 609    3 8810   57 1105    1]]
ba6422e22297c7eb0baa381225a2f146b9621791,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Difference is around 1 BLEU score lower on average than state of the art methods.,"[[27187    19   300   209     3  8775 12062  2604  1364    30  1348   145
    538    13     8   768  2254     5     1]]"
7488855f09b97eb6a027212fb7ace1d338f36a2b,100.0,No,[[  0 465   1]],No,[[465   1]]
9df4a7bd0abb99ae81f0ebb29c488f1caa0f268f,5.8824,they use a hierarchical pipeline system to train the retrieval modules,"[[    0    79   169     3     9  1382  7064  1950 12045   358    12  2412
      8 24515   138 10561     1]]",We treated the neural semantic retrieval at both the paragraph and sentence level as binary classification problems with models' parameters updated by minimizing binary cross entropy loss.,"[[  101  4260     8 24228 27632 24515   138    44   321     8  8986    11
   7142   593    38 14865 13774   982    28  2250    31  8755  3250    57
      3 28807 14865  2269     3    35 12395    63  1453     5     1]]"
e40df8c685a28b98006c47808f506def68f30e26,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
578d0b23cb983b445b1a256a34f969b34d332075,0.0,"BIBREF13, BIBREF21","[[    0     3  5972 25582   371  2368     6     3  5972 25582   371  2658
      1]]","Arap-Tweet BIBREF19 , an in-house Twitter dataset for gender, the MADAR shared task 2 BIBREF20, the LAMA-DINA dataset from BIBREF22, LAMA-DIST, Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34","[[   71  5846    18   382  1123    15    17     3  5972 25582   371  2294
      3     6    46    16    18  1840  3046 17953    21  7285     6     8
    283 16759   448  2471  2491   204     3  5972 25582   371  1755     6
      8   301 21250    18   308 21116 17953    45     3  5972 25582   371
   2884     6   301 21250    18   308 13582     6 19248 10657     7  1883
     57  4699  5767  1741   371 14132  8584  2471    18 23615     3  5972
  25582   371  2266     6     3  5972 25582   371  1828     6     3  5972
  25582   371  2688     6     3  5972 25582   371  2555     6     3  5972
  25582   371  4347     3  5972 25582   371  2577     6     3  5972 25582
    371  3166     6     3  5972 25582   371  1458     6     3  5972 25582
    371  3341     6     3  5972 25582   371  2668     6     3  5972 25582
    371  4201     6     3  5972 25582   371  3710     1]]"
b24b56ccc5d4b04fee85579b2dee77306ec829b2,7.3171,"discussion opportunities related to a specific academic discipline, such as physics, English Language Arts,","[[    0  3071  1645  1341    12     3     9   806  2705  7998     6   224
     38     3 11599     6  1566 10509  4218     6]]","Our annotation scheme introduces opportunities for the educational community to conduct further research , Once automated classifiers are developed, such relations between talk and learning can be examined at scale,  automatic labeling via a standard coding scheme can support the generalization of findings across studies, and potentially lead to automated tools for teachers and students, collecting and annotating corpora that can be used by the NLP community to advance the field in this particular area","[[  421 30729  5336  4277     7  1645    21     8  3472   573    12  3498
    856   585     3     6  1447 10069   853  7903     7    33  1597     6
    224  5836   344  1350    11  1036    54    36 14650    44  2643     6
   6569  3783    53  1009     3     9  1068     3  9886  5336    54   380
      8   879  1707    13  7469   640  2116     6    11  6149   991    12
  10069  1339    21  3081    11   481     6 10858    11    46  2264  1014
  11736   127     9    24    54    36   261    57     8   445  6892   573
     12  3245     8  1057    16    48  1090   616     1]]"
c7f087c78768d5c6f3ff26921858186d627fd4fd,0.0,feature extraction from clinical notes,[[    0  1451 16629    45  3739  3358     1]],features per admission were extracted as inputs to the readmission risk classifier,"[[  753   399  7209   130 21527    38  3785     7    12     8   608  5451
   1020   853  7903     1]]"
3aee5c856e0ee608a7664289ffdd11455d153234,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81","[[ 242  794   18   60  855  920  356    6    3 6037 2604   13    3 4241
     5 2517    6  377  536   13    3 4271    5 5062    6    3 2326   13
  4097 3072   11    3 7381   13    3 4241    5 3420    5  242  794   18
  5534  356    6    3 6037 2604   13 8798    5 4450    6  377  536   13
     3 4729    5 4305    6    3 2326   13 1300 2884   11    3 7381   13
  8798    5 4959    1]]"
6ead576ee5813164684a8cdda36e6a8c180455d9,0.0,BLEU scores,[[    0     3  8775 12062  7586     1]],"Rouge-L, Bleu-1",[[23777    18   434     6 11805    76  2292     1]]
7920f228de6ef4c685f478bac4c7776443f19f39,100.0,English,[[   0 1566    1]],English,[[1566    1]]
c0122190119027dc3eb51f0d4b4483d2dbedc696,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Stacking method, LSTMCNN, SARNN, simple LSTM bidirectional model, TextCNN","[[    3 19814    53  1573     6   301  4209  3698 17235     6   180 24947
    567     6   650     3  7600  2305  2647 26352   825     6  5027   254
  17235     1]]"
d4b9cdb4b2dfda1e0d96ab6c3b5e2157fd52685e,6.0606,"a common assumption, namely that an explanation is faithful, and that an explanation is consistent","[[    0     3     9  1017 20662     6     3 17332    24    46  7295    19
  13855     6    11    24    46  7295    19  4700]]","Two models will make the same predictions if and only if they use the same reasoning process., On similar inputs, the model makes similar decisions if and only if its reasoning is similar., Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other.","[[ 2759  2250    56   143     8   337 20099     3    99    11   163     3
     99    79   169     8   337 20893   433     5     6   461  1126  3785
      7     6     8   825   656  1126  3055     3    99    11   163     3
     99   165 20893    19  1126     5     6 18264  1467    13     8  3785
     33    72   359    12     8   825 20893   145   717     5     3  7371
      6     8  7548    13   315  1467    13     8  3785    33  2547    45
    284   119     5     1]]"
982979cb3c71770d8d7d2d1be8f92b66223dec85,4.878,consistent distribution of the embeddings BIBREF0,"[[    0  4700  3438    13     8 25078    26    53     7     3  5972 25582
    371   632     1]]"," For example, one metric could consist in checking whether for any given word, all words that are known to belong to the same class are closer than any words belonging to different classes, independently of the actual cosine","[[  242   677     6    80     3  7959   228  5608    16  6450   823    21
    136   787  1448     6    66  1234    24    33   801    12 13000    12
      8   337   853    33  4645   145   136  1234 12770    12   315  2287
      6 13971    13     8  1805   576     7   630     1]]"
02e4bf719b1a504e385c35c6186742e720bcb281,0.0,using discourse relations,[[    0   338 22739  5836     1]],"based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event ","[[   3  390   30    8 4689  344  984    6    8 5259    3 9618  485   13
    80  605   54 2082    8  487    3 9618  485   13    8  119  605    1]]"
e86d381322c8db2b74a13a8e23082ddb010c1e40,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
7c792cda220916df40edb3107e405c86455822ed,100.0,METEOR,[[   0 7934 3463 2990    1]],METEOR,[[7934 3463 2990    1]]
cac0119681f311b2efd14b3251a2a5b69ad5d0cd,100.0,No,[[  0 465   1]],No,[[465   1]]
eae13c9693ace504eab1f96c91b16a0627cd1f75,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
8486e06c03f82ebd48c7cfbaffaa76e8b899eea5,0.0,Unanswerable,[[   0  597 3247 3321  179    1]], hand-curated collection of complete inflection tables for 198 lemmata,"[[  609    18 22579  1232    13   743    16    89 12252  5056    21     3
  24151    90   635   144     9     1]]"
20eb673b01d202b731e7ba4f84efc10a18616dd3,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"the number of speakers of each gender category, their speech duration",[[   8  381   13 7215   13  284 7285 3295    6   70 5023 8659    1]]
da4d25dd9de09d16168788bb02ad600f5b0b3ba4,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"single head, disabling a whole layer, that is, all 12 heads in a given layer","[[ 712  819    6 1028    9 7428    3    9  829 3760    6   24   19    6
    66  586 7701   16    3    9  787 3760    1]]"
0b9021cefca71081e617a362e7e3995c5f1d2a88,20.0,"Bi-LSTM, BOW",[[    0  2106    18  7600  2305     6   272 15251     1]],"CNN-C, CNN-W, CNN-Lex-C, CNN-Lex-W, Bi-LSTM-C , Bi-LSTM-W, Lex-rule, BOW","[[19602    18   254     6 19602    18   518     6 19602    18   434   994
     18   254     6 19602    18   434   994    18   518     6  2106    18
   7600  2305    18   254     3     6  2106    18  7600  2305    18   518
      6 17546    18  5155    15     6   272 15251     1]]"
70797f66d96aa163a3bee2be30a328ba61c40a18,100.0,SRCC,[[   0    3 6857 2823    1]],SRCC,[[   3 6857 2823    1]]
dcb18516369c3cf9838e83168357aed6643ae1b8,0.0,"BIBREF2, BIBREF3","[[    0     3  5972 25582   371   357     3     6     3  5972 25582   371
    519     1]]",The dataset comes with a ranked set of relevant documents. Hence the baselines do not use a retrieval system.,"[[   37 17953   639    28     3     9     3  8232   356    13  2193  2691
      5     3 13151     8 20726     7   103    59   169     3     9 24515
    138   358     5     1]]"
a7d72f308444616a0befc8db7ad388b3216e2143,0.0,WN18 and FB15k,[[    0     3 21170  2606    11     3 15586  1808   157     1]],"in-house dataset, ACE05 dataset ",[[   16    18  1840 17953     6     3 11539  3076 17953     1]]
0fce128b8aaa327ac0d58ec30cd2ecbea2019baa,0.0,"ROCStories, BLEU",[[    0     3 26893   134   235  2593     6     3  8775 12062     1]],"Seq2Seq, HLSTM, HLSTM+Copy, HLSTM+Graph Attention, HLSTM+Contextual Attention","[[  679  1824   357   134    15  1824     6   454  7600  2305     6   454
   7600  2305  1220  3881   102    63     6   454  7600  2305  1220 21094
  20748     6   454  7600  2305  1220  4302  6327  3471 20748     1]]"
5a02a3dd26485a4e4a77411b50b902d2bda3731b,4.0816,using sequence-based tagging,[[    0   338  5932    18   390     3    17 15242     1]],"To model an answer which is a collection of spans, the multi-span head uses the $\mathtt {BIO}$ tagging format BIBREF8: $\mathtt {B}$ is used to mark the beginning of a span, $\mathtt {I}$ is used to mark the inside of a span and $\mathtt {O}$ is used to mark tokens not included in a span","[[  304   825    46  1525    84    19     3     9  1232    13  8438     7
      6     8  1249    18     7  2837   819  2284     8  1514     2  3357
    107    17    17     3     2   279  7550     2  3229     3    17 15242
   1910     3  5972 25582   371   927    10  1514     2  3357   107    17
     17     3     2   279     2  3229    19   261    12  3946     8  1849
     13     3     9  8438     6  1514     2  3357   107    17    17     3
      2   196     2  3229    19   261    12  3946     8  1096    13     3
      9  8438    11  1514     2  3357   107    17    17     3     2   667
      2  3229    19   261    12  3946 14145     7    59  1285    16     3
      9  8438     1]]"
73bb8b7d7e98ccb88bb19ecd2215d91dd212f50d,0.0,LSTM,[[   0    3 7600 2305    1]],comparing the summary with the text instead of the reference and labeling the candidate bad if it is incorrect or irrelevant,"[[    3 14622     8  9251    28     8  1499  1446    13     8  2848    11
   3783    53     8  4775  1282     3    99    34    19 12153    42 26213
      1]]"
b366706e2fff6dd8edc89cc0c6b9d5b0790f43aa,25.0,"BPRA, BPRA, BPRA, BPRA, BPRA","[[    0     3 11165  4763     6     3 11165  4763     6     3 11165  4763
      6     3 11165  4763     6     3 11165  4763]]","BPRA, APRA, BLEU",[[    3 11165  4763     6     3  2965  4763     6     3  8775 12062     1]]
2c8d5e3941a6cc5697b242e64222f5d97dba453c,10.5263,"Compared to baseline, the results were 0.86 vs 0.86 for both models.","[[    0     3 25236    12 20726     6     8   772   130  4097  3840     3
    208     7  4097  3840    21   321  2250     5]]",BERT on Quora drops from 94.6% to 24.1%,"[[  272 24203    30  2415   127     9 11784    45     3  4240     5  6370
     12   997     5  4704     1]]"
f608fbc7a4a10a79698f340e2948c4c7034642d5,40.0,Bi-directional LSTM with a corresponding target in bold,"[[    0  2106    18 26352     3  7600  2305    28     3     9     3  9921
   2387    16  8197     1]]","Bi-directional LSTM, self-attention ",[[ 2106    18 26352     3  7600  2305     6  1044    18 25615     1]]
47726be8641e1b864f17f85db9644ce676861576,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"We compare this method of bias mitigation with the no bias mitigation (""Orig""), geometric bias mitigation (""Geo""), the two pieces of our method alone (""Prob"" and ""KNN"") and the composite method (""KNN+Prob""). We note that the composite method performs reasonably well according the the RIPA metric, and much better than traditional geometric bias mitigation according to the neighborhood metric, without significant performance loss according to the accepted benchmarks. To our knowledge this is the first bias mitigation method to perform reasonably both on both metrics.","[[  101  4048    48  1573    13 14387 27193    28     8   150 14387 27193
  13308   667  3380  8512     6 21910 14387 27193 13308   517    15    32
   8512     6     8   192  2161    13    69  1573  2238 13308  3174   115
    121    11    96   439 17235  8512    11     8 13723  1573 13308   439
  17235  1220  3174   115   121   137   101  2232    24     8 13723  1573
   1912     7 13145   168  1315     8     8   391 25981     3  7959     6
     11   231   394   145  1435 21910 14387 27193  1315    12     8  5353
      3  7959     6   406  1516   821  1453  1315    12     8  4307 15705
      7     5   304    69  1103    48    19     8   166 14387 27193  1573
     12  1912 13145   321    30   321 15905     5     1]]"
df25dd9004a3b367202d7731ee912a8052a35780,100.0,No,[[  0 465   1]],No,[[465   1]]
5d22746b3004c5e90ea714b24bf8bc7b4d15bd88,0.0,COCO5K,[[   0 2847 5911  755  439    1]],Karpathy and Fei-Fei's split for MS-COCO dataset BIBREF10,"[[ 4556 24136    11  4163    23    18   371    15    23    31     7  5679
     21  5266    18  5911  5911 17953     3  5972 25582   371  1714     1]]"
8c0e8a312b85c4ffdffabeef0d29df1ef8ff7fb2,100.0,"3,200 sentences",[[    0  6180  3632 16513     1]],"3,200 sentences",[[ 6180  3632 16513     1]]
b556fd3a9e0cff0b33c63fa1aef3aed825f13e28,0.0,"WN18S, WSJ15S, WSJ15S","[[    0     3 21170  2606   134     6     3  8439   683  1808   134     6
      3  8439   683  1808   134     1]]","16 different datasets from several popular review corpora used in BIBREF20, CoNLL 2000 BIBREF22","[[  898   315 17953     7    45   633  1012  1132 11736   127     9   261
     16     3  5972 25582   371  1755     6   638   567 10376  2766     3
   5972 25582   371  2884     1]]"
d4b9cdb4b2dfda1e0d96ab6c3b5e2157fd52685e,6.0606,"a common assumption, namely that an explanation is faithful, and that an explanation is consistent","[[    0     3     9  1017 20662     6     3 17332    24    46  7295    19
  13855     6    11    24    46  7295    19  4700]]","Two models will make the same predictions if and only if they use the same reasoning process., On similar inputs, the model makes similar decisions if and only if its reasoning is similar., Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other.","[[ 2759  2250    56   143     8   337 20099     3    99    11   163     3
     99    79   169     8   337 20893   433     5     6   461  1126  3785
      7     6     8   825   656  1126  3055     3    99    11   163     3
     99   165 20893    19  1126     5     6 18264  1467    13     8  3785
     33    72   359    12     8   825 20893   145   717     5     3  7371
      6     8  7548    13   315  1467    13     8  3785    33  2547    45
    284   119     5     1]]"
5787ac3e80840fe4cf7bfae7e8983fa6644d6220,0.0,"LSTM BIBREF2, LSTM BIBREF3","[[    0     3  7600  2305     3  5972 25582   371  4482     3  7600  2305
      3  5972 25582   371   519     1]]",We collected a corpus of poems and a corpus of vernacular literature from online resources,"[[  101  4759     3     9 11736   302    13 18460    11     3     9 11736
    302    13   548    29     9  4866  6678    45   367  1438     1]]"
0c1663a7f7750b399f40ef7b4bf19d5c598890ff,0.0,they are given only a prefix of the text which requires completing the query from text,"[[    0    79    33   787   163     3     9   554 12304    13     8  1499
     84  2311     3  8828     8 11417    45  1499]]",we replace user embeddings with a low-dimensional image representation,"[[   62  3601  1139 25078    26    53     7    28     3     9   731    18
  11619  1023  6497     1]]"
d4a6f5034345036dbc2d4e634a8504f79d42ca69,0.0,English-Japanese,[[   0 1566   18  683 9750 1496   15    1]],the WMT'14 English-French (En-Fr) and English-German (En-De) datasets.,"[[    8   549  7323    31  2534  1566    18   371    60  5457    41  8532
     18   371    52    61    11  1566    18 24518    41  8532    18  2962
     61 17953     7     5     1]]"
0f928732f226185c76ad5960402e9342c0619310,0.0,LSTMs,[[   0    3 7600 2305    7    1]],"feedforward neural networks (DNNs), convolutional neural networks (CNNs)","[[ 3305 26338 24228  5275    41   308 17235     7   201   975 24817   138
  24228  5275    41   254 17235     7    61     1]]"
06202ab8b28dcf3991523cf163b8844b42b9fc99,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"10k training and 1k test, 1,101 sentences (26k tokens)","[[  335   157   761    11   209   157   794     6  1914 19621 16513    41
   2688   157 14145     7    61     1]]"
fb3687ea05d38b5e65fdbbbd1572eacd82f56c0b,100.0,No,[[  0 465   1]],No,[[465   1]]
f704d182c9e01a2002381b76bf21e4bb3c0d3efc,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
594a6bf37eab64a16c6a05c365acc100e38fcff1,0.0,language prediction task,[[    0  1612 21332  2491     1]],"sentiment analysis, the disambiguation of demonstrative pronouns,","[[ 6493  1693     6     8  1028 24621   257    13 13191  1528   813 15358
     29     7     6     1]]"
102de97c123bb1e247efec0f1d958f8a3a86e2f6,100.0,BLEU and TER scores,[[    0     3  8775 12062    11     3  5946  7586     1]],BLEU and TER scores,[[    3  8775 12062    11     3  5946  7586     1]]
af7a9b56596f90c84f962098f7e836309161badf,60.0,"vSTS contains 2,516 pairs for training","[[    0     3   208  4209   134  2579  3547   755  2938 14152    21   761
      1]]",1338 pairs for training,[[ 1179  3747 14152    21   761     1]]
99a10823623f78dbff9ccecb210f187105a196e9,25.0,public Portuguese word2vec embeddings model,"[[    0   452 21076  1448   357   162    75 25078    26    53     7   825
      1]]",large Portuguese corpus,[[  508 21076 11736   302     1]]
993ee7de848ab6adfe02fa728b3a2c896238859b,100.0,No,[[  0 465   1]],No,[[465   1]]
ba48c095c496d01c7717eaa271470c3406bf2d7c,100.0,Chinese,[[   0 2830    1]],Chinese,[[2830    1]]
880a76678e92970791f7c1aad301b5adfc41704f,0.0,NELL,[[    0   445 12735     1]],"ML  logistic regression classifier combined with a Convolutional Neural Network (CNN) to identify self-reported diagnostic tweets.
NLP methods:  tweet conversion to numeric word vector,  removing tweets containing hyperlinks, removing ""retweets"", removing all tweets containing horoscope indicators,  lowercasing and  removing punctuation.","[[    3  6858 28820 26625   853  7903  3334    28     3     9  1193 24817
    138  1484  9709  3426    41   254 17235    61    12  2862  1044    18
     60 16262  7028 10657     7     5   445  6892  2254    10 10657  6113
     12   206 17552  1448 12938     6     3  8499 10657     7     3  6443
  29588     7     6     3  8499    96    60    17  1123    15    17     7
   1686     3  8499    66 10657     7     3  6443     3   107   127    32
  11911 15600     6  1364  6769    53    11     3  8499  5427    76   257
      5     1]]"
5e9732ff8595b31f81740082333b241d0a5f7c9a,10.0,"significantly improves the F1 score of the proposed model by 0.2 points., compared","[[    0  4019  1172     7     8   377   536  2604    13     8  4382   825
     57     3 18189   979     5     6     3  2172]]",on diversity 6.87 and on relevance 4.6 points higher,[[   30  7322  4357  4225    11    30 20208     3 25652   979  1146     1]]
4140d8b5a78aea985546aa1e323de12f63d24add,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
63337fd803f6fdd060ebd0f53f9de79d451810cd,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
f6a1125c5621a2f32c9bcdd188dff14efa096083,100.0,2.2 BLEU gains,[[    0     3 15300     3  8775 12062 11391     1]],2.2 BLEU gains,[[    3 15300     3  8775 12062 11391     1]]
47796c7f0a7de76ccb97ccbd43dc851bb8a613d5,20.0,morpheme segmentation method uses morpheme segmentation to segment a corp,"[[    0     3  8886    15   526  5508   257  1573  2284     3  8886    15
    526  5508   257    12  5508     3     9 11736]]","morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5, Zemberek, BIBREF12","[[    3  8886    15   526  5508   257     3  5972 25582   371   591    11
    938    17    15 25072   695  9886    41   279  5668    61     3  5972
  25582   371 11116  1027 18247    15   157     6     3  5972 25582   371
   2122     1]]"
20ec88c45c1d633adfd7bff7bbf3336d01fb6f37,100.0,"Precision, Recall, F1",[[    0 28464     6   419 16482     6   377   536     1]],"Precision, Recall, F1",[[28464     6   419 16482     6   377   536     1]]
29bdd1fb20d013b23b3962a065de3a564b14f0fb,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
4bdad5a20750c878d1a891ef255621f6172b6a79,34.4828,a discrete latent variable has an explicit semantic meaning to improve the CVAE on short,"[[    0     3     9 19217    15    50  4669  7660    65    46 17623 27632
   2530    12  1172     8 10430 14611    30   710]]","we connect each latent variable with a word in the vocabulary, thus each latent variable has an exact semantic meaning.","[[   62  1979   284    50  4669  7660    28     3     9  1448    16     8
  19067     6  2932   284    50  4669  7660    65    46  2883 27632  2530
      5     1]]"
bc1bc92920a757d5ec38007a27d0f49cb2dde0d1,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
cfd67b9eeb10e5ad028097d192475d21d0b6845b,100.0,No,[[  0 465   1]],No,[[465   1]]
9447ec36e397853c04dcb8f67492ca9f944dbd4b,11.7647,WN2Vec BIBREF5 and GloVe BIBREF,"[[    0     3 21170   357   553    15    75     3  5972 25582   371   755
     11  9840   553    15     3  5972 25582   371]]",Italian Wikipedia and Google News extraction producing final vocabulary of 618224 words,"[[ 4338 16885    11  1163  3529 16629  5874   804 19067    13   431  2606
  24622  1234     1]]"
d353a6bbdc66be9298494d0c853e0d8d752dec4b,0.0,manually verified,[[    0 12616 17087     1]],"empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method)","[[23941   120  4048  6569 27910    41  6762     6    69  4432     3   390
     30  7781    26    61    11 25194 27910    41 10604     6     3   390
     30   361  7980  1750  1573    61     1]]"
8a94766f8251fa0bce0e09e5c69ce05761811a62,0.0,manually reviewed,[[    0 12616  9112     1]],"plan-to-DFS mapping to perform the correct sequence of traversals, and train a neural classifier to act as a controller","[[  515    18   235    18 10665   134 14670    12  1912     8  2024  5932
     13  5187  5405     6    11  2412     3     9 24228   853  7903    12
   1810    38     3     9  8612     1]]"
6548db45fc28e8a8b51f114635bad14a13eaec5b,12.1212,a classifier which takes the input data into account and then uses it to classify the,"[[   0    3    9  853 7903   84 1217    8 3785  331  139  905   11  258
  2284   34   12  853 4921    8]]","We construct a GAN model which combines different sets of word embeddings INLINEFORM4 , INLINEFORM5 , into a single set of word embeddings INLINEFORM6 . ","[[  101  6774     3     9   350  5033   825    84     3 15256   315  3369
     13  1448 25078    26    53     7  3388 20006 24030   591     3     6
   3388 20006 24030   755     3     6   139     3     9   712   356    13
   1448 25078    26    53     7  3388 20006 24030   948     3     5     1]]"
c30c3e0f8450b1c914d29f41c17a22764fa078e0,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],The system extends BiDAF BIBREF4 with self-attention,"[[   37   358  4285     7  2106  4296   371     3  5972 25582   371   591
     28  1044    18 25615     1]]"
8d793bda51a53a4605c1c33e7fd20ba35581a518,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Confusion in recognizing the words that are active at a given node by a speech recognition solution developed for Indian Railway Inquiry System.,"[[ 1193  7316    16     3 22873     8  1234    24    33  1676    44     3
      9   787   150   221    57     3     9  5023  5786  1127  1597    21
   2557 18025    86  1169   651  2149     5     1]]"
518d0847b02b4f23a8f441faa38b935c9b892e1e,100.0,"Honk, DeepSpeech-finetune","[[    0 11772   157     6  9509   134   855 10217    18 13536    17   444
      1]]","Honk, DeepSpeech-finetune",[[11772   157     6  9509   134   855 10217    18 13536    17   444     1]]
9bd938859a8b063903314a79f09409af8801c973,26.6667,"WSJ6 and WSJ7-M, WSJ7-M and","[[   0    3 8439  683  948   11    3 8439  683  940   18  329    6    3
  8439  683  940   18  329   11]]","WMT14 En-Fr and En-De datasets, IWSLT De-En and En-Vi datasets","[[  549  7323  2534   695    18   371    52    11   695    18  2962 17953
      7     6    27  8439  9012   374    18  8532    11   695    18   553
     23 17953     7     1]]"
64ab2b92e986e0b5058bf4f1758e849f6a41168b,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
77cf4379106463b6ebcb5eb8fa5bb25450fa5fb8,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
97dac7092cf8082a6238aaa35f4b185343b914af,15.7895,Demographic data are used to identify users' tweets and their location in the user's,"[[    0 10007 16587   331    33   261    12  2862  1105    31 10657     7
     11    70  1128    16     8  1139    31     7]]","either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age, more women than men were given a diagnosis of depression","[[  893   952    20  8918    18 10041  2074    19  5868     6    42    20
   8918 26068    33    72   952    12 14765    70  1246     6    72   887
    145  1076   130   787     3     9  8209    13  7562     1]]"
c348a8c06e20d5dee07443e962b763073f490079,100.0,evidence extraction and answer synthesis,[[    0  2084 16629    11  1525     3 17282     1]],evidence extraction and answer synthesis,[[ 2084 16629    11  1525     3 17282     1]]
4226a1830266ed5bde1b349205effafe7a0e2337,20.0,"information about a relation, entity, or fact about a relation, relation, or concept","[[    0   251    81     3     9  4689     6 10409     6    42   685    81
      3     9  4689     6  4689     6    42  2077]]","high-order representation of a relation, loss gradient of relation meta","[[  306    18  9397  6497    13     3     9  4689     6  1453 26462    13
   4689 10531     1]]"
d39c911bf2479fdb7af339b59acb32073242fab3,0.0,"Xinhua, Xinhua, Xinhu","[[  0   3   4  77 107  76   9   6   3   4  77 107  76   9   6   3   4  77
  107  76]]","Car, Phone, Notebook, Camera",[[ 1184     6  8924     6 29125     6 12148     1]]
8985ead714236458a7496075bc15054df0e3234e,8.6957,"GPT-2 scores highest on elliptic and control pairs, while Transformer-XL","[[    0   350  6383  4949  7586  2030    30     3  7999   102  1225    11
    610 14152     6   298 31220    18     4   434]]","Overall accuracy per model is: 5-gram (60.5), LSTM (68.9), TXL (68.7), GPT-2 (80.1)","[[ 9126  7452   399   825    19    10  7670  5096 11372 12100   201     3
   7600  2305    41  3651     5 11728     6   332     4   434    41  3651
      5 12703     6   350  6383  4949 13642 16029    61     1]]"
4f1a5eed730fdcf0e570f9118fc09ef2173c6a1b,13.3333,"Open-domain response generation, CVAE",[[    0  2384    18 22999  1773  3381     6 10430 14611     1]]," Seq2seq, CVAE, Hierarchical Gated Fusion Unit (HGFU), Mechanism-Aware Neural Machine (MANM)","[[  679  1824   357     7    15  1824     6 10430 14611     6  3204  7064
   1950 11118    26 23230  5579    41 21855 19813   201 31529    18   188
   3404  1484  9709  5879    41  9312   329    61     1]]"
9785ecf1107090c84c57112d01a8e83418a913c1,100.0,"German, Spanish, Chinese",[[   0 2968    6 5093    6 2830    1]],"German, Spanish, Chinese",[[2968    6 5093    6 2830    1]]
19608e727b527562b750949e41e763908566b58e,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
bbc58b193c08ccb2a1e8235a36273785a3b375fb,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
03502826f4919e251edba1525f84dd42f21b0253,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
a45edc04277a458911086752af4f17405501230f,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
a48c6d968707bd79469527493a72bfb4ef217007,100.0,MultiNLI,[[    0  4908 18207   196     1]],MultiNLI,[[ 4908 18207   196     1]]
b8137eb0fa0b41f871c899a54154f640f0e9aca1,85.7143,"relational entities, general text-based attributes, descriptive text of images, nodes in graph","[[    0  4689   138 12311     6   879  1499    18   390 12978     6 25444
   1499    13  1383     6   150  1395    16  8373]]","relational entities, general text-based attributes, descriptive text of images, nodes in graph structure of networks, queries","[[ 4689   138 12311     6   879  1499    18   390 12978     6 25444  1499
     13  1383     6   150  1395    16  8373  1809    13  5275     6 13154
      1]]"
51aaec4c511d96ef5f5c8bae3d5d856d8bc288d3,0.0,"LSTM, LSTM-B, LSTM-C, LSTM","[[   0    3 7600 2305    6    3 7600 2305   18  279    6    3 7600 2305
    18  254    6    3 7600 2305]]",the baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search,"[[    8 20726   213  2071  3381  2284     3     9  1068  5932    18   235
     18     7    15   835  3772   825     3 28984    28  1388  8557    11
   2071 17549  2284  4963    18 14672   960     1]]"
03895bc75e4d01c359cd269a9eb3b6ea57039817,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],CNN model which additionally learns intermediate hidden layer representations and convolutional filters. Moreover the CNN model can take advantage of the semantic similarity encoded in the distributed word2vec representations,"[[19602   825    84 11700   669     7 18957  5697  3760  6497     7    11
    975 24817   138 10607     5     3  7371     8 19602   825    54   240
   2337    13     8 27632  1126   485 23734    26    16     8  8308  1448
    357   162    75  6497     7     1]]"
ec91b87c3f45df050e4e16018d2bf5b62e4ca298,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
a6717e334c53ebbb87e5ef878a77ef46866e3aed,100.0,No,[[  0 465   1]],No,[[465   1]]
5ea87432b9166d6a4ab8806599cd2b1f9178622f,3.8462,predicting the word given its context,[[    0     3 29856     8  1448   787   165  2625     1]],"best results were obtained using new word embeddings, best group of word embeddings is EC, The highest type F1-score was obtained for EC1 model, built using binary FastText Skip-gram method utilising subword information, ability of the model to provide vector representation for the unknown words seems to be the most important","[[  200   772   130  5105   338   126  1448 25078    26    53     7     6
    200   563    13  1448 25078    26    53     7    19     3  3073     6
     37  2030   686   377   536    18     7  9022    47  5105    21     3
   3073   536   825     6  1192   338 14865  6805 13598    17 25378    18
   5096  1573 19388    53   769  6051   251     6  1418    13     8   825
     12   370 12938  6497    21     8  7752  1234  1330    12    36     8
    167   359     1]]"
3c34187a248d179856b766e9534075da1aa5d1cf,8.6957,F1 score of 0.83,[[   0  377  536 2604   13 4097 4591    1]],"the results obtained on development and test set (F1 = 89.60, F1 = 87.82) and the results on the supplemental test set (F1 = 71.49)","[[    8   772  5105    30   606    11   794   356    41   371   536  3274
      3  3914     5  3328     6   377   536  3274     3  4225     5  4613
     61    11     8   772    30     8     3 29672   794   356    41   371
    536  3274   489 14912 11728     1]]"
03fb4b31742820df58504575c562bee672e016be,0.0,No,[[  0 465   1]],Unanswerable,[[ 597 3247 3321  179    1]]
faa4f28a2f2968cecb770d9379ab2cfcaaf5cfab,0.0,morphological analysis,[[   0    3 8886 4478 1693    1]],"Speaker's Gender Effects, Interlocutors' Gender and Number Effects","[[16778    31     7   350  3868 14247     7     6  3037  5133    76  5535
     31   350  3868    11  7720 14247     7     1]]"
75773ee868c0429ccb913eceb367ff0782eeda8a,100.0,No,[[  0 465   1]],No,[[465   1]]
1a7d28c25bb7e7202230e1b70a885a46dac8a384,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
551cc0401674f7c363e0018b8186a125f7b17e99,0.0,Twitter users' opinions were analyzed by annotators on the topic of hateful conduct,"[[    0  3046  1105    31  8479   130     3 16466    57    46  2264  6230
     30     8  2859    13  5591  1329  3498     1]]",10 hashtags that can be used in an insulting or offensive way,"[[  335 25354     7    24    54    36   261    16    46 21548    53    42
  12130   194     1]]"
aaec98481defc4c230f84a64cdcf793d89081a76,100.0,Lead-3,[[    0 12208  3486     1]],Lead-3,[[12208  3486     1]]
75b69eef4a38ec16df63d60be9708a3c44a79c56,0.0,Perplexity improves the model's accuracy by 0.1 points.,"[[    0  1915  9247   485  1172     7     8   825    31     7  7452    57
      3 16029   979     5     1]]","Pearson correlation to human judgement - proposed vs next best metric
Sample level comparison:
- Story generation: 0.387 vs 0.148
- Dialogue: 0.472 vs 0.341
Model level comparison:
- Story generation:  0.631 vs 0.302
- Dialogue: 0.783 vs 0.553","[[29300 18712    12   936 22555     3    18  4382     3   208     7   416
    200     3  7959 12474   593  4993    10     3    18  8483  3381    10
      3 19997  4225     3   208     7  4097 24748     3    18  5267 10384
     10     3 22776  5865     3   208     7     3 19997  4853  5154   593
   4993    10     3    18  8483  3381    10     3 22787  3341     3   208
      7     3 19997  4305     3    18  5267 10384    10     3 22426  4591
      3   208     7     3 12100  4867     1]]"
b14f13f2a3a316e5a5de9e707e1e6ed55e235f6f,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
7f9bc06cfa81a4e3f7df4c69a1afef146ed5a1cf,28.5714,SST-2 accuracy increase by 0.88 when error rate increases by 0.88,"[[   0  180 4209 4949 7452  993   57 4097 4060  116 3505 1080 5386   57
  4097 4060    1]]","10 Epochs: pearson-Spearman correlation  drops  60 points when error increase by 20%
50 Epochs: pearson-Spearman correlation  drops  55 points when error increase by 20%","[[  335 10395  6322     7    10     3 29892   106    18   134   855   291
    348 18712 11784  1640   979   116  3505   993    57  7580   943 10395
   6322     7    10     3 29892   106    18   134   855   291   348 18712
  11784  6897   979   116  3505   993    57  7580     1]]"
951098f0b7169447695b47c142384f278f451a1e,0.0,"ethos, traits, prior beliefs of the audience",[[    0     3 30397     6 16843     6  1884 11393    13     8  2417     1]],"5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact","[[  305   487  1113 11241    21     3     9  1090  1988    10   150  1113
      6   731  1113     6  2768  1113     6   306  1113    11   182   306
   1113     1]]"
48fb76ae9921c9d181f65afc63a42af8ba3bc519,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"large raw Czech corpora available from the LINDAT/CLARIN repository, Czech Wikipedia","[[  508  5902 16870 11736   127     9   347    45     8   301 13885  5767
     87   254 22492  3162 22109     6 16870 16885     1]]"
76d62e414a345fe955dc2d99562ef5772130bc7e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"neural question-answering technique to extract relations from a story text, OpenIE5, a commonly used rule-based information extraction technique","[[24228   822    18  3247  3321    53  3317    12  5819  5836    45     3
      9   733  1499     6  2384  5091 11116     3     9  5871   261  3356
     18   390   251 16629  3317     1]]"
a6d00f44ff8f83b6c1787e39333e759b0c3daf15,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"user holds or points weapons, is seen in a group fashion which displays a gangster culture, or is showing off graffiti, hand signs, tattoos and bulk cash","[[ 1139  4532    42   979  7749     6    19   894    16     3     9   563
   2934    84  8397     3     9     3  3810  1370  1543     6    42    19
   2924   326 30196     6   609  3957     6 16713     7    11  7942  1723
      1]]"
f887d5b7cf2bcc1412ef63bff4146f7208818184,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
3703433d434f1913307ceb6a8cfb9a07842667dd,0.0,"LSTM, LSTM+N, LSTM+N, LSTM","[[   0    3 7600 2305    6    3 7600 2305 1220  567    6    3 7600 2305
  1220  567    6    3 7600 2305]]","Considering ""What"" and ""How"" separately versus jointly optimizing for both.","[[    3 21419    96  5680   121    11    96  7825   121 12000     3  8911
  22801 19769    53    21   321     5     1]]"
285858416b1583aa3d8ba0494fd01c0d4332659f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"1) investigate errors produced by the end-to-end methods and explore several approaches to correct common errors done in French, 2) compare the end-to-end methods in a SLU context and evaluate the semantic value of the partially correct produced words","[[ 8925  9127  6854  2546    57     8   414    18   235    18   989  2254
     11  2075   633  6315    12  2024  1017  6854   612    16  2379     6
   9266  4048     8   414    18   235    18   989  2254    16     3     9
    180  9138  2625    11  6825     8 27632   701    13     8 14610  2024
   2546  1234     1]]"
f0e8f045e2e33a2129e67fb32f356242db1dc280,11.7647,"MH17 disinformation detection system, MH17 disinformation detection system, MH17","[[    0     3 20131  2517  1028  6391 10664   358     6     3 20131  2517
   1028  6391 10664   358     6     3 20131  2517]]",applying reasoning BIBREF36 or irony detection methods BIBREF37,"[[ 6247 20893     3  5972 25582   371  3420    42  3575    63 10664  2254
      3  5972 25582   371  4118     1]]"
54c9147ffd57f1f7238917b013444a9743f0deb8,0.0,"Bi-LSTM, BERT",[[    0  2106    18  7600  2305     6   272 24203     1]],The sequence model architectures which this method is transferred to are: LSTM and Transformer-based models,"[[   37  5932   825  4648     7    84    48  1573    19 10250    12    33
     10     3  7600  2305    11 31220    18   390  2250     1]]"
9653c89a93ac5c717a0a26cf80e9aa98a5ccf910,0.0,"BIBREF2, BIBREF3",[[    0     3  5972 25582   371  4482     3  5972 25582   371   519     1]],"WDAqua BIBREF0 , QAKiS BIBREF7 , gAnswer BIBREF6 and Platypus BIBREF8","[[  549  4296  4960     3  5972 25582   371   632     3     6  1593 12396
     23   134     3  5972 25582   371   940     3     6     3   122   188
     29     7  3321     3  5972 25582   371   948    11  8422 12575   302
      3  5972 25582   371   927     1]]"
73bbe0b6457423f08d9297a0951381098bd89a2b,2.8571,"LSTM-based, dependency-based, and span-based SRL","[[    0     3  7600  2305    18   390     3     6 27804    18   390     6
     11  8438    18   390 16626     1]]","2008 Punyakanok et al. 
2009 Zhao et al. + ME 
2008 Toutanova et al. 
2010 Bjorkelund et al.  
2015 FitzGerald et al. 
2015 Zhou and Xu 
2016 Roth and Lapata 
2017 He et al. 
2017 Marcheggiani et al.
2017 Marcheggiani and Titov 
2018 Tan et al. 
2018 He et al. 
2018 Strubell et al. 
2018 Cai et al. 
2018 He et al. 
2018 Li et al. 
","[[ 2628 18266 20856   152  1825     3    15    17   491     5  2464 30680
      3    15    17   491     5  1768  7934  2628  8786     9 14979     3
     15    17   491     5  2735   272 12775  5768  1106     3    15    17
    491     5  1230  9783   172   517    49   138    26     3    15    17
    491     5  1230  1027  9492    11     3     4    76  1421 17269    11
    325  4665     9  1233   216     3    15    17   491     5  1233  1332
     15 15406  2738     3    15    17   491     5  1233  1332    15 15406
   2738    11  2262   235   208   846  8331     3    15    17   491     5
    846   216     3    15    17   491     5   846  5500    76  7708     3
     15    17   491     5   846  1336    23     3    15    17   491     5
    846   216     3    15    17   491     5   846  1414     3    15    17
    491     5     1]]"
6e2899c444baaeb0469599f65722780894f90f29,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],we use mean average precision (MAP) as the main evaluation metric,"[[   62   169  1243  1348 11723    41 25790    61    38     8   711  5002
      3  7959     1]]"
bdd8368debcb1bdad14c454aaf96695ac5186b09,8.6957,PTSD Peer Support Programs (PSP) use twitter posts as a baseline to,"[[    0     3 29609  1276    49  4224  2350     7    41  4176   345    61
    169 19010  3489    38     3     9 20726    12]]","defined into four categories from high risk, moderate risk, to low risk","[[4802  139  662 5897   45  306 1020    6 8107 1020    6   12  731 1020
     1]]"
11c5b12e675cfd8d1113724f019d8476275bd700,100.0,No,[[  0 465   1]],No,[[465   1]]
572458399a45fd392c3a4e07ce26dcff2ad5a07d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"For the Oshiete-goo dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, Trans, by 0.021, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.037.  For the nfL6 dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, CLSTM, by 0.028, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.040. Human evaluation of the NAGM's generated outputs for the Oshiete-goo dataset had 47% ratings of (1), the highest rating, while CLSTM only received 21% ratings of (1). For the nfL6 dataset, the comparison of (1)'s was NAGM's 50% to CLSTM's 30%. ","[[  242     8   411     7   107 22881    18   839    32 17953     6     8
  10144  7381   825    31     7   391 26260   427    18   434  2604    19
   1146   145     8  2030  5505  7450   825     6  4946     6    57     3
  11739  2658     6    11   165     3  8775 12062  4278  2604    19  1146
    145     8  2030  5505   825 11175   134  2305    57     3 11739  4118
      5   242     8     3    29    89   434   948 17953     6     8 10144
   7381   825    31     7   391 26260   427    18   434  2604    19  1146
    145     8  2030  5505  7450   825     6 11175   134  2305     6    57
      3 11739  2577     6    11   165     3  8775 12062  4278  2604    19
   1146   145     8  2030  5505   825 11175   134  2305    57     3 11739
   2445     5  3892  5002    13     8 10144  7381    31     7  6126  3911
      7    21     8   411     7   107 22881    18   839    32 17953   141
    314  6170  9712    13  5637     6     8  2030  5773     6   298 11175
    134  2305   163  1204   204  4704  9712    13  5637     5   242     8
      3    29    89   434   948 17953     6     8  4993    13  5637    31
      7    47 10144  7381    31     7  5743    12 11175   134  2305    31
      7 10738     5     1]]"
8a276dfe748f07e810b3944f4f324eaf27e4a52c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The distribution of joke scores varies wildly, ranging from 0 to 136,354 upvotes. We found that there is a major jump between the 0-200 upvote range and the 200 range and onwards, with only 6% of jokes scoring between 200-20,000. We used this natural divide as the cutoff to decide what qualified as a funny joke, giving us 13884 not-funny jokes and 2025 funny jokes.","[[   37  3438    13 10802  7586     3 15550     3 28890     6     3  6836
     45     3   632    12     3 23459     6  2469   591    95  1621  1422
      5   101   435    24   132    19     3     9   779  4418   344     8
      3  9498  3632    95  1621    17    15   620    11     8  2382   620
     11    30  2239     7     6    28   163     3  6370    13 10802     7
  10389   344  2382    18 13922     5   101   261    48   793 14514    38
      8  1340  1647    12  2204   125  4717    38     3     9  6613 10802
      6  1517   178     3 22744  4608    59    18    89   202    29    63
  10802     7    11   460  1828  6613 10802     7     5     1]]"
b546f14feaa639e43aa64c799dc61b8ef480fb3d,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
0d1408744651c3847469c4a005e4a9dccbd89cf1,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
ec8043290356fcb871c2f5d752a9fe93a94c2f71,0.0,"long-range syntactical links, though less frequent than adjacent syntactical relationships","[[    0   307    18  5517  8953    17  2708  1950  2416     6   713   705
   8325   145 12487  8953    17  2708  1950  3079]]","general classification tasks, use of the methodology in other networked systems, a network could be enriched with embeddings obtained from graph embeddings techniques","[[  879 13774  4145     6   169    13     8 15663    16   119  1229    15
     26  1002     6     3     9  1229   228    36     3 27307    28 25078
     26    53     7  5105    45  8373 25078    26    53     7  2097     1]]"
26c290584c97e22b25035f5458625944db181552,100.0,"10,001 utterances",[[    0 10372 17465     3  5108   663     7     1]],"10,001 utterances",[[10372 17465     3  5108   663     7     1]]
4d30c2223939b31216f2e90ef33fe0db97e962ac,100.0,11'248,[[   0  850   31  357 3707    1]],11'248,[[ 850   31  357 3707    1]]
fa3663567c48c27703e09c42930e51bacfa54905,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"deep convolutional networks BIBREF53 , BIBREF54","[[ 1659   975 24817   138  5275     3  5972 25582   371  4867     3     6
      3  5972 25582   371  5062     1]]"
1d6c42e3f545d55daa86bea6fabf0b1c52a93bbb,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
0460019eb2186aef835f7852fc445b037bd43bb7,100.0,two,[[  0 192   1]],two,[[192   1]]
fc69f5d9464cdba6db43a525cecde2bf6ddaaa57,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
bfad30f51ce3deea8a178944fa4c6e8acdd83a48,10.5263,a general framework for data exploration by semantic queries,[[    0     3     9   879  4732    21   331  9740    57 27632 13154     1]],"three main components, namely data processing, task processing, and query processing","[[  386   711  3379     6     3 17332   331  3026     6  2491  3026     6
     11 11417  3026     1]]"
e37c32fce68759b2272adc1e44ea91c1a7c47059,0.0,"Movie reviews in two main channels, one for short and one for long","[[    0 10743  2456    16   192   711  6047     6    80    21   710    11
     80    21   307     1]]","movies , restaurants, English , Korean",[[4876    3    6 3661    6 1566    3    6 9677    1]]
a1c4f9e8661d4d488b8684f055e0ee0e2275f767,0.0,"BiLSTM, BERT",[[    0  2106  7600  2305     6   272 24203     1]],"Recurrent Neural Network (RNN), ActionLSTM, Generative Recurrent Neural Network Grammars (RNNG)","[[  419 14907  1484  9709  3426    41 14151   567   201  6776  7600  2305
      6  5945    49  1528   419 14907  1484  9709  3426 30751     7    41
  14151 12531    61     1]]"
8434974090491a3c00eed4f22a878f0b70970713,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Proposed model has 1.16 million parameters and 11.04 MB.,"[[  749 12151   825    65  1300  2938   770  8755    11  7806  6348     3
   4633     5     1]]"
a4422019d19f9c3d95ce8dc1d529bf3da5edcfb1,100.0,No,[[  0 465   1]],No,[[465   1]]
1b9119813ea637974d21862a8ace83bc1acbab8e,0.0,"BIBREF13, BIBREF18","[[    0     3  5972 25582   371  2368     6     3  5972 25582   371  2606
      1]]",They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).,"[[  328   261  2142  2168 24532  1612    11 24532 16265    12   554  9719
  25078    26    53     7    11 17953   937    16   454  7331  2491    12
   2412   825    41   221  5756     7    59  2799    16  1040   137     1]]"
43878a6a8fc36aaae29d95815355aaa7d25c3b53,100.0,personalized bAbI dialog dataset,[[    0  9354     3   115  8952   196 13463 17953     1]],the personalized bAbI dialog dataset,[[    8  9354     3   115  8952   196 13463 17953     1]]
6147846520a3dc05b230241f2ad6d411d614e24c,66.6667,"scientific writing, named entity recognition, paper acceptance prediction",[[    0  4290   913     6  2650 10409  5786     6  1040 11122 21332     1]],"paper acceptance prediction, Named Entity Recognition (NER), author stance prediction","[[ 1040 11122 21332     6  5570    26  4443   485 31110    41 18206   201
   2291     3  8389 21332     1]]"
557d1874f736d9d487eb823fe8f6dab4b17c3c42,100.0,Bantu,[[   0 5185   17   76    1]],Bantu,[[5185   17   76    1]]
0428e06f0550e1063a64d181210795053a8e6436,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
cb4727cd5643dabc3f5c95e851d5313f5d979bdc,20.0,"Compared to the state-of-the-art performance, the EM model achieved ","[[    0     3 25236    12     8   538    18   858    18   532    18  1408
    821     6     8     3  6037   825  5153     3]]",our Open model achieves more than 3 points of f1-score than the state-of-the-art result,"[[  69 2384  825 1984    7   72  145  220  979   13    3   89  536   18
     7 9022  145    8  538   18  858   18  532   18 1408  741    1]]"
3b090b416c4ad7d9b5b05df10c5e7770a4590f6a,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
e8e6719d531e7bef5d827ac92c7b1ab0b8ec3c8e,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
02348ab62957cb82067c589769c14d798b1ceec7,30.0,"BERT language representation model BIBREF6, ROUGE, Language model, and ","[[    0   272 24203  1612  6497   825     3  5972 25582   371 11071   391
  26260   427     6 10509   825     6    11     3]]","BiGRU s with attention, ROUGE, Language model (LM), Next sentence prediction","[[ 2106   517  8503     3     7    28  1388     6   391 26260   427     6
  10509   825    41 11160   201  3021  7142 21332     1]]"
d5fa26a2b7506733f3fa0973e2fe3fc1bbd1a12d,100.0,"Yes, as new sentences.",[[    0  2163     6    38   126 16513     5     1]],"Yes, as new sentences.",[[ 2163     6    38   126 16513     5     1]]
71538776757a32eee930d297f6667cd0ec2e9231,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions","[[    3 25125     8  1675   344  1448  3476    11     8   192 15905    13
   1139  3813    41  1890  1748  5773     6  1243   381    13  5050    61
     16  2450 13080 26625     7     1]]"
47726be8641e1b864f17f85db9644ce676861576,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
39a450ac15688199575798e72a2cc016ef4316b5,21.6216,"Compared to baseline, the improved F1 score of BIBREF9 was 0.86","[[    0     3 25236    12 20726     6     8  3798   377   536  2604    13
      3  5972 25582   371  1298    47  4097  3840]]",Compared to baselines SAN (Table 1) shows  improvement of 1.096% on EM and 0.689% F1. Compared to other published SQuAD results (Table 2) SAN is ranked second. ,"[[    3 25236    12 20726     7     3 19976    41 20354  8925  1267  4179
     13  1300  4198  6370    30     3  6037    11  4097  3651  7561   377
   5411     3 25236    12   119  1790   180  5991  6762   772    41 20354
   9266     3 19976    19     3  8232   511     5     1]]"
4319a13a6c4a9494ccb465509c9d4265f63dc9b5,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],the generated sentences often did not include all desired attributes.,[[    8  6126 16513   557   410    59   560    66  5327 12978     5     1]]
41174d8b176cb8549c2d83429d94ba8218335c84,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
4debd7926941f1a02266b1a7be2df8ba6e79311a,100.0,No,[[  0 465   1]],No,[[465   1]]
691cba5713c76a6870e35bc248ce1d29c0550bc7,100.0,No,[[  0 465   1]],No,[[465   1]]
f4238f558d6ddf3849497a130b3a6ad866ff38b3,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Answer with content missing: (formula 1) bias(q, a, b) = cos(a, q) − cos(b, q)
Bias is calculated as substraction of cosine similarities of question and some answer for two opposite answers.","[[11801    28   738  3586    10    41  2032    83     9  8925 14387   599
   1824     6     3     9     6     3   115    61  3274   576     7   599
      9     6     3  1824    61     3     2   576     7   599   115     6
      3  1824    61  2106     9     7    19 11338    38   769     7 10559
     13   576     7   630 25758    13   822    11   128  1525    21   192
   6401  4269     5     1]]"
edcde2b675cf8a362a63940b2bbdf02c150fe01f,26.087,a black-box context injection system that uses a combination of textual hints and,"[[    0     3     9  1001    18  2689  2625 10672   358    24  2284     3
      9  2711    13  1499  3471     3 23041    11]]",supply an NMT system with knowledge regarding the speaker and interlocutor of first-person sentences,"[[ 1899    46   445  7323   358    28  1103  1918     8  5873    11  1413
     40    32  3044   127    13   166    18  6075 16513     1]]"
13e87f6d68f7217fd14f4f9a008a65dd2a0ba91c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Experiment 1: ACC around 0.5 with 50% noise rate in worst case - clearly higher than baselines for all noise rates
Experiment 2: ACC on real noisy datasets: 0.7 on Movie, 0.79 on Laptop, 0.86 on Restaurant (clearly higher than baselines in almost all cases)","[[ 1881  4267   297   209    10     3 14775   300     3 12100    28  5743
   4661  1080    16  6025   495     3    18  3133  1146   145 20726     7
     21    66  4661  1917  1881  4267   297   204    10     3 14775    30
    490 26847 17953     7    10     3 22426    30 10743     6  4097  4440
     30 19416     6  4097  3840    30  6233    41  2482   291   120  1146
    145 20726     7    16   966    66  1488    61     1]]"
146fe3e97d8080f04222ed20903dd0d5fd2f551c,0.0,"78,976",[[   0    3 3940    6 4327  948    1]],"the food dataset has 3,806 images for training ",[[    8   542 17953    65  6180  2079   948  1383    21   761     1]]
40c2bab4a6bf3c0628079fcf19e8b52f27f51d98,100.0,using generative process,[[    0   338     3 25181   433     1]],using generative process,[[  338     3 25181   433     1]]
ff9495982b8821240b8a65eafcc9bb8ed8b8e084,10.5263,"power law behavior approximates the error well both with respect to data size, when holding model size","[[    0   579   973  3889 24672     7     8  3505   168   321    28  1445
     12   331   812     6   116  3609   825   812]]","estimated test accuracy is highly correlated with actual test accuracy for various datasets, appropriateness of the proposed function for modeling the complex error landscape","[[ 5861   794  7452    19  1385     3 29604    28  1805   794  7452    21
    796 17953     7     6  2016   655    13     8  4382  1681    21 15309
      8  1561  3505  3283     1]]"
bd78483a746fda4805a7678286f82d9621bc45cf,0.0,"Q&A BIBREF9, Q&A BIBREF12","[[    0  1593   184   188     3  5972 25582   371  1298     6  1593   184
    188     3  5972 25582   371  2122     1]]",state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26,"[[  538    18   858    18   532    18  1408   822 18243  2250    41    23
      5    15     5     3 23008  9688     3  5972 25582   371  3288    61
     11   272 24203    18 14885    15     3  5972 25582   371  2688     1]]"
61404466cf86a21f0c1783ce535eb39a01528ce8,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
b02d2d351bd2e49d4d59db0a8a6ef23cb90bfbc4,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
d6a27c41c81f12028529e97e255789ec2ba39eaa,14.8148,accuracy is generally higher on test than on test for BLEU,"[[    0  7452    19  2389  1146    30   794   145    30   794    21     3
   8775 12062     1]]","accuracy can change up to 5 percentage points, whereas BLEU can vary up to 8 points","[[ 7452    54   483    95    12   305  5294   979     6     3 10339     3
   8775 12062    54  5215    95    12   505   979     1]]"
5f7850254b723adf891930c6faced1058b99bd57,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. 
The interpretability of the model is shown in Figure 2. ","[[   71  7558 24578   454  8257  2284     8  5697  2315    13     3     9
   8401 18270     3  7600  2305    38   753    11     3     9 19217    15
  24578   454  8257  2284   331    38   753     5    37  7280  2020    13
      8   825    19  2008    16  7996  1682     1]]"
39492338e27cb90bf1763e4337c2f697cf5082ba,40.0,two CNNs and one LSTM,[[    0   192 19602     7    11    80     3  7600  2305     1]],10 CNNs and 10 LSTMs,[[  335 19602     7    11   335     3  7600  2305     7     1]]
20f7b359f09c37e6aaaa15c2cdbb52b031ab4809,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
74fe054f5243c8593ddd2c0628f91657246b7dfa,100.0,No,[[  0 465   1]],No,[[465   1]]
8cc56fc44136498471754186cfa04056017b4e54,11.1111,BLEU outperforms by 0.86 on relevance and 0.88 on average on relevance,"[[    0     3  8775 12062    91   883  2032     7    57  4097  3840    30
  20208    11  4097  4060    30  1348    30 20208]]","Proposed model is better than both lexical based models by significan margin in all metrics: BLEU 0.261 vs 0.250, ROUGLE 0.162 vs 0.155 etc.","[[  749 12151   825    19   394   145   321     3 30949   138     3   390
   2250    57  1320  3286   152  6346    16    66 15905    10     3  8775
  12062     3 18189  4241     3   208     7  4097 11434     6   391 26260
   3765     3 16029  4056     3   208     7  4097 20896   672     5     1]]"
b921a1771ed0ba9dbeff9da000336ecf2bb38322,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5471766ca7c995dd7f0f449407902b32ac9db269,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"2.36 point increase in the F1 score with respect to the best SEM architecture, on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM), lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa, For POS tagging, we observe error reductions of respectively 0.71% for GSD, 0.81% for Sequoia, 0.7% for Spoken and 0.28% for ParTUT, For parsing, we observe error reductions in LAS of 2.96% for GSD, 3.33% for Sequoia, 1.70% for Spoken and 1.65% for ParTUT","[[ 1682  3420   500   993    16     8   377   536  2604    28  1445    12
      8   200   180  6037  4648     6    30     8 26585   434  6048    18
  11359  3162  1898 13642 10917     3   208     7     5   505 18189    21
      3     4 11160   201     3  5430     7  1187  2250  4252    30     8
    926  1566   761   356    16     8 26585   434  6048    18   382  6038
   1898     6   505 10917     3   208     7     5     3  4613     5  4729
     21  2158 12920   382     9     6   242     3 16034     3    17 15242
      6    62  7743  3505  4709     7    13  6898     3 22426  4704    21
    350  7331     6     3 22384  4704    21 26859    32    23     9     6
   4097  6170    21  8927  2217    11     3 18189  5953    21  2180   382
   6675     6   242   260     7    53     6    62  7743  3505  4709     7
     16     3 20245    13     3 27297  6370    21   350  7331     6     3
  19660  5170    21 26859    32    23     9     6     3 18596  6932    21
   8927  2217    11  1300 28168    21  2180   382  6675     1]]"
52e8c9ed66ace1780e41815260af1309064d20de,25.0,Wikipedea Corpus and Reddit Corpus,"[[    0  2142  2168  3138    15     9 10052   302    11  1624    26   155
  10052   302     1]]",WN18 and FB15k,[[    3 21170  2606    11     3 15586  1808   157     1]]
591231d75ff492160958f8aa1e6bfcbbcd85a776,0.0,linearly map source word embedding into target word embedding space BIBREF,"[[    0 13080   120  2828  1391  1448 25078    26    53   139  2387  1448
  25078    26    53   628     3  5972 25582   371]]","CNN-mean, CNN-avgmax",[[19602    18   526   152     6 19602    18     9   208   122  9128     1]]
b5d6357d3a9e3d5fdf9b344ae96cddd11a407875,11.1111,"high-rank neural network with a corresponding heuristics, low-rank","[[    0   306    18  6254 24228  1229    28     3     9     3  9921     3
     88   450  3040     7     6   731    18  6254]]","PCFGLA-based parser, viz. Berkeley parser BIBREF5, minimal span-based neural parser BIBREF6","[[ 2104 22807  4569    18   390   260     7    49     6     3  7302     5
  20776   260     7    49     3  5972 25582   371 11116  6211  8438    18
    390 24228   260     7    49     3  5972 25582   371   948     1]]"
540e9db5595009629b2af005e3c06610e1901b12,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia.,"[[   37  5921   857    24     8  2142  2168  4907     7 11736   302  2579
   1591  2827 30729     7   298   271 26847     5   328 27324  2652     7
     24  1178    43  1591    18  2666   189 17087    57  4993    28 16885
      5     1]]"
a234bcbf2e41429422adda37d9e926b49ef66150,22.2222,accuracy,[[   0 7452    1]],"classification accuracy, BLEU scores, model perplexities of the reconstruction","[[13774  7452     6     3  8775 12062  7586     6   825   399  9247  2197
     13     8 20532     1]]"
4f4892f753b1d9c5e5e74c7c94d8c9b6ef523e7b,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
9c2de35d07f0d536bfdefe4828d66dd450de2b61,100.0,No,[[  0 465   1]],No,[[465   1]]
1244cf6d75e3aa6d605a0f4b141c015923a3f2e7,12.1951,a sound metric should have a high correlation with human judgment,"[[    0     3     9  1345     3  7959   225    43     3     9   306 18712
     28   936  7661     1]]","The first one is that it should be highly correlated with human judgement of similarity. The second one is that it should be able to distinguish sentences which are in logical contradiction, logically unrelated or in logical agreement. The third one is that a robust evaluator should also be able to identify unintelligible sentences. The last criteria is that a good evaluation metric should not give high scores to semantically distant sentences and low scores to semantically related sentences.","[[   37   166    80    19    24    34   225    36  1385     3 29604    28
    936 22555    13  1126   485     5    37   511    80    19    24    34
    225    36     3   179    12 15849 16513    84    33    16     3  6207
  27252     6     3  6207   120    73  3897    42    16     3  6207  2791
      5    37  1025    80    19    24     3     9  6268     3    15  7480
   1016   225    92    36     3   179    12  2862    73  2429    40  2825
   2317 16513     5    37   336  6683    19    24     3     9   207  5002
      3  7959   225    59   428   306  7586    12 27632  1427 10382 16513
     11   731  7586    12 27632  1427  1341 16513     5     1]]"
144714fe0d5a2bb7e21a7bf50df39d790ff12916,53.3333,"ISOT dataset, LIAR dataset",[[    0  8162   382 17953     6  8729  4280 17953     1]],"ISOT dataset: LLVM
Liar dataset: Hybrid CNN and LSTM with attention","[[ 8162   382 17953    10     3 10376 12623  1414   291 17953    10  5555
   2160    26 19602    11     3  7600  2305    28  1388     1]]"
07ee4e0277ad1083270131d32a71c3fe062a916d,10.0,languages based on Arabic culture,[[    0  8024     3   390    30 19248  1543     1]],"Author profiling and deception detection in Arabic, LAMA+DINA Emotion detection, Sentiment analysis in Arabic tweets","[[10236  9639    53    11    20  7239 10664    16 19248     6   301 21250
   1220   308 21116   262  7259 10664     6  4892  2998   295  1693    16
  19248 10657     7     1]]"
f52ec4d68de91dba66668f0affc198706949ff90,0.0,"People who follow vegetarian diet/vegan diet, people who do yoga","[[    0  2449   113  1130 16364  3178    87   162  2565  3178     6   151
    113   103  6325     1]]",Women-Yoga,[[ 4047    18   476 19914     1]]
63b0c93f0452d0e1e6355de1d0f3ff0fd67939fb,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Quora duplicate question dataset BIBREF22,[[ 2415   127     9 19197   822 17953     3  5972 25582   371  2884     1]]
a0876fcbcb5a5944b412613e885703f14732676c,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
f33236ebd6f5a9ccb9b9dbf05ac17c3724f93f91,16.6667,neutral class,[[   0 7163  853    1]],"balanced accuracy, i.e., the average of the three accuracies on each class","[[8965 7452    6    3   23    5   15    5    6    8 1348   13    8  386
     3 6004 2414 6267    7   30  284  853    1]]"
003f884d3893532f8c302431c9f70be6f64d9be8,0.0,No,[[  0 465   1]],Unanswerable,[[ 597 3247 3321  179    1]]
cd1ad7e18d8eef8f67224ce47f3feec02718ea1a,44.4444,"TransE, DistMult, Composition, Syntactic","[[   0 4946  427    6 2678   17  329   83   17    6 2570 4718    6 8951
    17 2708  447    1]]","TransE, DistMult, ComplEx, ConvE, RotatE","[[4946  427    6 2678   17  329   83   17    6 4961   40 5420    6 1193
   208  427    6 8704  144  427    1]]"
8a5254ca726a2914214a4c0b6b42811a007ecfc6,58.3333,Transcribing data is available for about 890 hours for 8 speakers.,"[[   0 4946 2685  115   53  331   19  347   21   81  505 2394  716   21
   505 7215    5    1]]",Transcribed data is available for duration of 38h 54m 38s for 8 speakers.,"[[ 3083    29 22573   331    19   347    21  8659    13  6654   107 10630
     51  6654     7    21   505  7215     5     1]]"
ddd6ba43c4e1138156dd2ef03c25a4c4a47adad0,100.0,No,[[  0 465   1]],No,[[465   1]]
a035472a5c6cf238bed62b63d28100c546d40bd5,13.9535,by using the variational inference method BIBREF11,"[[    0    57   338     8 12338   138    16 11788  1573     3  5972 25582
    371  2596     1]]","target some heuristically extracted contents, treat INLINEFORM1 as a latent variable and co-train selector and generator by maximizing the marginal data likelihood, reinforcement learning to approximate the marginal likelihood,  Variational Reinforce-Select (VRS) which applies variational inference BIBREF10 for variance reduction","[[ 2387   128     3    88   450   343  6402 21527 10223     6  2665  3388
  20006 24030   536    38     3     9    50  4669  7660    11   576    18
   9719  1738   127    11  9877    57     3 30259     8 18777   331 17902
      6 28050  1036    12 24672     8 18777 17902     6 12928   257   138
  14317 10880    18   134    15  3437    41 13556   134    61    84  8275
  12338   138    16 11788     3  5972 25582   371  1714    21 27154  4709
      1]]"
07f5e360e91b99aa2ed0284d7d6688335ed53778,100.0,No,[[  0 465   1]],No,[[465   1]]
134598831939a3ae20d177cec7033d133625a88e,5.102,by fine-tun the weights during learning of the target dataset.,"[[    0    57  1399    18    17   202 32134     8  1293     7   383  1036
     13     8  2387 17953     5     1]]","In the MULT method, two datasets are simultaneously trained, and the weights are tuned based on the inputs which come from both datasets. The hyper-parameter $\lambda \in (0,1)$ is calculated based on a brute-force search or using general global search. This hyper parameter is used to calculate the final cost function which is computed from the combination of the cost function of the source dataset and the target datasets. , this paper proposes to use the most relevant samples from the source dataset to train on the target dataset. One way to find the most similar samples is finding the pair-wise distance between all samples of the development set of the target dataset and source dataset., we propose using a clustering algorithm on the development set. The clustering algorithm used ihere is a hierarchical clustering algorithm. The cosine similarity is used as a criteria to cluster each question and answer. Therefore, these clusters are representative of the development set of the target dataset and the corresponding center for each cluster is representative of all the samples on that cluster. In the next step, the distance of each center is used to calculate the cosine similarity. Finally, the samples in the source dataset which are far from these centers are ignored. In other words, the outliers do not take part in transfer learning.","[[   86     8     3  9696  9012  1573     6   192 17953     7    33 11609
   4252     6    11     8  1293     7    33 14007     3   390    30     8
   3785     7    84   369    45   321 17953     7     5    37  6676    18
   6583  4401  1514     2    40   265   115    26     9     3     2    77
  17482     6  6982  3229    19 11338     3   390    30     3     9 18343
     15    18 10880   960    42   338   879  1252   960     5   100  6676
  15577    19   261    12 11837     8   804   583  1681    84    19 29216
     26    45     8  2711    13     8   583  1681    13     8  1391 17953
     11     8  2387 17953     7     5     3     6    48  1040  4230     7
     12   169     8   167  2193  5977    45     8  1391 17953    12  2412
     30     8  2387 17953     5   555   194    12   253     8   167  1126
   5977    19  2342     8  3116    18 10684  2357   344    66  5977    13
      8   606   356    13     8  2387 17953    11  1391 17953     5     6
     62  4230   338     3     9  9068    53 12628    30     8   606   356
      5    37  9068    53 12628   261     3    23    88    60    19     3
      9  1382  7064  1950  9068    53 12628     5    37   576     7   630
   1126   485    19   261    38     3     9  6683    12  9068   284   822
     11  1525     5  4063     6   175  9068     7    33  6978    13     8
    606   356    13     8  2387 17953    11     8     3  9921  1530    21
    284  9068    19  6978    13    66     8  5977    30    24  9068     5
     86     8   416  1147     6     8  2357    13   284  1530    19   261
     12 11837     8   576     7   630  1126   485     5  4213     6     8
   5977    16     8  1391 17953    84    33   623    45   175  6881    33
  14986     5    86   119  1234     6     8    91  3299     7   103    59
    240   294    16  2025  1036     5     1]]"
5112bbf13c7cf644bf401daecb5e3265889a4bfc,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
9aa52b898d029af615b95b18b79078e9bed3d766,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Proposed vs best baseline:
Decoding: 8541 vs 8532 tokens/sec
Training: 8h vs 8h","[[  749 12151     3   208     7   200 20726    10   374  9886    10 11989
   4853     3   208     7 11989  2668 14145     7    87  7549  4017    10
    505   107     3   208     7   505   107     1]]"
58f3bfbd01ba9768172be45a819faaa0de2ddfa4,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
e98d331faacd50f8ec588d2466b5a85da1f37e6f,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
50e3fd6778dadf8ec0ff589aa8b18c61bdcacd41,100.0,WikiText-TL-39,[[    0  2142  2168 13598    17    18 12733    18  3288     1]],WikiText-TL-39,[[ 2142  2168 13598    17    18 12733    18  3288     1]]
356e462f7966e30665a387ed7a9ad2e830479da6,0.0,multilingual POS corpus,[[    0  1249 25207     3 16034 11736   302     1]],The dataset was created by using translations provided by Tatoeba and OpenSubtitles BIBREF16.,"[[   37 17953    47   990    57   338  7314     7   937    57  9233    32
     15   115     9    11  2384 25252 21869     7     3  5972 25582   371
   2938     5     1]]"
1db37e98768f09633dfbc78616992c9575f6dba4,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
ef07ec34433221d4da79d5923fb00d8ac446b92c,0.0,by the number of times a word piece is aligned with the source,"[[   0   57    8  381   13  648    3    9 1448 1466   19 7901   15   26
    28    8 1391    1]]",median cosine similarity,[[15572   576     7   630  1126   485     1]]
b13cf4205f3952c3066b9fb81bd5c4277e2bc7f5,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
1ce26783f0ff38925bfc07bbbb65d206e52c2d21,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
c47e87efab11f661993a14cf2d7506be641375e4,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Answer with content missing: (formula for CIC) it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities,"[[11801    28   738  3586    10    41  2032    83     9    21   205  4666
     61    34  3744    21     8   167   359   251   441   284 13463  3303
      5   205  4666    54    36  2930    12   136  4505  1635  1707  2491
     28   554 17094  1832 12311     1]]"
0b411f942c6e2e34e3d81cc855332f815b6bc123,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Two neural networks: an extractor based on an encoder (BERT) and a decoder (LSTM Pointer Network BIBREF22) and an abstractor identical to the one proposed in BIBREF8.,"[[ 2759 24228  5275    10    46  5819   127     3   390    30    46 23734
     52    41 12920   382    61    11     3     9    20  4978    52    41
   7600  2305  4564    49  3426     3  5972 25582   371  2884    61    11
     46  9838   127 12022    12     8    80  4382    16     3  5972 25582
    371   927     5     1]]"
bc1e3f67d607bfc7c4c56d6b9763d3ae7f56ad5b,15.3846,performance drops of 0.62%,[[    0   821 11784    13     3 22787  5406     1]], improvements of up to 1.5 BLEU over the seq2seq baseline,"[[ 6867    13    95    12  8613     3  8775 12062   147     8   142  1824
    357     7    15  1824 20726     1]]"
54be3541cfff6574dba067f1e581444537a417db,6.25,"Compared to the best state-of-the-art methods, their accuracy improves by","[[    0     3 25236    12     8   200   538    18   858    18   532    18
   1408  2254     6    70  7452  1172     7    57]]","Compared with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively.","[[    3 25236    28     8  3447  1002    13   679    51   427  2165 20798
    188     6    69  4732  1984     7  3594  6932     6  1300  6370     6
   1300  5988     6  1300  5170  6097 11391    30  3388 20006 24030   632
      3     6  3388 20006 24030   536     3     6  3388 20006 24030   357
     11  3388 20006 24030   519  6898     5     1]]"
c9ee70c481c801892556eb6b9fd8ee38197923be,0.0,"satire, parody, sarcasm","[[   0    3 9275   60    6  260 9666    6    3    7 4667    9    7   51
     1]]","language-independent (e.g., punctuation marks, positive and negative emoticons, quotations, personal pronouns, tweet's length, named entities),  language-dependent relying on dedicated lexicons (e.g., negation, opinion lexicons, opposition words)","[[ 1612    18    77 17631    41    15     5   122     5     6  5427    76
    257  6784     6  1465    11  2841 25993  8056     6 20222     7     6
    525   813 15358    29     7     6 10657    31     7  2475     6  2650
  12311   201  1612    18 17631     3  4610    53    30  2425     3 30949
    106     7    41    15     5   122     5     6 14261   257     6  3474
      3 30949   106     7     6  8263  1234    61     1]]"
ee27e5b56e439546d710ce113c9be76e1bfa1a3d,100.0,No,[[  0 465   1]],No,[[465   1]]
203d322743353aac8a3369220e1d023a78c2cae3,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5c26388a2c0b0452d529d5dd565a5375fdabdb70,18.1818,"Axelrod's TextWorld, Axelrod's TextWorld","[[    0    71   226    15    40  9488    31     7  5027 17954     6    71
    226    15    40  9488    31     7  5027 17954]]","Lurking Horror, Afflicted, Anchorhead, 9:05, TextWorld games","[[ 2318    52  1765 29792     6    71    89    89  2176  1054     6 25874
   3313     6   668    10  3076     6  5027 17954  1031     1]]"
7a70fb11cb3449749f0c2c06101965bf5d02c54a,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
4dad15fee1fe01c3eadce8f0914781ca0a6e3f23,22.8571,They remove slot dependency and use local attention to train slot-specific networks,"[[    0   328  2036  4918 27804    11   169   415  1388    12  2412  4918
     18  9500  5275     1]]",They exclude slot-specific parameters and incorporate better feature representation of user utterance and dialogue states using syntactic information and convolutional neural networks (CNN).,"[[  328 17981  4918    18  9500  8755    11  6300   394  1451  6497    13
   1139     3  5108   663    11  7478  2315   338  8953    17  2708   447
    251    11   975 24817   138 24228  5275    41   254 17235   137     1]]"
9f6e877e3bde771595e8aee10c2656a0e7b9aeb2,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
269b05b74d5215b09c16e95a91ae50caedd9e2aa,100.0,"agreement rates, Kappa value",[[    0  2791  1917     6 12232   102     9   701     1]],"agreement rates, Kappa value",[[ 2791  1917     6 12232   102     9   701     1]]
21cbcd24863211b02b436f21deaf02125f34da4c,0.0,Conversation Type dataset,[[    0 28941  6632 17953     1]],Couples Therapy Corpus (CoupTher) BIBREF21,"[[25185     7 13587 10052   302    41  3881   413   634    52    61     3
   5972 25582   371  2658     1]]"
bd74452f8ea0d1d82bbd6911fbacea1bf6e08cab,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
ac706631f2b3fa39bf173cd62480072601e44f66,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
249f2a9bd9d59679cbe82b3fa01572fc7a04f81b,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
81cee2fc6edd9b7bc65bbf6b4aa35782339e6cff,0.0,"documents that are related to contracts, purchase orders, financial statements, regulatory filings, and more","[[   0 2691   24   33 1341   12 8201    6 1242 5022    6  981 6643    6
  8253 9479    7    6   11   72]]","Variety of formats supported (PDF, Word...), user can define content elements of document","[[31884    13 10874  3510    41 17559     6  4467 11439     6  1139    54
   6634   738  2479    13  1708     1]]"
a516b37ad9d977cb9d4da3897f942c1c494405fe,0.0,"squad, newsqa, and msmarco","[[    0 12025     6  1506  1824     9     6    11     3    51     7  1635
    509     1]]","DocQA, SAN, QANet, ASReader, LM, Random Guess","[[17268 23008     6     3 19976     6     3 23008  9688     6  6157 19915
     49     6     3 11160     6 25942 28291     1]]"
649d6dc076251547aece6532f75d00fc99081d2b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]]," some configurations used in some architectures (e.g., additional RNN layers) are actually irrelevant","[[  128  5298     7   261    16   128  4648     7    41    15     5   122
      5     6  1151   391 17235  7500    61    33   700 26213     1]]"
fb2593de1f5cc632724e39d92e4dd82477f06ea1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],performances of a purely content-based model naturally stays stable,"[[ 7357    13     3     9     3 18760   738    18   390   825  6212 13628
   5711     1]]"
8602160e98e4b2c9c702440da395df5261f55b1f,100.0,Data released for APDA shared task contains 3 datasets.,"[[    0  2747  1883    21     3  2965  4296  2471  2491  2579   220 17953
      7     5     1]]",Data released for APDA shared task contains 3 datasets.,"[[ 2747  1883    21     3  2965  4296  2471  2491  2579   220 17953     7
      5     1]]"
d469c7de5c9e6dd8a901190e95688c446f12118f,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
72ceeb58e783e3981055c70a3483ea706511fac3,100.0,joint goal accuracy,[[   0 4494 1288 7452    1]],joint goal accuracy,[[4494 1288 7452    1]]
372fbf2d120ca7a101f70d226057f9639bf1f9f2,0.0,crowd-sourcing annotations,[[    0  4374    18 19035 30729     7     1]],"Crowd workers were asked to mark whether a triple was correct, namely, did the triple reflect the consequence of the sentence.","[[15343    26  2765   130  1380    12  3946   823     3     9 12063    47
   2024     6     3 17332     6   410     8 12063  3548     8 17009    13
      8  7142     5     1]]"
68ff2a14e6f0e115ef12c213cf852a35a4d73863,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],The dataset contains about 590 tweets about DDos attacks.,"[[   37 17953  2579    81   305  2394 10657     7    81   309  4135     7
   6032     5     1]]"
602396d1f5a3c172e60a10c7022bcfa08fa6cbc9,25.0,RCRN outperforms by 0.82 compared to LSTMs,"[[    0     3  4902 14151    91   883  2032     7    57  4097  4613     3
   2172    12     3  7600  2305     7     1]]",Proposed RCRN outperforms ablative baselines BiLSTM by +2.9% and 3L-BiLSTM by +1.1% on average across 16 datasets.,"[[  749 12151     3  4902 14151    91   883  2032     7     3 15403  1528
  20726     7  2106  7600  2305    57  1768  4416  7561    11   220   434
     18   279    23  7600  2305    57     3 18446     5  4704    30  1348
    640   898 17953     7     5     1]]"
3786164eaf3965c11c9969c4463b8c3223627067,6.4516,a classifier to detect “hot spots”,[[    0     3     9   853  7903    12  8432   105 10718  6883   153     1]],"8 levels and degrees, ranging from `not hot' to `luke warm' to `hot +'. Every utterance was labeled with one of these discrete labels by a single annotator","[[  505  1425    11  4526     6     3  6836    45     3     2  2264  1312
     31    12     3     2    40  1598    15  1978    31    12     3     2
  10718  1768    31     5  2181     3  5108   663    47  3783    15    26
     28    80    13   175 19217    15 11241    57     3     9   712    46
   2264  1016     1]]"
3334f50fe1796ce0df9dd58540e9c08be5856c23,9.5238,"PTSD Peer Support Network (PSN) BIBREF12, LIWC ","[[    0     3 29609  1276    49  4224  3426    41  4176   567    61     3
   5972 25582   371  2122     6  8729 10038     3]]"," For each user, we calculate the proportion of tweets scored positively by each LIWC category.","[[  242   284  1139     6    62 11837     8  7385    13 10657     7  5799
  18294    57   284  8729 10038  3295     5     1]]"
a1d422cb2e428333961370496ca281a1be99fdff,20.0,BLEU and TER scores,[[    0     3  8775 12062    11     3  5946  7586     1]],"coherence, logical consistency, fluency and diversity","[[  576   760  1433     6     3  6207 12866     6  6720  4392    11  7322
      1]]"
61272b1d0338ed7708cf9ed9c63060a6a53e97a2,66.6667,accuracy of 82.0%,[[   0 7452   13    3 4613    5 6932    1]],accuracy of 82.6%,[[7452   13    3 4613    5 6370    1]]
31b6544346e9a31d656e197ad01756813ee89422,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
da4535b75e360604e3ce4bb3631b0ba96f4dadd3,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"in an ME game there are typically several interpretive biases at work: each player has her own bias, as does the Jury","[[   16    46  7934   467   132    33  3115   633  7280   757 14387    15
      7    44   161    10   284  1959    65   160   293 14387     6    38
    405     8 15598    63     1]]"
fd7f13b63f6ba674f5d5447b6114a201fe3137cb,100.0,english language,[[    0 22269  1612     1]],english language,[[22269  1612     1]]
4b41f399b193d259fd6e24f3c6e95dc5cae926dd,9.5238,images and text,[[   0 1383   11 1499    1]],"For the question generation model 15,000 images with 75,000 questions. For the chatbot model, around 460k utterances over 230k dialogues.","[[  242     8   822  3381   825   209  5898  1383    28   489  5898   746
      5   242     8  3582  4045   825     6   300     3 25991   157     3
   5108   663     7   147     3 13427   157  7478     7     5     1]]"
eced6a6dffe43c28e6d06ab87eed98c135f285a3,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
3a62dd5fece70f8bf876dcbb131223682e3c54b7,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"ttention vector sequence is segmented into several subsequences and each subsequence represents the attention of one word, we devise an appropriate aggregation module to fuse the inner-word character attention","[[    3    17  9174 12938  5932    19  5508    15    26   139   633   769
      7    15   835  3772     7    11   284   769     7    15   835  3772
   5475     8  1388    13    80  1448     6    62 13282    15    46  2016
      3 31761    23   106  6008    12 17165     8  4723    18  6051  1848
   1388     1]]"
3de9bf4b0b667b3f1181da9f006da1354565bcbd,10.0,F1 and F2 score,[[   0  377  536   11  377  357 2604    1]],"BLEU, embedding-based metrics (Average, Extrema, Greedy and Coherence), , entropy-based metrics (Ent-{1,2}), distinct metrics (Dist-{1,2,3} and Intra-{1,2,3})","[[    3  8775 12062     6 25078    26    53    18   390 15905    41   188
    624   545     6 20355     9     6  7186    15    26    63    11   638
    760  1433   201     3     6     3    35 12395    63    18   390 15905
     41 16924    18     2  4347   357     2   201  6746 15905    41   308
    343    18     2  4347  4482   519     2    11  9874     9    18     2
   4347  4482   519     2    61     1]]"
edb2d24d6d10af13931b3a47a6543bd469752f0c,11.1111,They selected 300 communities from Reddit for comparison.,[[   0  328 2639 3147 2597   45 1624   26  155   21 4993    5    1]],"They collect subreddits from January 2013 to December 2014,2 for which there are at
least 500 words in the vocabulary used to estimate the measures,
in at least 4 months of the subreddit’s history. They compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language.","[[  328  2868   769  1271    26  7085    45  1762  2038    12  1882  1412
      6   357    21    84   132    33    44   709  2899  1234    16     8
  19067   261    12  7037     8  3629     6    16    44   709   314   767
     13     8   769  1271    26   155    22     7   892     5   328 29216
     69  3629   147     8  2622  1545    57  1105    16     3     9   573
     16    97  3196    13   767     6    21   284 23750  1676   847     6
     11 12616  2036  2597   213     8  7942    13     8  7548    33    16
      3     9  2959  1612     5     1]]"
085147cd32153d46dd9901ab0f9195bfdbff6a85,0.0,MGNC-CNN,[[    0     3  9306  8137    18   254 17235     1]],"MC-CNN
MVCNN
CNN",[[    3  3698    18   254 17235   283  7431 17235 19602     1]]
6e962f1f23061f738f651177346b38fd440ff480,9.0909,"attention-based model with a bidirectional component, a recurrent part, and","[[    0  1388    18   390   825    28     3     9  2647 26352  3876     6
      3     9     3    60 14907   294     6    11]]","BERTje BIBREF8, an ULMFiT model (Universal Language Model Fine-tuning for Text Classification model) BIBREF19., mBERT","[[  272 24203  1924     3  5972 25582   371 11864    46     3  4254 13286
     23   382   825    41  8313   138 10509  5154 11456    18    17   202
     53    21  5027  4501  2420   825    61     3  5972 25582   371  2294
      5     6     3    51 12920   382     1]]"
a0fbf90ceb520626b80ff0f9160b3cd5029585a5,100.0,BIBREF16,[[    0     3  5972 25582   371  2938     1]],BIBREF16,[[    3  5972 25582   371  2938     1]]
0bfed6f9cfe93617c5195c848583e3945f2002ff,0.0,"BiLSTM, BERT",[[    0  2106  7600  2305     6   272 24203     1]],gated neural network ,[[10530    26 24228  1229     1]]
b0dca7b74934f51ff3da0c074ad659c25d84174d,100.0,CAEVO,[[   0 3087  427 8040    1]],CAEVO,[[3087  427 8040    1]]
427252648173c3ba78c211b86fa89fc9f4406653,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Answer with content missing: (Experimental setup not properly rendered) In our experiments we used seven target domains: “Business and Commerce” (BUS), “Government and Politics” (GOV), “Physical and Mental Health” (HEA), “Law and Order” (LAW),
“Lifestyle” (LIF), “Military” (MIL), and “General Purpose” (GEN). Exceptionally, GEN does
not have a natural root category.","[[11801    28   738  3586    10    41  5420  4267 13974  5818    59  3085
  20518    61    86    69 12341    62   261  2391  2387  3303     7    10
    105 26269    11 12949   153    41   279  3063   201   105 27304   297
     11 14984     7   153    41  5577   553   201   105 16977     7  1950
     11 17054  1685   153    41  6021   188   201   105  3612   210    11
   5197   153    41  4569   518   201   105 16427  4084   153    41   434
   6058   201   105   329   173   155  1208   153    41   329  3502   201
     11   105 20857  7333  2748    15   153    41 18464   137     3 26586
   1427     6     3 18464   405    59    43     3     9   793  5465  3295
      5     1]]"
16646ee77975fed372b76ce639e2664ae2105dcf,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
c1429f7fed5a4dda11ac7d9643f97af87a83508b,100.0,empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation,"[[    0 23941   120   794    12   125  5996  1112    16     8  5002   408
   2603     8  6138    13     8   936  5002     1]]",empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation,"[[23941   120   794    12   125  5996  1112    16     8  5002   408  2603
      8  6138    13     8   936  5002     1]]"
3bf429633ecbbfec3d7ffbcfa61fa90440cc918b,0.0,identifying aspect that is related to an opinion target,[[   0    3 9690 2663   24   19 1341   12   46 3474 2387    1]],apply an ensemble of deep learning and linguistics t,"[[ 1581    46  8784    13  1659  1036    11     3 24703     7     3    17
      1]]"
e87f47a293e0b49ab8b15fc6633d9ca6dc9de071,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Egyptian (EGY), Levantine (LEV), Gulf (GLF), North African (NOR)","[[16341    41  8579   476   201   312  6451   630    41  3765   553   201
  13566    41 13011   371   201  1117  3850    41 24833    61     1]]"
e9cfe3f15735e2b0d5c59a54c9940ed1d00401a2,100.0,No,[[  0 465   1]],No,[[465   1]]
5b2480c6533696271ae6d91f2abe1e3a25c4ae73,0.0,No,[[  0 465   1]],It is not completely valid for natural languages because of diversity of language - this is called smoothing requirement.,"[[  94   19   59 1551 3982   21  793 8024  250   13 7322   13 1612    3
    18   48   19  718 3050   53 5971    5    1]]"
d2b3f2178a177183b1aeb88784e48ff7e3e5070c,100.0,between 0.81 and 0.88,[[   0  344 4097 4959   11 4097 4060    1]], between 0.81 and 0.88,[[ 344 4097 4959   11 4097 4060    1]]
ffc5ad48b69a71e92295a66a9a0ff39548ab3cf1,0.0,"LSTM, LSTM-L, LSTM-L, LSTM","[[   0    3 7600 2305    6    3 7600 2305   18  434    6    3 7600 2305
    18  434    6    3 7600 2305]]","GloVe embeddings trained by BIBREF10 on Wikipedia and Gigaword 5 (vocab: 400K, dim: 300), w2v-gn, Word2vec BIBREF5 trained on the Google News dataset (vocab: 3M, dim: 300), DeepWalk , node2vec","[[ 9840   553    15 25078    26    53     7  4252    57     3  5972 25582
    371  1714    30 16885    11     3 20640     9  6051   305    41  1621
  10891    10  4837   439     6 15688    10  3147   201     3   210   357
    208    18   122    29     6  4467   357   162    75     3  5972 25582
    371   755  4252    30     8  1163  3529 17953    41  1621 10891    10
    220   329     6 15688    10  3147   201  9509 29873     3     6   150
    221   357   162    75     1]]"
cca74448ab0c518edd5fc53454affd67ac1a201c,100.0,"198,112",[[    0     3 24151     6  2596   357     1]],"198,112",[[    3 24151     6  2596   357     1]]
31ee92e521be110b6a5a8d08cc9e6f90a3a97aae,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
415f35adb0ef746883fb9c33aa53b79cc4e723c3,100.0,Gendered characters in the dataset,[[    0  5945    26  3737  2850    16     8 17953     1]],Gendered characters in the dataset,[[ 5945    26  3737  2850    16     8 17953     1]]
db39a71080e323ba2ddf958f93778e2b875dcd24,17.7778,"Adding speaker role, semantic slot and dialog domain annotations helps to reduce the number of dialogue","[[    0     3 22612  5873  1075     6 27632  4918    11 13463  3303 30729
      7  1691    12  1428     8   381    13  7478]]","Our encoder-decoder framework employs separate encoding for different speakers in the dialog., We integrate semantic slot scaffold by performing delexicalization on original dialogs., We integrate dialog domain scaffold through a multi-task framework.","[[  421 23734    52    18   221  4978    52  4732  5936     7  2450     3
     35  9886    21   315  7215    16     8 13463     5     6   101  9162
  27632  4918 30777    57  5505    20 30949   138  1707    30   926 13463
      7     5     6   101  9162 13463  3303 30777   190     3     9  1249
     18 23615  4732     5     1]]"
6ff240d985bbe96b9d5042c9b372b4e8f498f264,100.0,$0.3$ million records,[[    0 20324   519  3229   770  3187     1]],$0.3$ million records,[[20324   519  3229   770  3187     1]]
d803b782023553bbf9b36551fbc888ad189b1f29,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],to judge each utterance from 1 (bad) to 3 (good) in terms of informativeness and naturalness,"[[   12  5191   284     3  5108   663    45   209    41  5514    61    12
    220    41 10452    61    16  1353    13 11152   655    11   793   655
      1]]"
abebf9c8c9cf70ae222ecb1d3cabf8115b9fc8ac,100.0,"political events such as elections, corruption cases or justice decisions","[[    0  1827   984   224    38  9768     6 13100  1488    42  4831  3055
      1]]","political events such as elections, corruption cases or justice decisions",[[ 1827   984   224    38  9768     6 13100  1488    42  4831  3055     1]]
26012f57cba21ba44b9a9f7ed8b1ed9e8ee7625d,100.0,PV-DM,[[    0 16303    18  7407     1]],PV-DM,[[16303    18  7407     1]]
f7789313a804e41fcbca906a4e5cf69039eeef9f,40.0,"Reuters-21578, LabelMe",[[    0     3 18844  4949  1808  3940     6 16229   329    15     1]],"Reuters-21578 BIBREF30,  LabelMe BIBREF31, 20-Newsgroups benchmark corpus BIBREF29 ","[[    3 18844  4949  1808  3940     3  5972 25582   371  1458     6 16229
    329    15     3  5972 25582   371  3341     6   460    18  6861     7
  10739     7 15705 11736   302     3  5972 25582   371  3166     1]]"
98daaa9eaa1e1e574be336b8933b861bfd242e5e,0.0,Combination of Vision and Language,[[    0 20578   257    13 10886    11 10509     1]],"weakly labeled into hate or non-hate memes, depending on their source","[[ 5676   120  3783    15    26   139  5591    42   529    18   547    15
  23289     7     6  3345    30    70  1391     1]]"
0fec9da2bc80a12a7a6d6600b9ecf3e122732b60,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
3213529b6405339dfd0c1d2a0f15719cdff0fa93,0.0,"a linear logistic regression model (LRM), a distributed neural network (DNN)","[[    0     3     9 13080 28820 26625   825    41 12564   329   201     3
      9  8308 24228  1229    41 12145   567    61]]","The baseline models used are DrQA modified to support answering no answer questions, DrQA+CoQA which is pre-tuned on CoQA dataset, vanilla BERT, BERT+review tuned on domain reviews, BERT+CoQA tuned on the supervised CoQA data","[[   37 20726  2250   261    33   707 23008  8473    12   380 18243   150
   1525   746     6   707 23008  1220  3881 23008    84    19   554    18
     17   444    26    30   638 23008 17953     6 13478   272 24203     6
    272 24203  1220    60  4576 14007    30  3303  2456     6   272 24203
   1220  3881 23008 14007    30     8     3 23313   638 23008   331     1]]"
417dabd43d6266044d38ed88dbcb5fdd7a426b22,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining,"[[ 3303    18 24315    15    26  1514     2  3229  2775   439 16513    11
      3 17518     3  3443    13  6080  1499 21527    45   765  1688   261
     57     3  5972 25582   371   948     3     9 17149 11505    10 13275
      1]]"
6959e87cf2668a03854da3f042c87e6fdb2ade8a,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
ffbd6f583692db66b719a846ba2b7f6474df481a,0.0,LSTM,[[   0    3 7600 2305    1]],model is composed of an encoder (§SECREF5) and a decoder with the attention mechanism (§SECREF7) that are both implemented using recurrent neural networks (RNNs),"[[  825    19 10431    13    46 23734    52    41     2  4132  4545  9976
   9120    11     3     9    20  4978    52    28     8  1388  8557    41
      2  4132  4545  9976 12703    24    33   321  6960   338     3    60
  14907 24228  5275    41 14151   567     7    61     1]]"
40f87db3a8d1ac49b888ce3358200f7d52903ce7,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
03a911049b6d7df2b6391ed5bc129a3b65133bcd,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
9d5153a7553b7113716420a6ddceb59f877eb617,100.0,No,[[  0 465   1]],No,[[465   1]]
751aa2b1531a17496536887288699cc8d5c3cec9,0.0,"spelling mistakes, typographic errors, colloquialisms, abbreviations, ","[[    0 19590  8176     6 23042 14797  6854     6  8029    32  1169  6835
      7     6   703  1999  2099  1628     6     3]]","Hence WordPiece tokenizer tokenizes noisy words into subwords. However, it ends up breaking them into subwords whose meaning can be very different from the meaning of the original word. Often, this changes the meaning of the sentence completely, therefore leading to substantial dip in the performance.","[[    3 13151  4467   345    23    15   565 14145  8585 14145  1737     7
  26847  1234   139   769  6051     7     5   611     6    34  5542    95
   7814   135   139   769  6051     7     3  2544  2530    54    36   182
    315    45     8  2530    13     8   926  1448     5     3 10084     6
     48  1112     8  2530    13     8  7142  1551     6  2459  1374    12
   7354 10823    16     8   821     5     1]]"
e2427f182d7cda24eb7197f7998a02bc80550f15,13.7931,the learning algorithm is fault-tolerant and persistent in real time,"[[    0     8  1036 12628    19  7828    18 23267    11 15803    16   490
     97     1]]",By using Apache Spark which stores all executions in a lineage graph and recovers to the previous steady state from any fault,"[[  938   338 24263 15036    84  3253    66  9328     7    16     3     9
    689   545  8373    11  8303     7    12     8  1767 11207   538    45
    136  7828     1]]"
b3bd217287b8c765b0d461dc283afec779dbf039,100.0,hybrid approach,[[   0 9279 1295    1]],hybrid approach,[[9279 1295    1]]
a7f07ae48eed084c3144214228f4ecb72bc0a0e3,0.0,SAN-VOES,[[    0     3 19976    18  8040  3205     1]],"IMG-only, QUES-only, SAN, SANDY,  VOES-Oracle, VOES","[[   27  9306    18  9926     6     3 15367   134    18  9926     6     3
  19976     6     3 19976 19409     6     3  8040  3205    18  7395     9
   2482     6     3  8040  3205     1]]"
5d5a571ff04a5fdd656ca87f6525a60e917d6558,100.0,No,[[  0 465   1]],No,[[465   1]]
26faad6f42b6d628f341c8d4ce5a08a591eea8c2,100.0,508,[[  0 943 927   1]],508,[[943 927   1]]
3fa638e6167e1c7a931c8ee5c0e2e397ec1b6cda,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
71d59c36225b5ee80af11d3568bdad7425f17b0c,14.2857,BLSTM-CNN-CRF - 82.0% - 82,"[[    0   272  7600  2305    18   254 17235    18  4545   371     3    18
      3  4613     5  6932     3    18     3  4613]]",Best BLSTM-CNN-CRF had F1 score 86.87 vs 86.69 of best BLSTM-CRF ,"[[ 1648   272  7600  2305    18   254 17235    18  4545   371   141   377
    536  2604     3  3840     5  4225     3   208     7     3  3840     5
   3951    13   200   272  7600  2305    18  4545   371     1]]"
9c4a4dfa7b0b977173e76e2d2f08fa984af86f0e,0.0,TP-N2F lags behind LSTM-based Seq2S,"[[   0    3 7150   18  567  357  371    3 5430    7 1187    3 7600 2305
    18  390  679 1824  357  134]]","Full Testing Set accuracy: 84.02
Cleaned Testing Set accuracy: 93.48","[[ 4043 19693  2821  7452    10     3  4608     5  4305   205 30233 19693
   2821  7452    10     3  4271     5  3707     1]]"
3996438cef34eb7bedaa6745b190c69553cf246b,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
4542a4e7eabb8006fb7bcff2ca6347cfb3fbc56b,100.0,No,[[  0 465   1]],No,[[465   1]]
e44a5514d7464993997212341606c2c0f3a72eb4,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
fbb85cbd41de6d2818e77e8f8d4b91e431931faa,0.0,Amazon Mechanical Turk (AMT),[[    0  2536 24483 23694    41   188  7323    61     1]],asked the authors to rank by closeness five citations we selected from their paper,"[[ 1380     8  5921    12 11003    57   885   655   874     3 13903     7
     62  2639    45    70  1040     1]]"
c45feda62f23245f53e855706e2d8ea733b7fd03,47.0588,attention-based translation model with convolutional neural network (CNN),"[[    0  1388    18   390  7314   825    28   975 24817   138 24228  1229
     41   254 17235    61     1]]",Attention-based translation model with convolution sequence to sequence model,"[[20748    18   390  7314   825    28   975 24817  5932    12  5932   825
      1]]"
48fa2ccc236e217fcf0e5aab0e7a146faf439b02,100.0,No,[[  0 465   1]],No,[[465   1]]
e12674f0466f8c0da109b6076d9939b30952c7da,0.0,Statistic model of word embeddings,[[    0  7135  3040   825    13  1448 25078    26    53     7     1]],FastText,[[ 6805 13598    17     1]]
58c1b162a4491d4a5ae0ff86cc8bd64e98739620,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
5d164651a4aed7cf24d53ba9685b4bee8c965933,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
095888f6e10080a958d9cd3f779a339498f3a109,0.0,MWP 2000 dataset,[[    0     3 16027   345  2766 17953     1]],"AI2 BIBREF2, CC BIBREF19, IL BIBREF4, MAWPS BIBREF20","[[ 7833   357     3  5972 25582   371  4482     3  2823     3  5972 25582
    371  2294     6     3  3502     3  5972 25582   371  8525  4800   518
   4176     3  5972 25582   371  1755     1]]"
63eb31f613a41a3ddd86f599e743ed10e1cd07ba,100.0,Hindi-English,[[    0 25763    18 26749     1]],Hindi-English,[[25763    18 26749     1]]
657edbf39c500b2446edb9cca18de2912c628b7d,33.3333,Perplexity score 82.0%,[[   0 1915 9247  485 2604    3 4613    5 6932    1]],Perplexity score 142.84 on dev and 138.91 on test,"[[ 1915  9247   485  2604     3 24978     5  4608    30    20   208    11
      3 22744     5  4729    30   794     1]]"
5f4e6ce4a811c4b3ab07335d89db2fd2a8d8d8b2,100.0,accuracy,[[   0 7452    1]],accuracy,[[7452    1]]
03b939ad70593f6475c56e9be73ba409d33faa62,0.0,"BIBREF2, BIBREF3","[[    0     3  5972 25582   371   357     3     6     3  5972 25582   371
    519     1]]","LEAD, QUERY_SIM, MultiMR, SVR, DocEmb, ISOLATION","[[  301 19552     6     3 15367 11824   834   134  5166     6  4908  9320
      6   180 13556     6 17268 17467     6  8162   434  8015     1]]"
a5abd4dd91e6f2855e9098bd6ae1481c0fdb0d4a,0.0,"TEXT-0, TEXT-0, TEXT-","[[   0    3 3463    4  382   18  632    6    3 3463    4  382   18  632
     6    3 3463    4  382   18]]","TB-Dense,  MATRES",[[    3  9041    18   308  5167     6     3 18169 12200     1]]
ad16c8261c3a0b88c685907387e1a6904eb15066,24.2424,"affective information into chatbots improve user-satisfaction, improve user-engagement","[[    0  2603   757   251   139  3582  4045     7  1172  1139    18  9275
      7    89  4787     6  1172  1139    18 25123]]","how to incorporate affective information into chatbots, what are resources that available and can be used to build EAC, and how to evaluate EAC performance","[[ 149   12 6300 2603  757  251  139 3582 4045    7    6  125   33 1438
    24  347   11   54   36  261   12  918  262 5173    6   11  149   12
  6825  262 5173  821    1]]"
45be26c01e82835d9949529003c6b64f90db3d1a,100.0,Twitter definition of hateful conduct,[[   0 3046 4903   13 5591 1329 3498    1]],Twitter definition of hateful conduct,[[3046 4903   13 5591 1329 3498    1]]
1d72770d075b22411ec86d8bdee532f8c643740b,0.0,4%,[[   0    3 5988    1]],"3.1 F1 gain on the original dev set, 11 F1 gain on the multi-hop dev set, 10 F1 gain on the out-of-domain dev set.","[[    3 18495   377   536  2485    30     8   926    20   208   356     6
    850   377   536  2485    30     8  1249    18 10776    20   208   356
      6   335   377   536  2485    30     8    91    18   858    18 22999
     20   208   356     5     1]]"
d28260b5565d9246831e8dbe594d4f6211b60237,0.0,Plackett-Luce Model for SMT Reranking,"[[    0  8422    75 12922    18 11748    15  5154    21   180  7323   419
   6254    53     1]]",We empirically provide a formula to measure the richness in the scenario of machine translation.,"[[  101 23941   120   370     3     9  5403    12  3613     8  2354   655
     16     8  8616    13  1437  7314     5     1]]"
98ba7a7aae388b1a77dd6cab890977251d906359,100.0,No,[[  0 465   1]],No,[[465   1]]
56c6ff65c64ca85951fdea54d6b096f28393c128,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
4d05a264b2353cff310edb480a917d686353b007,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],The HMM can identify punctuation or pick up on vowels.,"[[   37   454  8257    54  2862  5427    76   257    42  1432    95    30
  22121  3573     5     1]]"
58e65741184c81c9e7fe0ca15832df2d496beb6f,100.0,No,[[  0 465   1]],No,[[465   1]]
d3092cd32cd581a57fa4844f80fe18d6b920e903,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"LSTM with text embedding, LSTM with emoji embedding, Attention-based LSTM with emojis","[[    3  7600  2305    28  1499 25078    26    53     6     3  7600  2305
     28     3    15    51 21892 25078    26    53     6 20748    18   390
      3  7600  2305    28     3    15    51 21892     7     1]]"
382bef47d316d7c12ea190ae160bf0912a0f4ffe,100.0,Manual verification,[[    0  9950 17549     1]],Manual verification,[[ 9950 17549     1]]
1383ddd4619cf81227c72f3d9f30c10c47a0cdad,0.0,"a CTC-trained DNN, a CTC-trained DNN","[[    0     3     9   205  3838    18    17 10761   309 17235     6     3
      9   205  3838    18    17 10761   309 17235]]",Our baseline system (Baseline_1850K) is taken from BIBREF13 . ,"[[  421 20726   358    41 14885    15   747   834  2606  1752   439    61
     19  1026    45     3  5972 25582   371  2368     3     5     1]]"
1898f999626f9a6da637bd8b4857e5eddf2fc729,4.1667,F1 score of 82.0% compared to previous work.,"[[   0  377  536 2604   13    3 4613    5 6932    3 2172   12 1767  161
     5    1]]","WordDecoding (WDec) model achieves F1 scores that are $3.9\%$ and $4.1\%$ higher than HRL on the NYT29 and NYT24 datasets respectively, PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\%$ and $1.3\%$ higher than HRL on the NYT29 and NYT24 datasets respectively","[[ 4467  2962  9886    41 17698    15    75    61   825  1984     7   377
    536  7586    24    33  1514 28640     2  1454  3229    11  1514 19708
      2  1454  3229  1146   145  8383   434    30     8  5825   382  3166
     11  5825   382  2266 17953     7  6898     6   276    17    52  9688
   2962  9886    41 15420  2962    75    61   825  1984     7   377   536
   7586    24    33  5583     5   632     2  1454  3229    11  1514 13606
      2  1454  3229  1146   145  8383   434    30     8  5825   382  3166
     11  5825   382  2266 17953     7  6898     1]]"
157b9f6f8fb5d370fa23df31de24ae7efb75d6f3,12.766,Gender prediction task,[[    0   350  3868 21332  2491     1]],"They achieved best result in the PAN 2017 shared task with accuracy for Variety prediction task 0.0013 more than the 2nd best baseline, accuracy for Gender prediction task 0.0029 more than 2nd best baseline and accuracy for Joint prediction task 0.0101 more than the 2nd best baseline","[[  328  5153   200   741    16     8   276  5033  1233  2471  2491    28
   7452    21 31884 21332  2491     3 10667  2368    72   145     8   204
    727   200 20726     6  7452    21   350  3868 21332  2491     3 10667
   3166    72   145   204   727   200 20726    11  7452    21 16761 21332
   2491     3 11739 19621    72   145     8   204   727   200 20726     1]]"
b217d9730ba469f48426280945dbb77542b39183,0.0,LSTM,[[   0    3 7600 2305    1]],"Caravel, COAV and NNCD",[[ 1184     9  4911     6  2847  6968    11     3 17235  6931     1]]
76405b76b930a5bbe895e9e96ce4a3cff1b0b1a1,11.4286,"image and question pairing, attention, projection, reasoning, input fusion, input attention, generation","[[    0  1023    11   822 22073     6  1388     6 13440     6 20893     6
   3785     3  7316     6  3785  1388     6  3381]]","pre-trained text representations, transformer-based encoders together with GRU models, attention mechanisms are paramount for learning top performing networks, Top-Down is the preferred attention method","[[  554    18    17 10761  1499  6497     7     6 19903    18   390 23734
     52     7   544    28   350  8503  2250     6  1388 12009    33 26979
     21  1036   420  5505  5275     6  2224    18   308  9197    19     8
   6241  1388  1573     1]]"
fc06502fa62803b62f6fd84265bfcfb207c1113b,100.0,"Annotators who were not security experts, researchers in either NLP or computer security","[[   0  389 2264 6230  113  130   59 1034 2273    6 4768   16  893  445
  6892   42 1218 1034    1]]","annotators who were not security experts, researchers in either NLP or computer security","[[  46 2264 6230  113  130   59 1034 2273    6 4768   16  893  445 6892
    42 1218 1034    1]]"
fe90eec1e3cdaa41d2da55864c86f6b6f042a56c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"User reviews written in Chinese collected online for hotel, mobile phone, and travel domains","[[6674 2456 1545   16 2830 4759  367   21 1595    6 1156  951    6   11
  1111 3303    7    1]]"
6c4d121d40ce6318ecdc141395cdd2982ba46cff,100.0,"BIBREF7, BIBREF26","[[    0     3  5972 25582   371   940     6     3  5972 25582   371  2688
      1]]","BIBREF7, BIBREF26 ",[[    3  5972 25582   371   940     6     3  5972 25582   371  2688     1]]
922f1b740f8b13fdc8371e2a275269a44c86195e,0.0,No,[[  0 465   1]],Yes,[[2163    1]]
a9acd1af4a869c17b95ec489cdb1ba7d76715ea4,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
2ff3898fbb5954aa82dd2f60b37dd303449c81ba,0.0,Wiktionary,[[    0  2142 12696  1208     1]],"Penn Treebank, Text8, WSJ10",[[11358  7552  4739     6  5027 11864     3  8439   683  1714     1]]
bfd4fc82ffdc5b2b32c37f4222e878106421ce2a,11.7647,DM+ model does not require that supporting facts (i.e. the facts that are,"[[   0    3 7407 1220  825  405   59 1457   24 3956 6688   41   23    5
    15    5    8 6688   24   33]]",the input fusion layer to allow interactions between input facts and a novel attention based GRU that allows for logical reasoning over ordered inputs. ,"[[    8  3785     3  7316  3760    12   995  9944   344  3785  6688    11
      3     9  3714  1388     3   390   350  8503    24  1250    21     3
   6207 20893   147  5563  3785     7     5     1]]"
79a28839fee776d2fed01e4ac39f6fedd6c6a143,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Proposing an improved RNN model, the phonetic temporal neural LID approach, based on phonetic features that results in better performance","[[  749  2748    53    46  3798   391 17235   825     6     8   951  1225
  10301  8563 24228   301  4309  1295     6     3   390    30   951  1225
    753    24   772    16   394   821     1]]"
e9209ebf38c4ae4d93884f68c7b5b3444e0604f3,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
62c4c8b46982c3fcf5d7c78cd24113635e2d7010,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
99760276cfd699e55b827ceeb653b31b043b9ceb,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Morphological analysis is the task of creating a morphosyntactic description for a given word,  inflectional realization is framed as a mapping from the pairing of a lemma with a set of morphological tags to the corresponding word form","[[ 4574  9553  6207  1693    19     8  2491    13  1577     3     9     3
   8886    32     7    63    29    17  2708   447  4210    21     3     9
    787  1448     6    16 26303  6318 23543    19     3 20157    38     3
      9 14670    45     8 22073    13     3     9    90   635     9    28
      3     9   356    13     3  8886  4478 12391    12     8     3  9921
   1448   607     1]]"
6ad92aad46d2e52f4e7f3020723922255fd2b603,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
37c7c62c9216d6cf3d0858cf1deab6db4b815384,64.0,Annotation was done using annotators from Amazon Mechanical Turk (AMT),"[[    0   389  2264   257    47   612   338    46  2264  6230    45  2536
  24483 23694    41   188  7323    61     1]]",Annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations,"[[  389  2264   257    47   612    28     8   199    13    46  2264  6230
     45  2536 24483 23694    30     3 20317  4995     7    13  9029     1]]"
3ebdc15480250f130cf8f5ab82b0595e4d870e2f,100.0,77 genres,[[   0    3 4013 5349    7    1]],77 genres,[[   3 4013 5349    7    1]]
c7d99e66c4ab555fe3d616b15a5048f3fe1f3f0e,0.0,"statistical machine translation, part-of-speech tagging, chunking, and","[[    0 11775  1437  7314     6   294    18   858    18     7   855 10217
      3    17 15242     6 16749    53     6    11]]",Visualization of State of the union addresses,[[10893  1707    13  1015    13     8  7021  7181     1]]
81064bbd0a0d72a82d8677c32fb71b06501830a0,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"ROUGE-1 increases by 0.05, ROUGE-2 by 0.06 and ROUGE-L by 0.09","[[  391 26260   427  2292  5386    57     3 25079     6   391 26260   427
   4949    57  4097  5176    11   391 26260   427    18   434    57  4097
   4198     1]]"
bc26eee4ef1c8eff2ab8114a319901695d044edb,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"pairing crowdworkers and having half of them acting as Wizards by limiting their dialogue options only to relevant and plausible ones, at any one point in the interaction","[[22073  4374  1981   277    11   578   985    13   135  6922    38 23382
      7    57     3 17979    70  7478   931   163    12  2193    11 31323
   2102     6    44   136    80   500    16     8  6565     1]]"
52f1a91f546b8a25a5d72325c503ec8f9c72de23,0.0,BIBREF14,[[    0     3  5972 25582   371  2534     1]],RNNLM BIBREF11,[[  391 17235 11160     3  5972 25582   371  2596     1]]
79bd2ad4cb5c630ce69d5a859ed118132cae62d7,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
3a9d391d25cde8af3334ac62d478b36b30079d74,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
f5707610dc8ae2a3dc23aec63d4afa4b40b7ec1e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Variables in the set {str, prec, attr} indicating in which mode the mention should be resolved.","[[12928   179     7    16     8   356     3     2     7    17    52     6
    554    75     6    44    17    52     2     3 15716    16    84  2175
      8  2652   225    36 13803     5     1]]"
63bb39fd098786a510147f8ebc02408de350cb7c,100.0,No,[[  0 465   1]],No,[[465   1]]
12eba1598dca14db64dbc8b73484639363a4618e,18.1818,"word embeddings, word embeddings, hyphens, hyp","[[    0  1448 25078    26    53     7     6  1448 25078    26    53     7
      6     3 13397  3225     7     6     3 13397]]","word unigrams, bigrams, and trigrams","[[1448   73   23 5096    7    6  600 2375    7    6   11 6467 5096    7
     1]]"
21a9f1cddd7cb65d5d48ec4f33fe2221b2a8f62e,100.0,"$150,000$ tweets",[[    0  1970  9286  3229 10657     7     1]],"$150,000$ tweets",[[ 1970  9286  3229 10657     7     1]]
40b9f502f15e955ba8615822e6fa08cb5fd29c81,0.0,"BIBREF0, BIBREF1","[[    0     3  5972 25582   371   632     6     3  5972 25582   371   536
      1]]","Corpus 5KL, Corpus 8KF",[[10052   302   305   439   434     6 10052   302   505   439   371     1]]
9651fbd887439bf12590244c75e714f15f50f73d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The advantage of pre-training gradually diminishes with the increase of labeled data, Fixed representations yield better results than fine-tuning in some cases, pre-training the Seq2Seq encoder outperforms pre-training the decoder","[[   37  2337    13   554    18 13023 11556 26999    15     7    28     8
    993    13  3783    15    26   331     6 21378  6497     7  6339   394
    772   145  1399    18    17   202    53    16   128  1488     6   554
     18 13023     8   679  1824   357   134    15  1824 23734    52    91
    883  2032     7   554    18 13023     8    20  4978    52     1]]"
b622f57c4e429b458978cb8863978d7facab7cfe,6.6667,they represent conceptual neighbours as vectors,[[    0    79  4221 17428 14245     7    38 12938     7     1]],"Once this classifier has been trained, we can then use it to predict conceptual neighborhood for categories for which only few instances are known.","[[ 1447    48   853  7903    65   118  4252     6    62    54   258   169
     34    12  9689 17428  5353    21  5897    21    84   163   360 10316
     33   801     5     1]]"
99cf494714c67723692ad1279132212db29295f3,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"AQA diverges from well structured language in favour of less fluent, but more effective, classic information retrieval (IR) query operations","[[   71 23008 12355  2897    45   168 14039  1612    16 14499    13   705
   6720   295     6    68    72  1231     6  2431   251 24515   138    41
   5705    61 11417  2673     1]]"
3a3a65c65cebc2b8c267c334e154517d208adc7d,0.0,LSTM,[[   0    3 7600 2305    1]],"Multi-Encoder, Constrained-Decoder model","[[ 4908    18  8532  4978    52     6  1193 22418    18  2962  4978    52
    825     1]]"
94e0cf44345800ef46a8c7d52902f074a1139e1a,20.0,"MUC, MUC2M, MUC3M",[[   0  283 6463    6  283 6463  357  329    6  283 6463  519  329    1]],"MUC, CoNLL, ACE, OntoNotes, MSM, Ritter, UMBC","[[  283  6463     6   638   567 10376     6     3 11539     6   461   235
  10358    15     7     6   283  4212     6 11671   449     6     3  6122
   7645     1]]"
8b43201e7e648c670c02e16ba189230820879228,100.0,No,[[  0 465   1]],No,[[465   1]]
8c78b21ec966a5e8405e8b9d3d6e7099e95ea5fb,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM),"[[ 4494  1036   445  6892  2250    24   169   975 24817   138 24228  1229
     41   254 17235    61     3  5972 25582   371   927    11  2647    18
  26352   307   710    18  1987  2594    41   279    23  7600  2305    61
      1]]"
7e51490a362135267e75b2817de3c38dfe846e21,16.6667,"local features such as price, size, battery life, etc.","[[   0  415  753  224   38  594    6  812    6 3322  280    6  672    5
     1]]"," Local, shallow features based mostly on orthographic, word shape and n-gram features plus their context","[[ 4593     6 16906   753     3   390  3323    30 26429 14797     6  1448
   2346    11     3    29    18  5096   753   303    70  2625     1]]"
fc6cfac99636adda28654e1e19931c7394d76c7c,0.0,a neural approach BIBREF6,[[    0     3     9 24228  1295     3  5972 25582   371   948     1]]," We devise a simple clustering algorithm to approximate this process. First, we initialize with random cluster assignments and define “cluster strength” to be the relative difference between “intra-group” Euclidean distance and “inter-group” Euclidean distance. Then, we iteratively propose random exchanges of memberships, only accepting these proposals when the cluster strength increases, until convergence. ","[[  101 13282    15     3     9   650  9068    53 12628    12 24672    48
    433     5  1485     6    62  2332  1737    28  6504  9068 14023    11
   6634   105    75 26156  2793   153    12    36     8  5237  1750   344
    105 20322     9    18 10739   153  4491 14758   221   152  2357    11
    105  3870    18 10739   153  4491 14758   221   152  2357     5    37
     29     6    62    34    49  1528   120  4230  6504  2509     7    13
   4757     7     6   163 13643   175 12152   116     8  9068  2793  5386
      6   552 31098     5     1]]"
00d6228bcd6b839529e52d0d622bf787a9356158,14.2857,Target accuracy is evaluated by comparing the model to the baseline in both TSA and target validation,"[[    0 12615  7452    19 14434    57     3 14622     8   825    12     8
  20726    16   321   332  4507    11  2387 16148]]","precision ($P.$), recall ($R.$) and $F_1$ scores for target recognition and targeted sentiment","[[11723  8785   345     5  3229   201  7881  8785   448     5  3229    61
     11  1514   371   834   536  3229  7586    21  2387  5786    11  7774
   6493     1]]"
e9d9bb87a5c4faa965ceddd98d8b80d4b99e339e,0.0,EV: EV: EV: EV: EV: EV: ,"[[   0    3 8878   10    3 8878   10    3 8878   10    3 8878   10    3
  8878   10    3 8878   10    3]]","On subtask 3 best proposed model has F1 score of 92.18 compared to best previous F1 score of 88.58.
On subtask 4 best proposed model has 85.9, 89.9 and 95.6 compared to best previous results of 82.9, 84.0 and 89.9 on 4-way, 3-way and binary aspect polarity.","[[  461   769 23615   220   200  4382   825    65   377   536  2604    13
      3  4508     5  2606     3  2172    12   200  1767   377   536  2604
     13     3  4060     5  3449     5   461   769 23615   314   200  4382
    825    65 11989     5  1298     6     3  3914     5  1298    11   668
  25134     3  2172    12   200  1767   772    13   505 27297     6   505
  15021    11     3  3914     5  1298    30  7769  1343     6  5354  1343
     11 14865  2663     3  9618   485     5     1]]"
2e9c6e01909503020070ec4faa6c8bf2d6c0af42,100.0,the author and the supervisor,[[    0     8  2291    11     8 14640     1]],the author and the supervisor,[[    8  2291    11     8 14640     1]]
9d016eb3913b41f7a18c6fa865897c12b5fe0212,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
827c58f6cab6c6fe7a6c43bdc71150b61ba0eed4,16.1616,"agreement score of 0.88 with respect to the vaccine category and the category, and the agreement score","[[    0  2791  2604    13  4097  4060    28  1445    12     8 12956  3295
     11     8  3295     6    11     8  2791  2604]]"," Relevance and Subject categorizations are annotated at a percent agreement of $0.71$ and $0.70$, their agreement scores are only fair, at $\alpha =0.27$ and $\alpha =0.29$, Stance and Sentiment, which carry more categories than the former two, is $0.54$ for both. Their agreement scores are also fair, at $\alpha =0.35$ and $\alpha =0.34$, This holds for the Relevant category ($0.81$), the Vaccine category ($0.79$) and the Positive category ($0.64$),  The Negative category yields a mutual F-score of $0.42$, which is higher than the more frequently annotated categories Neutral ($0.23$) and Not clear ($0.31$).","[[31484   565    11 19237  9624   122   127  1707     7    33    46  2264
    920    44     3     9  1093  2791    13 20324  4450  3229    11 20324
   2518  3229     6    70  2791  7586    33   163  2725     6    44  1514
      2   138  6977  3274 18189   940  3229    11  1514     2   138  6977
   3274 18189  1298  3229     6   472   663    11  4892  2998   295     6
     84  2331    72  5897   145     8  1798   192     6    19 20324  5062
   3229    21   321     5  2940  2791  7586    33    92  2725     6    44
   1514     2   138  6977  3274 19997   755  3229    11  1514     2   138
   6977  3274 19997   591  3229     6   100  4532    21     8 31484    17
   3295  8785 22384   536  3229   201     8     3 31334    15  3295  8785
  22426  1298  3229    61    11     8 24972  3295  8785 22787   591  3229
    201    37 17141  1528  3295  6339     7     3     9  8543   377    18
      7  9022    13 20324  4165  3229     6    84    19  1146   145     8
     72  4344    46  2264   920  5897  3617  8792  8785 18189   519  3229
     61    11   933   964  8785 19997   536  3229   137     1]]"
999b20dc14cb3d389d9e3ba5466bc3869d2d6190,0.0,systematic survey on recent progress in NQG,[[    0 20036  3719    30  1100  2188    16   445  2247   517     1]],Kim et al. (2019),[[6777    3   15   17  491    5   41 8584   61    1]]
64b65687b82ddb17c3d068381aaee56eb7fc02cd,50.0,Twitter dataset from the original BIBREF11,[[    0  3046 17953    45     8   926     3  5972 25582   371  2596     1]],Twitter dataset obtained from the authors of BIBREF12,"[[ 3046 17953  5105    45     8  5921    13     3  5972 25582   371  2122
      1]]"
22b8836cb00472c9780226483b29771ae3ebdc87,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data.,"[[  328  2332  1737    70  1448    11 10409 25078    26    53     7    28
  12938     7   554    18    17 10761   147     3     9   508 11736   302
     13    73  9339   400    26   331     5     1]]"
4146e1d8f79902c0bc034695998b724515b6ac81,25.0,WN18 context corpus,[[    0     3 21170  2606  2625 11736   302     1]],CoNLL-2012 shared task BIBREF21 corpus,"[[  638   567 10376    18 12172  2471  2491     3  5972 25582   371  2658
  11736   302     1]]"
0fa81adf00662694e1dc74475ae2b9283c50748c,100.0,parameter sharing,[[    0 15577  2178     1]],parameter sharing,[[15577  2178     1]]
8f8f2b0046e1a78bd34c0c3d6b6cb24463a8ed7f,100.0,"English, Chinese",[[   0 1566    6 2830    1]],"English, Chinese",[[1566    6 2830    1]]
4e468ce13b7f6ac05371c62c08c3cec1cd760517,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
41bff17f7d7e899c03b051e20ef01f0ebc5c8bb1,22.2222,BLEU and TER scores,[[    0     3  8775 12062    11     3  5946  7586     1]],ROUGE BIBREF29 and METEOR BIBREF30,"[[  391 26260   427     3  5972 25582   371  3166    11  7934  3463  2990
      3  5972 25582   371  1458     1]]"
aeab5797b541850e692f11e79167928db80de1ea,9.5238,they use the knowledge graph embeddings as a text representation model and enrich BERT,"[[    0    79   169     8  1103  8373 25078    26    53     7    38     3
      9  1499  6497   825    11 12812   272 24203]]",all three representations are concatenated and passed into a MLP,"[[  66  386 6497    7   33  975 2138   35  920   11 2804  139    3    9
   283 6892    1]]"
ace60950ccd6076bf13e12ee2717e50bc038a175,0.0,partisan news detection,[[    0     3 18237  1506 10664     1]],They pre-train the models using 600000 articles as an unsupervised dataset and then fine-tune the models on small training set.,"[[  328   554    18  9719     8  2250   338  7366  2313  2984    38    46
     73 23313 17953    11   258  1399    18    17   444     8  2250    30
    422   761   356     5     1]]"
ba05a53f5563b9dd51cc2db241c6e9418bc00031,9.7561,"Different from the standard discriminative training formulation, we focus on an unsupervised inference method,","[[    0 13936    45     8  1068  9192  1528   761 20029     6    62   992
     30    46    73 23313    16 11788  1573     6]]","the best permutation is decided by $\mathcal {J}_{\text{SEQ}}(\mathbf {L}_{un}^{(s^{\prime })},\mathbf {L}_{un}^{(r)})$ , which is the sequence discriminative criterion of taking the $s^{\prime }$ -th permutation in $n$ -th output inference stream at utterance $u$","[[    8   200   399  4246   257    19  1500    57  1514     2  3357   107
   1489     3     2   683     2   834     2  6327     2   134 23346     2
    599     2  3357   107   115    89     3     2   434     2   834     2
    202     2   599     7     2  8234    15     3     2    61     2     6
      2  3357   107   115    89     3     2   434     2   834     2   202
      2   599    52    61     2    61  3229     3     6    84    19     8
   5932  9192  1528     3  2685 10140    29    13   838     8  1514     7
      2  8234    15     3     2  3229     3    18   189   399  4246   257
     16  1514    29  3229     3    18   189  3911    16 11788  6093    44
      3  5108   663  1514    76  3229     1]]"
f74eaee72cbd727a6dffa1600cdf1208672d713e,100.0,QA pairs per predicate,[[    0     3 23008 14152   399   554  4370   342     1]],QA pairs per predicate,[[    3 23008 14152   399   554  4370   342     1]]
d76ecdc0743893a895bc9dc3772af47d325e6d07,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
05196588320dfb0b9d9be7d64864c43968d329bc,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
569ad21441e99ae782d325d5f5e1ac19e08d5e76,70.0,"screen name of the user, screen name of the news article the comment is for","[[   0 1641  564   13    8 1139    6 1641  564   13    8 1506 1108    8
  1670   19   21    1]]","title of the news article, screen name of the user",[[2233   13    8 1506 1108    6 1641  564   13    8 1139    1]]
9f3444c9fb2e144465d63abf58520cddd4165a01,0.0,korean-english,[[   0    3 5543   15  152   18 4606   40 1273    1]],gu-EtAl:2018:EMNLP1,"[[   3 1744   18  427   17  188   40   10 9457   10 6037  567 6892  536
     1]]"
42812113ec720b560eb9463ff5e74df8764d1bff,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
891c4af5bb77d6b8635ec4109572de3401b60631,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
7d5ba230522df1890619dedcfb310160958223c1,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
f7070b2e258beac9b09514be2bfcc5a528cc3a0e,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
dcd22abfc9e7211925c0393adc30dbd4711a9f88,0.0,300-word N-gram corpus,[[    0  3147    18  6051   445    18  5096 11736   302     1]],10 million sentences gathered from Wikipedia,[[  335   770 16513     3  9094    45 16885     1]]
10ddc5caf36fe9d7438eb5a3936e24580c4ffe6a,0.0,"LSTM, MATRES",[[    0     3  7600  2305     6     3 18169 12200     1]],For relation prediction they test TransE and for relation extraction they test position aware neural sequence model,"[[  242  4689 21332    79   794  4946   427    11    21  4689 16629    79
    794  1102  2718 24228  5932   825     1]]"
5a33ec23b4341584a8079db459d89a4e23420494,56.25,public dashboard where competitors can see their results after competition has finished completing task,"[[    0   452 16740   213  9216    54   217    70   772   227  2259    65
   2369     3  8828  2491     1]]","Public dashboard where competitors can see their results during competition, on part of the test set (public test set).","[[ 2575 16740   213  9216    54   217    70   772   383  2259     6    30
    294    13     8   794   356    41 15727   794   356   137     1]]"
65e6a1cc2590b139729e7e44dce6d9af5dd2c3b5,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"do not follow a particular plan or pursue a particular fixed information need,  integrating content found via search with content from structured data, at each system turn, there are a large number of conversational moves that are possible, most other domains do not have such high quality structured data available, live search may not be able to achieve the required speed and efficiency","[[  103    59  1130     3     9  1090   515    42  6665     3     9  1090
   3599   251   174     6     3 20030   738   435  1009   960    28   738
     45 14039   331     6    44   284   358   919     6   132    33     3
      9   508   381    13  3634   138  6914    24    33   487     6   167
    119  3303     7   103    59    43   224   306   463 14039   331   347
      6   619   960   164    59    36     3   179    12  1984     8   831
   1634    11  3949     1]]"
2941874356e98eb2832ba22eae9cb08ec8ce0308,0.0,BLEU scores,[[    0     3  8775 12062  7586     1]],"TF-IDF + SVM, Depeche + SVM, NRC + SVM, TF-NRC + SVM, Doc2Vec + SVM,  Hierarchical RNN, BiRNN + Self-Attention, ELMo + BiRNN,  Fine-tuned BERT","[[    3  9164    18  4309   371  1768   180 12623     6   374   855  1033
   1768   180 12623     6   445  4902  1768   180 12623     6     3  9164
     18  9142   254  1768   180 12623     6 17268   357   553    15    75
   1768   180 12623     6  3204  7064  1950   391 17235     6  2106 14151
    567  1768  8662    18   188    17  9174     6   262 11160    32  1768
   2106 14151   567     6 11456    18    17   444    26   272 24203     1]]"
fa5357c56ea80a21a7ca88a80f21711c5431042c,100.0,36,[[   0 4475    1]],36,[[4475    1]]
f9de9ddea0c70630b360167354004ab8cbfff041,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
e2f269997f5a01949733c2ec8169f126dabd7571,0.0,CNN,[[    0 19602     1]],"- En-Fr (WMT14)
- En-De (WMT15)
- Skipthought (BookCorpus)
- AllNLI (SNLI + MultiNLI)
- Parsing (PTB + 1-billion word)","[[    3    18   695    18   371    52    41   518  7323  2534    61     3
     18   695    18  2962    41   518  7323  1808    61     3    18 25378
  11841    17    41 13355 13026  7800    61     3    18   432 18207   196
     41  8544  8159  1768  4908 18207   196    61     3    18  2180     7
     53    41  6383   279  1768  8218   115 14916  1448    61     1]]"
675d7c48541b6368df135f71f9fc13a398f0c8c6,100.0,the transformer models,[[    0     8 19903  2250     1]],the transformer models,[[    8 19903  2250     1]]
df8cc1f395486a12db98df805248eb37c087458b,0.0,WSJ4 and WSJ4-CV,"[[    0     3  8439   683   591    11     3  8439   683   591    18 20410
      1]]","SST (Stanford Sentiment Treebank), Subj (Subjectivity dataset), MPQA Opinion Corpus, RT is another movie review sentiment dataset, TREC is a dataset for classification of the six question types","[[  180  4209    41   134    17   152  2590  4892  2998   295  7552  4739
    201  3325   354    41 25252 11827 10696 17953   201  5220 23008   411
  22441 10052   302     6     3  5934    19   430  1974  1132  6493 17953
      6   332 20921    19     3     9 17953    21 13774    13     8  1296
    822  1308     1]]"
141dab98d19a070f1ce7e7dc384001d49125d545,5.5556,a new GRU formulation incorporates attention gates that are computed using global knowledge over the,"[[    0     3     9   126   350  8503 20029  6300     7  1388 18975    24
     33 29216    26   338  1252  1103   147     8]]","For the DMN+, we propose replacing this single GRU with two different components. The first component is a sentence reader, The second component is the input fusion layer","[[  242     8     3  7407   567  1220     6    62  4230 11906    48   712
    350  8503    28   192   315  3379     5    37   166  3876    19     3
      9  7142  5471     6    37   511  3876    19     8  3785     3  7316
   3760     1]]"
b464bc48f176a5945e54051e3ffaea9a6ad886d7,16.6667,"flight booking, task of finding the right slot",[[   0 3777 5038    6 2491   13 2342    8  269 4918    1]],"Slot filling, we consider the actions that a user might perform via apps on their phone, The corresponding actions are booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant","[[12094    14    53     6    62  1099     8  2874    24     3     9  1139
    429  1912  1009  4050    30    70   951     6    37     3  9921  2874
     33  5038     3     9  3777     6 17992     3     9   234     6  2611
   2601  3500     6    11   492     3     9 13019    44     3     9  2062
      1]]"
cebf3e07057339047326cb2f8863ee633a62f49f,100.0,"Arabic, German, Portuguese, Russian, Swedish",[[    0 19248     6  2968     6 21076     6  4263     6 16531     1]],"Arabic, German, Portuguese, Russian, Swedish",[[19248     6  2968     6 21076     6  4263     6 16531     1]]
340501f23ddc0abe344a239193abbaaab938cc3a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations","[[ 2777    46  2264   920  2691    28   305     3 13903     7   284     3
   8232   209    12  7836   213   209    19   709  2193    11   305    19
    167  2193    21     3     9   792    13     3 10593    46  2264   920
      3 13903     7     1]]"
89ce18ee52c52a78b38c49b14574407b7ea2fb02,50.0,SOTA with fewer emojis,[[    0   180 27976    28     3 10643     3    15    51 21892     7     1]],Attention-based LSTM with emojis,"[[20748    18   390     3  7600  2305    28     3    15    51 21892     7
      1]]"
0b31eb5bb111770a3aaf8a3931d8613e578e07a8,0.0,"Random selection criteria, a random selection criteria, and a random selection criteria","[[    0 25942  1801  6683     6     3     9  6504  1801  6683     6    11
      3     9  6504  1801  6683     1]]","Presence of only the exact unigrams 'caused', 'causing', or 'causes'","[[10131  1433    13   163     8  2883    73    23  5096     7     3    31
    658 10064    31     6     3    31  5885    31     6    42     3    31
    658  1074     7    31     1]]"
ba973b13f26cd5eb1da54663c0a72842681d5bf5,20.0,"Word2vec, Word2vec, and Word2vec","[[   0 4467  357  162   75    6 4467  357  162   75    6   11 4467  357
   162   75    1]]","news publications, wine reviews, and Reddit",[[ 1506 10142     6  2013  2456     6    11  1624    26   155     1]]
cc5d3903913fa2e841f900372ec74b0efd5e0c71,0.0,cross-domain sentiment analysis,[[    0  2269    18 22999  6493  1693     1]],12 binary-class classification and multi-class classification of reviews based on rating,"[[  586 14865    18  4057 13774    11  1249    18  4057 13774    13  2456
      3   390    30  5773     1]]"
66d743b735ba75589486e6af073e955b6bb9d2a4,0.0,"pipeline system, attention network",[[    0 12045   358     6  1388  1229     1]],"Attn seq2seq, Ptr-UNK, KV Net, Mem2Seq, DSR","[[ 486   17   29  142 1824  357    7   15 1824    6  276   17   52   18
  7443  439    6  480  553 6540    6 1212   51  357  134   15 1824    6
   309 6857    1]]"
586566de02abdf20b7bfd0d5a43ba93cb02795c3,0.0,a representation learning approach based on a bidirectional LSTM and pre-t,"[[    0     3     9  6497  1036  1295     3   390    30     3     9  2647
  26352     3  7600  2305    11   554    18    17]]",a non-parameter optimized linear-kernel SVM that uses TF-IDF bag-of-word vectors as inputs,"[[    3     9   529    18  6583  4401 18149 13080    18   157 11965    40
    180 12623    24  2284     3  9164    18  4309   371  2182    18   858
     18  6051 12938     7    38  3785     7     1]]"
ff27d6e6eb77e55b4d39d343870118d1a6debd5e,100.0,SVM,[[    0   180 12623     1]],SVM,[[  180 12623     1]]
bcec22a75c1f899e9fcea4996457cf177c50c4c5,0.0,"The variables were: NP-NP, NP-NP, NP-NP,","[[    0    37 11445   130    10     3  9082    18  9082     6     3  9082
     18  9082     6     3  9082    18  9082     6]]","(i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero task-specific BiRNN layers of any kind","[[   41    23    61  5733     3 18206    18  9500  2106 14151   567  7500
      6    41    23    23    61  5733  4083    18  9500  2106 14151   567
   7500     6    42    41    23    23    23    61  5733  2491    18  9500
   2106 14151   567  7500    13   136   773     1]]"
63850ac98a47ae49f0f49c1c1a6e45c6c447272c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Answer with content missing: (whole introduction) However, recent
studies observe the limits of ROUGE and find in
some cases, it fails to reach consensus with human.
judgment (Paulus et al., 2017; Schluter, 2017).","[[11801    28   738  3586    10    41  8903   109  5302    61   611     6
   1100  2116  7743     8  6790    13   391 26260   427    11   253    16
    128  1488     6    34 13288    12  1535 16698    28   936     5  7661
     41 23183   302     3    15    17   491     5     6  1233   117 13675
  10525     6  1233   137     1]]"
363ddc06db5720786ed440927d7fbb7d0a8078ae,66.6667,"stylistic, lexical, and semantic",[[    0 22308   447     6     3 30949   138     6    11 27632     1]],"stylometric, lexical, grammatical, and semantic","[[22437    32  7959     6     3 30949   138     6     3  5096  4992   138
      6    11 27632     1]]"
e3dc8689d8db31f04797f515fe224f6075f5cb16,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
da544015511e535503dee2eaf4912a5e36c806cd,52.6316,BIBREF5 to train neural sequence-to-sequence ST models using,"[[    0     3  5972 25582   371   755    12  2412 24228  5932    18   235
     18     7    15   835  3772  5097  2250   338]]","BIBREF5 to train neural sequence-to-sequence, NMF topic model with scikit-learn BIBREF14","[[    3  5972 25582   371   755    12  2412 24228  5932    18   235    18
      7    15   835  3772     6   445 13286  2859   825    28 17201  9229
     18   109   291    29     3  5972 25582   371  2534     1]]"
f4e17b14318b9f67d60a8a2dad1f6b506a10ab36,18.1818,BLEU scores,[[    0     3  8775 12062  7586     1]],Comparing BLEU score of model with and without attention,"[[20959    53     3  8775 12062  2604    13   825    28    11   406  1388
      1]]"
b5a2b03cfc5a64ad4542773d38372fffc6d3eac7,0.0,Effect of emotion on the EAC,[[    0 14247    13 13868    30     8   262  5173     1]],"Qualitatively through efficiency, effectiveness and satisfaction aspects and quantitatively through metrics such as precision, recall, accuracy, BLEU score and even human judgement.","[[25388    17  1528   120   190  3949     6  9570    11  5044  3149    11
  18906   120   190 15905   224    38 11723     6  7881     6  7452     6
      3  8775 12062  2604    11   237   936 22555     5     1]]"
ff0f77392abc905fe76e0b8c28a76dfb0372a0ec,37.2093,"direct similarity over ConceptNet Numberbatch embeddings, the relationships inferred","[[    0  1223  1126   485   147 16688  9688  7720   115 14547 25078    26
     53     7     6     8  3079    16  1010  1271]]","direct similarity over ConceptNet Numberbatch embeddings, the relationships inferred over ConceptNet by SME, features that compose ConceptNet with other resources (WordNet and Wikipedia), and a purely corpus-based feature that looks up two-word phrases in the Google Books dataset","[[ 1223  1126   485   147 16688  9688  7720   115 14547 25078    26    53
      7     6     8  3079    16  1010  1271   147 16688  9688    57   180
   4369     6   753    24 15283 16688  9688    28   119  1438    41   518
    127    26  9688    11 16885   201    11     3     9     3 18760 11736
    302    18   390  1451    24  1416    95   192    18  6051 15101    16
      8  1163 10022 17953     1]]"
67131c15aceeb51ae1d3b2b8241c8750a19cca8e,100.0,Oracle,[[    0 14617     1]],Oracle ,[[14617     1]]
6b6360fab2edc836901195c0aba973eae4891975,0.0,"WN16S, WSJ10",[[    0     3 21170  2938   134     6     3  8439   683  1714     1]],Switchboard conversational English corpus,[[13218  1976  3634   138  1566 11736   302     1]]
4c854d33a832f3f729ce73b206ff90677e131e48,0.0,"LSTM, LSTM+attention",[[    0     3  7600  2305     6     3  7600  2305  1220 25615     1]],"tried many configurations of our network models, but report results with only three configurations, Transformer Type 1, Transformer Type 2, Transformer Type 3","[[ 1971   186  5298     7    13    69  1229  2250     6    68   934   772
     28   163   386  5298     7     6 31220  6632  1914 31220  6632  3547
  31220  6632   220     1]]"
e09e89b3945b756609278dcffb5f89d8a52a02cd,100.0,5575 speeches,[[    0  6897  3072 26147     1]],5575 speeches,[[ 6897  3072 26147     1]]
9c94ff8c99d3e51c256f2db78c34b2361f26b9c2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard.","[[   37 23382    54  1738    80    13   633   554 17094  4175    12  1299
      6    42   686    70   293  1569     3    99   906     5  1443  1499
   4175   103    59   483     8  7478   538    16     8   377  4212     6
     78    34    19   359    12 25733    70   169    57  1260   631  7478
    931    12     8 23382     5     1]]"
94dc437463f7a7d68b8f6b57f1e3606eacc4490a,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
f14ff780c28addab1d738f676c4ec0b4106356b6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Meta vertex candidate identification. Edit distance and word lengths distance are used to determine whether two words should be merged into a meta vertex (only if length distance threshold is met, the more expensive edit distance is computed)., The meta vertex creation. As common identifiers, we use the stemmed version of the original vertices and if there is more than one resulting stem, we select the vertex from the identified candidates that has the highest centrality value in the graph and its stemmed version is introduced as a novel vertex (meta vertex).","[[14204   548 10354  4775 10356     5 11664  2357    11  1448  2475     7
   2357    33   261    12  2082   823   192  1234   225    36     3 21726
    139     3     9 10531   548 10354    41  9926     3    99  2475  2357
  12709    19  1736     6     8    72  2881  4777  2357    19 29216    26
    137     6    37 10531   548 10354  3409     5   282  1017     3  8826
     52     7     6    62   169     8  6269  2726   988    13     8   926
      3  3027   867     7    11     3    99   132    19    72   145    80
      3  5490  6269     6    62  1738     8   548 10354    45     8  4313
   4341    24    65     8  2030  2069   485   701    16     8  8373    11
    165  6269  2726   988    19  3665    38     3     9  3714   548 10354
     41  3493     9   548 10354   137     1]]"
9fd137bf7eabaf8bc234a18b6ea34471cf4a3b95,0.0,neural machine translation,[[    0 24228  1437  7314     1]],"trained using Nematus, default configuration",[[4252  338 1484 3357  302    6 4647 5298    1]]
b458ebca72e3013da3b4064293a0a2b4b5ef1fa6,0.0,Bi-directional neural coreference resolution model (Bi-directional neural coreference resolution model),"[[    0  2106    18 26352 24228  2583 11788  3161   825    41   279    23
     18 26352 24228  2583 11788  3161   825    61]]","BIBREF2 , BIBREF1 ","[[    3  5972 25582   371   357     3     6     3  5972 25582   371   536
      1]]"
baeb6785077931e842079e9d0c9c9040947ffa4e,0.0,"WSJN19, NLG20",[[    0     3  8439   683   567  2294     6   445 24214  1755     1]],The E2E NLG challenge dataset BIBREF21,"[[   37   262   357   427   445 24214  1921 17953     3  5972 25582   371
   2658     1]]"
519db0922376ce1e87fcdedaa626d665d9f3e8ce,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
10091275f777e0c2890c3ac0fd0a7d8e266b57cf,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
e755fb599690d0d0c12ddb851ac731a0a7965797,37.5,"English, French, Spanish, Portuguese, Spanish, Portuguese, Spanish, Portuguese, Spanish, Portuguese","[[    0  1566     6  2379     6  5093     6 21076     6  5093     6 21076
      6  5093     6 21076     6  5093     6 21076]]","Dutch, French, Russian, Spanish , Turkish, English ","[[10098     6  2379     6  4263     6  5093     3     6 15423     6  1566
      1]]"
ef081d78be17ef2af792e7e919d15a235b8d7275,0.0,"WNeWPro dataset, WNeWPro dataset","[[    0     3 21170    15   518  3174 17953     6     3 21170    15   518
   3174 17953     1]]","MNLI, AG's News, DBPedia","[[  283 18207   196     6  8859    31     7  3529     6   309 11165 18999
      1]]"
a81941f933907e4eb848f8aa896c78c1157bff20,33.3333,just new relations,[[   0  131  126 5836    1]],The model does not add new relations to the knowledge graph.,[[  37  825  405   59  617  126 5836   12    8 1103 8373    5    1]]
51de39c8bad62d3cbfbec1deb74bd8a3ac5e69a8,7.4074,"Adaptive frontends, duration model, Acoustic prediction model and vocoder models","[[    0     3 14808   757   851   989     7     6  8659   825     6 31614
  21332   825    11     3  6117 13487  2250     1]]","Replacing attention mechanism to query-key attention, and adding a loss to make the attention mask as diagonal as possible","[[ 7144  9700    53  1388  8557    12 11417    18  4397  1388     6    11
   2651     3     9  1453    12   143     8  1388  8181    38 26184    38
    487     1]]"
51d03f0741b72ae242c380266acd2321baf43444,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
90dd5c0f5084a045fd6346469bc853c33622908f,26.087,"correctness is evaluated by two different arithmetic test sets, namely, ","[[    0  2024   655    19 14434    57   192   315     3     9 30922    51
   7578   794  3369     6     3 17332     6     3]]","BLEU-2, average accuracies over 3 test trials on different randomly sampled test sets","[[    3  8775 12062  4949     6  1348     3  6004  2414  6267     7   147
    220   794 10570    30   315 21306  3106    26   794  3369     1]]"
e9cf1b91f06baec79eb6ddfd91fc5d434889f652,25.0,Ctrl+F (search for token) and stop (stop for searching through partially,"[[    0   205    17    52    40  1220   371    41 13173    21 14145    61
     11  1190    41  7618    21  4549   190 14610]]","previous, next, Ctrl+F $<$query$>$, stop","[[1767    6  416    6  205   17   52   40 1220  371 1514    2 3229  835
   651 3229 3155 3229    6 1190    1]]"
c84590ba32df470a7c5343d8b99e541b217f10cf,0.0,WN18 and FB15k,[[    0     3 21170  2606    11     3 15586  1808   157     1]],The Wikipedia Toxic Comments dataset,[[   37 16885   304   226   447 16243 17953     1]]
e28019afcb55c01516998554503bc1b56f923995,100.0,Personal thought of the annotator.,[[   0 4239  816   13    8   46 2264 1016    5    1]],Personal thought of the annotator.,[[4239  816   13    8   46 2264 1016    5    1]]
de03e8cc1ceaf2108383114460219bf46e00423c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"a subset of the population where we can quantitatively measure reactions: the popular Reddit r/Jokes thread, These Reddit posts consist of the body of the joke, the punchline, and the number of reactions or upvotes. ","[[    3     9   769  2244    13     8  2074   213    62    54 18906   120
   3613 14081    10     8  1012  1624    26   155     3    52    87   683
     32  7735  4546     6   506  1624    26   155  3489  5608    13     8
    643    13     8 10802     6     8 11462   747     6    11     8   381
     13 14081    42    95  1621  1422     5     1]]"
a1557ec0f3deb1e4cd1e68f4880dcecda55656dd,100.0,"Northeast U.S., West U.S. and South U.S.","[[    0 21732   412     5   134     5     6  1244   412     5   134     5
     11  1013   412     5   134     5     1]]","Northeast U.S., West U.S. and South U.S.","[[21732   412     5   134     5     6  1244   412     5   134     5    11
   1013   412     5   134     5     1]]"
db264e363f3b3aa83526952bef02f826dff70042,100.0,No,[[  0 465   1]],No,[[465   1]]
093dd1e403eac146bcd19b51a2ace316b36c6264,100.0,No,[[  0 465   1]],No,[[465   1]]
4d8ca3f7aa65dcb42eba72acf3584d37b416b19c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"A mechanism ${f}$ is a random function that takes a dataset $\mathcal {N}$ as input, and outputs a random variable ${f}(\mathcal {N})$.","[[   71  8557  1514     2    89     2  3229    19     3     9  6504  1681
     24  1217     3     9 17953  1514     2  3357   107  1489     3     2
    567     2  3229    38  3785     6    11  3911     7     3     9  6504
   7660  1514     2    89     2   599     2  3357   107  1489     3     2
    567     2    61  3229     5     1]]"
25b24ab1248f14a621686a57555189acc1afd49c,100.0,DyNet,[[    0 12991  9688     1]],DyNet,[[12991  9688     1]]
45b28a6ce2b0f1a8b703a3529fd1501f465f3fdf,0.0,"Bi-directional, LSTM, LSTM, LSTM, LSTM","[[    0  2106    18 26352     6     3  7600  2305     6     3  7600  2305
      6     3  7600  2305     6     3  7600  2305]]","special dedicated discriminator is added to the model to control that the latent representation does not contain stylistic information, shifted autoencoder or SAE, combination of both approaches","[[  534  2425  9192  1016    19   974    12     8   825    12   610    24
      8    50  4669  6497   405    59  3480 22308   447   251     6     3
  21082  1510    35  4978    52    42  4646   427     6  2711    13   321
   6315     1]]"
676c874266ee0388fe5b9a75e1006796c68c3c13,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
1a678d081f97531d54b7122254301c20b3531198,57.1429,"Wikidata, FB15K, FB16K","[[    0  2142  2168  6757     6     3 15586  1808   439     6     3 15586
   2938   439     1]]","Wikidata, ReVerb, FB15K, TACRED","[[ 2142  2168  6757     6   419  5000   115     6     3 15586  1808   439
      6   332  5173 13729     1]]"
8abb96b2450ebccfcc5c98772cec3d86cd0f53e0,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
defc17986d3c4aed9eccdbaebda5eb202fbcb6cf,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
04b43deab0fd753e3419ed8741c10f652b893f02,9.5238,"differential similarity function, BIBREF10, BIBREF11","[[    0 21701  1126   485  1681     6     3  5972 25582   371  1714     6
      3  5972 25582   371  2596     1]]","a linear projection and a bijective function with continuous transformation though  ‘affine coupling layer’ of (Dinh et al.,2016). ","[[    3     9 13080 13440    11     3     9  2647 11827   757  1681    28
   7558  6586   713   458 18581    15  4897   697  3760    22    13    41
    308    77   107     3    15    17   491     5     6 11505   137     1]]"
d3a1a53521f252f869fdae944db986931d9ffe48,0.0,the experts in the field,[[   0    8 2273   16    8 1057    1]],political pundits of the Washington Post,[[1827 4930   26 7085   13    8 2386 1844    1]]
af75ad21dda25ec72311c2be4589efed9df2f482,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The system outperforms by 27.7% the LSTM model, 38.5% the RL-SPINN model and 41.6% the Gumbel Tree-LSTM","[[   37   358    91   883  2032     7    57  2307     5  6170     8     3
   7600  2305   825     6  6654     5  2712     8     3 12831    18  4274
   3162   567   825    11  8798     5  6370     8  2846    51  2370  7552
     18  7600  2305     1]]"
c359ab8ebef6f60c5a38f5244e8c18d85e92761d,0.0,1001,[[  0 910 536   1]],"10*n paraphrases, where n depends on the number of paraphrases that contain the entity mention spans","[[  335  1935    29  3856 27111     7     6   213     3    29  5619    30
      8   381    13  3856 27111     7    24  3480     8 10409  2652  8438
      7     1]]"
242f96142116cf9ff763e97aecd54e22cb1c8b5a,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
26b5c090f72f6d51e5d9af2e470d06b2d7fc4a98,10.5263,"LSTM-based encoder-decoder framework, ResNet-based model","[[    0     3  7600  2305    18   390 23734    52    18   221  4978    52
   4732     6  7127  9688    18   390   825     1]]"," 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256","[[ 7769 18270 23734    52     6  7769 18270    20  4978    52     6 27687
     18 19386   825     6    28 25078    26    53    11  5697   812   356
     38     3 19337     1]]"
65ad17f614b7345f0077424c04c94971c831585b,100.0,BiLSTM with max pooling,[[   0 2106 7600 2305   28 9858 2201   53    1]],BiLSTM with max pooling,[[2106 7600 2305   28 9858 2201   53    1]]
dd2046f5481f11b7639a230e8ca92904da75feed,0.0,"combining character-level n-gram features, syntactic features, linguistic","[[    0     3 13275  1848    18  4563     3    29    18  5096   753     6
   8953    17  2708   447   753     6     3 24703]]","maximum of two scores assigned by the two separate models, average score",[[2411   13  192 7586 7604   57    8  192 2450 2250    6 1348 2604    1]]
90741b227b25c42e0b81a08c279b94598a25119d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation","[[ 1612    84 21119    42 21773   120 20143     7    42   340    15  3247
      3     9   568    42     3     9   563     3   390  1286     3     9
    522    17    13    70  4810   224    38  7285     6 11655   485     6
     42  6949 12602     1]]"
90ad8d7ee27192b89ffcfa4a68302f370e6333a8,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
b9c0049a7a5639c33efdb6178c2594b8efdefabb,23.5294,"Compared to the baseline, the results were significantly better.","[[    0     3 25236    12     8 20726     6     8   772   130  4019   394
      5     1]]",our method outperforms the baseline in both relevance and fluency significantly.,"[[   69  1573    91   883  2032     7     8 20726    16   321 20208    11
   6720  4392  4019     5     1]]"
96be67b1729c3a91ddf0ec7d6a80f2aa75e30a30,100.0,English,[[   0 1566    1]],English,[[1566    1]]
8c0846879771c8f3915cc2e0718bee448f5cb007,0.0,100 000 documents,[[   0  910 6078 2691    1]],"80 excerpts from scientific works, collection of 1,645 chat conversations, collection of 200 aggregated postings","[[ 2775 29603     7    45  4290   930     6  1232    13  1914   948  2128
   3582  9029     6  1232    13  2382 12955    26  5910     7     1]]"
4c4f76837d1329835df88b0921f4fe8bda26606f,100.0,No,[[  0 465   1]],No,[[465   1]]
c691b47c0380c9529e34e8ca6c1805f98288affa,100.0,No,[[  0 465   1]],No,[[465   1]]
186ccc18c6361904bee0d58196e341a719fb31c2,3.7037,"clinical sentiment analysis, topic extraction, and sentiment analysis.","[[    0  3739  6493  1693     6  2859 16629     6    11  6493  1693     5
      1]]","Sociodemographics: gender, age, marital status, etc., Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc., Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc.","[[16874    32  1778 16587     7    10  7285     6  1246     6  2774  1947
   2637     6   672     5     6 10180  1035   892    10   381    13  1767
   7209     7     6   892    13  2629  1294    26 10355     6  1348  2475
     13  1049    41   413   552    24  7209   201   672     5     6  2784
     45     8   750  7209    10  2475    13  1049    41 26342   201  2629
   1294    26   138  1020     6   381    11  2475    13  3358     6    97
     13 12445     6  5002  7586     6   672     5     1]]"
7d841b98bcee29aaa9852ef7ceea1213d703deba,100.0,No,[[  0 465   1]],No,[[465   1]]
0fcac64544842dd06d14151df8c72fc6de5d695c,0.0,"DA recognition is compared with several approaches such as BLEU, DA-C","[[    0     3  4296  5786    19     3  2172    28   633  6315   224    38
      3  8775 12062     6     3  4296    18   254]]","BLSTM+Attention+BLSTM
Hierarchical BLSTM-CRF
CRF-ASN
Hierarchical CNN (window 4)
mLSTM-RNN
DRLM-Conditional
LSTM-Softmax
RCNN
CNN
CRF
LSTM
BERT","[[  272  7600  2305  1220   188    17  9174  1220  8775   134  2305  3204
   7064  1950   272  7600  2305    18  4545   371   205  8556    18   188
   8544  3204  7064  1950 19602    41  5165  2381     3  7256     3    51
   7600  2305    18 14151   567     3  3913 11160    18  4302 10569   138
      3  7600  2305    18 28017  9128     3  4902 17235 19602   205  8556
      3  7600  2305   272 24203     1]]"
6852217163ea678f2009d4726cb6bd03cf6a8f78,0.0,"TransE BIBREF8, DistMult BIBREF7, Rot","[[    0  4946   427     3  5972 25582   371 11864  2678    17   329    83
     17     3  5972 25582   371   940     6  8704]]","WN18RR, FB15k-237, YAGO3-10","[[    3 21170  2606 12224     6     3 15586  1808   157  4949  4118     6
      3 17419  5577   519  4536     1]]"
6adfa9eee76b96953a76c03356bf41d8a9378851,33.3333,"On average, the model outperforms by 0.61 on average on 16 low-re","[[   0  461 1348    6    8  825   91  883 2032    7   57 4097 4241   30
  1348   30  898  731   18   60]]",1.6% lower phone error rate on average,[[1300 6370 1364  951 3505 1080   30 1348    1]]
02945c85d6cc4cdd1757b2f2bfa5e92ee4ed14a0,100.0,dialectal tweet data,[[    0 28461   138 10657   331     1]],dialectal tweet data,[[28461   138 10657   331     1]]
bed527bcb0dd5424e69563fba4ae7e6ea1fca26a,100.0,GermEval 2019 shared task,[[   0 5744   51  427 2165 1360 2471 2491    1]],GermEval 2019 shared task,[[5744   51  427 2165 1360 2471 2491    1]]
b43fa27270eeba3e80ff2a03754628b5459875d6,0.0,dialogue simulator,[[    0  7478 26927     1]],"Alarm, Banks, Buses, Calendar, Events, Flights, Homes, Hotels, Media, Messaging, Movies, Music, Payment, Rental Cars, Restaurants, Ride Sharing, Services, Train, Travel, Weather","[[24920     6  1925     7     6  5703    15     7     6 18783     6 11137
      6 16736     7     6 14093     6 13300     6  3159     6  6598  5855
      6 10743     7     6  3057     6 12248     6 14157  1184     7     6
   6233     7     6 17392 25037     6  1799     6 15059     6  4983     6
  17709     1]]"
2d47cdf2c1e0c64c73518aead1b94e0ee594b7a5,0.0,edit an image,[[   0 4777   46 1023    1]],"Dataset has 1737 train, 497 dev and 559 test sentences.","[[ 2747  2244    65  1003  4118  2412     6   314  4327    20   208    11
    305  3390   794 16513     5     1]]"
15a1df59ed20aa415a4daf0acb256747f6766f77,0.0,phenomenon of anaphoric pronouns,[[    0 15037    13    46     9 19968   447   813 15358    29     7     1]],"shining through, explicitation",[[23215   190     6 17623   257     1]]
0747cecb3c72594c5d15ba18490566be1ffdbfad,23.5294,"Many other approaches like SQuAD, MS-MARCO, TriviaQA, CNN/","[[    0  1404   119  6315   114   180  5991  6762     6  5266    18 13845
   5911     6  2702  5907 23008     6 19602    87]]", strong competitive methods on the SQuAD leaderboard and TriviaQA leaderboard,"[[ 1101  3265  2254    30     8   180  5991  6762  2488  1976    11  2702
   5907 23008  2488  1976     1]]"
dcdcd977f18206da3ff8ad0ffb14f7bc5e126c7d,0.0,edit an image,[[   0 4777   46 1023    1]],we use the context of the word to predict its label and by doing so our model learns label-aware context for each word in the sentence,"[[  62  169    8 2625   13    8 1448   12 9689  165 3783   11   57  692
    78   69  825  669    7 3783   18    9 3404 2625   21  284 1448   16
     8 7142    1]]"
4bae74eb707ed71d5f438ddb3d9c2192ac490f66,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
83f22814aaed9b5f882168e22a3eac8f5fda3882,0.0,correlation between model groundings and the correlation between the image and the groundings,"[[    0 18712   344   825  1591    53     7    11     8 18712   344     8
   1023    11     8  1591    53     7     1]]",rank-correlation BIBREF25,[[11003    18  5715    60  6105     3  5972 25582   371  1828     1]]
4c7ac51a66c15593082e248451e8f6896e476ffb,0.0,TP-N2F achieves a 78.9 BLEU score on,"[[    0     3  7150    18   567   357   371  1984     7     3     9     3
   3940     5  1298     3  8775 12062  2604    30]]","Full Testing Set Accuracy: 84.02
Cleaned Testing Set Accuracy: 93.48","[[ 4043 19693  2821  4292  3663  4710    10     3  4608     5  4305   205
  30233 19693  2821  4292  3663  4710    10     3  4271     5  3707     1]]"
73633afbefa191b36cca594977204c6511f9dad4,0.0,No,[[  0 465   1]],"Not at the moment, but summaries can be additionaly extended with this annotations.","[[  933    44     8   798     6    68  4505    51  5414    54    36  1151
     63  4760    28    48 30729     7     5     1]]"
9d9d84822a9c42eb0257feb7c18086d390dae3be,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
91c81807374f2459990e5f9f8103906401abc5c2,13.7931,barycentric coordinates,[[    0  1207    63 17456 11639     7     1]]," The basic idea of the visualization, drawing on Isaac Newton’s visualization of the color spectrum BIBREF8 , is to express a mixture in terms of its constituents as represented in barycentric coordinates.","[[   37  1857   800    13     8 21744     6  5364    30 20876 20126    22
      7 21744    13     8   945 10113     3  5972 25582   371   927     3
      6    19    12  3980     3     9  4989    16  1353    13   165 17429
      7    38  7283    16  1207    63 17456 11639     7     5     1]]"
87357448ce4cae3c59d4570a19c7a9df4c086bd8,5.8824,"The fusion process involves a series of steps, one for each recurrent unit,","[[    0    37     3  7316   433  5806     3     9   939    13  2245     6
     80    21   284     3    60 14907  1745     6]]",The most simple one is to directly apply a CNN layer after the embedding layer to obtain blended contextual representations. Then a GRU layer is applied afterward.,"[[   37   167   650    80    19    12  1461  1581     3     9 19602  3760
    227     8 25078    26    53  3760    12  3442 19731 28131  6497     7
      5    37    29     3     9   350  8503  3760    19  2930   227  2239
      5     1]]"
4904ef32a8f84cf2f53b1532ccf7aa77273b3d19,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
52e8f79814736fea96fd9b642881b476243e1698,14.2857,"i-vector, PLDA",[[   0    3   23   18  162 5317    6    3 5329 4296    1]],"BULATS i-vector/PLDA
BULATS x-vector/PLDA
VoxCeleb x-vector/PLDA
PLDA adaptation (X1)
 Extractor fine-tuning (X2) ","[[  272  4254 23377     3    23    18   162  5317    87  5329  4296   272
   4254 23377     3   226    18   162  5317    87  5329  4296  3152   226
    254   400   115     3   226    18   162  5317    87  5329  4296     3
   5329  4296 14340    41     4  6982 18742   127  1399    18    17   202
     53    41     4  7318     1]]"
5712a0b1e33484ebc6d71c70ae222109c08dede2,100.0,VQA and GeoQA,[[    0   584 23008    11 10107 23008     1]],VQA and GeoQA,[[  584 23008    11 10107 23008     1]]
f08502e952e711c629d40b22ee3f5ff626d62ba8,25.0,"standard MT metrics, BLEU metrics",[[    0  1068     3  7323 15905     6     3  8775 12062 15905     1]],"MT metrics, Readability metrics and other sentence-level features, Metrics based on the baseline QuEst features, Metrics based on other features","[[    3  7323 15905     6  3403  2020 15905    11   119  7142    18  4563
    753     6  1212  3929     7     3   390    30     8 20726  2415 14997
    753     6  1212  3929     7     3   390    30   119   753     1]]"
3c3cb51093b5fd163e87a773a857496a4ae71f03,0.0,LSTM with a corresponding q-score,"[[   0    3 7600 2305   28    3    9    3 9921    3 1824   18    7 9022
     1]]"," the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history","[[    8 10389   825  1217     8  1448 25078    26    53  5932    38  3785
      6 10389   147   284   928  4775  1448    45   192 14013    10  5637
      8  5113    24     8  4775  1448  1402    54    36     3 12327    38
      3     9  1281  1448   117  6499     8 12226   485    13     8  1309
     24     8  4775  1448  1461  6963  1767  5508   257   892     1]]"
5c90e1ed208911dbcae7e760a553e912f8c237a5,0.0,8000,[[    0     3 25129     1]],"In-house dataset consists of  3716 documents 
ACE05 dataset consists of  1635 documents","[[   86    18  1840 17953     3  6848    13  6862  2938  2691     3 11539
   3076 17953     3  6848    13   898  2469  2691     1]]"
e930f153c89dfe9cff75b7b15e45cd3d700f4c72,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
559c1307610a15427caeb8aff4d2c01ae5c9de20,0.0,"BIBREF13, BIBREF14","[[    0     3  5972 25582   371  2368     6     3  5972 25582   371  2534
      1]]","For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 .","[[  242     8     3    35  5756   297   853  7903    62  4048   374   287
   2748   179 20748     3  5972 25582   371   357     3     6     3  5972
  25582   371   519    38  6960    16     8  2314 20726     6     3  3205
   5166     3  5972 25582   371   591     3     6    11     3     9 19903
   1229    28   554    18    17 10761  1293     7     3  5972 25582   371
    755     3     5     1]]"
c309e87c9e08cf847f31e554577d6366faec1ea0,100.0,No,[[  0 465   1]],No,[[465   1]]
b27f7993b1fe7804c5660d1a33655e424cea8d10,25.0,Users' tweets,[[    0 13504    31 10657     7     1]],Profile pictures from the Twitter users' profiles.,[[14226  1933    45     8  3046  1105    31 10958     5     1]]
babe72f0491e65beff0e5889380e8e32d7a81f78,0.0,Unanswerable,[[   0  597 3247 3321  179    1]]," Moreover, our TL-TranSum method also outperforms other approaches such as MaxCover ( $5\%$ ) and MRMR ( $7\%$ )","[[    3  7371     6    69     3 12733    18  9402    29   134   440  1573
     92    91   883  2032     7   119  6315   224    38  5370   254  1890
     41  6422     2  1454  3229     3    61    11     3  9320  9320    41
  11301     2  1454  3229     3    61     1]]"
3bfdbf2d4d68e01bef39dc3371960e25489e510e,0.0,mean number of turns,[[   0 1243  381   13 5050    1]],"Measuring three aspects: argumentation, specificity and knowledge domain.","[[1212    9    7 7920  386 3149   10 5464  257    6  806  485   11 1103
  3303    5    1]]"
8255f74cae1352e5acb2144fb857758dda69be02,33.3333,by the number of grammatical matches between the two sentences,"[[    0    57     8   381    13     3  5096  4992   138  6407   344     8
    192 16513     1]]",by calculating log ratio of grammatical phrase over ungrammatical phrase,"[[   57     3 25956  4303  5688    13     3  5096  4992   138  9261   147
     73  5096  4992   138  9261     1]]"
2caa8726222237af482e170c51c88099cefef6fc,100.0,No,[[  0 465   1]],No,[[465   1]]
709a4993927187514701fe3cc491ac3030da1215,0.0,"QuaSP+, QuaSP+Zero",[[   0 2415    9 4274 1220    6 2415    9 4274 1220  956   49   32    1]],"information retrieval system, word-association method,  CCG-style rule-based semantic parser written specifically for friction questions, state-of-the-art neural semantic parser","[[  251 24515   138   358     6  1448    18 13443  1573     6   205 12150
     18  4084  3356    18   390 27632   260     7    49  1545  3346    21
  21764   746     6   538    18   858    18   532    18  1408 24228 27632
    260     7    49     1]]"
6073fa9050da76eeecd8aa3ccc7ecb16a238d83f,100.0,F1 score,[[   0  377  536 2604    1]],F1 score,[[ 377  536 2604    1]]
6852217163ea678f2009d4726cb6bd03cf6a8f78,0.0,"TransE BIBREF8, DistMult BIBREF7, Rot","[[    0  4946   427     3  5972 25582   371 11864  2678    17   329    83
     17     3  5972 25582   371   940     6  8704]]","WN18RR BIBREF26, FB15k-237 BIBREF18, YAGO3-10 BIBREF27","[[    3 21170  2606 12224     3  5972 25582   371  2688     6     3 15586
   1808   157  4949  4118     3  5972 25582   371  2606     6     3 17419
   5577   519  4536     3  5972 25582   371  2555     1]]"
8d258899e36326183899ebc67aeb4188a86f682c,0.0,a simple LSTM with a few simple optimizations,"[[    0     3     9   650     3  7600  2305    28     3     9   360   650
  11295     7     1]]","$ f_r(h, t) & = & \Vert \textbf {W}_{r,1}\textbf {h} + \textbf {r} - \textbf {W}_{r,2}\textbf {t}\Vert _{\ell _{1/2}} $","[[ 1514     3    89   834    52   599   107     6     3    17    61     3
    184  3274     3   184     3     2  5000    17     3     2  6327   115
     89     3     2   518     2   834     2    52     6   536     2  6327
    115    89     3     2   107     2  1768     3     2  6327   115    89
      3     2    52     2     3    18     3     2  6327   115    89     3
      2   518     2   834     2    52     6   357     2  6327   115    89
      3     2    17     2  5000    17     3   834     2  3820     3   834
      2 17637     2  1514     1]]"
58d50567df71fa6c3792a0964160af390556757d,100.0,No,[[  0 465   1]],No,[[465   1]]
955ca31999309685c1daa5cb03867971ca99ec52,100.0,"WN18, FB15k",[[    0     3 21170  2606     6     3 15586  1808   157     1]],"WN18, FB15k",[[    3 21170  2606     6     3 15586  1808   157     1]]
c54de73b36ab86534d18a295f3711591ce9e1784,100.0,No,[[  0 465   1]],No,[[465   1]]
58edc6ed7d6966715022179ab63137c782105eaf,33.3333,the LSTM model,[[   0    8    3 7600 2305  825    1]],the hybrid model MinAvgOut + RL,[[    8  9279   825  4765   188   208   122 15767  1768     3 12831     1]]
33f72c8da22dd7d1378d004cbd8d2dcd814a5291,0.0,accuracy,[[   0 7452    1]],error rate in a minimal pair ABX discrimination task,[[3505 1080   16    3    9 6211 3116    3 5359    4 9192  257 2491    1]]
948327d7aa9f85943aac59e3f8613765861f97ff,100.0,No,[[  0 465   1]],No,[[465   1]]
7a53668cf2da4557735aec0ecf5f29868584ebcf,100.0,tutorial videos for a photo-editing software,[[    0  7114  3075    21     3     9  1202    18 11272    53   889     1]],tutorial videos for a photo-editing software,[[ 7114  3075    21     3     9  1202    18 11272    53   889     1]]
458f3963387de57fdc182875c9ca3798b612b633,0.0,"LSTM, MATRES",[[    0     3  7600  2305     6     3 18169 12200     1]],"GPT2, SciBERT model of BIBREF11","[[  350  6383  4482 16021 12920   382   825    13     3  5972 25582   371
   2596     1]]"
3b40799f25dbd98bba5b526e0a1d0d0bb51173e0,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
10f560fe8e1c0c7dea5e308ee4cec16d07874f1d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"selects the next summary sentence based not only on properties of the source text, but also on the previously selected sentences in the summary","[[ 1738     7     8   416  9251  7142     3   390    59   163    30  2605
     13     8  1391  1499     6    68    92    30     8  3150  2639 16513
     16     8  9251     1]]"
5f0bb32d70ee8e4c4c59dc5c193bc0735fd751cc,100.0,dialogue simulator,[[    0  7478 26927     1]],dialogue simulator,[[ 7478 26927     1]]
41d3750ae666ea5a9cea498ddfb973a8366cccd6,0.0,BLEU score,[[    0     3  8775 12062  2604     1]],"annotators are asked how attractive the headlines are, Likert scale from 1 to 10 (integer values)","[[   46  2264  6230    33  1380   149  5250     8 12392     7    33     6
   2792    52    17  2643    45   209    12   335    41  2429  1304  2620
     61     1]]"
feb4e92ff1609f3a5e22588da66532ff689f3bcc,0.0,entity-grid model,[[    0 10409    18  3496    26   825     1]],character bigram CNN classifier,[[ 1848   600  2375 19602   853  7903     1]]
c1c611409b5659a1fd4a870b6cc41f042e2e9889,18.1818,BLEU scores,[[    0     3  8775 12062  7586     1]],"BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence.","[[    3  8775 12062  7586     6  2883  6407    13  1234    16   321  7314
      7    11  2859 11800     6    11   576     7   630 25758    13 12487
  16513    21   576   760  1433     5     1]]"
42394c54a950bae8cebecda9de68ee78de69dc0d,25.0,Wikipedia,[[    0 16885     1]],counts of predicate-argument tuples from English Wikipedia,"[[12052    13   554  4370   342    18   291  1744   297     3    17   413
    965    45  1566 16885     1]]"
2c576072e494ab5598667cd6b40bc97fdd7d92d7,15.0,Data released for APDA shared task contains 3 datasets.,"[[    0  2747  1883    21     3  2965  4296  2471  2491  2579   220 17953
      7     5     1]]","we manually label an in-house dataset of 1,100 users with gender tags, we randomly sample 20,000 tweets for each class from an in-house dataset gold labeled with the same 15 classes as the shared task","[[   62 12616  3783    46    16    18  1840 17953    13  1914  2915  1105
     28  7285 12391     6    62 21306  3106     3 13922 10657     7    21
    284   853    45    46    16    18  1840 17953  2045  3783    15    26
     28     8   337   627  2287    38     8  2471  2491     1]]"
05238d1fad2128403577822aa4822ef8ca9570ac,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
84737d871bde8058d8033e496179f7daec31c2d3,100.0,No,[[  0 465   1]],No,[[465   1]]
84d36bca06786070e49d3db784e42a51dd573d36,100.0,conceptualization task,[[    0 17428  1707  2491     1]],conceptualization task,[[17428  1707  2491     1]]
7b44bee49b7cb39cb7d5eec79af5773178c27d4d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner","[[    3  3626     3     9   356    13 30729  1339   224    38  4574    89
     15   302   172     6   276  9156 18888     6  8974   354    26     6
      3 18206   371    11  4919    52     1]]"
46563a1fb2c3e1b39a185e4cbb3ee1c80c8012b7,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
0cd0755ac458c3bafbc70e4268c1e37b87b9721b,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
ebeedbb8eecdf118d543fdb5224ae610eef212c8,0.0,bi-directional NMT tasks,[[    0  2647    18 26352   445  7323  4145     1]],"Procrustes, GPA, GeoMM, GeoMM$_{semi}$, Adv-C-Procrustes, Unsup-SL, Sinkhorn-BT","[[  749 14127   849     7     6   350  3965     6 10107  8257     6 10107
   8257  3229   834     2     7    15    51    23     2  3229     6     3
  21021    18   254    18  3174 14127   849     7     6   597     7   413
     18  5629     6 26560  6293    18  9021     1]]"
ea51aecd64bd95d42d28ab3f1b60eecadf6d3760,0.0,"TF-IDF, TF-IDF, TF-IDF, ","[[   0    3 9164   18 4309  371    6    3 9164   18 4309  371    6    3
  9164   18 4309  371    6    3]]","Books, DVDs, Electronics, Kitchen appliances",[[10022     6  5677     7     6  9885     7     6  5797  9403     1]]
37103369e5792ece49a71666489016c4cea94cda,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
ff8557d93704120b65d9b597a4fab40b49d24b6d,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
fc5f9604c74c9bb804064f315676520937131e17,92.3077,BLEU scores and the slot error rate,[[    0     3  8775 12062  7586    11     8  4918  3505  1080     1]],BLEU scores and the slot error rate (ERR),"[[    3  8775 12062  7586    11     8  4918  3505  1080    41  3316   448
     61     1]]"
7aaaf7bff9947c6d3b954ae25be87e6e1c49db6d,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
12cfbaace49f9363fcc10989cf92a50dfe0a55ea,41.6667,91.93% INLINEFORM0 for the CoNLL 2003 NER task,"[[    0   668 22493  5170  3388 20006 24030   632    21     8   638   567
  10376  3888     3 18206  2491     1]]",91.93% F1 score on CoNLL 2003 NER task and 96.37% F1 score on CoNLL 2000 Chunking task,"[[  668 22493  5170   377   536  2604    30   638   567 10376  3888     3
  18206  2491    11   668 27865  6170   377   536  2604    30   638   567
  10376  2766 16636    29  1765  2491     1]]"
c180f44667505ec03214d44f4970c0db487a8bae,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"the neural approach is generally preferred by a greater percentage of participants than the rules or random, human-made game outperforms them all","[[    8 24228  1295    19  2389  6241    57     3     9  2123  5294    13
   3008   145     8  2219    42  6504     6   936    18  4725   467    91
    883  2032     7   135    66     1]]"
846a1992d66d955fa1747bca9a139141c19908e8,11.7647,TEXT-Stim (Twitter 2016),"[[   0    3 3463    4  382   18  134 2998   41  382 7820  449 1421   61
     1]]","Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, Health Care Reform (HCR)","[[19796     3    18  3046  4892  2998   295 10052   302    41  4209   134
  10052   302   201 22439     3    18  3046  4892  2998   295 10052   302
      6  1685  2686 18490    41   566  4545    61     1]]"
ad6415f4351c44ffae237524696a3f76f383bfd5,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
8fcbae7c3bd85034ae074fa58a35e773936edb5b,31.5789,"Topical Classifier, Random Forest, and Logistic Logistic Logistic Model","[[    0 18059   138  4501  7903     6 25942  6944     6    11  7736  3040
   7736  3040  7736  3040  5154     1]]","Support Vector Machine (SVM), Logistic Regression (LR), Random Forest (RF)","[[ 4224 29011  5879    41   134 12623   201  7736  3040   419 22430    41
  12564   201 25942  6944    41  8556    61     1]]"
01dc6893fc2f49b732449dfe1907505e747440b0,31.25,"morality, rationality, religion, culture, politics, economy, culture, science, technology","[[    0  4854   485     6 12226   485     6  5562     6  1543     6  6525
      6  2717     6  1543     6  2056     6   748]]","Ethics, Gender, Human rights, Sports, Freedom of Speech, Society, Religion, Philosophy, Health, Culture, World, Politics, Environment, Education, Digital Freedom, Economy, Science and Law","[[25577     6   350  3868     6  3892  2166     6  5716     6 14179    13
  26351     6  3467     6 18182     6 26597     6  1685     6 10160     6
   1150     6 14984     7     6 13706     6  2855     6  4190 14179     6
  22077     6  2854    11  2402     1]]"
fac052c4ad6b19a64d7db32fd08df38ad2e22118,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Calinski-Harabasz score, t-SNE, UMAP","[[ 3104    77  4009    18 15537     9  4883   172  2604     6     3    17
     18   134  4171     6   412 25790     1]]"
14c0328e8ec6360a913b8ecb3e50cb27650ff768,15.0,F1 score of 0.91 on OP and 0.91 on FB and 0.91 on,"[[    0   377   536  2604    13  4097  4729    30     3  4652    11  4097
   4729    30     3 15586    11  4097  4729    30]]","all of our models outperform the random baseline by a wide margin, he best F1 score in content words more than doubles that of the random baseline (0.286 vs. 0.116)","[[   66    13    69  2250    91   883  2032     8  6504 20726    57     3
      9  1148  6346     6     3    88   200   377   536  2604    16   738
   1234    72   145  1486     7    24    13     8  6504 20726    41 18189
   3840     3   208     7     5  4097 20159    61     1]]"
5499527beadb7f5dd908bd659cad83d6a81119bd,10.5263,"Lexical Dictionary, Snips",[[    0 17546  1950 28767     6   180    29 15432     1]],"Wiktionary, Oxford Dictionary of English Idioms, UsingEnglish.com (UE), Sporleder corpus, VNC dataset, SemEval-2013 Task 5 dataset","[[ 2142 12696  1208     6 10274 28767    13  1566    27    26    23    32
     51     7     6     3  3626 26749     5   287    41  5078   201  2526
    127  1361    49 11736   302     6   584  8137 17953     6   679    51
    427  2165    18 11138 16107   305 17953     1]]"
0bd992a6a218331aa771d922e3c7bb60b653949a,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
8816333fbed2bfb1838407df9d6c084ead89751c,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
1959e0ebc21fafdf1dd20c6ea054161ba7446f61,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.,"[[  205  4578    19  5819    53  8649   331    45  1035   585   331    41
    202 16180    26   137 10236     7  6634     3 23008    18  6227   134
   2491    24     3  8345    12  2928   167  1341  1499    45   926  1499
      5     1]]"
675f28958c76623b09baa8ee3c040ff0cf277a5a,0.0,"70,000",[[    0     3 28891     1]],"300,000 sentences with 1.5 million single-quiz questions","[[ 3147     6  2313 16513    28  8613   770   712    18  1169   172   746
      1]]"
cfb5ab893ed77f9df7eeb4940b6bacdef5acccea,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
f1c70baee0fd02b8ecb0af4b2daa5a56f3e9ccc3,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"239,357 English question-answer pairs",[[  204  3288     6   519  3436  1566   822    18  3247  3321 14152     1]]
4c7b29f6e3cc1e902959a1985146ccc0b15fe521,100.0,Wikipedia,[[    0 16885     1]],Wikipedia,[[16885     1]]
f258ada8577bb71873581820a94695f4a2c223b3,100.0,"70,000",[[    0     3 28891     1]],"70,000",[[    3 28891     1]]
4fcc668eb3a042f60c4ce2e7d008e7923b25b4fc,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
efc65e5032588da4a134d121fe50d49fe8fe5e8c,10.8108,"Task A, Task B, and Task C",[[    0 16107    71     6 16107   272     6    11 16107   205     1]],"Multitask learning is used for the task of predicting relevance of a comment on a different question to a given question, where the supplemental tasks are predicting relevance between the questions, and between the comment and the corresponding question","[[ 4908 23615  1036    19   261    21     8  2491    13     3 29856 20208
     13     3     9  1670    30     3     9   315   822    12     3     9
    787   822     6   213     8     3 29672  4145    33     3 29856 20208
    344     8   746     6    11   344     8  1670    11     8     3  9921
    822     1]]"
ff83eea2df9976c1a01482818340871b17ad4f8c,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
6992f8e5a33f0af0f2206769484c72fecc14700b,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
d8bf4a29c7af213a9a176eb1503ec97d01cc8f51,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
56123dd42cf5c77fc9a88fc311ed2e1eb672126e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"SPTree, Tagging, CopyR, HRL, GraphR, N-gram Attention","[[  180  6383    60    15     6   332 15242     6 20255   448     6  8383
    434     6     3 21094   448     6   445    18  5096 20748     1]]"
53807f435d33fe5ce65f5e7bda7f77712194f6ab,0.0,Unanswerable,[[   0  597 3247 3321  179    1]]," only 1 in 9 qualitative papers in Human-Computer Interaction reported inter-rater reliability metrics, low-effort responses from crowdworkers","[[  163   209    16   668 19647  5778    16  3892    18  5890  2562    49
   3037  4787  2196  1413    18  1795    49 10581 15905     6   731    18
  30475  7216    45  4374  1981   277     1]]"
abc5836c54fc2ac8465aee5a83b9c0f86c6fd6f5,100.0,No,[[  0 465   1]],No,[[465   1]]
41844d1d1ee6d6d38f31b3a17a2398f87566ed92,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"two parallel convolutional networks, INLINEFORM0 , that share the same set of weights","[[  192  8449   975 24817   138  5275     6  3388 20006 24030   632     3
      6    24   698     8   337   356    13  1293     7     1]]"
171ebfdc9b3a98e4cdee8f8715003285caeb2f39,7.1429,0.986 vs 0.9898 on visual and 0.986 on text,"[[    0     3 23758  3840     3   208     7  4097  3916  3916    30  3176
     11     3 23758  3840    30  1499     1]]","Average accuracy of proposed model vs best prevous result:
Single-task Training: 57.57 vs 55.06
Multi-task Training: 50.17 vs 50.59","[[23836  7452    13  4382   825     3   208     7   200   554  3249   741
     10  7871    18 23615  4017    10     3  3436     5  3436     3   208
      7  6897     5  5176  4908    18 23615  4017    10   943     5  2517
      3   208     7   943     5  3390     1]]"
21f615bf19253fc27ea838012bc088f4d10cdafd,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
352c081c93800df9654315e13a880d6387b91919,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
4fa851d91388f0803e33f6cfae519548598cd37c,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
0be0c8106df5fde4b544af766ec3d4a3d7a6c8a2,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
96dcabaa8b6bd89b032da609e709900a1569a0f9,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"These ask which choices are morally required, forbidden, or permitted, norms are understood as universal rules of what to do and what not to do","[[  506   987    84  3703    33  4854   120   831     6 29387     6    42
  10548     6  7982     7    33  7571    38  7687  2219    13   125    12
    103    11   125    59    12   103     1]]"
2869d19e54fb554fcf1d6888e526135803bb7d75,100.0,F1 score of 82.10%,[[    0   377   536  2604    13   505 14489  6932     1]],F1 score of 82.10%,[[  377   536  2604    13   505 14489  6932     1]]
fab4ec639a0ea1e07c547cdef1837c774ee1adb8,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
100cf8b72d46da39fedfe77ec939fb44f25de77f,100.0,Chinese dataset BIBREF0,[[    0  2830 17953     3  5972 25582   371   632     1]],Chinese dataset BIBREF0,[[ 2830 17953     3  5972 25582   371   632     1]]
b99948ac4810a7fe3477f6591b8cf211d6398e67,100.0,five,[[  0 874   1]],five,[[874   1]]
b236b9827253037b2fd7884d7bfec74619d96293,43.4783,"Compared to the previous best model, the proposed model achieved a 78.98%","[[    0     3 25236    12     8  1767   200   825     6     8  4382   825
   5153     3     9     3  3940     5  1298  5953]]","significant MAP performance improvement compared to the previous best model, CompClip-LM (0.696 to 0.734 absolute)","[[ 1516     3 25790   821  4179     3  2172    12     8  1767   200   825
      6  4961   254  7446    18 11160    41 22787  4314    12     3 22426
   3710  6097    61     1]]"
f0317e48dafe117829e88e54ed2edab24b86edb1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"if the attention loose track of the objects in the picture and ""gets lost"", the model still takes it into account and somehow overrides the information brought by the text-based annotations","[[    3    99     8  1388  6044  1463    13     8  4820    16     8  1554
     11    96  2782     7  1513  1686     8   825   341  1217    34   139
    905    11  9296   147    52  9361     8   251  1940    57     8  1499
     18   390 30729     7     1]]"
208e667982160cfbce49ef49ad96f6ab094292ac,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"“coding scheme” is defined, coders are trained with the coding scheme, Training sometimes results in changes to the coding scheme, calculation of “inter-annotator agreement” or “inter-rater reliability.”, there is a process of “reconciliation” for disagreements","[[  105  9886  5336   153    19  4802     6  1081    52     7    33  4252
     28     8     3  9886  5336     6  4017  1664   772    16  1112    12
      8     3  9886  5336     6 18643    13   105  3870    18   152  2264
   1016  2791   153    42   105  3870    18  1795    49 10581     5  1241
    132    19     3     9   433    13   105    60 31739   153    21 28155
      7     1]]"
12f1919a3e8ca460b931c6cacc268a926399dff4,0.0,LSTM,[[   0    3 7600 2305    1]],AdaBoost-based classifier,[[ 1980     9 16481    18   390   853  7903     1]]
cf3af2b68648fa8695e7234b6928d014e3b141f1,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
09621c9cd762e1409f22d501513858d67dcd3c7c,0.0,YouTube videos,[[   0 5343 3075    1]],a tutorial website about an image editing program ,[[   3    9 7114  475   81   46 1023 8414  478    1]]
e24fbcc8be922c43f6b6037cdf2bfd4c0a926c08,100.0,the Meta-LSTM BIBREF0,[[    0     8 14204    18  7600  2305     3  5972 25582   371   632     1]], the Meta-LSTM BIBREF0,[[    8 14204    18  7600  2305     3  5972 25582   371   632     1]]
52ed2eb6f4d1f74ebdc4dcddcae201786d4c0463,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
e72a672f8008bbc52b93d8037a5fedf8956136af,0.0,"DSTC2, Maluuba",[[    0     3  3592  3838  4482  2148    76 17309     1]],"E2ECM, CDM",[[ 262  357 3073  329    6 3190  329    1]]
7348e781b2c3755b33df33f4f0cab4b94fcbeb9b,100.0,Only automatic methods,[[   0 3462 6569 2254    1]],Only automatic methods,[[3462 6569 2254    1]]
fee5aef7ae521ccd1562764a91edefecec34624d,0.0,a regularizer which prevents the inference network from copying $x$,"[[    0     3     9  1646  8585    84  1709     7     8    16 11788  1229
     45  2405    53  1514     2   226     2  3229]]","Answer with content missing: (Formula 2) Formula 2 is an answer: 
\big \langle\! \log p_\theta({x}|{z}) \big \rangle_{q_\phi({z}|{x})}  -  \beta |D_{KL}\big(q_\phi({z}|{x}) || p({z})\big)-C|","[[11801    28   738  3586    10    41  3809  4115     9  9266 13786   204
     19    46  1525    10     3     2 12911     3     2    40 13247     2
     55     3     2  2152     3   102   834     2   532    17     9   599
      2   226     2  9175     2   172     2    61     3     2 12911     3
      2    52 13247   834     2  1824   834     2 11692   599     2   172
      2  9175     2   226     2    61     2     3    18     3     2   346
     17     9  1820   308   834     2   439   434     2 12911   599  1824
    834     2 11692   599     2   172     2  9175     2   226     2    61
   1820  9175     3   102   599     2   172     2    61     2 12911    61
     18   254  9175     1]]"
7f5ab9a53aef7ea1a1c2221967057ee71abb27cb,100.0,No,[[  0 465   1]],No,[[465   1]]
c029deb7f99756d2669abad0a349d917428e9c12,100.0,3%,[[   0    3 5170    1]],3%,[[   3 5170    1]]
d3a1a53521f252f869fdae944db986931d9ffe48,100.0,the experts in the field,[[   0    8 2273   16    8 1057    1]],the experts in the field,[[   8 2273   16    8 1057    1]]
8c073b7ea8cb5cc54d7fecb8f4bf88c1fb621b19,100.0,cosine similarity,[[   0  576    7  630 1126  485    1]],cosine similarity,[[ 576    7  630 1126  485    1]]
8e2b125426d1220691cceaeaf1875f76a6049cbd,8.1633,"Compared to state-of-the-art methods, their accuracy improves by 0.86","[[    0     3 25236    12   538    18   858    18   532    18  1408  2254
      6    70  7452  1172     7    57  4097  3840]]","ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.
On Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively.","[[ 9191  8042   357 12858    26     6     8  7452    13  4382  1573    19
   3798    57  6097     3  8775  5078     3 27297     6  5477  4225     6
   1300  4440    21     3   226  1570  4669     6     3   226  1649  2708
     11     3    32  1649  2708  6898     5   461 20474   447 17953     6
      8  7452    13  4382  1573    19  3798    57  6097     3  8775  5078
   1877  3301     5  2853  2596     6  2853  3647    21     3   226  1570
   4669     6     3   226  1649  2708    11     3    32  1649  2708     5
     60  5628 13830     5     1]]"
ab3737fbf17b7a0e790e1315fffe46f615ebde64,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
670c464a5dba78e0be7ec168fe36db604e172ea7,22.2222,word embeddings from a medical record,[[    0  1448 25078    26    53     7    45     3     9  1035  1368     1]],"GloVe, concatenation of average embeddings calculated separately for the interview and for the medical examination","[[ 9840   553    15     6   975  2138    35   257    13  1348 25078    26
     53     7 11338 12000    21     8  2772    11    21     8  1035  6498
      1]]"
864b5c1fe8c744f80a55e87421b29d6485b7efd0,66.6667,"Precision, Recall and F1",[[    0 28464     6   419 16482    11   377   536     1]],"Precision, Recall and INLINEFORM0 score",[[28464     6   419 16482    11  3388 20006 24030   632  2604     1]]
24d06808fa3b903140659ee5a471fdfa86279980,100.0,standard Transformer Base model,[[    0  1068 31220  8430   825     1]],standard Transformer Base model,[[ 1068 31220  8430   825     1]]
5a9f94ae296dda06c8aec0fb389ce2f68940ea88,22.2222,their best performance outperforms by 0.86 compared to the best average of 0.86,"[[   0   70  200  821   91  883 2032    7   57 4097 3840    3 2172   12
     8  200 1348   13 4097 3840]]",Their average improvement in Character Error Rate over the best MHA model was 0.33 percent points.,"[[ 2940  1348  4179    16 20087   848    52   127 13002   147     8   200
    283  5478   825    47  4097  4201  1093   979     5     1]]"
f052444f3b3bf61a3f226645278b780ebd7774db,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
e4cc2e73c90e568791737c97d77acef83588185f,100.0,8000,[[    0     3 25129     1]],8000,[[    3 25129     1]]
3aa43a0d543b88d40e4f3500c7471e263515be40,4.0816,supervised learning and unsupervised learning,[[    0     3 23313  1036    11    73 23313  1036     1]],"translated the responses in multiple languages into English using machine translation, words without functional meaning (e.g. `I'), rare words that occurred in only one narrative, numbers, and punctuation were all removed, remaining words were stemmed to remove plural forms of nouns or conjugations of verbs","[[15459     8  7216    16  1317  8024   139  1566   338  1437  7314     6
   1234   406  5014  2530    41    15     5   122     5     3     2   196
     31   201  3400  1234    24  6935    16   163    80  8109     6  2302
      6    11  5427    76   257   130    66  3641     6  4080  1234   130
   6269  2726    12  2036 28037  2807    13   150   202     7    42 26648
   1628    13  7375     7     1]]"
37e8f5851133a748c4e3e0beeef0d83883117a98,7.4074,"Compared to baselines, we show that generalises to goals and environment dynamics not seen","[[    0     3 25236    12 20726     7     6    62   504    24   879    23
   2260    12  1766    11  1164 14966    59   894]]","Proposed model achive 66+-22 win rate, baseline CNN 13+-1  and baseline FiLM 32+-3 .","[[  749 12151   825     3  1836   757     3  3539  1220 16149  1369  1080
      6 20726 19602  1179  1220  2292    11 20726  3188 11160  3538  1220
   3486     3     5     1]]"
f103789b85b00ec973076652c639bd31c605381e,0.0,"WN15, WN15, WN15, WN16, WN16","[[    0     3 21170  1808     6     3 21170  1808     6     3 21170  1808
      6     3 21170  2938     6     3 21170  2938]]","Senseval-2 (SE2), Senseval-3 (SE3), SemEval 2013 task 12 (SE13), and SemEval 2015 task 13 (SE15), OntoNotes Release 5.0","[[    3 19003  2165  4949    41  4132  7318     6     3 19003  2165  3486
     41  4132  5268     6   679    51   427  2165  2038  2491   586    41
   4132  2368   201    11   679    51   427  2165  1230  2491  1179    41
   4132  1808   201   461   235 10358    15     7 13048     3 20734     1]]"
3fddd9f6707b9e40e35518dae7f6da7c4cb77d16,100.0,No,[[  0 465   1]],No,[[465   1]]
2a1e6a69e06da2328fc73016ee057378821e0754,0.0,They used a multi-hop approach.,[[    0   328   261     3     9  1249    18 10776  1295     5     1]],Exact matches to the entity string and predictions from a coreference resolution system,"[[ 1881  2708  6407    12     8 10409  6108    11 20099    45     3     9
   2583 11788  3161   358     1]]"
2fffff59e57b8dbcaefb437a6b3434fc137f813b,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
d72548fa4d29115252605d5abe1561a3ef2430ca,0.0,they use n-grams as inputs,[[   0   79  169    3   29   18 5096    7   38 3785    7    1]],represent every sentence by their reduced n-gram set,[[4221  334 7142   57   70 3915    3   29   18 5096  356    1]]
dac2591f19f5bbac3d4a7fa038ff7aa09f6f0d96,0.0,"NNIM, NNIM-GROUNDING, NNIM-GRO","[[    0     3 17235  5166     6     3 17235  5166    18   517  4630 13110
   2365     6     3 17235  5166    18   517  4630]]","Optimized TF-IDF, iterated TF-IDF, BERT re-ranking.","[[30543    15    26     3  9164    18  4309   371     6    34    15  4094
      3  9164    18  4309   371     6   272 24203     3    60    18  6254
     53     5     1]]"
82c4863293a179fe5c0d9a1ff17d224bde952f54,0.0,image,[[   0 1023    1]],The proposed Multimodal Differential Network (MDN) consists of a representation module and a joint mixture module.,"[[   37  4382  4908 20226 13936    23   138  3426    41   329 12145    61
      3  6848    13     3     9  6497  6008    11     3     9  4494  4989
   6008     5     1]]"
04495845251b387335bf2e77e2c423130f43c7d9,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
00bcdffff7e055f99aaf1b05cf41c98e2748e948,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"For the emotion recognition from text they use described neural network as baseline.
For audio and face there is no baseline.","[[  242     8 13868  5786    45  1499    79   169  3028 24228  1229    38
  20726     5   242  2931    11   522   132    19   150 20726     5     1]]"
185841e979373808d99dccdade5272af02b98774,11.7647,We use a combination of LSTM and LSTM to detect incorrect facts.,"[[    0   101   169     3     9  2711    13     3  7600  2305    11     3
   7600  2305    12  8432 12153  6688     5     1]]","if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. ","[[    3    99   132    19    46  3505    16     8  8373     6     8 12063
     19   952    12    36 26608    28   165  5353     6    11  2932     8
    825   225   474   709  2019    30    48 12063     5    86   119  1234
      6     8  3505 12063   225    43     8   709  2860    30     8   825
     31     7 21332    13     8   761   331     5     1]]"
9257c578ee19a7d93e2fba866be7b0bf1142c393,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
d71772bfbc27ff1682e552484bc7c71818be50cf,20.0,CAIS,[[    0   205 25018     1]],the $\mathbf {C}$hinese $\mathbf {A}$rtificial $\mathbf {I}$ntelligence $\mathbf {S}$peakers (CAIS),"[[    8  1514     2  3357   107   115    89     3     2   254     2  3229
    107  4477    15  1514     2  3357   107   115    89     3     2   188
      2  3229    52    17    23 22816  1514     2  3357   107   115    89
      3     2   196     2  3229    29  1625  2825  1433  1514     2  3357
    107   115    89     3     2   134     2  3229 14661   277    41   254
  25018    61     1]]"
d24acc567ebaec1efee52826b7eaadddc0a89e8b,0.0,58,[[   0    3 3449    1]],10700,[[ 335 9295    1]]
0a3a8d1b0cbac559f7de845d845ebbfefb91135e,14.2857,accuracy of 0.82,[[   0 7452   13 4097 4613    1]],"Accuracy not available: WER results are reported 42.6 German, 35.9 English","[[ 4292  3663  4710    59   347    10   549  3316   772    33  2196   314
  22724  2968     6  3097     5  1298  1566     1]]"
9d336c4c725e390b6eba8bb8fe148997135ee981,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
24014a040447013a8cf0c0f196274667320db79f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"model overall still gives 1.0% higher average UAS and LAS than the previous best parser, BIAF, our model reports more than 1.0% higher average UAS than STACKPTR and 0.3% higher than BIAF","[[  825  1879   341  1527  1300  6932  1146  1348   412  3291    11     3
  20245   145     8  1767   200   260     7    49     6     3  5972  6282
      6    69   825  2279    72   145  1300  6932  1146  1348   412  3291
    145  5097 15339  6383   448    11  4097  5170  1146   145     3  5972
   6282     1]]"
efb3a87845460655c53bd7365bcb8393c99358ec,0.0,82.0%,[[   0    3 4613    5 6932    1]],"accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR","[[ 7452    13     3  3840     5  3891    30  5097   134     6 11989     5
   2534    30 22439    11   505 23758    30   454  4545     1]]"
f062723bda695716aa7cb0f27675b7fc0d302d4d,11.1111,a distribution of high-dimensional vectors that is then modeled on a set of,"[[    0     3     9  3438    13   306    18 11619 12938     7    24    19
    258     3 25125    30     3     9   356    13]]","judged by 10 raters on a [0,10] scale","[[5191   26   57  335 1080   52    7   30    3    9  784  632    6 1714
   908 2643    1]]"
4e2cb1677df949ee3d1d3cd10962b951da907105,0.0,image,[[   0 1023    1]],Decoder that generates question using an LSTM-based language model,"[[ 4451 13487    24  3806     7   822   338    46     3  7600  2305    18
    390  1612   825     1]]"
a6665074b067abb2676d5464f36b2cb07f6919d3,19.0476,CTB reaches new state-of-the-art for all parsing tasks,"[[    0   205  9041     3 12763   126   538    18   858    18   532    18
   1408    21    66   260     7    53  4145     1]]",". On PTB, our model achieves 93.90 F1 score of constituent parsing and 95.91 UAS and 93.86 LAS of dependency parsing., On CTB, our model achieves a new state-of-the-art result on both constituent and dependency parsing.","[[    3     5   461   276  9041     6    69   825  1984     7     3  4271
      5  2394   377   536  2604    13 17429   260     7    53    11 11923
      5  4729   412  3291    11     3  4271     5  3840     3 20245    13
  27804   260     7    53     5     6   461   205  9041     6    69   825
   1984     7     3     9   126   538    18   858    18   532    18  1408
    741    30   321 17429    11 27804   260     7    53     5     1]]"
da82b6dad2edd4911db1dc59e4ccd7f66c5fd79c,0.0,"BABEL, BIBREF16, BIBREF17, BIBRE","[[    0   272  5359  3577     6     3  5972 25582   371  2938     6     3
   5972 25582   371  2517     6     3  5972 25582]]","VGG-BLSTM, character-level RNNLM","[[  584 21320    18  8775   134  2305     6  1848    18  4563   391 17235
  11160     1]]"
0481a8edf795768d062c156875d20b8fb656432c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"output of global LSTM network at time $V_{m_i}^t$5 , which encodes the mention context and target entity information from $V_{m_i}^t$6 to $V_{m_i}^t$7","[[ 3911    13  1252     3  7600  2305  1229    44    97  1514   553   834
      2    51   834    23     2    17  3229   755     3     6    84 23734
      7     8  2652  2625    11  2387 10409   251    45  1514   553   834
      2    51   834    23     2    17  3229   948    12  1514   553   834
      2    51   834    23     2    17  3229   940     1]]"
3103502cf07726d3eeda34f31c0bdf1fc0ae964e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Zipf's law describes change of word frequency rate, while Heaps-Herdan describes different word number in large texts (assumed that Hepas-Herdan is consequence of Zipf's)","[[22296    89    31     7   973  8788   483    13  1448  7321  1080     6
    298   216     9   102     7    18 12636  3768  8788   315  1448   381
     16   508 14877    41     9     7  4078    15    26    24   216  8020
     18 12636  3768    19 17009    13 22296    89    31     7    61     1]]"
c9e9c5f443649593632656a5934026ad8ccc1712,4.8193,"We use machine reading to discover strategies required to solve a task, thereby generalising to","[[    0   101   169  1437  1183    12  2928  3266   831    12  4602     3
      9  2491     6     3 12550   879  4890    12]]"," We first encode text inputs using bidirectional LSTMs, then compute summaries using self-attention and conditional summaries using attention. We concatenate text summaries into text features, which, along with visual features, are processed through consecutive layers. In this case of a textual environment, we consider the grid of word embeddings as the visual features for . The final output is further processed by MLPs to compute a policy distribution over actions and a baseline for advantage estimation.","[[  101   166 23734  1499  3785     7   338  2647 26352     3  7600  2305
      7     6   258 29216  4505    51  5414   338  1044    18 25615    11
   1706   138  4505    51  5414   338  1388     5   101   975  2138    35
    342  1499  4505    51  5414   139  1499   753     6    84     6   590
     28  3176   753     6    33  8534   190 12096  7500     5    86    48
    495    13     3     9  1499  3471  1164     6    62  1099     8  8634
     13  1448 25078    26    53     7    38     8  3176   753    21     3
      5    37   804  3911    19   856  8534    57   283  6892     7    12
  29216     3     9  1291  3438   147  2874    11     3     9 20726    21
   2337 22781     5     1]]"
542a87f856cb2c934072bacaa495f3c2645f93be,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Care / Harm, Fairness / Cheating, Loyalty / Betrayal, Authority / Subversion, and Sanctity / Degradation","[[2686    3   87 3504   51    6 4506  655    3   87 2556 1014    6 1815
    63 2920   63    3   87 7912 2866  138    6 9293    3   87 3325 8674
     6   11 1051   75   17  485    3   87  374 3987  257    1]]"
3c378074111a6cc7319c0db0aced5752c30bfffb,0.0,Yes,[[   0 2163    1]],"The multi-task model outperforms the single-task model at all data sizes, but none have an overall benefit from the open vocabulary system","[[   37  1249    18 23615   825    91   883  2032     7     8   712    18
  23615   825    44    66   331  4342     6    68  5839    43    46  1879
   1656    45     8   539 19067   358     1]]"
32e8eda2183bcafbd79b22f757f8f55895a0b7b2,100.0,3,[[  0 220   1]],3,[[220   1]]
95d98b2a7fbecd1990ec9a070f9d5624891a4f26,84.2105,"2,396 ironic and 2,396 non-ironic tweets is provided","[[    0  3547   519  4314  3575   447    11  3547   519  4314   529    18
  17773   447 10657     7    19   937     1]]","a balanced dataset of 2,396 ironic and 2,396 non-ironic tweets is provided","[[    3     9  8965 17953    13  3547   519  4314  3575   447    11  3547
    519  4314   529    18 17773   447 10657     7    19   937     1]]"
98515bd97e4fae6bfce2d164659cd75e87a9fc89,0.0,Users' tweets,[[    0 13504    31 10657     7     1]],Sociability from ego-network on Twitter,[[16874  2020    45     3  6066    18  1582  1981    30  3046     1]]
526ae24fa861d52536b66bcc2d2ddfce483511d6,100.0,relative WER improvement of 10%.,[[   0 5237  549 3316 4179   13 6389    5    1]],relative WER improvement of 10%.,[[5237  549 3316 4179   13 6389    5    1]]
afa94772fca7978f30973c43274ed826c40369eb,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
a74886d789a5d7ebcf7f151bdfb862c79b6b8a12,0.0,BIBREF32,[[    0     3  5972 25582   371  2668     1]],"a BiLSTM over all words in the respective sequences with randomly initialised word embeddings, following BIBREF30","[[    3     9  2106  7600  2305   147    66  1234    16     8  6477  5932
      7    28 21306  2332  3375  1448 25078    26    53     7     6   826
      3  5972 25582   371  1458     1]]"
9595bf228c9e859b0dc745e6c74070be2468d2cf,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
b8d5e9fa08247cb4eea835b19377262d86107a9d,0.0,"WN18, WN18, WN19, WN19, WN19","[[    0     3 21170  2606     6     3 21170  2606     6     3 21170  2294
      6     3 21170  2294     6     3 21170  2294]]","IBM-UB-1 dataset BIBREF25, IAM-OnDB dataset BIBREF42, The ICDAR-2013 Competition for Online Handwriting Chinese Character Recognition BIBREF45, ICFHR2018 Competition on Vietnamese Online Handwritten Text Recognition using VNOnDB BIBREF50","[[11045    18 10134  2292 17953     3  5972 25582   371  1828     6    27
   4815    18  7638  9213 17953     3  5972 25582   371  4165     6    37
     27  6931  4280    18 11138 15571    21  1777  2263  9933  2830 20087
  31110     3  5972 25582   371  2128     6    27  7380 11120  9457 15571
     30 24532  1777  2263 14973  5027 31110   338   584  7400    29  9213
      3  5972 25582   371  1752     1]]"
9368471073c66fefebc04f1820209f563a840240,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
6743c1dd7764fc652cfe2ea29097ea09b5544bc3,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
87cb19e453cf7e248f24b5f7d1ff9f02d87fc261,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule;, A Naïve Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class;","[[   71 18434  8184   825 21603     7   284   356    13  6677  1234    57
    165  1644 12938    16 25078    26    53   628     6    11   853 15821
   6085   139     8   853    13 12257  1644 25078    26    53    16  4491
  14758   221   152  2357   826     3     9  1835  9128  3356   117     6
     71  1823     2   162  2474    15     7   825  1099     7   321  1243
     11 27154     6   365     8 20662    13 12315   859 25078    26    53
   8393     6    57  8213     3     9  1389  3438    28  1243 12938    11
  26184   576  9504   663 16826    12     8   356    13  6677  1234    13
    284   853   117     1]]"
9d1135303212356f3420ed010dcbe58203cc7db4,0.0,WSJT BIBREF21,[[    0     3  8439   683   382     3  5972 25582   371  2658     1]],"English$\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)","[[ 1566  3229     2  3535  6770  1514   196    17     9  9928    87 24518
  17622    13     8  4159  4209    18   254 11736   302     6   282  1151
    331     6    62   169     3     9  2153    13   452    11 16950   331
     21    81   898   770  7142 14152    21  1566    18   196    17     9
   9928    41  8532    18   196    17    61    11  1514 23444  3229   770
    549  7323  2534  7142 14152    21     8  1566    18 24518    41  8532
     18  2962    61     1]]"
1a43df221a567869964ad3b275de30af2ac35598,85.7143,Yelp Challenge dataset BIBREF2,[[    0  7271    40   102  7729 17953     3  5972 25582   371   357     1]],the Yelp Challenge dataset,[[    8  7271    40   102  7729 17953     1]]
18fbf9c08075e3b696237d22473c463237d153f5,0.0,Yes,[[   0 2163    1]],"Moderate agreement of 0.64-0.68 Fleiss’ Kappa over event type labels, 0.77 Fleiss’ Kappa over participant labels, and good agreement of 90.5% over coreference information.","[[ 9258  2206  2791    13     3 22787 26814     5  3651   377  4460     7
      7    22 12232   102     9   147   605   686 11241     6  4097  4013
    377  4460     7     7    22 12232   102     9   147  8344 11241     6
     11   207  2791    13  2777     5  2712   147  2583 11788   251     5
      1]]"
e07df8f613dbd567a35318cd6f6f4cb959f5c82d,100.0,perplexity,[[   0  399 9247  485    1]],perplexity,[[ 399 9247  485    1]]
0e45aae0e97a6895543e88705e153f084ce9c136,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
c3d50f1e6942c9894f9a344e7cbc411af01e419c,100.0,No,[[  0 465   1]],No,[[465   1]]
30dad5d9b4a03e56fa31f932c879aa56e11ed15b,0.0,bilingual tweets,[[    0 30521 10657     7     1]],"Appreciation, Satisfied, Peripheral complaint, Demanded inquiry, Corruption, Lagged response, Unresponsive, Medicine payment, Adverse behavior, Grievance ascribed and Obnoxious/irrelevant","[[ 2276  7886   257     6  8821   159  8549    26     6  1276  5082   760
    138 10394     6 20625    15    26 15736     6   638 27599     6   325
  11918  1773     6   597 31847     6  6852  1942     6  1980  7583  3889
      6 26039   208   663     3     9 22573    11  4249 19864  2936    87
     23    52 30374     1]]"
35915166ab2fd3d39c0297c427d4ac00e8083066,100.0,No,[[  0 465   1]],No,[[465   1]]
3c0d66f9e55a89d13187da7b7128666df9a742ce,6.4516,speaker-closed model,[[    0  5873    18 16221    26   825     1]],"In the speaker-closed condition, two episodes were set aside from each speaker as development and test sets., In the speaker-open condition, all the data except for the test speaker's were used for training","[[   86     8  5873    18 16221    26  1706     6   192 13562   130   356
   5915    45   284  5873    38   606    11   794  3369     5     6    86
      8  5873    18  8751  1706     6    66     8   331  3578    21     8
    794  5873    31     7   130   261    21   761     1]]"
08333e4dd1da7d6b5e9b645d40ec9d502823f5d7,8.3333,high-rank method outperforms the weaker approach by 0.3 points on average,"[[    0   306    18  6254  1573    91   883  2032     7     8  5676    49
   1295    57     3 19997   979    30  1348     1]]","0.007 MAP on Task A, 0.032 MAP on Task B, 0.055 MAP on Task C","[[    3 10667   940     3 25790    30 16107    71     6     3 11739  2668
      3 25790    30 16107   272     6     3 25079   755     3 25790    30
  16107   205     1]]"
6b55b558ed581759425ede5d3a6fcdf44b8082ac,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Naive Bayes, SVM, Maximum Entropy classifiers","[[ 1823   757  2474    15     7     6   180 12623     6 24210   695 12395
     63   853  7903     7     1]]"
d40662236eed26f17dd2a3a9052a4cee1482d7d6,0.0,using fixed-dimensional embeddings,[[    0   338  3599    18 11619 25078    26    53     7     1]],a vector of frame-level acoustic features,"[[    3     9 12938    13  2835    18  4563     3     9  3422     7  1225
    753     1]]"
b44ce9aae8b1479820555b99ce234443168dc1fe,0.0,multi-language corpus,[[    0  1249    18 24925 11736   302     1]],"MultiUN BIBREF20, OpenSubtitles BIBREF21","[[ 4908  7443     3  5972 25582   371  1755     6  2384 25252 21869     7
      3  5972 25582   371  2658     1]]"
44104668796a6ca10e2ea3ecf706541da1cec2cf,0.0,between LSTM and ELMo,[[    0   344     3  7600  2305    11   262 11160    32     1]],Accuracy of best interpretible system was 0.3945 while accuracy of LSTM-ELMo net was 0.6818.,"[[4292 3663 4710   13  200 7280 2317  358   47 4097 3288 2128  298 7452
    13    3 7600 2305   18 3577  329   32 3134   47 4097 3651 2606    5
     1]]"
564dcaf8d0bcc274ab64c784e4c0f50d7a2c17ee,0.0,"German-English, French-English, and Japanese-English","[[    0  2968    18 26749     6  2379    18 26749     6    11  4318    18
  26749     1]]","Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur","[[1533    6  272  122    6 1336    6  205    7    6  878    6  374    6
   695    6 1122    6 4491    6 1699    6 3188    6 6248    6 2776    6
   216    6 2018    6 3455    6   94    6  325    6  301   17    6  301
   208    6  445  115    6  445   40    6  445   29    6    3 5329    6
   276   17    6 2158    6 2770    6  180   40    6  180  208    6 6087
     6  412  157    6 4575    1]]"
3ac30bd7476d759ea5d9a5abf696d4dfc480175b,100.0,LSTM LMs,[[    0     3  7600  2305     3 11160     7     1]],LSTM LMs,[[    3  7600  2305     3 11160     7     1]]
1870f871a5bcea418c44f81f352897a2f53d0971,0.0,PTSD Peer Support Survey (PSS),[[    0     3 29609  1276    49  4224 11418    41  4176   134    61     1]],"DOSPERT, BSSS and VIAS",[[ 6054  4274 24203     6     3  4547  4256    11  9766  3291     1]]
94bee0c58976b58b4fef9e0adf6856fe917232e5,100.0,Switchboard-2000 contains 1700 more hours of speech data.,"[[    0 13218  1976    18 13527  2579     3 26774    72   716    13  5023
    331     5     1]]",Switchboard-2000 contains 1700 more hours of speech data.,"[[13218  1976    18 13527  2579     3 26774    72   716    13  5023   331
      5     1]]"
5a06f11aa75a8affde3d595c40fb03e06769e368,100.0,No,[[  0 465   1]],No,[[465   1]]
4ce4db7f277a06595014db181342f8cb5cb94626,21.0526,"inappropriate, offensive, sexist, homophobic","[[    0 21001     6 12130     6     3     7 12135     6 13503  9553 15979
      1]]","No attacks to any community,  racist, sexist, homophobic, religion based attacks, attacks to other communities","[[  465  6032    12   136   573     6 23713     6     3     7 12135     6
  13503  9553 15979     6  5562     3   390  6032     6  6032    12   119
   2597     1]]"
3c3b4797e2b21e2c31cf117ad9e52f327721790f,42.8571,"German, English, Chinese",[[   0 2968    6 1566    6 2830    1]],"English, German, Spanish, Italian, Japanese and Portuguese,  English, Arabic and Chinese","[[ 1566     6  2968     6  5093     6  4338     6  4318    11 21076     6
   1566     6 19248    11  2830     1]]"
cd9776d03fe48903e43e916385df12e1e798070a,100.0,No,[[  0 465   1]],No,[[465   1]]
18412237f7faafc6befe975d5bcd348e2b499b55,0.0,$3th$,[[   0 5583  189 3229    1]],$4th$,[[7514  189 3229    1]]
1ccc4f63268aa7841cc6fd23535c9cbe85791007,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
70c2dc170a73185c9d1a16953f85aca834ead6d3,100.0,Mean Average Precision,[[    0 23045 23836 28464     1]],Mean Average Precision,[[23045 23836 28464     1]]
a9d530d68fb45b52d9bad9da2cd139db5a4b2f7c,100.0,Kneser–Ney smoothing,[[    0 22860     7    49   104   567    15    63  3050    53     1]],Kneser–Ney smoothing,[[22860     7    49   104   567    15    63  3050    53     1]]
41d3ab045ef8e52e4bbe5418096551a22c5e9c43,0.0,"LSTM-Dense, LSTM-Dense, LSTM-D","[[   0    3 7600 2305   18  308 5167    6    3 7600 2305   18  308 5167
     6    3 7600 2305   18  308]]","IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German","[[   27  8439  9012  2534  2968    18 26749     6    27  8439  9012  2534
  15423    18 26749     6   549  7323  2534  1566    18 24518     1]]"
b6fb72437e3779b0e523b9710e36b966c23a2a40,0.0,two,[[  0 192   1]],"WikiSQL - 2 rules (SELECT, WHERE)
SimpleQuestions - 1 rule
SequentialQA - 3 rules (SELECT, WHERE, COPY)","[[ 2142  2168   134  2247   434     3    18   204  2219    41 23143 14196
      6   549 17444   427    61  9415  5991   222  2865     3    18   209
   3356 26859  7220 23008     3    18   220  2219    41 23143 14196     6
    549 17444   427     6     3 25032   476    61     1]]"
574f17134e4dd041c357ebb75a7ef590da294d22,0.0,nearly frozen binomials,[[    0  2111 10451  2701 30019    40     7     1]],null model ,[[206 195 825   1]]
547be35cff38028648d199ad39fb48236cfb99ee,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
753990d0b621d390ed58f20c4d9e4f065f0dc672,100.0,seed lexicon consists of positive and negative predicates,"[[    0  6677     3 30949   106     3  6848    13  1465    11  2841   554
  11346  1422     1]]",seed lexicon consists of positive and negative predicates,"[[ 6677     3 30949   106     3  6848    13  1465    11  2841   554 11346
   1422     1]]"
f7789313a804e41fcbca906a4e5cf69039eeef9f,57.1429,"Reuters-21578, LabelMe",[[    0     3 18844  4949  1808  3940     6 16229   329    15     1]]," 20-Newsgroups benchmark corpus , Reuters-21578, LabelMe","[[  460    18  6861     7 10739     7 15705 11736   302     3     6     3
  18844 16539  3436 11864 16229   329    15     1]]"
dbf606cb6fc1d070418cc25e38ae57bbbb7087a0,0.0,"LSTMs, LMs, LMs, LMs, LM","[[    0     3  7600  2305     7     6     3 11160     7     6     3 11160
      7     6     3 11160     7     6     3 11160]]","1) How to introduce unsupervised pre-training into NLG tasks with cross-modal context?, 2) How to design a generic pre-training algorithm to fit a wide range of NLG tasks?, 3) How to reduce the computing resources required for large-scale pre-training?, 4) What aspect of knowledge do the pre-trained models provide for better language generation?","[[ 8925   571    12  4277    73 23313   554    18 13023   139   445 24214
   4145    28  2269    18 20226  2625    58     6  9266   571    12   408
      3     9  8165   554    18 13023 12628    12  1400     3     9  1148
    620    13   445 24214  4145    58     6     3  5268   571    12  1428
      8 10937  1438   831    21   508    18  6649   554    18 13023    58
      6     3  7256   363  2663    13  1103   103     8   554    18    17
  10761  2250   370    21   394  1612  3381    58     1]]"
cacb83e15e160d700db93c3f67c79a11281d20c5,16.6667,No,[[  0 465   1]],"No, there has been previous work on recognizing social norm violation.","[[  465     6   132    65   118  1767   161    30     3 22873   569  7982
  12374     5     1]]"
73cd785d474bae7af974802715ef7ba5468d9139,100.0,manually inspect,[[    0 12616 15614     1]],manually inspect,[[12616 15614     1]]
9174aded45bc36915f2e2adb6f352f3c7d9ada8b,0.0,"WSJ2V, WSJ3V, WSJ4V, ","[[   0    3 8439  683  357  553    6    3 8439  683  519  553    6    3
  8439  683  591  553    6    3]]","SST-2 (Stanford Sentiment Treebank, version 2), Snips","[[  180  4209  4949    41   134    17   152  2590  4892  2998   295  7552
   4739     6   988  9266     6   180    29 15432     1]]"
abc5836c54fc2ac8465aee5a83b9c0f86c6fd6f5,100.0,No,[[  0 465   1]],No,[[465   1]]
aeda22ae760de7f5c0212dad048e4984cd613162,8.6957,"a new dataset of COmplex Sentence TRAnsformations, ","[[    0     3     9   126 17953    13  2847    51  9247  4892    17  1433
      3 11359    29     7 14678     7     6     3]]","For each source sentence, transformation sentences that are transformed according to some criteria (paraphrase, minimal change etc.)","[[  242   284  1391  7142     6  6586 16513    24    33 13421  1315    12
    128  6683    41  6583 27111     6  6211   483   672     5    61     1]]"
2bc0bb7d3688fdd2267c582ca593e2ce72718a91,100.0,Wiktionary,[[    0  2142 12696  1208     1]],Wiktionary,[[ 2142 12696  1208     1]]
95bb3ea4ebc3f2174846e8d422abc076e1407d6a,100.0,between 1900s and 2010s,[[    0   344 19036     7    11  2735     7     1]],between 1900s and 2010s,[[  344 19036     7    11  2735     7     1]]
b512ab8de26874ee240cffdb3c65d9ac8d6023d9,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
32ba4d2d15194e889cbc9aa1d21ff1aa6fa27679,0.0,"effective guidelines, short-term training",[[   0 1231 5749    6  710   18 1987  761    1]],extensive personal feedback,[[3616  525 3160    1]]
2cc42d14c8c927939a6b8d06f4fdee0913042416,100.0,No,[[  0 465   1]],No,[[465   1]]
942eb1f7b243cdcfd47f176bcc71de2ef48a17c4,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
028910d643c103abd90045ccb07ee8adc5a3e177,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
df510c85c277afc67799abcb503caa248c448ad2,100.0,No,[[  0 465   1]],No,[[465   1]]
2e632eb5ad611bbd16174824de0ae5efe4892daf,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Noisy data, Complexity and diversity of multimodal relations, Small set of multimodal examples","[[14445     7    63   331     6 16926   485    11  7322    13  1249 20226
   5836     6  4872   356    13  1249 20226  4062     1]]"
70e596dd4334a94844454fa7b565889556e2358d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],180221 of 231162 author names could be matched successfully,"[[  507  4305  2658    13  1902 20159   357  2291  3056   228    36     3
  10304  4234     1]]"
6b4de7fef3a543215f16042ce6a29186bf84fea4,100.0,"BERT, ERNIE, and BERT-wwm","[[    0   272 24203     6     3 25992  5091     6    11   272 24203    18
    210   210    51     1]]","BERT, ERNIE, and BERT-wwm","[[  272 24203     6     3 25992  5091     6    11   272 24203    18   210
    210    51     1]]"
78577fd1c09c0766f6e7d625196adcc72ddc8438,0.0,WSJ 2020,[[   0    3 8439  683 6503    1]],Training datasets: TTS System dataset and embedding selection dataset. Evaluation datasets: Common Prosody Errors dataset and LFR dataset.,"[[ 4017 17953     7    10   332  4578  2149 17953    11 25078    26    53
   1801 17953     5 22714 17953     7    10  7155   749     7  9666   848
     52   127     7 17953    11   301  7422 17953     5     1]]"
6412e97373e8e9ae3aa20aa17abef8326dc05450,0.0,HATE,[[   0  454 6048    1]],Human evaluators,[[3892    3   15 7480 6230    1]]
a9cc4b17063711c8606b8fc1c5eaf057b317a0c9,0.0,F1 score,[[   0  377  536 2604    1]],"For task 1, we use F1-score, Task completion ratio, User satisfaction degree, Response fluency, Number of dialogue turns, Guidance ability for out of scope input","[[  242  2491  1914    62   169   377   536    18     7  9022     6 16107
   6929  5688     6  6674  5044  1952     6 16361  6720  4392     6  7720
     13  7478  5050     6 15703    26   663  1418    21    91    13  7401
   3785     1]]"
238ec3c1e1093ce2f5122ee60209b969f7669fae,7.767,The fluctuation is measured by the mean square error (MAP) of the neighbor.,"[[    0    37 23460   257    19  8413    57     8  1243  2812  3505    41
  25790    61    13     8 10678     5     1]]","Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:
1) Setting N, the size of the neighbor.
2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.
3) Computing the surrounding uniformity for ai(0 < i ≤ N) and w.
4) Computing the mean m and the sample variance σ for the uniformities of ai .
5) Checking whether the uniformity of w is less than m − 3σ. If the value is less than m − 3σ, we may regard w as a polysemic word.","[[  421  1573  1912     7     3     9 11775   794    12  2082   823     3
      9   787  1448    19   261  4251     7    15    51 11937    16     8
   1499     6  1315    12     8   826  2245    10  8925 23495   445     6
      8   812    13     8 10678     5  9266     3 22065   445 10678    53
   1234     3     9    23    16     8   455     3  2544  7669    28     8
  12938    13     8   787  1448     3   210    19     8     3 17924     5
      3  5268 25274     8  3825  7117   485    21     3     9    23   599
    632     3     2     3    23     3     2   445    61    11     3   210
      5     3  7256 25274     8  1243     3    51    11     8  3106 27154
      3     2    21     8  7117  2197    13     3     9    23     3     5
      3  9120  1972    53   823     8  7117   485    13     3   210    19
    705   145     3    51     3     2   220     2     5   156     8   701
     19   705   145     3    51     3     2   220     2     6    62   164
   3553     3   210    38     3     9  4251     7    15  3113  1448     5
      1]]"
bd99aba3309da96e96eab3e0f4c4c8c70b51980a,0.0,LSTMs with 20% higher accuracy,[[   0    3 7600 2305    7   28 7580 1146 7452    1]],"RNN-context, SRB, CopyNet, RNN-distract, DRGD","[[  391 17235    18  1018  6327     6   180 12108     6 20255  9688     6
    391 17235    18    26   159  6471    17     6     3  3913 18405     1]]"
a779d452d11f368c66f7b51f7190d0fe9402f505,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"(infixes 700K, 318K, and 40K) each representing the number of approximate parameters","[[   41    77 12304    15     7 12283   439     6   220  2606   439     6
     11  1283   439    61   284  9085     8   381    13 24672  8755     1]]"
cf93a209c8001ffb4ef505d306b6ced5936c6b63,100.0,late 2014,[[   0 1480 1412    1]],late 2014,[[1480 1412    1]]
4e841138f307839fd2c212e9f02489e27a5f830c,21.4286,DA recognition is an important part of the dialogue process,[[   0    3 4296 5786   19   46  359  294   13    8 7478  433    1]],DA recognition is aimed to assign a label to each utterance in a conversation. It can be formulated as a supervised classification problem. ,"[[    3  4296  5786    19     3  8287    12 12317     3     9  3783    12
    284     3  5108   663    16     3     9  3634     5    94    54    36
      3 23148    38     3     9     3 23313 13774   682     5     1]]"
44a2a8e187f8adbd7d63a51cd2f9d2d324d0c98d,25.0,HEOT,[[   0    3 6021 6951    1]],"HEOT , A labelled dataset for a corresponding english tweets","[[    3  6021  6951     3     6    71     3 29506 17953    21     3     9
      3  9921 22269 10657     7     1]]"
6f2118a0c64d5d2f49eee004d35b956cb330a10e,0.0,"WSJ2K, WSJ3K, WSJ4","[[   0    3 8439  683  357  439    6    3 8439  683  519  439    6    3
  8439  683  591    1]]","Microsoft Research dataset containing movie, taxi and restaurant domains.","[[ 2803  2200 17953     3  6443  1974     6  9256    11  2062  3303     7
      5     1]]"
db9021ddd4593f6fadf172710468e2fdcea99674,0.0,text-code parallel branch,[[   0 1499   18 4978 8449 6421    1]],incorporating coding syntax tree model,[[    3 18218     3  9886 28230  2195   825     1]]
5fb6a21d10adf4e81482bb5c1ec1787dc9de260d,16.6667,By the number of possible moral dimensions.,[[   0  938    8  381   13  487 4854 8393    5    1]],By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence,"[[  938 10090    53  4854   120  2193  6677  1234    28     3     9   356
     13  4854   120 26213  6677  1234     3   390    30     8  9347    13
      3  2165  1433     1]]"
551f77b58c48ee826d78b4bf622bb42b039eca8c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],can be biased by dataset used and may generate categories which are suboptimal compared to human designed categories,"[[   54    36 30026    57 17953   261    11   164  3806  5897    84    33
    769 12331  1982     3  2172    12   936   876  5897     1]]"
0c7823b27326b3f5dff51f32f45fc69c91a4e06d,18.1818,in a virtual reality environment,[[   0   16    3    9 4291 2669 1164    1]],in open-ended task esp. for counting-type questions ,"[[   16   539    18 14550  2491 16159     5    21 15899    18  6137   746
      1]]"
cee29acec4da1b247795daa4e2e82ef8a7b25a64,100.0,DL-61,[[    0     3 10013    18  4241     1]],DL-61,[[    3 10013    18  4241     1]]
2a950ede24b26a45613169348d5db9176fda4f82,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
cc28919313f897358ef864948c65318dc61cb03c,21.0526,"string kernels, SST-1, SST-2, SST-3, SST-4","[[    0  6108 20563     7     6   180  4209  2292     6   180  4209  4949
      6   180  4209  3486     6   180  4209  4278]]","string kernels, SST, KE-Meta, SFA, CORAL, TR-TrAdaBoost, Transductive string kernels, transductive kernel classifier","[[ 6108 20563     7     6   180  4209     6     3  9914    18 23351     9
      6   180  4795     6  2847 21415     6 11466    18   382    52   188
     26     9 16481     6  4946 28668  6108 20563     7     6  3017 28668
  20563   853  7903     1]]"
23d32666dfc29ed124f3aa4109e2527efa225fbc,42.8571,They use it as standard for all word embeddings,"[[    0   328   169    34    38  1068    21    66  1448 25078    26    53
      7     1]]",They use it as addition to previous model - they add new edge between words if word embeddings are similar.,"[[  328   169    34    38   811    12  1767   825     3    18    79   617
    126  3023   344  1234     3    99  1448 25078    26    53     7    33
   1126     5     1]]"
8e898bec123c70315db44f6c8002adc8bf4486ad,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
d51069595f67a3a53c044c8a37bae23facbfa45d,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
74db8301d42c7e7936eb09b2171cd857744c52eb,46.6667,The accuracy of the neural network models on the task is evaluated by the mean of the error scores,"[[    0    37  7452    13     8 24228  1229  2250    30     8  2491    19
  14434    57     8  1243    13     8  3505  7586]]",Comparison of test accuracies of neural network models on an inflection task and qualitative analysis of the errors,"[[30760    13   794     3  6004  2414  6267     7    13 24228  1229  2250
     30    46    16    89 12252  2491    11 19647  1693    13     8  6854
      1]]"
0457242fb2ec33446799de229ff37eaad9932f2a,0.0,"a decentralized platform with a single user interface, a distributed decentralized decentralized","[[    0     3     9    20 21411  1585    28     3     9   712  1139  3459
      6     3     9  8308    20 21411    20 21411]]","handling large volume incoming data, sentiment analysis on tweets and predictive online learning","[[ 5834   508  2908     3 19583   331     6  6493  1693    30 10657     7
     11 27875   367  1036     1]]"
75043c17a2cddfce6578c3c0e18d4b7cf2f18933,0.0,a trend in the psychology of music that studies how the musical preferences are reflected in the,"[[    0     3     9  4166    16     8 14153    13   723    24  2116   149
      8  4183 11177    33     3 12999    16     8]]","audiences wanted products more and more contemporary, intense and a little bit novel or sophisticated, but less and less mellow and (surprisingly) unpretentious","[[11209  1114   494    72    11    72  4092     6  6258    11     3     9
    385   720  3714    42  8732     6    68   705    11   705     3  2341
   3216    11    41 12713    61    73  2026  4669  2936     1]]"
8951fde01b1643fcb4b91e51f84e074ce3b69743,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"They  evaluate newly proposed models in several low-resource settings across different languages with real, distantly supervised data with non-synthetic noise","[[  328  6825  6164  4382  2250    16   633   731    18    60  7928  3803
    640   315  8024    28   490     6 10382   120     3 23313   331    28
    529    18     7    63    29 17194  4661     1]]"
326e08a0f5753b90622902bd4a9c94849a24b773,0.0,"70,000",[[    0     3 28891     1]],"17,833 sentences, 826,987 characters and 2,714 question-answer pairs","[[12864  4591   519 16513     6   505  2688     6  3916   940  2850    11
   3547   940  2534   822    18  3247  3321 14152     1]]"
f513e27db363c28d19a29e01f758437d7477eb24,0.0,"low-frequency reading comprehension, high-frequency reading comprehension","[[    0   731    18 30989  1183 27160     6   306    18 30989  1183 27160
      1]]","AS Reader, GA Reader, CAS Reader",[[ 6157 16120     6 10615 16120     6     3 18678 16120     1]]
94c5f5b1eb8414ad924c3568cedd81dc35f29c48,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],3000 hard samples are selected from the test set,[[ 220 2313  614 5977   33 2639   45    8  794  356    1]]
182b6d77b51fa83102719a81862891f49c23a025,16.0,a large number of articles that were labeled with partisanship were annotated,"[[    0     3     9   508   381    13  2984    24   130  3783    15    26
     28     3 18237  2009   130    46  2264   920]]","deciding publisher partisanship, risk annotator bias because of short description text provided to annotators","[[    3 12053 14859     3 18237  2009     6  1020    46  2264  1016 14387
    250    13   710  4210  1499   937    12    46  2264  6230     1]]"
785c054f6ea04701f4ab260d064af7d124260ccc,0.0,"WSJ, WSJ2V, WSJ2V, WSJ","[[   0    3 8439  683    6    3 8439  683  357  553    6    3 8439  683
   357  553    6    3 8439  683]]","SearchSnippets, StackOverflow, Biomedical","[[ 4769   134    29    23  6811    17     7     6     3 19814 23847  7631
      6  3318  2726  1950     1]]"
c20bb0847ced490a793657fbaf6afb5ef54dad81,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
3aa7173612995223a904cc0f8eef4ff203cbb860,0.0,"POS and NER embedding, Relation embedding, BiLSTM BI","[[    0     3 16034    11     3 18206 25078    26    53     6 28898 25078
     26    53     6  2106  7600  2305     3  5972]]","SLQA, Rusalka, HMA Model (single), TriAN (single), jiangnan (ensemble), MITRE (ensemble), TriAN (ensemble), HMA Model (ensemble)","[[    3  5629 23008     6  2770     7   138  1258     6   454  4148  5154
     41     7    53   109   201  2702  5033    41     7    53   109   201
      3   354    23  1468    29   152    41  7408   201  8161 20371    41
   7408   201  2702  5033    41  7408   201   454  4148  5154    41  7408
     61     1]]"
7eb3852677e9d1fb25327ba014d2ed292184210c,8.0,"Word & Trace (WTM) dataset is collected from a variety of sources,","[[    0  4467     3   184  3083   565    41   518  2305    61 17953    19
   4759    45     3     9  1196    13  2836     6]]","from YouTube videos, with associated transcripts obtained from semi-supervised caption filtering, from a Voice Search service","[[   45  5343  3075     6    28  1968 20146     7  5105    45  4772    18
  23313 25012  4191    53     6    45     3     9 12347  4769   313     1]]"
93b299acfb6fad104b9ebf4d0585d42de4047051,0.0,WN18 and FB15k,[[    0     3 21170  2606    11     3 15586  1808   157     1]],"ABSA SemEval 2014-2016 datasets
Yelp Academic Dataset
Wikipedia dumps","[[20798   188   679    51   427  2165  1412    18 11505 17953     7  7271
     40   102 16682  2747  2244 16885 11986     7     1]]"
be7b375b22d95d1f6c68c48f57ea87bf82c72123,0.0,accuracy,[[   0 7452    1]],"ROUGE F1, METEOR",[[  391 26260   427   377  4347  7934  3463  2990     1]]
0038b073b7cca847033177024f9719c971692042,10.5263,"Using a BLEU-style constraint, we reduce relation extraction to a","[[    0     3  3626     3     9     3  8775 12062    18  4084 27354    17
      6    62  1428  4689 16629    12     3     9]]","The relation R(x,y) is mapped onto a question q whose answer is y","[[   37  4689   391   599   226     6    63    61    19     3 28400  2400
      3     9   822     3  1824     3  2544  1525    19     3    63     1]]"
9ae084e76095194135cd602b2cdb5fb53f2935c1,100.0,word error rate,[[   0 1448 3505 1080    1]],word error rate,[[1448 3505 1080    1]]
4f243056e63a74d1349488983dc1238228ca76a7,100.0,No,[[  0 465   1]],No,[[465   1]]
a6b99b7f32fb79a7db996fef76e9d83def05c64b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Active Intent Accuracy, Requested Slot F1, Average Goal Accuracy, Joint Goal Accuracy","[[11383    86  4669  4292  3663  4710     6 15374    15    26 12094   377
   4347 23836 17916  4292  3663  4710     6 16761 17916  4292  3663  4710
      1]]"
9ef182b61461d0d8b6feb1d6174796ccde290a15,100.0,Use an existing one,[[   0 2048   46 1895   80    1]],Use an existing one,[[2048   46 1895   80    1]]
e9ccc74b1f1b172224cf9f01e66b1fa9e34d2593,37.8378,"claim number, date, URL, URL, claim source, claim description, claim description, claim","[[   0 1988  381    6  833    6 8889    6 8889    6 1988 1391    6 1988
  4210    6 1988 4210    6 1988]]","besides claim, label and claim url, it also includes a claim ID, reason, category, speaker, checker, tags, claim entities, article title, publish data and claim date","[[    3 15262  1988     6  3783    11  1988     3 16137     6    34    92
    963     3     9  1988  4699     6  1053     6  3295     6  5873     6
    691    49     6 12391     6  1988 12311     6  1108  2233     6  8099
    331    11  1988   833     1]]"
8ea4bd4c1d8a466da386d16e4844ea932c44a412,100.0,text-code parallel corpus,[[    0  1499    18  4978  8449 11736   302     1]], text-code parallel corpus,[[ 1499    18  4978  8449 11736   302     1]]
b2254f9dd0e416ee37b577cef75ffa36cbcb8293,0.0,2,[[  0 204   1]],"5 domains: software, stuff, african wildlife, healthcare, datatypes","[[  305  3303     7    10   889     6  2005     6 24040    29  9106     6
   4640     6   331  6137     7     1]]"
aa60b0a6c1601e09209626fd8c8bdc463624b0b3,0.0,"Result: 81.0%, 81.0%, 81.0%","[[    0     3 20119    10     3  4959     5  6932     6     3  4959     5
   6932     6     3  4959     5  6932     1]]","With both test sets performances decrease, varying between 94-97%","[[  438   321   794  3369  7357  6313     6     3 14177   344     3  4240
   7141  6170     1]]"
9299fe72f19c1974564ea60278e03a423eb335dc,9.7561,"selection of raters, use of context, creation of reference translations","[[   0 1801   13 1080   52    7    6  169   13 2625    6 3409   13 2848
  7314    7    1]]","MT developers to which crowd workers were compared are usually not professional translators, evaluation of sentences in isolation prevents raters from detecting translation errors, used not originally written Chinese test set
","[[    3  7323  5564    12    84  4374  2765   130     3  2172    33  1086
     59   771 22770     7     6  5002    13 16513    16 15997  1709     7
   1080    52     7    45     3 29782  7314  6854     6   261    59  5330
   1545  2830   794   356     1]]"
a1097ce59270d6f521d92df8d2e3a279abee3e67,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"model points out plausible signals which were passed over by an annotator, it also picks up on a recurring tendency in how-to guides in which the second person pronoun referring to the reader is often the benefactee of some action","[[  825   979    91 31323  9650    84   130  2804   147    57    46    46
   2264  1016     6    34    92  1432     7    95    30     3     9     3
  21557 17174    16   149    18   235  9314    16    84     8   511   568
    813 15358    29     3 13215    12     8  5471    19   557     8     3
  15719  8717    15    15    13   128  1041     1]]"
f2c5da398e601e53f9f545947f61de5f40ede1ee,23.5294,The coefficients are then interpreted as mean squared average precision,"[[    0    37 27742     7    33   258     3 20603    38  1243  2812    26
   1348 11723     1]]",The coefficients are projected back to the dummy variable space.,"[[   37 27742     7    33 16037   223    12     8     3    26 16056  7660
    628     5     1]]"
13b36644357870008d70e5601f394ec3c6c07048,100.0,No,[[  0 465   1]],No,[[465   1]]
1e2ffa065b640e912d6ed299ff713a12195e12c4,0.0,"a training set of table tennis demonstrations, a training set of locomotion demonstration","[[    0     3     9   761   356    13   953  9999 10686     7     6     3
      9   761   356    13  2072    32  7259 10686]]",a simulated binning task in which the robot is tasked to place a cube into a bowl as outlined by the verbal command,"[[    3     9     3 31126     3   115 10503  2491    16    84     8  7567
     19     3    17 23552    12   286     3     9   123   346   139     3
      9  3047    38     3 17403    57     8  7375   138  4106     1]]"
bd6cec2ab620e67b3e0e7946fc045230e6906020,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"F1 score of 0.71 for this task without any specific training, simply by choosing a threshold below which all sentence pairs are considered duplicates, distances between duplicate and non-duplicate questions using different embedding systems","[[  377   536  2604    13  4097  4450    21    48  2491   406   136   806
    761     6   914    57  4622     3     9 12709   666    84    66  7142
  14152    33  1702 19197     7     6  2357     7   344 19197    11   529
     18    26   413 26221   746   338   315 25078    26    53  1002     1]]"
be1c0816793a4549c811480170f30fab52a7a157,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
b11ee27f3de7dd4a76a1f158dc13c2331af37d9f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]], path ranking-based KGC (PRKGC),[[ 2071 11592    18   390   480 11055    41  5554   439 11055    61     1]]
1baf87437b70cc0375b8b7dc2cfc2830279bc8b5,57.1429,Randomly selected from Twitter,[[    0 25942   120  2639    45  3046     1]],"Randomly selected from a Twitter dump, temporally matched to causal documents","[[25942   120  2639    45     3     9  3046 11986     6 10301   127  1427
      3 10304    12 31161  2691     1]]"
1951cde612751410355610074c3c69cec94824c2,0.0,CNN,[[    0 19602     1]],autoencoders,[[1510   35 4978   52    7    1]]
2b61893b22ac190c94c2cb129e86086888347079,100.0,DBpedia,[[    0     3  9213 24477     1]],DBpedia,[[    3  9213 24477     1]]
ca4b66ffa4581f9491442dcec78ca556253c8146,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
f42e61f9ad06fb782d1574eb973c880add4f76d2,0.0,"hierarchical generalizations, linear order, logical consistency","[[    0  1382  7064  1950   879  1707     7     6 13080   455     6     3
   6207 12866     1]]","type of recurrent unit, type of attention, choice of sequential vs. tree-based model structure","[[  686    13     3    60 14907  1745     6   686    13  1388     6  1160
     13 29372     3   208     7     5  2195    18   390   825  1809     1]]"
0bd683c51a87a110b68b377e9a06f0a3e12c8da0,57.1429,"bilingual dictionary induction, cross-lingual word linking and entity linking","[[    0 30521 24297    16  8291     6  2269    18 25207  1448 17988    11
  10409 17988     1]]","bilingual dictionary induction, monolingual and cross-lingual word similarity, and cross-lingual hypernym discovery","[[30521 24297    16  8291     6  7414 25207    11  2269    18 25207  1448
   1126   485     6    11  2269    18 25207  6676    29    63    51  9087
      1]]"
4688534a07a3cbd8afa738eea02cc6981a4fd285,52.6316,They use Monalog to combine BERT with BERT,"[[    0   328   169  2963     9  2152    12  5148   272 24203    28   272
  24203     1]]",They use Monalog for data-augmentation to fine-tune BERT on this task,"[[  328   169  2963     9  2152    21   331    18 19260    12  1399    18
     17   444   272 24203    30    48  2491     1]]"
90f80a94fabaab72833256572db1d449c2779beb,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
5dc2f79cd8078d5976f2df9ab128d4517e894257,100.0,BBC and CNN,[[    0  9938    11 19602     1]],BBC and CNN ,[[ 9938    11 19602     1]]
891cab2e41d6ba962778bda297592c916b432226,100.0,Python,[[    0 20737     1]],Python,[[20737     1]]
c383fa9170ae00a4a24a8e39358c38395c5f034b,14.2857,"they know what they are, they use it to classify their work as content","[[   0   79  214  125   79   33    6   79  169   34   12  853 4921   70
   161   38  738    1]]"," words found in the control word lists are then removed, The remaining words, which represent the content","[[1234  435   16    8  610 1448 7809   33  258 3641    6   37 4080 1234
     6   84 4221    8  738    1]]"
aa7decee4e3006c2c99b1f331a5b32d44a565ef6,100.0,No,[[  0 465   1]],No,[[465   1]]
b8cee4782e05afaeb9647efdb8858554490feba5,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
222b2469eede9a0448e0226c6c742e8c91522af3,100.0,No,[[  0 465   1]],No,[[465   1]]
1a69696034f70fb76cd7bb30494b2f5ab97e134d,6.6667,"Using Deep Learning, our model outperforms by 0.81 BLEU and 0.","[[    0     3  3626  9509  6630     6    69   825    91   883  2032     7
     57  4097  4959     3  8775 12062    11  4097]]",Answer with content missing: (Table II) Proposed model has F1 score of  0.7220 compared to 0.7148 best state-state-of-the-art result.,"[[11801    28   738  3586    10    41 20354  2466    61   749 12151   825
     65   377   536  2604    13  4097  5865  1755     3  2172    12     3
  22426 24748   200   538    18  5540    18   858    18   532    18  1408
    741     5     1]]"
cb8e2069218e30c643013c20e93ebe23525d9f55,0.0,"Editing framework, WSJ-Base, WSJ-Base","[[    0 11664    53  4732     6     3  8439   683    18 14885    15     6
      3  8439   683    18 14885    15     1]]","Adobe internal NLU tool, Pytext, Rasa",[[11551  3224   445  9138  1464     6 12901  6327     6  9053     9     1]]
2898e4aa7a3496c628e7ddf2985b48fb11aa3bba,0.0,accuracy,[[   0 7452    1]],"test-set perplexity, likelihood convergence and clustering measures, visualizing the topic summaries, authors' topic distributions and by performing an automatic labeling task","[[  794    18  2244   399  9247   485     6 17902 31098    11  9068    53
   3629     6  3176  2610     8  2859  4505    51  5414     6  5921    31
   2859  3438     7    11    57  5505    46  6569  3783    53  2491     1]]"
3fae289ab1fc023bce2fa4f1ce4d9f828074f232,0.0,Unanswerable,[[   0  597 3247 3321  179    1]]," restrict the content of each text to the abstract and conclusion of the original work, considered other parts of the original works such as introduction or discussion sections, extracted text portions are appropriate for the AV task, each original work was preprocessed manually, removed tables, formulas, citations, quotes and sentences that include non-language content such as mathematical constructs or specific names of researchers, systems or algorithms","[[18395     8   738    13   284  1499    12     8  9838    11  7489    13
      8   926   161     6  1702   119  1467    13     8   926   930   224
     38  5302    42  3071  6795     6 21527  1499 17622    33  2016    21
      8     3  6968  2491     6   284   926   161    47   554 15056    15
     26 12616     6  3641  5056     6  5403     7     6     3 13903     7
      6  7599    11 16513    24   560   529    18 24925   738   224    38
  18913  6774     7    42   806  3056    13  4768     6  1002    42 16783
      1]]"
d3014683dff9976b7c56b72203df99f0e27e9989,0.0,F1 score,[[   0  377  536 2604    1]],"we report P@1, which is equivalent to accuracy, we also provide results with P@5 and P@10 in the Appendix","[[  62  934  276 1741 4347   84   19 7072   12 7452    6   62   92  370
   772   28  276 1741  755   11  276 1741 1714   16    8 2276  989 2407
     1]]"
4c822bbb06141433d04bbc472f08c48bc8378865,0.0,Randomly from a Twitter dump,[[    0 25942   120    45     3     9  3046 11986     1]],"They identify documents that contain the unigrams 'caused', 'causing', or 'causes'","[[  328  2862  2691    24  3480     8    73    23  5096     7     3    31
    658 10064    31     6     3    31  5885    31     6    42     3    31
    658  1074     7    31     1]]"
74261f410882551491657d76db1f0f2798ac680f,0.0,"Spanish, Catalan, Czech, Mandarin",[[    0  5093     6  3431     9  1618     6 16870     6 31057     1]],"Answer with content missing: (3 Experimental Setup) We experiment with six target languages: French (FR), Brazilian Portuguese (PT), Italian (IT), Polish (PL), Croatian (HR), and Finnish (FI).","[[11801    28   738  3586    10  6918 30871  2821   413    61   101  5016
     28  1296  2387  8024    10  2379    41  7422   201 18065 21076    41
   6383   201  4338    41  3177   201 16073    41  5329   201 19789    29
     41 11120   201    11 28124    41  4936   137     1]]"
32a232310babb92991c4b1b75f7aa6b4670ec447,100.0,No,[[  0 465   1]],No,[[465   1]]
f6e5febf2ea53ec80135bbd532d6bb769d843dd8,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
3e0c9469821cb01a75e1818f2acb668d071fcf40,100.0,"overall rating, mean number of turns",[[   0 1879 5773    6 1243  381   13 5050    1]],"overall rating, mean number of turns",[[1879 5773    6 1243  381   13 5050    1]]
2677b88c2def3ed94e25a776599555a788d197f2,0.0,BERT,[[    0   272 24203     1]],6-layer bLSTM with 1024 hidden units,[[12405 18270     3   115  7600  2305    28   335  2266  5697  3173     1]]
78661bdd4d11148e07bdf17141cf088db4ad60c6,0.0,82.0%,[[   0    3 4613    5 6932    1]],an official F1-score of 0.2905 on the test set,"[[  46 2314  377  536   18    7 9022   13 4097 3166 3076   30    8  794
   356    1]]"
98b11f70239ef0e22511a3ecf6e413ecb726f954,100.0,No,[[  0 465   1]],No,[[465   1]]
5a2c0c55a43dcc0b9439d330d2cbe1d5d444bf36,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
9a4aa0e4096c73cd2c3b1eab437c1bf24ae7bf03,100.0,"abstracts, sentences",[[    0  9838     7     6 16513     1]],"abstracts, sentences",[[ 9838     7     6 16513     1]]
53b02095ba7625d85721692fce578654f66bbdf0,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
d10e256f2f724ad611fd3ff82ce88f7a78bad7f7,60.0,Sub-task A: F1 score of 0.82,"[[    0  3325    18 23615    71    10   377   536  2604    13  4097  4613
      1]]",macro F1 score of 0.62,[[11663   377   536  2604    13  4097  4056     1]]
0cd90e5b79ea426ada0203177c28812a7fc86be5,90.9091,the number of experts between models,[[   0    8  381   13 2273  344 2250    1]],varied the number of experts between models,[[10535     8   381    13  2273   344  2250     1]]
afdad4c9bdebf88630262f1a9a86ac494f06c4c1,50.0,DM+ model does not require that supporting facts (i.e. the facts that are,"[[   0    3 7407 1220  825  405   59 1457   24 3956 6688   41   23    5
    15    5    8 6688   24   33]]","the new DMN+ model does not require that supporting facts (i.e. the facts that are relevant for answering a particular question) are labeled during training., In addition, we introduce a new input module to represent images.","[[    8   126     3  7407   567  1220   825   405    59  1457    24  3956
   6688    41    23     5    15     5     8  6688    24    33  2193    21
  18243     3     9  1090   822    61    33  3783    15    26   383   761
      5     6    86   811     6    62  4277     3     9   126  3785  6008
     12  4221  1383     5     1]]"
bea60603d78baeeb6df1afb53ed08d8296b42f1e,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
1a252ffeaebdb189317aefd6c606652ba9677112,20.0,disabling certain heads increases performance by up to 3.2%,[[   0 1028    9 7428  824 7701 5386  821   57   95   12 1877 5406    1]],"disabling the first layer in the RTE task gives a significant boost, resulting in an absolute performance gain of 3.2%,  this operation vary across tasks","[[1028    9 7428    8  166 3760   16    8  391 3463 2491 1527    3    9
  1516 4888    6    3 5490   16   46 6097  821 2485   13 1877 5406    6
    48 2986 5215  640 4145    1]]"
f4e1d2276d3fc781b686d2bb44eead73e06fbf3f,100.0,Language Modeling,[[    0 10509  5154    53     1]],Language Modeling,[[10509  5154    53     1]]
12eaaf3b6ebc51846448c6e1ad210dbef7d25a96,0.0,1,[[  0 209   1]],wav2vec has 12 convolutional layers,[[ 8036   208   357   162    75    65   586   975 24817   138  7500     1]]
0062ad4aed09a57d0ece6aa4b873f4a4bf65d165,0.0,standard deviation,[[    0  1068 25291     1]],"we define the similarity between INLINEFORM7 and INLINEFORM8 by, DISPLAYFORM0","[[   62  6634     8  1126   485   344  3388 20006 24030   940    11  3388
  20006 24030   927    57     6     3 15438   345 29002 24030   632     1]]"
0c557b408183630d1c6c325b5fb9ff1573661290,17.3913,"Compared to corrected SNLI-VE dataset, SNLI-VE has a","[[    0     3 25236    12 23006     3  8544  8159    18  8575 17953     6
      3  8544  8159    18  8575    65     3     9]]","73.02% on the uncorrected SNLI-VE test set, achieves 73.18% balanced accuracy when tested on the corrected test set","[[    3  4552     5   632  5406    30     8    73 28832    15    26     3
   8544  8159    18  8575   794   356     6  1984     7   489 18495  5953
   8965  7452   116  5285    30     8 23006   794   356     1]]"
3f326c003be29c8eac76b24d6bba9608c75aa7ea,100.0,F1 and Weighted-F1,[[    0   377   536    11 14230    15    26    18   371   536     1]],F1 and Weighted-F1,[[  377   536    11 14230    15    26    18   371   536     1]]
9c0cf1630804366f7a79a40934e7495ad9f32346,100.0,No,[[  0 465   1]],No,[[465   1]]
568ce2f5355d009ec9bc1471fb5ea74655f7e554,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
f3766c6937a4c8c8d5e954b4753701a023e3da74,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],fine-tuned the GPT-2 medium model BIBREF51 on our collected headlines and then used it to measure the perplexity (PPL) on the generated outputs,"[[ 1399    18    17   444    26     8   350  6383  4949  2768   825     3
   5972 25582   371  5553    30    69  4759 12392     7    11   258   261
     34    12  3613     8   399  9247   485    41   345  5329    61    30
      8  6126  3911     7     1]]"
ead5dc1f3994b2031a1852ecc4f97ac5760ea977,100.0,14 categories,[[   0  968 5897    1]],14 categories,[[ 968 5897    1]]
f5db12cd0a8cd706a232c69d94b2258596aa068c,13.3333,performance improves by 0.86 on comparison and 0.86 on comparison,"[[   0  821 1172    7   57 4097 3840   30 4993   11 4097 3840   30 4993
     1]]","Answer with content missing: (Table 1) The performance of all the target models raises significantly, while that on the original
examples remain comparable (e.g. the overall accuracy of BERT on modified examples raises from 24.1% to 66.0% on Quora)","[[11801    28   738  3586    10    41 20354  8925    37   821    13    66
      8  2387  2250  3033     7  4019     6   298    24    30     8   926
   4062  2367 13289    41    15     5   122     5     8  1879  7452    13
    272 24203    30  8473  4062  3033     7    45   997     5  4704    12
      3  3539     5  6932    30  2415   127     9    61     1]]"
c0a11ba0f6bbb4c69b5a0d4ae9d18e86a4a8f354,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
85912b87b16b45cde79039447a70bd1f6f1f8361,0.0,"70,000",[[    0     3 28891     1]],449050,[[8537 2394 1752    1]]
975a4ac9773a4af551142c324b64a0858670d06e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"17,833 sentences, 826,987 characters and 2,714 question-answer pairs","[[12864  4591   519 16513     6   505  2688     6  3916   940  2850    11
   3547   940  2534   822    18  3247  3321 14152     1]]"
e4a19b91b57c006a9086ae07f2d6d6471a8cf0ce,9.6774,are identified the main international development topics that states raise,[[   0   33 4313    8  711 1038  606 4064   24 2315 3033    1]]," They focus on exclusivity and semantic coherence measures: Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. They select select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence.","[[  328   992    30 16288   485    11 27632   576   760  1433  3629    10
  20600  8325  1234    16     3     9   787  2859    24   103    59  2385
    182   557    16   119  4064    33     3  4931    38   492    24  2859
   3839     5   328  1738  1738     8   898    18 19710   825     6    84
     65     8  2015  1465 27687    16     8 26625  1400     6    11   795
   1146 16288   485    44     8   337   593    13 27632   576   760  1433
      5     1]]"
482b4cc7676cf13912e27899c718f4dc5d92846d,20.0,by using the vector adverbs,[[    0    57   338     8 12938     3     9    26 11868     7     1]],identify all abbreviations using regular expressions,[[2862   66  703 1999 2099 1628  338 1646 3893    7    1]]
e111925a82bad50f8e83da274988b9bea8b90005,100.0,Randomly from Twitter,[[    0 25942   120    45  3046     1]],Randomly from Twitter,[[25942   120    45  3046     1]]
dad8cc543a87534751f9f9e308787e1af06f0627,0.0,WN18KO,[[    0     3 21170  2606 12725     1]],"AIDA-B, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI","[[   71 26483    18   279     6     3 11539 21653     6  5266 15829     6
     71 16892   188 13777     6     3 21170  2326    18 18105 15658     6
      3 21170  2326    18 16785 14108     1]]"
95abda842c4df95b4c5e84ac7d04942f1250b571,66.6667,"German-English, French-English, and Japanese-English","[[    0  2968    18 26749     6  2379    18 26749     6    11  4318    18
  26749     1]]","multiple language pairs including German-English, French-English, and Japanese-English.","[[ 1317  1612 14152   379  2968    18 26749     6  2379    18 26749     6
     11  4318    18 26749     5     1]]"
6e2ad9ad88cceabb6977222f5e090ece36aa84ea,0.0,"BERT-Base, LSTM-Base, LSTM-Base","[[    0   272 24203    18 14885    15     6     3  7600  2305    18 14885
     15     6     3  7600  2305    18 14885    15]]",The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254.,"[[   37 20726   825    19     3     9  1659  5932    18   235    18     7
     15   835  3772 23734    52    87   221  4978    52   825    28  1388
      5    37 23734    52    19     3     9  2647 26352  3230    18   134
  14184     3 11679 19159   599  7600  2305    61  2358     3  5972 25582
    371  2534    11     8    20  4978    52     3     9   712     3  7600
   2305  2358    28  1388  8557     5    37  1388  8557    19 29216    26
     38    16     3  5972 25582   371  1298    11    62   169     3     9
  30337    63   960    21    20  9886     5   101  2412   414    18   235
     18   989   379     8  1234 25078    26    53     7     5    37 25078
     26    53   812   261    19    13   209  2577    11     8  5697   538
    812    13     8     3  7600  2305  2640    19    13   944  7984     1]]"
4d5e2a83b517e9c082421f11a68a604269642f29,100.0,2,[[  0 204   1]],2,[[204   1]]
a2a3af59f3f18a28eb2ca7055e1613948f395052,100.0,Twitter,[[   0 3046    1]],Twitter,[[3046    1]]
ac148fb921cce9c8e7b559bba36e54b63ef86350,20.0,The Gigaword corpus,[[    0    37     3 20640     9  6051 11736   302     1]],The same 2K set from Gigaword used in BIBREF7,"[[   37   337   204   439   356    45     3 20640     9  6051   261    16
      3  5972 25582   371   940     1]]"
702e2d02c25a2f3f6b1be8ad3d448b502b8ced9c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"derive rewards from human-human dialogues by assigning positive values to contextualised responses seen in the data, and negative values to randomly chosen responses due to lacking coherence","[[   74   757 11157    45   936    18 12450  7478     7    57 12317    53
   1465  2620    12 28131  3375  7216   894    16     8   331     6    11
   2841  2620    12 21306  3934  7216   788    12 16914   576   760  1433
      1]]"
cc2b98b46497c71e955e844fb36e9ef6e2784640,0.0,accuracy,[[   0 7452    1]],"BLEU BIBREF8, RIBES BIBREF9, token-level delay","[[    3  8775 12062     3  5972 25582   371 11864     3  5593   279  3205
      3  5972 25582   371  1298     6 14145    18  4563  7230     1]]"
2ba0c7576eb5b84463a59ff190d4793b67f40ccc,91.6667,"attention matrices, using visualizations of the activations created by different pieces of text","[[    0  1388     3 20705    15     7     6   338 21744     7    13     8
   5817  1628   990    57   315  2161    13  1499]]","attention probes, using visualizations of the activations created by different pieces of text","[[ 1388 12732     7     6   338 21744     7    13     8  5817  1628   990
     57   315  2161    13  1499     1]]"
8cb9006bcbd2f390aadc6b70d54ee98c674e45cc,0.0,"SNLP-R, SNLP-R",[[   0    3 8544 6892   18  448    6    3 8544 6892   18  448    1]],"daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary website, novels, history and religious books from Sindhi Adabi Board,  tweets regarding news and sports are collected from twitter","[[ 1444  2209   210  1273    11    71   210  3690    71   210     9   172
  17542   107    23 16265     6 16885 11986     7     6   710  1937    11
   2100  1506    45  2142  3441   291   569   875     6  1506    45 11561
   4467  2785   875     6  4332   913     7     6 16438     6  1937     6
   1335    45 17542   107  8930  3357 14551   475     6 16438     6   892
     11  4761  1335    45 17542   107    23  1980 15975  2086     6 10657
      7  1918  1506    11  2100    33  4759    45 19010     1]]"
728a55c0f628f2133306b6bd88af00eb54017b12,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Winter and summer words formed two separate clusters. Week day and week-end day words also formed separate clusters.,"[[4272   11 1248 1234 5147  192 2450 9068    7    5 6551  239   11  471
    18  989  239 1234   92 5147 2450 9068    7    5    1]]"
1adbdb5f08d67d8b05328ccc86d297ac01bf076c,0.0,"English, Chinese",[[   0 1566    6 2830    1]],"Train languages are: Cantonese, Bengali, Pashto, Turkish, Vietnamese, Haitian, Tamil, Kurdish, Tokpisin and Georgian, while Assamese, Tagalog, Swahili, Lao are used as target languages.","[[15059  8024    33    10  1072  6948     7    15     6 20008    23     6
   6156   107   235     6 15423     6 24532     6 22179   152     6 22503
      6  8333    26  1273     6   304   157   102   159    77    11  5664
     29     6   298   282     7     9  2687    15     6  3284     9  2152
      6 17085   107   173    23     6   325    32    33   261    38  2387
   8024     5     1]]"
f197e0f61f7980c64a76a3a9657762f1f0edb65b,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
b3fcab006a9e51a0178a1f64d1d084a895bd8d5c,0.0,"LSTM, BERT",[[    0     3  7600  2305     6   272 24203     1]],"S2VT, RGB (VGG), RGB (VGG)+Flow (AlexNet), LSTM-E (VGG), LSTM-E (C3D) and Yao et al.","[[  180   357 18645     6 27412    41 17217   517   201 27412    41 17217
    517    61  1220 15390    41 27280  9688   201     3  7600  2305    18
    427    41 17217   517   201     3  7600  2305    18   427    41   254
    519   308    61    11  4701    32     3    15    17   491     5     1]]"
bf2ebc9bbd4cbdf8922c051f406effc97fd16e54,100.0,two,[[  0 192   1]],two,[[192   1]]
25fd61bb20f71051fe2bd866d221f87367e81027,0.0,"BERT-Base, BT-Base, BT-Base, ","[[    0   272 24203    18 14885    15     6     3  9021    18 14885    15
      6     3  9021    18 14885    15     6     3]]","NDM, LIDM, KVRN, and TSCP/RL","[[  445  7407     6  8729  7407     6   480 13556   567     6    11     3
   4578  4184    87 12831     1]]"
936878cff0e6e327b2554ee5d46686797ee92cf2,100.0,No,[[  0 465   1]],No,[[465   1]]
16fa6896cf4597154363a6c9a98deb49fffef15f,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
a6950c22c7919f86b16384facc97f2cf66e5941d,0.0,"WN18N, FB15k",[[    0     3 21170  2606   567     3     6     3 15586  1808   157     1]],"INLINEFORM0 (SemEval 2014) contains reviews of the laptop domain and those of INLINEFORM1 (SemEval 2014), INLINEFORM2 (SemEval 2015) and INLINEFORM3 (SemEval 2016) are for the restaurant domain.","[[ 3388 20006 24030   632    41   134    15    51   427  2165  1412    61
   2579  2456    13     8  4544  3303    11   273    13  3388 20006 24030
    536    41   134    15    51   427  2165  1412   201  3388 20006 24030
    357    41   134    15    51   427  2165  1230    61    11  3388 20006
  24030   519    41   134    15    51   427  2165  1421    61    33    21
      8  2062  3303     5     1]]"
3d49b678ff6b125ffe7fb614af3e187da65c6f65,4.878,Their probabilistic correlation is explicitly guided by the model of QG training,"[[    0  2940  9551  3040 18712    19 21119 10995    57     8   825    13
   1593   517   761     1]]","The framework jointly learns parametrized QA and QG models subject to the constraint in equation 2. In more detail, they minimize QA and QG loss functions, with a third dual loss for regularization.","[[   37  4732 22801   669     7 30706   776    26     3 23008    11  1593
    517  2250  1426    12     8 27354    17    16 13850  1682    86    72
   2736     6    79 10558     3 23008    11  1593   517  1453  3621     6
     28     3     9  1025  7013  1453    21  1646  1707     5     1]]"
84aef81dae38e0dca0ad041141df60ab9ac29761,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
dfc393ba10ec4af5a17e5957fcbafdffdb1a6443,0.0,"BiLSTM, LSTM, LSTM, LSTM, LSTM","[[   0 2106 7600 2305    6    3 7600 2305    6    3 7600 2305    6    3
  7600 2305    6    3 7600 2305]]","BiMPM, ESIM, Decomposable Attention Model, KIM, BERT","[[ 2106  5244   329     6     3  3205  5166     6   374   287  2748   179
  20748  5154     6   480  5166     6   272 24203     1]]"
e84e80067b3343d136fd75300691c8b3d3efbdac,6.7797,by combining the bilingual data with the real bilingual data,"[[    0    57     3 13275     8 30521   331    28     8   490 30521   331
      1]]","By choosing English (En) as the pivot language, we perform pivot alignments for identical English segments on Europarl Fr-En and En-De parallel corpora BIBREF18 , constructing a multi-parallel corpus of Fr-En-De. Then each of the Fr*-De and Fr-De* pseudo parallel corpora is established from the multi-parallel data by applying the pivot language-based translation described in the previous section.","[[  938  4622  1566    41  8532    61    38     8 16959  1612     6    62
   1912 16959 14632     7    21 12022  1566 15107    30  5578    52    40
   6248    18  8532    11   695    18  2962  8449 11736   127     9     3
   5972 25582   371  2606     3     6     3 24246     3     9  1249    18
   1893 13701    40 11736   302    13  6248    18  8532    18  2962     5
     37    29   284    13     8  6248  1935    18  2962    11  6248    18
   2962  1935 22726  8449 11736   127     9    19  2127    45     8  1249
     18  1893 13701    40   331    57  6247     8 16959  1612    18   390
   7314  3028    16     8  1767  1375     5     1]]"
cb78e280e3340b786e81636431834b75824568c3,100.0,9,[[  0 668   1]],9,[[668   1]]
86a93a2d1c19cd0cd21ad1608f2a336240725700,100.0,interpretation of Frege's work are examples of holistic approaches to meaning,"[[    0  8868    13  5532   397    31     7   161    33  4062    13 16676
   6315    12  2530     1]]",interpretation of Frege's work are examples of holistic approaches to meaning,"[[ 8868    13  5532   397    31     7   161    33  4062    13 16676  6315
     12  2530     1]]"
d824f837d8bc17f399e9b8ce8b30795944df0d51,31.5789,"by comparing syntactic distance between two words, using the same lexical information","[[    0    57     3 14622  8953    17  2708   447  2357   344   192  1234
      6   338     8   337     3 30949   138   251]]",By visualizing syntactic distance estimated by the parsing network,"[[ 938 3176 2610 8953   17 2708  447 2357 5861   57    8  260    7   53
  1229    1]]"
e6c163f80a11bd057bbd0b6e1451ac82edddc78d,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
6cd874c4ae8e70f3c98c7176191c13a7decfbc45,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"BERT-ADA, BERT-PT, AEN-BERT, SDGCN-BERT","[[  272 24203    18 16759     6   272 24203    18  6383     6    71  5332
     18 12920   382     6   180 13535 10077    18 12920   382     1]]"
3cca26a9474d3b0d278e4dd57e24b227e7c2cd41,0.0,"WN16N, FB15K",[[    0     3 21170  2938   567     6     3 15586  1808   439     1]],"Brent corpus, PTB , Beijing University Corpus, Penn Chinese Treebank","[[21985 11736   302     6   276  9041     3     6 14465   636 10052   302
      6 11358  2830  7552  4739     1]]"
da845a2a930fd6a3267950bec5928205b6c6e8e8,9.5238,Lemmatization is done by a team of experts who used a multi-task,"[[    0   312   635   144  1707    19   612    57     3     9   372    13
   2273   113   261     3     9  1249    18 23615]]",how long it takes the system to lemmatize a set number of words,"[[ 149  307   34 1217    8  358   12   90  635  144 1737    3    9  356
   381   13 1234    1]]"
7b47aa6ba247874eaa8ab74d7cb6205251c01eb5,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
d5105a6a6d5d1931b0729dcf15ca862d6eac770f,100.0,62,[[   0    3 4056    1]],62,[[   3 4056    1]]
1d9b953a324fe0cfbe8e59dcff7a44a2f93c568d,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
0137ecebd84a03b224eb5ca51d189283abb5f6d9,0.0,"LSTM, LSTM+Attention, LSTM+B, ","[[   0    3 7600 2305    6    3 7600 2305 1220  188   17 9174    6    3
  7600 2305 1220  279    6    3]]","BERTNLU from ConvLab-2, a rule-based model (RuleDST) , TRADE (Transferable Dialogue State Generator) , a vanilla policy trained in a supervised fashion from ConvLab-2 (SL policy)","[[    3 12920 11053  9138    45  1193   208 18506  4949     6     3     9
   3356    18   390   825    41 17137   109   308  4209    61     3     6
  11466 20458    41 18474  1010   179  5267 10384  1015 27478    61     3
      6     3     9 13478  1291  4252    16     3     9     3 23313  2934
     45  1193   208 18506  4949    41  5629  1291    61     1]]"
33957fde72f9082a5c11844e7c47c58f8029c4ae,100.0,Freebase,[[    0  1443 10925     1]],Freebase,[[ 1443 10925     1]]
9cba2ee1f8e1560e48b3099d0d8cf6c854ddea2e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The system benefits from filters of each size., features of multigranular phrases are extracted with variable-size convolution filters.","[[   37   358  1393    45 10607    13   284   812     5     6   753    13
   1249  7662  4885 15101    33 21527    28  7660    18  7991   975 24817
  10607     5     1]]"
565d668947ffa6d52dad019af79289420505889b,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
317a6f211ecf48c58f008c12fbd5d41901db3e36,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
24c0f3d6170623385283dfda7f2b6ca2c7169238,100.0,Twitter API,[[   0 3046 6429    1]],Twitter API,[[3046 6429    1]]
41e300acec35252e23f239772cecadc0ea986071,100.0,multilingual neural machine translation models,[[    0  1249 25207 24228  1437  7314  2250     1]],Multilingual Neural Machine Translation Models,[[ 4908 25207  1484  9709  5879 24527  5154     7     1]]
7f5059b4b5e84b7705835887f02a51d4d016316a,100.0,No,[[  0 465   1]],No,[[465   1]]
c2da598346b74541c78ecff5c9586b3857dd01b5,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
f6380c60e2eb32cb3a9d3bca17cf4dc5ae584eca,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Training embeddings from small-corpora can increase the performance of some tasks,"[[ 4017 25078    26    53     7    45   422    18 14723   127     9    54
    993     8   821    13   128  4145     1]]"
90bc60320584ebba11af980ed92a309f0c1b5507,25.0,they use length information from the positional embeddings,"[[    0    79   169  2475   251    45     8  1102   138 25078    26    53
      7     1]]",They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative).,"[[  328  4277   126  6467  5307    32  7959     3    35  9886    84     3
  15262   251    81  1102  2284  1151  2475   251    41 14623    42  5237
    137     1]]"
a43c400ae37a8705ff2effb4828f4b0b177a74c4,50.0,character embeddings for taggers from different languages are shared,"[[    0  1848 25078    26    53     7    21  7860  1304     7    45   315
   8024    33  2471     1]]",shared character embeddings for taggers in both languages together through optimization of a joint loss function,"[[ 2471  1848 25078    26    53     7    21  7860  1304     7    16   321
   8024   544   190 11295    13     3     9  4494  1453  1681     1]]"
7081b6909cb87b58a7b85017a2278275be58bf60,100.0,210,[[    0     3 15239     1]],210,[[    3 15239     1]]
707db46938d16647bf4b6407b2da84b5c7ab4a81,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Simple Skip improves F1 from 74.34 to 74.81
Transformer Skip improes F1 from 74.34 to 74.95 ","[[ 9415 25378  1172     7   377   536    45     3  4581     5  3710    12
      3  4581     5  4959 31220 25378     3 16260    15     7   377   536
     45     3  4581     5  3710    12     3  4581     5  3301     1]]"
fb96c0cd777bb2961117feca19c6d41bfd8cfd42,0.0,debate websites related to the claim (disputed),[[    0  5054  3395  1341    12     8  1988    41 26986    61     1]],"idebate.com, debatewise.org, procon.org","[[    3  1599  3697    15     5   287     6  5054 10684     5  1677     6
    813  1018     5  1677     1]]"
4f12b41bd3bb2610abf7d7835291496aa69fb78c,12.5,"2domain>"" are used as the domain tags","[[    0     3     2   357 22999  3155   121    33   261    38     8  3303
  12391     1]]","Appending the domain tag “<2domain>"" to the source sentences of the respective corpora","[[ 2276  9303     8  3303  7860   105     2   357 22999  3155   121    12
      8  1391 16513    13     8  6477 11736   127     9     1]]"
515e10a71d78ccd9c7dc93cd942924a4c85d3a30,13.3333,by evaluating the accuracy of the model on the basis of the following criteria: (1) the accuracy,"[[    0    57     3 17768     8  7452    13     8   825    30     8  1873
     13     8   826  6683    10  5637     8  7452]]",perplexity of the models,[[ 399 9247  485   13    8 2250    1]]
8b99767620fd4efe51428b68841cc3ec06699280,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
47e6c3e6fcc9be8ca2437f41a4fef58ef4c02579,100.0,logistic regression model with character-level n-gram features,"[[    0  4303  3040 26625   825    28  1848    18  4563     3    29    18
   5096   753     1]]",Logistic regression model with character-level n-gram features,"[[ 7736  3040 26625   825    28  1848    18  4563     3    29    18  5096
    753     1]]"
9a9338d0e74fd315af643335e733445031bd7656,0.0,WSJ4 speech recognition dataset,[[    0     3  8439   683   591  5023  5786 17953     1]], AMI IHM meeting corpus,[[   71  7075    27 20611  1338 11736   302     1]]
58a00ca123d67b9be55021493384c0acef4c568d,7.4074,"we use answerable question as a prototype and its answer span as a plausible answer,","[[    0    62   169  1525   179   822    38     3     9 14402    11   165
   1525  8438    38     3     9 31323  1525     6]]","learn to ask unanswerable questions by editing answerable ones with word exchanges, negations, etc","[[  669    12   987    73  3247  3321   179   746    57  8414  1525   179
   2102    28  1448  2509     7     6 14261  1628     6   672     1]]"
8e630c5a4a8ba0a4f5d8c483a2bf09c4ac8020ce,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
ef4d6c9416e45301ea1a4d550b7c381f377cacd9,18.1818,"linguistic features, partly taken from the NLP literature and partly specifically designed for this task,","[[    0     3 24703   753     6 16509  1026    45     8   445  6892  6678
     11 16509  3346   876    21    48  2491     6]]","standard linguistic features, such as Part-Of-Speech (POS) and chunk tag, series of features representing tokens' left and right context","[[ 1068     3 24703   753     6   224    38  2733    18   667    89    18
    134   855 10217    41 16034    61    11 16749  7860     6   939    13
    753  9085 14145     7    31   646    11   269  2625     1]]"
a978a1ee73547ff3a80c66e6db3e6c3d3b6512f4,25.8065,2.9 points on the BLEU dataset and 0.9 points on the WS13 dataset,"[[    0     3 27297   979    30     8     3  8775 12062 17953    11     3
  23758   979    30     8     3  8439  2368 17953]]","0.08 points on the 2011 test set, 0.44 points on the 2012 test set, 0.42 points on the 2013 test set for IWSLT-CE.","[[4097 4018  979   30    8 2722  794  356    6 4097 3628  979   30    8
  1673  794  356    6 4097 4165  979   30    8 2038  794  356   21   27
  8439 9012   18 4770    5    1]]"
d95180d72d329a27ddf2fd5cc6919f469632a895,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
d8de12f5eff64d0e9c9e88f6ebdabc4cdf042c22,0.0,1.6 BLEU gains,[[    0     3 15062     3  8775 12062 11391     1]],0.8 points on Binary; 0.7 points on Fine-Grained; 0.6 points on Senti140; 0.7 points on Subj,"[[    3 22384   979    30 29221   117     3 22426   979    30 11456    18
    517 10761   117     3 22787   979    30  4892    17    23 22012   117
      3 22426   979    30  3325   354     1]]"
8c0621016e96d86a7063cb0c9ec20c76a2dba678,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
e9b1e8e575809f7b80b1125305cfa76ae4f5bdfb,0.0,LSTM with MH17 content classifier,[[    0     3  7600  2305    28     3 20131  2517   738   853  7903     1]], convolutional neural network (CNN) BIBREF29,"[[  975 24817   138 24228  1229    41   254 17235    61     3  5972 25582
    371  3166     1]]"
a88a454ac1a1230263166fd824e5daebb91cb05a,12.5,joint modeling across APIs and zero-shot generalization to new APIs,"[[    0  4494 15309   640  6429     7    11  5733    18 11159   879  1707
     12   126  6429     7     1]]",back translation between English and Chinese,[[ 223 7314  344 1566   11 2830    1]]
4266aacb575b4be7dbcdb8616766324f8790763c,0.0,CAEVO,[[   0 3087  427 8040    1]],"neural network-based models can outperform feature-based models with wide margins, contextualized representation learning can boost performance of NN models","[[24228  1229    18   390  2250    54    91   883  2032  1451    18   390
   2250    28  1148  6346     7     6 28131  1601  6497  1036    54  4888
    821    13     3 17235  2250     1]]"
53a0763eff99a8148585ac642705637874be69d4,2.8571,LSTM with a corresponding sequence of words,[[   0    3 7600 2305   28    3    9    3 9921 5932   13 1234    1]],"Active learning methods has a learning engine (mainly used for training of classification problems) and the selection engine (which chooses samples that need to be relabeled by annotators from unlabeled data). Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively.","[[11383  1036  2254    65     3     9  1036  1948    41  4894   261    21
    761    13 13774   982    61    11     8  1801  1948    41  3339   854
      7  5977    24   174    12    36  8318 10333    15    26    57    46
   2264  6230    45    73  9339   400    26   331   137    37    29     6
   8318 10333    15    26  5977    33   974    12   761   356    21   853
   7903    12     3    60    18  9719     6  2932 11721  4863     8  7452
     13     8   853  7903     5    86    48  1040     6   205  8556    18
    390  5508    49    11     3     9 10389   825    33  8152    38  1036
   1948    11  1801  1948     6  6898     5     1]]"
1ba28338d3f993674a19d2ee2ec35447e361505b,0.0,combination of drugs,[[   0 2711   13 4845    1]],"Chowdhury BIBREF14 and Thomas et al. BIBREF11, FBK-irst BIBREF10, Liu et al. BIBREF9, Sahu et al. BIBREF12","[[  205  4067    26 10666    63     3  5972 25582   371  2534    11  3576
      3    15    17   491     5     3  5972 25582   371  2596     6     3
  15586   439    18    23    52     7    17     3  5972 25582   371  1714
      6  1414    76     3    15    17   491     5     3  5972 25582   371
   1298     6  1138   107    76     3    15    17   491     5     3  5972
  25582   371  2122     1]]"
e1c681280b5667671c7f78b1579d0069cba72b0e,0.0,HEOT,[[   0    3 6021 6951    1]],"Ternary Trans-CNN , Hybrid multi-channel CNN and LSTM","[[ 9934    29  1208  4946    18   254 17235     3     6  5555  2160    26
   1249    18 19778 19602    11     3  7600  2305     1]]"
9e2e5918608a2911b341d4887f58a4595d7d1429,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
d151327c93b67928313f8fad8079a4ff9ef89314,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
5a23f436a7e0c33e4842425cf86d5fd8ba78ac92,0.0,"$150,000$ shares traded on the DAXXXXXXXXXX","[[    0  1970  9286  3229  4776 18107    30     8     3  4296     4     4
      4     4     4     4     4     4     4     4]]","553,451 documents",[[6897 6355 2128  536 2691    1]]
ecb680d79e847beb7c1aa590d288a7313908d64a,6.0606,"Using a comparison dataset, we obtain a BLEU score of 89.","[[    0     3  3626     3     9  4993 17953     6    62  3442     3     9
      3  8775 12062  2604    13     3  3914     5]]"," To test our proposed category induction model, we consider all BabelNet categories with fewer than 50 known instances. This is motivated by the view that conceptual neighborhood is mostly useful in cases where the number of known instances is small. For each of these categories, we split the set of known instances into 90% for training and 10% for testing.","[[  304   794    69  4382  3295    16  8291   825     6    62  1099    66
    272 10333  9688  5897    28     3 10643   145   943   801 10316     5
    100    19 11361    57     8   903    24 17428  5353    19  3323  1934
     16  1488   213     8   381    13   801 10316    19   422     5   242
    284    13   175  5897     6    62  5679     8   356    13   801 10316
    139 12669    21   761    11  6389    21  2505     5     1]]"
1a06b7a2097ebbad0afc787ea0756db6af3dadf4,0.0,"Uganda, Kenya, Tanzania, Pakistan, Myanmar, South Sudan, South Sudan, South Sudan,","[[    0 22093     6 12605     6 24677     6  6697     6 27274     6  1013
  21986     6  1013 21986     6  1013 21986     6]]","Bulgarian, Czech, French, German, Korean, Polish, Portuguese, Russian, Thai, Vietnamese","[[15536    29     6 16870     6  2379     6  2968     6  9677     6 16073
      6 21076     6  4263     6 12806     6 24532     1]]"
6a7370dd12682434248d006ffe0a72228c439693,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
7917d44e952b58ea066dc0b485d605c9a1fe3dda,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
bc473c5bd0e1a8be9b2037aa7006fd68217c3f47,22.8571,"selection of raters, use, creation of reference translations","[[    0  1801    13  1080    52     7     6   169 32128     6  3409    13
   2848  7314     7     1]]"," Choose professional translators as raters,  Evaluate documents, not sentences, Evaluate fluency in addition to adequacy, Do not heavily edit reference translations for fluency, Use original source texts","[[ 7023   771 22770     7    38  1080    52     7     6 17627 22156  2691
      6    59 16513     6 17627 22156  6720  4392    16   811    12     3
      9   221  2436  4710     6   531    59  8672  4777  2848  7314     7
     21  6720  4392     6  2048   926  1391 14877     1]]"
27b01883ed947b457d3bab0c66de26c0736e4f90,100.0,syllables,[[  0   3   7  63 195 179   7   1]],syllables,[[  3   7  63 195 179   7   1]]
04aff4add28e6343634d342db92b3ac36aa8c255,14.2857,"attention is distributed evenly among models, resulting in no gains at all.","[[    0  1388    19  8308 18756   859  2250     6     3  5490    16   150
  11391    44    66     5     1]]","visual attention is very sparse,  visual component of the attention hasn't learnt any variation over the source encodings","[[ 3176  1388    19   182 14144     7    15     6  3176  3876    13     8
   1388    65    29    31    17   669    17   136 12338   147     8  1391
      3    35  9886     7     1]]"
4b8257cdd9a60087fa901da1f4250e7d910896df,20.0,Correctness is defined as the percentage of words that are correct or missing from a sentence,"[[    0 28223   655    19  4802    38     8  5294    13  1234    24    33
   2024    42  3586    45     3     9  7142     1]]",typos in spellings or ungrammatical words,[[23042     7    16 19590     7    42    73  5096  4992   138  1234     1]]
412aff0b2113b7d61c914edf90b90f2994390088,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
c10f38ee97ed80484c1a70b8ebba9b1fb149bc91,100.0,SVMRank,[[    0   180 12623 22557     1]],SVMRank,[[  180 12623 22557     1]]
c431c142f5b82374746a2b2f18b40c6874f7131d,2.0833,WN16 and FB16k,[[    0     3 21170  2938    11     3 15586  2938   157     1]],"WMT18 EnDe bitext, WMT16 EnRo bitext, WMT15 EnFr bitext, We perform our experiments on WMT18 EnDe bitext, WMT16 EnRo bitext, and WMT15 EnFr bitext respectively. We use WMT Newscrawl for monolingual data (2007-2017 for De, 2016 for Ro, 2007-2013 for En, and 2007-2014 for Fr). For bitext, we filter out empty sentences and sentences longer than 250 subwords. We remove pairs whose whitespace-tokenized length ratio is greater than 2. This results in about 5.0M pairs for EnDe, and 0.6M pairs for EnRo. We do not filter the EnFr bitext, resulting in 41M sentence pairs.","[[  549  7323  2606   695  2962   720 10398     6   549  7323  2938   695
    448    32   720 10398     6   549  7323  1808   695   371    52   720
  10398     6   101  1912    69 12341    30   549  7323  2606   695  2962
    720 10398     6   549  7323  2938   695   448    32   720 10398     6
     11   549  7323  1808   695   371    52   720 10398  6898     5   101
    169   549  7323  3529  2935   210    40    21  7414 25207   331    41
  20615    18  9887    21   374     6  1421    21  2158     6  4101    18
  11138    21   695     6    11  4101    18 10218    21  6248   137   242
    720 10398     6    62  4191    91  6364 16513    11 16513  1200   145
   5986   769  6051     7     5   101  2036 14152     3  2544   872  6633
     18   235  2217  1601  2475  5688    19  2123   145  1682   100   772
     16    81     3 20734   329 14152    21   695  2962     6    11     3
  22787   329 14152    21   695   448    32     5   101   103    59  4191
      8   695   371    52   720 10398     6     3  5490    16  8798   329
   7142 14152     5     1]]"
50c8b821191339043306fd28e6cda2db400704f9,100.0,We collected Japanese fictional stories from the Web,[[    0   101  4759  4318 24460  1937    45     8  1620     1]],We collected Japanese fictional stories from the Web,[[  101  4759  4318 24460  1937    45     8  1620     1]]
9893c5f36f9d503678749cb0466eeaa0cfc9413f,100.0,five-character window context,[[    0   874    18 31886  2034  2625     1]],five-character window context,[[  874    18 31886  2034  2625     1]]
f399d5a8dbeec777a858f81dc4dd33a83ba341a2,84.2105,"QnAMaker Portal, QnaMaker Management APIs, Azure Search Index","[[    0  1593    29  4815     9  2304 16290     6  1593    29     9 22638
     52  2159  6429     7     6 21808  4769 11507]]","QnAMaker Portal, QnaMaker Management APIs, Azure Search Index, QnaMaker WebApp, Bot","[[ 1593    29  4815     9  2304 16290     6  1593    29     9 22638    52
   2159  6429     7     6 21808  4769 11507     6  1593    29     9 22638
     52  1620  9648     6 10091     1]]"
8cc56fc44136498471754186cfa04056017b4e54,9.8361,BLEU outperforms by 0.86 on relevance and 0.88 on average on relevance,"[[    0     3  8775 12062    91   883  2032     7    57  4097  3840    30
  20208    11  4097  4060    30  1348    30 20208]]","Under the retrieval evaluation setting, their proposed model + IR2 had better MRR than NVDM by 0.3769, better MR by 4.6, and better Recall@10 by  20 . 
Under the generative evaluation setting the proposed model + IR2 had better BLEU by 0.044 , better CIDEr by 0.033, better ROUGE by 0.032, and better METEOR by 0.029","[[ 3526     8 24515   138  5002  1898     6    70  4382   825  1768     3
   5705   357   141   394   283 12224   145     3 17058  7407    57  4097
   4118  3951     6   394     3  9320    57     3 25652     6    11   394
    419 16482  1741  1714    57   460     3     5  3526     8     3 25181
   5002  1898     8  4382   825  1768     3  5705   357   141   394     3
   8775 12062    57     3 11739  3628     3     6   394   205 13162    52
     57     3 11739  4201     6   394   391 26260   427    57     3 11739
   2668     6    11   394  7934  3463  2990    57     3 11739  3166     1]]"
f416c6818a7a8acb7ec4682ed424ecdbd7dd6df1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The multi-curricula learning scheme is scheduled according to the model's performance on the validation set, where the scheduling mechanism acts as the policy $\pi $ interacting with the dialogue model to acquire the learning status $s$. The reward of the multi-curricula learning mechanism $m_t$ indicates how well the current dialogue model performs.","[[   37  1249    18  3663  2234    83     9  1036  5336    19  5018  1315
     12     8   825    31     7   821    30     8 16148   356     6   213
      8 16131  8557  6775    38     8  1291  1514     2   102    23  1514
      3 23396    28     8  7478   825    12  7464     8  1036  2637  1514
      7  3229     5    37  9676    13     8  1249    18  3663  2234    83
      9  1036  8557  1514    51   834    17  3229  9379   149   168     8
    750  7478   825  1912     7     5     1]]"
c1ce652085ef9a7f02cb5c363ce2b8757adbe213,0.0,tourist information is collected from Wikipedia,[[    0  8548   251    19  4759    45 16885     1]],crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk),"[[ 4374    18 15551     8  1232    13     8 17953    30  2536 24483 23694
     41  7323   450   157    61     1]]"
47822fec590e840438a3054b7f512fec09dbd1e1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each","[[ 7491     7  3480 14985  5426  1234     6    11    79    33 18756  8308
    859   175   489  5349     7    28   335  2984   399   284     1]]"
d79d897f94e666d5a6fcda3b0c7e807c8fad109e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Average reward across 5 seeds show that NLP representations are robust to changes in the environment as well task-nuisances,"[[23836  9676   640   305  7299   504    24   445  6892  6497     7    33
   6268    12  1112    16     8  1164    38   168  2491    18 17965     7
    663     7     1]]"
5a29b1f9181f5809e2b0f97b4d0e00aea8996892,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],It takes into account the agreement between different systems,[[  94 1217  139  905    8 2791  344  315 1002    1]]
fa3312ae4bbed11a5bebd77caf15d651962e0b26,69.5652,F1 score of 82.11 on slot filling and 82.11 on intent,"[[   0  377  536 2604   13    3 4613    5 2596   30 4918   14   53   11
     3 4613    5 2596   30 9508]]",F1 scores of 86.16 on slot filling and 94.56 on intent detection,"[[  377   536  7586    13     3  3840     5  2938    30  4918    14    53
     11     3  4240     5  4834    30  9508 10664     1]]"
79bb1a1b71a1149e33e8b51ffdb83124c18f3e9c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Combined per-pixel accuracy for character line segments is 74.79,"[[    3 28257   399    18 14251  7452    21  1848   689 15107    19     3
   4581     5  4440     1]]"
320d72a9cd19b52c29dda9ddecd520c9938a717f,100.0,No,[[  0 465   1]],No,[[465   1]]
f63519bb5e116671cebd65cc78880c5cb573c570,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
d9eacd965bbdc468da522e5e6fe7491adc34b93b,0.0,LSTM with a corresponding LSTM with corresponding LSTM with ,"[[   0    3 7600 2305   28    3    9    3 9921    3 7600 2305   28    3
  9921    3 7600 2305   28    3]]","Support Vector Machines (SVM), Gaussian Naive Bayes, Multinomial Naive Bayes, Decision Trees, Random Forests and a Maximum Entropy classifier","[[ 4224 29011  5879     7    41   134 12623   201 12520     7 10488  1823
    757  2474    15     7     6  4908  3114    23   138  1823   757  2474
     15     7     6 25006  7552     7     6 25942  6944     7    11     3
      9 24210   695 12395    63   853  7903     1]]"
578d0b23cb983b445b1a256a34f969b34d332075,0.0,"BIBREF13, BIBREF21","[[    0     3  5972 25582   371  2368     6     3  5972 25582   371  2658
      1]]"," Arap-Tweet , UBC Twitter Gender Dataset, MADAR , LAMA-DINA , IDAT@FIRE2019, 15 datasets related to sentiment analysis of Arabic, including MSA and dialects","[[   71  5846    18   382  1123    15    17     3     6   412  7645  3046
    350  3868  2747  2244     6   283 16759   448     3     6   301 21250
     18   308 21116     3     6  4699  5767  1741   371 14132  8584     6
    627 17953     7  1341    12  6493  1693    13 19248     6   379  5266
    188    11 28461     7     1]]"
bbcd77aac74989f820e84488c52f3767d0405d51,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
9349acbfce95cb5d6b4d09ac626b55a9cb90e55e,0.0,citation context includes text spans in a citing paper describing a referenced,"[[    0     3 13903  2625   963  1499  8438     7    16     3     9     3
  17994  1040     3 16012     3     9  2848    26]]","Background, extends, uses, motivation, compare/contrast, and future work for the ACL-ARC dataset. Background, method, result comparison for the SciCite dataset.","[[23023     6  4285     7     6  2284     6  6620     6  4048    87  1018
     17 20484     6    11   647   161    21     8    71  8440    18 18971
  17953     5 23023     6  1573     6   741  4993    21     8 16021   254
    155    15 17953     5     1]]"
f5cf8738e8d211095bb89350ed05ee7f9997eb19,0.0,BLEU outperforms by a large margin,"[[    0     3  8775 12062    91   883  2032     7    57     3     9   508
   6346     1]]",up to four percentage points in accuracy,[[  95   12  662 5294  979   16 7452    1]]
d1ff6cba8c37e25ac6b261a25ea804d8e58e09c0,0.0,F1 score,[[   0  377  536 2604    1]],"F-score, Area Under the ROC Curve (AUC), mean accuracy (ACC), Precision vs Recall plot, ROC curve (which plots the True Positive Rate vs the False Positive Rate)","[[  377    18     7  9022     6  5690  3526     8     3 26893  4116   162
     41   188  6463   201  1243  7452    41 14775   201 28464     3   208
      7   419 16482  5944     6     3 26893  8435    41  3339  5944     7
      8 10998 24972 13002     3   208     7     8 10747     7    15 24972
  13002    61     1]]"
f7c34b128f8919e658ba4d5f1f3fc604fb7ff793,46.1538,"Textual inputs, database inputs, and semantic representations","[[    0  5027  3471  3785     7     6  3501  3785     7     6    11 27632
   6497     7     1]]","Textual inputs, knowledge bases, and images.",[[ 5027  3471  3785     7     6  1103 14701     6    11  1383     5     1]]
442f8da2c988530e62e4d1d52c6ec913e3ec5bf1,61.5385,"Cebuano and Mandarin, Mandarin and Mandarin, Chinese","[[    0  1064  3007   152    32    11 31057     6 31057    11 31057     6
   2830     1]]","Cebuano and Mandarin, Tagalog and Cantonese","[[ 1064  3007   152    32    11 31057     6  3284     9  2152    11  1072
   6948     7    15     1]]"
8bfbf78ea7fae0c0b8a510c9a8a49225bbdb5383,0.0,Yes,[[   0 2163    1]],"the task of detecting anglicisms can be approached as a sequence labeling problem where only certain spans of texts will be labeled as anglicism (in a similar way to an NER task). The chosen model was conditional random field model (CRF), which was also the most popular model in both Shared Tasks on Language Identification for Code-Switched Data","[[    8  2491    13     3 29782     3  1468    40 20231     7    54    36
  15319    38     3     9  5932  3783    53   682   213   163   824  8438
      7    13 14877    56    36  3783    15    26    38     3  1468    40
  20231    41    77     3     9  1126   194    12    46     3 18206  2491
    137    37  3934   825    47  1706   138  6504  1057   825    41  4545
    371   201    84    47    92     8   167  1012   825    16   321  7105
     26 16107     7    30 10509 31474    21  3636    18   134  7820  4513
   2747     1]]"
58f50397a075f128b45c6b824edb7a955ee8cba1,100.0,1,[[  0 209   1]],1,[[209   1]]
44c4bd6decc86f1091b5fc0728873d9324cdde4e,0.0,100 million sentences,[[    0   910   770 16513     1]],"7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus","[[12283 19568 14152    13   984   130 21527    45     8  4318  1620 11736
    302     6   305  3166 17246 14152    13   984   130 21527    45     8
     71  4184 11736   302     1]]"
5cd5864077e4074bed01e3a611b747a2180088a0,100.0,2000 images,[[   0 2766 1383    1]],2000 images,[[2766 1383    1]]
71f135be79341e61c28c3150b1822d0c4d0ca8d6,8.3333,"compared to baselines, our model achieves better performance on SF task.","[[    0     3  2172    12 20726     7     6    69   825  1984     7   394
    821    30     3  7016  2491     5     1]]"," improves the F1 score by almost 2%, which corresponds to a 12.3% error rate reduction","[[ 1172     7     8   377   536  2604    57   966     3  5406     6    84
  10423     7    12     3     9  8013  5170  3505  1080  4709     1]]"
3837ae1e91a4feb27f11ac3b14963e9a12f0c05e,0.0,biLSTM-networks,[[    0  2647  7600  2305    18  1582 13631     1]],"6)Contributor first names, 7)Contributor last names, 8)Contributor types (""soprano"", ""violinist"", etc.), 9)Classical work types (""symphony"", ""overture"", etc.), 10)Musical instruments, 11)Opus forms (""op"", ""opus""), 12)Work number forms (""no"", ""number""), 13)Work keys (""C"", ""D"", ""E"", ""F"" , ""G"" , ""A"", ""B"", ""flat"", ""sharp""), 14)Work Modes (""major"", ""minor"", ""m"")","[[    3 10938  4302  5135    17   127   166  3056     6     3 12703  4302
   5135    17   127   336  3056     6     3 13520  4302  5135    17   127
   1308 13308     7    32  5319    29    32  1686    96 11275    77   343
   1686   672     5   201     3 11728 21486  1950   161  1308 13308     7
     63    51  9621    63  1686    96  1890  2693  1686   672     5   201
      3 16968 29035   138  7778     6   850    61   667  7800  2807 13308
     32   102  1686    96    32  7800  8512     6   586    61 12492   381
   2807 13308    29    32  1686    96  5525  1152  8512     6  1179    61
  12492  9060 13308   254  1686    96   308  1686    96   427  1686    96
    371   121     3     6    96   517   121     3     6    96   188  1686
     96   279  1686    96 13710  1686    96     7  3272   102  8512     6
    968    61 12492  9258     7 13308 16547   127  1686    96  1109   127
   1686    96    51  8512     1]]"
58a340c338e41002c8555202ef9adbf51ddbb7a1,100.0,SST-2 dataset,[[    0   180  4209  4949 17953     1]],SST-2 dataset,[[  180  4209  4949 17953     1]]
2a6469f8f6bf16577b590732d30266fd2486a72e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"They use self-play learning , optimize the model for specific metrics, train separate models per user, use model  and response classification predictors, and filter the dataset to obtain higher quality training data.","[[  328   169  1044    18  4895  1036     3     6 13436     8   825    21
    806 15905     6  2412  2450  2250   399  1139     6   169   825    11
   1773 13774  9689   127     7     6    11  4191     8 17953    12  3442
   1146   463   761   331     5     1]]"
dd2f21d60cfca3917a9eb8b192c194f4de85e8b2,7.8431,"a closed-form solution, a reconstruction term, a KL term, and","[[    0     3     9  3168    18  2032  1127     6     3     9 20532  1657
      6     3     9   480   434  1657     6    11]]","interdependence between rate and distortion, impact of KL on the sharpness of the approximated posteriors, demonstrate how certain generative behaviours could be imposed on VAEs via a range of maximum channel capacities, some experiments to find if any form of syntactic information is encoded in the latent space","[[ 1413 18790  1433   344  1080    11 26095     6  1113    13   480   434
     30     8  4816   655    13     8 24672    26 29928     7     6  5970
    149   824     3 25181  7916     7   228    36     3 16068    30  9039
    427     7  1009     3     9   620    13  2411  4245 23875     6   128
  12341    12   253     3    99   136   607    13  8953    17  2708   447
    251    19 23734    26    16     8    50  4669   628     1]]"
c9c85eee41556c6993f40e428fa607af4abe80a9,66.6667,$sim $ 8.7M utterances,[[   0 1514    2    7  603 1514 4848  940  329    3 5108  663    7    1]],$\sim $ 8.7M annotated anonymised user utterances,"[[ 1514     2     7   603  1514  4848   940   329    46  2264   920 19581
   3375  1139     3  5108   663     7     1]]"
b43a8a0f4b8496b23c89730f0070172cd5dca06a,4.7619,LSTM with a linguistic component,[[    0     3  7600  2305    28     3     9     3 24703  3876     1]],"we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent.","[[   62  5148     3     9  1499  5932   769    18  1582  1981    28     3
      9 12938  6497   769    18  1582  1981    38  2008    16  7996 11376
   4386   371   755     3     5    37  1499  5932   769    18  1582  1981
      3  6848    13    46 25078    26    53  3760  2332  1601    28  2382
     18 11619  9840   553    15 25078    26    53     7     3  5972 25582
    371  1808  2348    57   192  8218 11619   975 24817  7500     6   258
      3     9  9858    18 13194    53  3760  2348    57     3     9 13809
   3760     5    37 12938  6497   769    18  1582  1981     3  6848    13
    192 13809  7500     5   101  6300   251    45   321   769    18  1582
  13631   190   975  2138    35   920     3 25412  1499  5932     7    11
  12938  6497     7    13  1389  1601  6741  1744  3040    86  1169   651
     11  4467     3 10628    41  8159 10038    61   753     3  5972 25582
    371  2938    21     8  1499    13   284   442    11   165  4208     5
      1]]"
e90ac9ee085dc2a9b6fe132245302bbce5f3f5ab,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
a7829abed2186f757a59d3da44893c0172c7012b,0.0,"Attention, Concatenation, Localized feature extraction, F1","[[    0 20748     6  1193  2138    35   257     6  4593  1601  1451 16629
      6   377   536     1]]","number of coattention blocks, the batch size, and the number of epochs trained and ensembled our three best networks","[[  381    13   576 25615  6438     6     8 11587   812     6    11     8
    381    13     3    15   102  6322     7  4252    11  8784    26    69
    386   200  5275     1]]"
5c059a13d59947f30877bed7d0180cca20a83284,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
9003c7041d3d2addabc2c112fa2c7efe5fab493c,100.0,"inappropriate, discriminating",[[    0 21001     6  9192  1014     1]],"inappropriate, discriminating",[[21001     6  9192  1014     1]]
0fd678d24c86122b9ab27b73ef20216bbd9847d1,0.0,F1 score,[[   0  377  536 2604    1]],Accuracy on each dataset and the average accuracy on all datasets.,"[[ 4292  3663  4710    30   284 17953    11     8  1348  7452    30    66
  17953     7     5     1]]"
5ba6f7f235d0f5d1d01fd97dd5e4d5b0544fd212,0.0,consistent large-scale evaluation,[[   0 4700  508   18 6649 5002    1]],"coverage metric, being distinct (cosine INLINEFORM0 0.7 or 0.8), belonging to the same class (cosine INLINEFORM1 0.7 or 0.8), being equivalent (cosine INLINEFORM2 0.85 or 0.95)","[[ 2591     3  7959     6   271  6746    41   509     7   630  3388 20006
  24030   632     3 22426    42     3 22384   201 12770    12     8   337
    853    41   509     7   630  3388 20006 24030   536     3 22426    42
      3 22384   201   271  7072    41   509     7   630  3388 20006 24030
    357  4097  4433    42     3 23758  9120     1]]"
9439430ff97c6e927d919860b1cb86a0dcff0038,100.0,10-fold cross validation,[[    0  9445 10533  2269 16148     1]],10-fold cross validation,[[ 9445 10533  2269 16148     1]]
e26e7e9bcd7e2cea561af596c59b98e823653a4b,61.5385,"telecommunication, electronics, and insurance",[[    0     3    17 18329     6 12800     6    11   958     1]]," four different companies in the telecommunication, electronics, and insurance industries","[[  662   315   688    16     8     3    17 18329     6 12800     6    11
    958  5238     1]]"
65d34041ffa4564385361979a08706b10b92ebc7,100.0,No,[[  0 465   1]],No,[[465   1]]
2d307b43746be9cedf897adac06d524419b0720b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Travel dataset contains 4100 raw samples, 11291 clauses, Hotel dataset contains 3825 raw samples, 11264 clauses, and the Mobile dataset contains 3483 raw samples and 8118 clauses","[[ 4983 17953  2579   314  2915  5902  5977     6   850   357  4729 14442
      7     6  2282 17953  2579  6654  1828  5902  5977     6   850 26755
  14442     7     6    11     8  4873 17953  2579  6154  4591  5902  5977
     11   505 20056 14442     7     1]]"
c58e60b99a6590e6b9a34de96c7606b004a4f169,0.0,semantic features,[[    0 27632   753     1]],"dependency relation between two words, word sense",[[27804  4689   344   192  1234     6  1448  1254     1]]
06c340c7c2ad57d7621c3e8baba6a3d0ed9f4696,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
79ed71a3505cf6f5e8bf121fd7ec1518cab55cae,21.4286,damaged neural modules exhibit the knowledge and lack expected of a Broca's apha,"[[    0  6780 24228 10561  6981     8  1103    11  2136  1644    13     3
      9  4027   658    31     7     3     9  6977]]","Damage to neural modules is done by randomly initializing their weights, causing the loss of all learned information.","[[26135    12 24228 10561    19   612    57 21306  2332  2610    70  1293
      7     6     3  5885     8  1453    13    66  2525   251     5     1]]"
327e6c6609fbd4c6ae76284ca639951f03eb4a4c,8.6957,"Compared to CN-Celeb, CN-Celeb achieved a","[[    0     3 25236    12     3 10077    18   254   400   115     6     3
  10077    18   254   400   115  5153     3     9]]","For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb","[[  242     3    23    18   162  5317   358     6  7357    33   209 18596
   2712 23447    12     3  1621   226    75   400   115     5   242     3
    226    18   162  5317   358     6  7357    33  5477   940  5988 23447
     12     3  1621   226    75   400   115     1]]"
536e4a39b654b78228bf55fd09d1b433e0dae447,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
e51d0c2c336f255e342b5f6c3cf2a13231789fed,0.0,Wikipedea Corpus,[[    0  2142  2168  3138    15     9 10052   302     1]],They collected tweets in Russian language using a heuristic query specific to Russian,"[[  328  4759 10657     7    16  4263  1612   338     3     9     3    88
    450  3040 11417   806    12  4263     1]]"
9eb5b336b3dcb7ab63f673ba9ab1818573cce6c3,22.2222,"4 million sentences, 8 million sentences extracted from Wikipedia",[[    0   314   770 16513     6   505   770 16513 21527    45 16885     1]],"1.1 million sentences, 119 different relation types (unique predicates)","[[    3 11039   770 16513     6     3 19993   315  4689  1308    41   202
   1495   554 11346  1422    61     1]]"
89e1e0dc5d15a05f8740f471e1cb3ddd296b8942,100.0,the punchline of the joke,[[    0     8 11462   747    13     8 10802     1]],the punchline of the joke ,[[    8 11462   747    13     8 10802     1]]
6e97c06f998f09256be752fa75c24ba853b0db24,100.0,Accuracy across six datasets,[[    0  4292  3663  4710   640  1296 17953     7     1]],Accuracy across six datasets,[[ 4292  3663  4710   640  1296 17953     7     1]]
bcce5eef9ddc345177b3c39c469b4f8934700f80,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
7c794fa0b2818d354ca666969107818a2ffdda0c,0.0,entity tagging,[[    0 10409     3    17 15242     1]],"We also report the metrics in BIBREF7 for consistency, we report the span F1,  Exact Match (EM) accuracy of the entire sequence of labels, metric that combines intent and entities","[[  101    92   934     8 15905    16     3  5972 25582   371   940    21
  12866     6    62   934     8  8438   377  4347  1881  2708 12296    41
   6037    61  7452    13     8  1297  5932    13 11241     6     3  7959
     24     3 15256  9508    11 12311     1]]"
fc8bc6a3c837a9d1c869b7ee90cf4e3c39bcd102,0.0,No,[[  0 465   1]],There were hierarchical and non-hierarchical baselines; BERT was one of those baselines,"[[  290   130  1382  7064  1950    11   529    18 18098  7064  1950 20726
      7   117   272 24203    47    80    13   273 20726     7     1]]"
7883a52f008f3c4aabfc9f71ce05d7c4107e79bb,100.0,No,[[  0 465   1]],No,[[465   1]]
b7708cbb50085eb41e306bd2248f1515a5ebada8,20.0,formal languages,[[   0 4727 8024    1]],These are well-known formal languages some of which was used in the literature to evaluate the learning capabilities of RNNs.,"[[  506    33   168    18  5661  4727  8024   128    13    84    47   261
     16     8  6678    12  6825     8  1036  5644    13   391 17235     7
      5     1]]"
234ccc1afcae4890e618ff2a7b06fc1e513ea640,7.5472,"Compared to baseline, the improvement was 0.8 points.","[[    0     3 25236    12 20726     6     8  4179    47     3 22384   979
      5     1]]","Data augmentation (es)  improved Adv es by 20% comparing to baseline 
Data augmentation (cs) improved Adv cs by 16.5% comparing to baseline
Data augmentation (cs+es) improved both Adv cs and Adv es by at least 10% comparing to baseline 
All models show improvements over adversarial sets  
","[[ 2747     3 19260    41    15     7    61  3798     3 21021     3    15
      7    57  7580     3 14622    12 20726  2747     3 19260    41    75
      7    61  3798     3 21021     3    75     7    57 10128  2712     3
  14622    12 20726  2747     3 19260    41    75     7  1220    15     7
     61  3798   321     3 21021     3    75     7    11     3 21021     3
     15     7    57    44   709  6389     3 14622    12 20726   432  2250
    504  6867   147 23210    23   138  3369     1]]"
29772ba04886bee2d26b7320e1c6d9b156078891,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
42be49b883eba268e3dbc5c3ff4631442657dcbb,36.8421,The dependency parsing results are verified on two benchmark treebanks.,"[[    0    37 27804   260     7    53   772    33 17087    30   192 15705
   2195  4739     7     5     1]]"," At last, our model is evaluated on two benchmark treebanks for both constituent and dependency parsing. The empirical results show that our parser reaches new state-of-the-art for all parsing tasks.","[[  486   336     6    69   825    19 14434    30   192 15705  2195  4739
      7    21   321 17429    11 27804   260     7    53     5    37 23941
    772   504    24    69   260     7    49     3 12763   126   538    18
    858    18   532    18  1408    21    66   260     7    53  4145     5
      1]]"
7ac0cec79c8c2b1909b0a1cc0d4646fce09884ee,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
c97a4a1c0e3d00137a9ae8d6fbb809ba6492991d,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
b8b588ca1e876b3094ae561a875dd949c8965b2e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue,"[[  150  2045  1068    21  3269     3 17768   192    41   127    72    61
   7478  1002   116  4014     8  5044    13     8   936    11     8  6720
   4392    13     8  6126  7478     1]]"
9aca4b89e18ce659c905eccc78eda76af9f0072a,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
be595b2017545b0359db6abf4914a155bdd10d23,15.7895,manually identified gang member profiles,[[    0 12616  4313     3  3810  1144 10958     1]]," text in their tweets and profile descriptions, their emoji use, their profile images, and music interests embodied by links to YouTube music videos, can help a classifier distinguish between gang and non-gang member profiles","[[ 1499    16    70 10657     7    11  3278 15293     6    70     3    15
     51 21892   169     6    70  3278  1383     6    11   723  3984     3
  24973    26    57  2416    12  5343   723  3075     6    54   199     3
      9   853  7903 15849   344     3  3810    11   529    18  3810  1144
  10958     1]]"
df934aa1db09c14b3bf4bc617491264e2192390b,12.5,automatic translator with Moses,[[    0  6569 22770    28 20064     1]],2-layer LSTM model with 500 hidden units in both encoder and decoder,"[[ 8401 18270     3  7600  2305   825    28  2899  5697  3173    16   321
  23734    52    11    20  4978    52     1]]"
0c78d2fe8bc5491b5fd8a2166190c59eba069ced,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
3fb4334e5a4702acd44bd24eb1831bb7e9b98d31,22.2222,two Chinese reading comprehension datasets CMRC 2018 BIBREF8 and DRCD ,"[[    0   192  2830  1183 27160 17953     7   205 27396   846     3  5972
  25582   371   927    11     3  3913  6931     3]]","Evaluation datasets used:
CMRC 2018 - 18939 questions, 10 answers
DRCD - 33953 questions, 5 answers
NIST MT02/03/04/05/06/08 Chinese-English - Not specified

Source language train data:
SQuAD - Not specified","[[22714 17953     7   261    10   205 27396   846     3    18     3 25312
   3288   746     6   335  4269     3  3913  6931     3    18   220  3288
   4867   746     6   305  4269   445 13582     3  7323  4305 31064  6348
  31911  5176    87  4018  2830    18 26749     3    18   933  7173  9149
   1612  2412   331    10   180  5991  6762     3    18   933  7173     1]]"
ebb7313eee2ea447abc83cb08b658b57c7eaa600,100.0,automatic translator with Moses,[[    0  6569 22770    28 20064     1]],automatic translator with Moses,[[ 6569 22770    28 20064     1]]
0b24b5a652d674d4694668d889643bc1accf18ef,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
736c74d2f61ac8d3ac31c45c6510a36c767a5d6d,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
35b10e0dc2cb4a1a31d5692032dc3fbda933bf7d,0.0,LSTM with a corresponding entailment model,"[[   0    3 7600 2305   28    3    9    3 9921    3   35 5756  297  825
     1]]",ensemble of hand-crafted syntactic and frame-semantic features BIBREF16,"[[ 8784    13   609    18  8810  8953    17  2708   447    11  2835    18
      7    15   348  1225   753     3  5972 25582   371  2938     1]]"
a725246bac4625e6fe99ea236a96ccb21b5f30c6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Amazon Conversational Bot Toolkit, natural language understanding (NLU) (nlu) module, dialog manager, knowledge bases, natural language generation (NLG) (nlg) module, text to speech (TTS) (tts)","[[ 2536 28941   138 10091 11640  9229     6   793  1612  1705    41   567
   9138    61    41    29    40    76    61  6008     6 13463  2743     6
   1103 14701     6   793  1612  3381    41 18207   517    61    41    29
     40   122    61  6008     6  1499    12  5023    41  9697   134    61
     41    17    17     7    61     1]]"
dc57ae854d78aa5d5e8c979826d3e2524d4e9165,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
2efdcebebeb970021233553104553205ce5d6567,100.0,two LSTM layers,[[   0  192    3 7600 2305 7500    1]],two LSTM layers,[[ 192    3 7600 2305 7500    1]]
6c0f97807cd83a94a4d26040286c6f89c4a0f8e0,13.3333,"representation of TDs as vectors in a vector space, represented in a vector","[[    0  6497    13     3 10494     7    38 12938     7    16     3     9
  12938   628     6  7283    16     3     9 12938]]",finite sequence of terms,[[ 361 7980 5932   13 1353    1]]
ec2b8c43f14227cf74f9b49573cceb137dd336e7,75.0,speech recognition system is evaluated using 6 indicators,[[    0  5023  5786   358    19 14434   338   431 15600     1]],Speech recognition system is evaluated using WER metric.,[[26351  5786   358    19 14434   338   549  3316     3  7959     5     1]]
dbfce07613e6d0d7412165e14438d5f92ad4b004,22.2222,"affective features such as recurrent recurrent lag, recurrent lag","[[    0  2603   757   753   224    38     3    60 14907     3    60 14907
      3  5430     6     3    60 14907     3  5430]]","affective features provided by different emotion models such as Emolex, EmoSenticNet, Dictionary of Affect in Language, Affective Norms for English Words and Linguistics Inquiry and Word Count","[[ 2603   757   753   937    57   315 13868  2250   224    38   262  4641
    994     6  3967    32   134    35  1225  9688     6 28767    13    71
     89  4075    16 10509     6    71    89  4075   757     3 19383     7
     21  1566  4467     7    11  6741  1744  3040     7    86  1169   651
     11  4467     3 10628     1]]"
094ce2f912aa3ced9eb97b171745d38f58f946dd,0.0,email,[[  0 791   1]],"The Online Retail Data Set consists of a clean list of 25873 invoices, totaling 541909 rows and 8 columns.","[[   37  1777 14530  2747  2821     3  6848    13     3     9  1349   570
     13   944  4225   519 10921     7     6   792    53 10630  2294  4198
  17918    11   505 15752     5     1]]"
3582fac4b2705db056f75a14949db7b80cbc3197,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
1e59263f7aa7dd5acb53c8749f627cf68683adee,100.0,No,[[  0 465   1]],No,[[465   1]]
d05d667822cb49cefd03c24a97721f1fe9dc0f4c,5.5556,they obtain relations between mentions using a combination of a supervised and unsupervised approach,"[[    0    79  3442  5836   344  2652     7   338     3     9  2711    13
      3     9     3 23313    11    73 23313  1295]]","Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain.","[[  282  6732     3     9   701    12     8  4689     3   390    30   823
   2652     7  4093    16     8   337  1708     6     3    99  2652     7
     33 12022     6    42     3    99  2652     7    33    16     8   337
   2583 11788  3741     5     1]]"
b573b36936ffdf1d70e66f9b5567511c989b46b2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],unshuffled version of the French OSCAR corpus,"[[   73 14279 10105    26   988    13     8  2379  6328   254  4280 11736
    302     1]]"
b37fd665dfa5fad43977069d5623f4490a979305,36.3636,SC-LSTM BIBREF3,[[    0  6508    18  7600  2305     3  5972 25582   371   519     1]],"$({1})$ SC-LSTM BIBREF3, $({2})$ GPT-2 BIBREF6 , $({3})$ HDSA BIBREF7","[[ 1514   599     2   536     2    61  3229  6508    18  7600  2305     3
   5972 25582   371  6355  1514   599     2   357     2    61  3229   350
   6383  4949     3  5972 25582   371   948     3     6  1514   599     2
    519     2    61  3229  3726  4507     3  5972 25582   371   940     1]]"
b2c8c90041064183159cc825847c142b1309a849,100.0,No,[[  0 465   1]],No,[[465   1]]
e6c872fea474ea96ca2553f7e9d5875df4ef55cd,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5871d258f66b00fb716065086f757ef745645bfe,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
f80d89fb905b3e7e17af1fe179b6f441405ad79b,100.0,No,[[  0 465   1]],No,[[465   1]]
77f04cd553df691e8f4ecbe19da89bc32c7ac734,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
f7070b2e258beac9b09514be2bfcc5a528cc3a0e,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
a3a871ca2417b2ada9df1438d282c45e4b4ad668,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Table TABREF20 , Table TABREF22, Table TABREF23","[[ 4398     3  3221 25582   371  1755     3     6  4398     3  3221 25582
    371  2884     6  4398     3  3221 25582   371  2773     1]]"
e8ca81d5b36952259ef3e0dbeac7b3a622eabe8e,100.0,IEMOCAP,[[    0     3  5091  5365 16986     1]],IEMOCAP,[[    3  5091  5365 16986     1]]
770aeff30846cd3d0d5963f527691f3685e8af02,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
16f71391335a5d574f01235a9c37631893cd3bb0,15.3846,"Compared with the state-of-the-art methods, the proposed method achieves ","[[    0     3 25236    28     8   538    18   858    18   532    18  1408
   2254     6     8  4382  1573  1984     7     3]]","Across 4 datasets, the best performing proposed model (CNN) achieved an average of 363% improvement over the state of the art method (LR-CNN)","[[    3 25087   314 17953     7     6     8   200  5505  4382   825    41
    254 17235    61  5153    46  1348    13  4475  5170  4179   147     8
    538    13     8   768  1573    41 12564    18   254 17235    61     1]]"
b69ffec1c607bfe5aa4d39254e0770a3433a191b,100.0,Chinese dataset BIBREF0,[[    0  2830 17953     3  5972 25582   371   632     1]],Chinese dataset BIBREF0,[[ 2830 17953     3  5972 25582   371   632     1]]
9efd025cfa69c6ff2777528bd158f79ead9353d1,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5f9bd99a598a4bbeb9d2ac46082bd3302e961a0f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],They evaluate F1 score and agent's test performance on their own built interactive datasets (iSQuAD and iNewsQA),"[[  328  6825   377   536  2604    11  3102    31     7   794   821    30
     70   293  1192  6076 17953     7    41    23   134  5991  6762    11
      3    23  6861     7 23008    61     1]]"
73abb173a3cc973ab229511cf53b426865a2738b,9.0909,"conventional methods, a slot-filling model",[[    0  7450  2254     6     3     9  4918    18 16326    53   825     1]],"a deep neural network (DNN) architecture proposed in BIBREF24 , maximum entropy (MaxEnt) proposed in BIBREF23 type of discriminative model","[[    3     9  1659 24228  1229    41   308 17235    61  4648  4382    16
      3  5972 25582   371  2266     3     6  2411     3    35 12395    63
     41 21298 16924    61  4382    16     3  5972 25582   371  2773   686
     13  9192  1528   825     1]]"
64c7545ce349265e0c97fd6c434a5f8efdc23777,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization","[[  312   635   144  1707    19   612    57    46  2205 19248     3   697
     76   343   213 19590 11698     7    33  7027     6    11    90    51
   2754    33   937    28   423  1227     9 12563  1707     1]]"
4e4d377b140c149338446ba69737ea191c4328d9,100.0,ACL Anthology Reference Corpus,[[    0    71  8440   389   189  1863 17881 10052   302     1]],ACL Anthology Reference Corpus,[[   71  8440   389   189  1863 17881 10052   302     1]]
33d2919f3400cd3c6fbb6960d74187ec80b41cd6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],The selection model selects the best answer from the set $\lbrace a_i\rbrace _{i=1}^N$ observed during the interaction by predicting the difference of the F1 score to the average F1 of all variants.,"[[   37  1801   825  1738     7     8   200  1525    45     8   356  1514
      2    40  1939   565     3     9   834    23     2    52  1939   565
      3   834     2    23  2423   536     2   567  3229  6970   383     8
   6565    57     3 29856     8  1750    13     8   377   536  2604    12
      8  1348   377   536    13    66  6826     7     5     1]]"
61b0db2b5718d409b07f83f912bad6a788bfee5a,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
dc5ff2adbe1a504122e3800c9ca1d348de391c94,5.3571,They evaluate the word-prediction representations using quality and consistency metrics and then compare them to,"[[    0   328  6825     8  1448    18  2026 12472  6497     7   338   463
     11 12866 15905    11   258  4048   135    12]]","The unsupervised tasks include five tasks from SemEval Semantic Textual Similarity (STS) in 2012-2016 BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 and the SemEval2014 Semantic Relatedness task (SICK-R) BIBREF35 .

The cosine similarity between vector representations of two sentences determines the textual similarity of two sentences, and the performance is reported in Pearson's correlation score between human-annotated labels and the model predictions on each dataset., Supervised Evaluation
It includes Semantic relatedness (SICK) BIBREF35 , SemEval (STS-B) BIBREF36 , paraphrase detection (MRPC) BIBREF37 , question-type classification (TREC) BIBREF38 , movie review sentiment (MR) BIBREF39 , Stanford Sentiment Treebank (SST) BIBREF40 , customer product reviews (CR) BIBREF41 , subjectivity/objectivity classification (SUBJ) BIBREF42 , opinion polarity (MPQA) BIBREF43 .","[[   37    73 23313  4145   560   874  4145    45   679    51   427  2165
    679   348  1225  5027  3471 18347   485    41  4209   134    61    16
   1673    18 11505     3  5972 25582   371  1458     3     6     3  5972
  25582   371  3341     3     6     3  5972 25582   371  2668     3     6
      3  5972 25582   371  4201     3     6     3  5972 25582   371  3710
     11     8   679    51   427  2165 10218   679   348  1225 16559   655
   2491    41   134 25286    18   448    61     3  5972 25582   371  2469
      3     5    37   576     7   630  1126   485   344 12938  6497     7
     13   192 16513  2082     7     8  1499  3471  1126   485    13   192
  16513     6    11     8   821    19  2196    16 29300    31     7 18712
   2604   344   936    18   152  2264   920 11241    11     8   825 20099
     30   284 17953     5     6  2011   208  3375 22714    94   963   679
    348  1225  1341   655    41   134 25286    61     3  5972 25582   371
   2469     3     6   679    51   427  2165    41  4209   134    18   279
     61     3  5972 25582   371  3420     3     6  3856 27111 10664    41
   9320  4051    61     3  5972 25582   371  4118     3     6   822    18
   6137 13774    41 20371   254    61     3  5972 25582   371  3747     3
      6  1974  1132  6493    41  9320    61     3  5972 25582   371  3288
      3     6 19796  4892  2998   295  7552  4739    41   134  4209    61
      3  5972 25582   371  2445     3     6   884   556  2456    41  4545
     61     3  5972 25582   371  4853     3     6  1426 10696    87 30536
  10696 13774    41  4138   279   683    61     3  5972 25582   371  4165
      3     6  3474     3  9618   485    41  5244 23008    61     3  5972
  25582   371  4906     3     5     1]]"
8829f738bcdf05b615072724223dbd82463e5de6,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
4a2248e1c71c0b0183ab0d225440cae2da396b8d,20.0,From Japanese Wikipedia,[[    0  1029  4318 16885     1]],Evaluation Dataset of Japanese Lexical Simplification kodaira,"[[22714  2747  2244    13  4318 17546  1950   925 27717     3   157    32
     26  2256     9     1]]"
b653f55d1dad5cd262a99502f63bf44c58ccc8cf,0.0,Pearson's correlation BIBREF0,[[    0 29300    31     7 18712     3  5972 25582   371   632     1]],Fisher Corpus English Part 1,[[14639 10052   302  1566  2733   209     1]]
0c09a0e8f9c5bdb678563be49f912ab6e3f97619,100.0,12,[[  0 586   1]],12,[[586   1]]
eacd7e540cc34cb45770fcba463f4bf968681d59,100.0,No,[[  0 465   1]],No,[[465   1]]
cf74ff49dfcdda2cd67a896b4b982a1c3ee51531,6.4516,"In the developing world, the Caucasus, the Azores and the Far East","[[    0    86     8  2421   296     6     8 16371  6769   302     6     8
  12611 14846    11     8  5186  1932     1]]","Nigeria, Benin, Ghana, Cameroon, Togo, Côte d'Ivoire, Chad, Burkina Faso, and Sudan, Republic of Togo, Ghana, Côte d'Ivoire, Sierra Leone, Cuba and Brazil","[[ 7904     6  2798    77     6 18406     6  5184    49    32   106     6
    304   839     6   205 10470    15     3    26    31   196 20106     6
   3643    26     6  4152  2917     9  1699     7    32     6    11 21986
      6  5750    13   304   839     6 18406     6   205 10470    15     3
     26    31   196 20106     6 17171 13918    15     6 13052    11  9278
      1]]"
65b579b2c62982e2ff154c8160288c2950d509f2,0.0,"sentiment analysis, inference",[[    0  6493  1693     6    16 11788     1]],"MRPC, STS-B, SST-2, QQP, RTE, QNLI, MNLI","[[    3  9320  4051     6  5097   134    18   279     6   180  4209  4949
      6  1593  2247   345     6   391  3463     6  1593 18207   196     6
    283 18207   196     1]]"
76c8aac84152fc4bbc0d5faa7b46e40438353e77,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
016b59daa84269a93ce821070f4f5c1a71752a8a,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
69b41524dc5820143e45f2f3545cd5c0a70e2922,14.2857,"naive Bayes, naive Dirichlet, TF-","[[   0    3   29    9  757 2474   15    7    6    3   29    9  757 7454
   362 1655    6    3 9164   18]]","SumBasic, Lsa, LexRank, TextRank, Bayes, Hmm, MaxEnt, NeuralSum, Lead-N","[[12198 14885   447     6   301     7     9     6 17546 22557     6  5027
  22557     6  2474    15     7     6   454   635     6  5370 16924     6
   1484  9709   134   440     6 12208    18   567     1]]"
368317b4fd049511e00b441c2e9550ded6607c37,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
6d0f2cce46bc962c6527f7b4a77721799f2455c6,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
99d7bef0ef395360b939a3f446eff67239551a9d,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
dc1fe3359faa2d7daa891c1df33df85558bc461b,100.0,No,[[  0 465   1]],No,[[465   1]]
b686e10a725254695821e330a277c900792db69f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]]," represent each word with an expressive multimodal distribution, for multiple distinct meanings, entailment, heavy tailed uncertainty, and enhanced interpretability. For example, one mode of the word `bank' could overlap with distributions for words such as `finance' and `money', and another mode could overlap with the distributions for `river' and `creek'.","[[ 4221   284  1448    28    46 27821  1249 20226  3438     6    21  1317
   6746  2530     7     6     3    35  5756   297     6  2437     3    17
  10990 14068     6    11  8358  7280  2020     5   242   677     6    80
   2175    13     8  1448     3     2  4739    31   228 21655    28  3438
      7    21  1234   224    38     3     2    89    77   663    31    11
      3     2 28442    31     6    11   430  2175   228 21655    28     8
   3438     7    21     3     2  5927    49    31    11     3     2  5045
     15   157    31     5     1]]"
957bda6b421ef7d2839c3cec083404ac77721f14,16.6667,"capitalisation, spelling errors, etc",[[    0  1784  2121     6 19590  6854     6   672     1]],"LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors, Spelling errors, Repeated characters, Capitalization, Length, Emoticon (Presence/Count), Sentiment Ratio.","[[  301  4296    73    23  5096     7    41 10572     7  1433    87 10628
    201     3 16034  6455    23    32     6  1713 23954    26  4443   485
   3137 10872     6  1713   308   159 19221  7878   127     7     6  8974
  12013  6854     6 20469    15    26  2850     6  5826  1707     6   312
   1725   189     6  3967  9798   106    41 10572     7  1433    87 10628
    201  4892  2998   295  6455    23    32     5     1]]"
c13fe4064df0cfebd0538f29cb13e917fc5c3be0,20.5128,a multi-task Recurrent Neural Network (RNN),"[[    0     3     9  1249    18 23615   419 14907  1484  9709  3426    41
  14151   567    61     1]]","The network architecture has a multi-task Bi-Directional Recurrent Neural Network, with an unsupervised sequence labeling task and a low-dimensional embedding layer between tasks. There is a hidden layer after each successive task with skip connections to the senior supervised layers.","[[   37  1229  4648    65     3     9  1249    18 23615  2106    18 23620
   6318   419 14907  1484  9709  3426     6    28    46    73 23313  5932
   3783    53  2491    11     3     9   731    18 11619 25078    26    53
   3760   344  4145     5   290    19     3     9  5697  3760   227   284
  25694  2491    28 11202  5992    12     8  2991     3 23313  7500     5
      1]]"
55588ae77496e7753bff18763a21ca07d9f93240,8.0,"a dialect that consists of two distinct parts, one for a male and one for","[[    0     3     9 28461    24     3  6848    13   192  6746  1467     6
     80    21     3     9  5069    11    80    21]]",It uses particular forms of a concept rather than all of them uniformly,"[[  94 2284 1090 2807   13    3    9 2077 1066  145   66   13  135 7117
   120    1]]"
9c2f306044b3d1b3b7fdd05d1c046e887796dd7a,0.0,"BIBREF16, BIBREF23, BIBREF24","[[    0     3  5972 25582   371  2938     6     3  5972 25582   371  2773
      6     3  5972 25582   371  2266     1]]","SemEval 2016 Task 6 BIBREF7, Stance Sentiment Emotion Corpus (SSEC) BIBREF15","[[  679    51   427  2165  1421 16107   431     3  5972 25582   371   940
      6   472   663  4892  2998   295   262  7259 10052   302    41  4256
   3073    61     3  5972 25582   371  1808     1]]"
4f0f446bf4518af7f539f6283145135192d5c00b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Logistic Regression (LR), Random Forest (RF), Support Vector Machines (SVM)","[[ 7736  3040   419 22430    41 12564   201 25942  6944    41  8556   201
   4224 29011  5879     7    41   134 12623    61     1]]"
b5c3787ab3784214fc35f230ac4926fe184d86ba,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
2cf8825639164a842c3172af039ff079a8448592,12.5,Randomly from Twitter,[[    0 25942   120    45  3046     1]],The data are self-reported by Twitter users and then verified by two human experts.,"[[   37   331    33  1044    18    60 16262    57  3046  1105    11   258
  17087    57   192   936  2273     5     1]]"
e1f559da7fa501d3190073bca9ce4d4a12149e80,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
0fd7d12711dfe0e35467a7ee6525127378a1bacb,100.0,listening comprehension task,[[    0  5351 27160  2491     1]], listening comprehension task ,[[ 5351 27160  2491     1]]
871c34219eb623bde9ac3937aa0f28fc3ad69445,7.1429,Bi-LSTMs with a maximum error rate of 0.88,"[[   0 2106   18 7600 2305    7   28    3    9 2411 3505 1080   13 4097
  4060    1]]","character unit the RNN-transducer with additional attention module, For subword units, classic RNN-transducer, RNN-transducer with attention and joint CTC-attention show comparable performance","[[ 1848  1745     8   391 17235    18  7031  4817    49    28  1151  1388
   6008     6   242   769  6051  3173     6  2431   391 17235    18  7031
   4817    49     6   391 17235    18  7031  4817    49    28  1388    11
   4494   205  3838    18 25615   504 13289   821     1]]"
fd08dc218effecbe5137a7e3b73d9e5e37ace9c1,100.0,No,[[  0 465   1]],No,[[465   1]]
f0317e48dafe117829e88e54ed2edab24b86edb1,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"if the attention loose track of the objects in the picture and ""gets lost"", the model still takes it into account and somehow overrides the information brought by the text-based annotations","[[    3    99     8  1388  6044  1463    13     8  4820    16     8  1554
     11    96  2782     7  1513  1686     8   825   341  1217    34   139
    905    11  9296   147    52  9361     8   251  1940    57     8  1499
     18   390 30729     7     1]]"
1e68a1232ab09b6bff506e442acc8ad742972102,37.5,"text-transformations to the messages, text-based classifiers, multilingual text","[[    0  1499    18  7031 14678     7    12     8  4175     6  1499    18
    390   853  7903     7     6  1249 25207  1499]]","text-transformations to the messages, vector space model, Support Vector Machine","[[ 1499    18  7031 14678     7    12     8  4175     6 12938   628   825
      6  4224 29011  5879     1]]"
d77c9ede2727c28e0b5a240b2521fd49a19442e0,100.0,word embeddings,[[    0  1448 25078    26    53     7     1]],word embeddings,[[ 1448 25078    26    53     7     1]]
b03e8e9a0cd2a44a215082773c7338f2f3be412a,0.0,LSTMs,[[   0    3 7600 2305    7    1]],"a two layer recurrent neural language model with GRU cells of hidden size 512, a two layer neural sequence to sequence model equipped with bi-linear attention function with GRU cells of hidden size 512, a linear dynamical system, semi-supervised SLDS models with varying amount of labelled sentiment tags","[[    3     9   192  3760     3    60 14907 24228  1612   825    28   350
   8503  2640    13  5697   812     3 24163     6     3     9   192  3760
  24228  5932    12  5932   825  5005    28  2647    18   747   291  1388
   1681    28   350  8503  2640    13  5697   812     3 24163     6     3
      9 13080  4896   138   358     6  4772    18 23313     3  5629  3592
   2250    28     3 14177   866    13     3 29506  6493 12391     1]]"
f0946fb9df9839977f4d16c43476e4c2724ff772,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5455b3cdcf426f4d5fc40bc11644a432fa7a5c8f,100.0,well-formed sentences vs concise answers,[[    0   168    18 10816 16513     3   208     7 22874  4269     1]],well-formed sentences vs concise answers,[[  168    18 10816 16513     3   208     7 22874  4269     1]]
60726d9792d301d5ff8e37fbb31d5104a520dea3,100.0,MH17 Twitter dataset,[[    0     3 20131  2517  3046 17953     1]],MH17 Twitter dataset,[[    3 20131  2517  3046 17953     1]]
ad5898fa0063c8a943452f79df2f55a5531035c7,42.1053,Word embeddings trained on a computer or a tablet,"[[    0  4467 25078    26    53     7  4252    30     3     9  1218    42
      3     9  7022     1]]",Word embeddings trained on GoogleNews and Word embeddings trained on Reddit dataset,"[[ 4467 25078    26    53     7  4252    30  1163  6861     7    11  4467
  25078    26    53     7  4252    30  1624    26   155 17953     1]]"
d60a3887a0d434abc0861637bbcd9ad0c596caf4,0.0,rule-based features,[[   0 3356   18  390  753    1]],rules that compute polarity of words after POS tagging or parsing steps,"[[ 2219    24 29216     3  9618   485    13  1234   227     3 16034     3
     17 15242    42   260     7    53  2245     1]]"
b9025c39838ccc2a79c545bec4a676f7cc4600eb,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"1. there may be situations where more than one action is reasonable, and also because writers tell a story playing with elements such as surprise or uncertainty.
2. Macro F1 = 14.6 (MLR, length 96 snippet)
Weighted F1 = 31.1 (LSTM, length 128 snippet)","[[ 1300   132   164    36  4147   213    72   145    80  1041    19  4360
      6    11    92   250  5943   817     3     9   733  1556    28  2479
    224    38  4158    42 14068     5  1682  1534  2771   377   536  3274
    209 25652    41  6858   448     6  2475     3  4314     3 20317  4995
     61 14230    15    26   377   536  3274   220 11039    41  7600  2305
      6  2475   209  2577     3 20317  4995    61     1]]"
58b3b630a31fcb9bffb510390e1ec30efe87bfbf,29.6296,"a general architecture for question answering (QA), it is composed of modules that allow different aspects","[[    0     3     9   879  4648    21   822 18243    41 23008   201    34
     19 10431    13 10561    24   995   315  3149]]", the facts that are relevant for answering a particular question) are labeled during training.,"[[    8  6688    24    33  2193    21 18243     3     9  1090   822    61
     33  3783    15    26   383   761     5     1]]"
d0b005cb7ed6d4c307745096b2ed8762612480d2,100.0,Transformer generation model,[[    0 31220  3381   825     1]],Transformer generation model,[[31220  3381   825     1]]
617c77a600be5529b3391ab0c21504cd288cc7c7,20.0,concept-sets of the same name and their associated sentence structure,"[[   0 2077   18 2244    7   13    8  337  564   11   70 1968 7142 1809
     1]]",These concept-sets are sampled from several large corpora of image/video captions,"[[  506  2077    18  2244     7    33  3106    26    45   633   508 11736
    127     9    13  1023    87 17241 25012     7     1]]"
f64531e460e0ac09b58584047b7616fdb7dd5b3f,0.0,DL-61,[[    0     3 10013    18  4241     1]],Unanswerable,[[ 597 3247 3321  179    1]]
cbf1137912a47262314c94d36ced3232d5fa1926,50.0,"fastText, CWE-LD",[[    0  1006 13598    17     6   205 15713    18  9815     1]],"fastText, CWE-LP",[[ 1006 13598    17     6   205 15713    18  6892     1]]
387970ebc7ef99f302f318d047f708274c0e8f21,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
03f4e5ac5a9010191098d6d66ed9bbdfafcbd013,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"by also learning source embeddings for not only unigrams but also n-grams present in each sentence, and averaging the n-gram embeddings along with the words","[[   57    92  1036  1391 25078    26    53     7    21    59   163    73
     23  5096     7    68    92     3    29    18  5096     7   915    16
    284  7142     6    11     3     9 23980     8     3    29    18  5096
  25078    26    53     7   590    28     8  1234     1]]"
b6e4b98fad3681691bcce13f57fb173aee30c592,31.5789,Using a combination of visual and textual representations,[[   0    3 3626    3    9 2711   13 3176   11 1499 3471 6497    7    1]],similarity is computed as the cosine of the produced $h_{L}$ and $h_{R}$ sentence/image representations,"[[ 1126   485    19 29216    26    38     8   576     7   630    13     8
   2546  1514   107   834     2   434     2  3229    11  1514   107   834
      2   448     2  3229  7142    87  8221  6497     7     1]]"
00e6324ecd454f5d4b2a4b27fcf4104855ff8ee2,37.5,"Using the t-SNE tool BIBREF14, we can determine the","[[    0     3  3626     8     3    17    18   134  4171  1464     3  5972
  25582   371  2534     6    62    54  2082     8]]",we use t-SNE tool BIBREF27 to visualize the learned embedding,"[[   62   169     3    17    18   134  4171  1464     3  5972 25582   371
   2555    12 25086     8  2525 25078    26    53     1]]"
eddb18109495976123e10f9c6946a256a55074bd,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"MVCNN, a novel CNN architecture for sentence classification. It combines multichannel initialization – diverse versions of pretrained word embeddings are used – and variable-size filters – features of multigranular phrases are extracted with variable-size convolution filters. ","[[  283  7431 17235     6     3     9  3714 19602  4648    21  7142 13774
      5    94     3 15256  1249 19778  2332  1707     3   104  2399  5204
     13  7140 10761  1448 25078    26    53     7    33   261     3   104
     11  7660    18  7991 10607     3   104   753    13  1249  7662  4885
  15101    33 21527    28  7660    18  7991   975 24817 10607     5     1]]"
06095a4dee77e9a570837b35fc38e77228664f91,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences ,"[[    8 17953     3  6848    13  2071  1863  2279   379 16513    11   746
     11  4269    81  8985   812    11     3    60 14309  6346     7    78
     34   405   560  1151 16513     1]]"
c21d26130b521c9596a1edd7b9ef3fe80a499f1e,7.4074,"conversational analytics, statistical analysis, and statistical analysis",[[    0  3634   138  9952     6 11775  1693     6    11 11775  1693     1]],"ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations","[[12320  6871     6  1577  1510  2096 14936     6  1898    95  2564   313
      6 12320  1974  3500     6 12320  1975  6750    11   492  2062 17653
      1]]"
e196e2ce72eb8b2d50732c26e9bf346df6643f69,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
af1439c68b28c27848203f863675946380d28943,0.0,a single-hop QA system BIBREF0,"[[    0     3     9   712    18 10776     3 23008   358     3  5972 25582
    371   632     1]]",RoBERTa baseline,[[ 2158 12920   382     9 20726     1]]
1fd969f53bc714d9b5e6604a7780cbd6b12fd616,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"fine-tuning schedules that elaborately design the control of learning rates for optimization, proxy tasks that leverage labeled data to help the pre-trained model better fit the target data distribution, knowledge distillation approaches that ditch the paradigm of initialization with pre-trained parameters by adopting the pre-trained model as a teacher network","[[ 1399    18    17   202    53  2023     7    24 16224   120   408     8
    610    13  1036  1917    21 11295     6 19784  4145    24 11531  3783
     15    26   331    12   199     8   554    18    17 10761   825   394
   1400     8  2387   331  3438     6  1103 20487   257  6315    24 24158
      8 20491    13  2332  1707    28   554    18    17 10761  8755    57
   4693    53     8   554    18    17 10761   825    38     3     9  3145
   1229     1]]"
76377e5bb7d0a374b0aefc54697ac9cd89d2eba8,9.5238,by using a distributed representation of word features,[[   0   57  338    3    9 8308 6497   13 1448  753    1]],By considering words as vertices and generating directed edges between neighboring words within a sentence,"[[  938  4014  1234    38     3  3027   867     7    11     3 11600  6640
   9804   344 10678    53  1234   441     3     9  7142     1]]"
fbd47705262bfa0a2ba1440a2589152def64cbbd,11.1111,"their model outperforms by 0.61, 0.61, 0.61,","[[   0   70  825   91  883 2032    7   57 4097 4241    3    6 4097 4241
     3    6 4097 4241    3    6]]","increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively, over INLINEFORM0 increase in EM and GM between our model and the next best two models","[[ 3094  7452    57   220  2712    11 12238    16  4993    12     8  8430
    747    11   891  6105  2250     6  6898     6   147  3388 20006 24030
    632   993    16     3  6037    11     3  7381   344    69   825    11
      8   416   200   192  2250     1]]"
eeaceee98ef1f6c971dac7b0b8930ee8060d71c2,8.3333,"a graded criteria for faithfulness, a graded criteria for plausibility and","[[    0     3     9  2769    26  6683    21 13855   655     6     3     9
   2769    26  6683    21  9564   302 11102    11]]","Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks., Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves.","[[    3 25087  2250    11  4145    10    37  1952    41     9     7  9954
   6649    61    13 13855   655    44     8   593    13   806  2250    11
   4145     5     6     3 25087  3785   628    10    37  1952    13 13855
    655    44     8   593    13   769  6633     7    13     8  3785   628
      6   224    38 17383    13  1126  3785     7     6    42 22166  3785
      7  1452     5     1]]"
ee31c8a94e07b3207ca28caef3fbaf9a38d94964,8.6957,"F1 score, F1 score",[[   0  377  536 2604    6  377  536 2604    1]],"BLEU, Micro Entity F1, quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5","[[    3  8775 12062     6  5893  4443   485   377  4347   463    13     8
   7216  1315    12  2024   655     6  6720  4392     6    11   936  2376
    655    30     3     9  2643    45   209    12   305     1]]"
1949d84653562fa9e83413796ae55980ab7318f2,100.0,mean reciprocal rank,[[    0  1243 22882   138 11003     1]],mean reciprocal rank,[[ 1243 22882   138 11003     1]]
81669c550d32d756f516dab5d2b76ff5f21c0f36,14.2857,"QANet, QANet2, QANet3, QANet4, QANet","[[    0     3 23008  9688     6     3 23008  9688  4482     3 23008  9688
   6355     3 23008  9688  8525     3 23008  9688]]","Syn Dep, OpenIE, SRL, BiDAF, QANet, BERT, NAQANet, NAQANet+","[[ 8951   374   102     6  2384  5091     6 16626     6  2106  4296   371
      6     3 23008  9688     6   272 24203     6 10144 23008  9688     6
  10144 23008  9688  1220     1]]"
1951cde612751410355610074c3c69cec94824c2,100.0,CNN,[[    0 19602     1]],CNN,[[19602     1]]
3a8d65eb8e1dbb995981a0e02d86ebf3feab107a,0.0,bi-directional regularizers,[[    0  2647    18 26352  1646  8585     7     1]],"an adversarial loss ($\ell _{adv}$) for each model as in the baseline, a cycle consistency loss ($\ell _{cycle}$) on each side","[[   46 23210    23   138  1453  8785     2  3820     3   834     2     9
     26   208     2  3229    61    21   284   825    38    16     8 20726
      6     3     9  4005 12866  1453  8785     2  3820     3   834     2
  10136     2  3229    61    30   284   596     1]]"
89d1687270654979c53d0d0e6a845cdc89414c67,100.0,Using crowdsourcing,[[    0     3  3626  4374 19035     1]],Using crowdsourcing ,[[    3  3626  4374 19035     1]]
fd8b6723ad5f52770bec9009e45f860f4a8c4321,5.5556,LSTM based NER models BIBREF11,"[[    0     3  7600  2305     3   390     3 18206  2250     3  5972 25582
    371  2596     1]]","A pointer network decodes the answer from a bidirectional LSTM with attention flow layer and self-matching layer, whose inputs come from word and character embeddings of the query and input text fed through a highway layer.","[[   71   500    49  1229    20  4978     7     8  1525    45     3     9
   2647 26352     3  7600  2305    28  1388  2537  3760    11  1044    18
  19515    53  3760     6     3  2544  3785     7   369    45  1448    11
   1848 25078    26    53     7    13     8 11417    11  3785  1499 13126
    190     3     9 10367  3760     5     1]]"
67a28fe78f07c1383176b89e78630ee191cf15db,0.0,LSTM,[[   0    3 7600 2305    1]],on the unlabeled data of each task,[[  30    8   73 9339  400   26  331   13  284 2491    1]]
b7fe91e71da8f4dc11e799b3bd408d253230e8c6,100.0,target-side affixes,[[   0 2387   18 1583    3 4127 2407   15    7    1]],target-side affixes,[[2387   18 1583    3 4127 2407   15    7    1]]
7efbe48e84894971d7cd307faf5f6dae9d38da31,100.0,300-hour English conversational speech,[[   0 3147   18 5842 1566 3634  138 5023    1]],300-hour English conversational speech,[[3147   18 5842 1566 3634  138 5023    1]]
9adcc8c4a10fa0d58f235b740d8d495ee622d596,0.0,1,[[  0 209   1]],2 for the ADE dataset and 3 for the CoNLL04 dataset,"[[  204    21     8     3 20458 17953    11   220    21     8   638   567
  10376  6348 17953     1]]"
b7291845ccf08313e09195befd3c8030f28f6a9e,0.0,using Wikipedia as the external knowledge source,[[    0   338 16885    38     8  3866  1103  1391     1]],BERT-Base BIBREF2 to provide the state-of-the-art contextualized modeling,"[[  272 24203    18 14885    15     3  5972 25582   371   357    12   370
      8   538    18   858    18   532    18  1408 28131  1601 15309     1]]"
e9d882775a132e172eea68ab6ab4621a924bb6b8,100.0,attention parsing,[[   0 1388  260    7   53    1]],attention parsing,[[1388  260    7   53    1]]
c34a15f1d113083da431e4157aceb11266e9a1b2,100.0,No,[[  0 465   1]],No,[[465   1]]
496b4ae3c0e26ec95ff6ded5e6790f24c35f0f5b,0.0,relations,[[   0 5836    1]],by converting human advice to first-order logic format and use as an input to calculate gradient,"[[   57     3 21049   936  1867    12   166    18  9397  9769  1910    11
    169    38    46  3785    12 11837 26462     1]]"
93e8ce62361b9f687d5200d2e26015723721a90f,100.0,No,[[  0 465   1]],No,[[465   1]]
e21a8581cc858483a31c6133e53dd0cfda76ae4c,100.0,No,[[  0 465   1]],No,[[465   1]]
6090d3187c41829613abe785f0f3665d9ecd90d9,26.087,"meaning is, therefore, something that words have in sentences; and it's something that sentences","[[    0  2530    19     6  2459     6   424    24  1234    43    16 16513
    117    11    34    31     7   424    24 16513]]",Only in the context of a sentence does a word have a meaning.,"[[3462   16    8 2625   13    3    9 7142  405    3    9 1448   43    3
     9 2530    5    1]]"
cc9f0ac8ead575a9b485a51ddc06b9ecb2e2a44d,20.0,"Compared to the previous best, the proposed model achieved a 62.0% improvement in","[[    0     3 25236    12     8  1767   200     6     8  4382   825  5153
      3     9     3  4056     5  6932  4179    16]]","Compared with the previous SOTA without BERT on SParC, our model improves Ques.Match and Int.Match by $10.6$ and $5.4$ points, respectively.","[[    3 25236    28     8  1767   180 27976   406   272 24203    30  6760
    291   254     6    69   825  1172     7  7227     7     5   329 14547
     11    86    17     5   329 14547    57  1970 22787  3229    11  1514
  27436  3229   979     6  6898     5     1]]"
7065e6140dbaffadebe62c9c9d3863ca0f829d52,100.0,seven,[[   0 2391    1]],seven,[[2391    1]]
34dd0ee1374a3afd16cf8b0c803f4ef4c6fec8ac,0.0,"MC-CNN, MGNC-CNN","[[    0     3  3698    18   254 17235     6     3  9306  8137    18   254
  17235     1]]","standard CNN, C-CNN, MVCNN ",[[ 1068 19602     6   205    18   254 17235     6   283  7431 17235     1]]
21a96b328b43a568f9ba74cbc6d4689dbc4a3d7b,100.0,No,[[  0 465   1]],No,[[465   1]]
d32b6ac003cfe6277f8c2eebc7540605a60a3904,0.0,SVMRank,[[    0   180 12623 22557     1]],"(1) Rank by the number of times a citation is mentioned in the document., (2) Rank by the number of times the citation is cited in the literature (citation impact)., (3) Rank using Google Scholar Related Articles., (4) Rank by the TF*IDF weighted cosine similarity., (5) Rank using a learning-to-rank model trained on text similarity rankings.","[[ 5637     3 22557    57     8   381    13   648     3     9     3 13903
     19  2799    16     8  1708     5     6  6499     3 22557    57     8
    381    13   648     8     3 13903    19     3 11675    16     8  6678
     41 13903  1113   137     6 10153     3 22557   338  1163 23229 16559
   7491     7     5     6     3 10820     3 22557    57     8     3  9164
   1935  4309   371  1293    15    26   576     7   630  1126   485     5
      6     3 15757     3 22557   338     3     9  1036    18   235    18
   6254   825  4252    30  1499  1126   485 16890     5     1]]"
327e06e2ce09cf4c6cc521101d0aecfc745b1738,100.0,accuracy with standard deviation,[[    0  7452    28  1068 25291     1]],accuracy with standard deviation,[[ 7452    28  1068 25291     1]]
e1b36927114969f3b759cba056cfb3756de474e4,33.3333,improves F1 score by 0.86 and a corresponding metric by 0.86,"[[   0 1172    7  377  536 2604   57 4097 3840   11    3    9    3 9921
     3 7959   57 4097 3840    1]]",Improved AECNN-T by 2.1 and AECNN-T-SM BY 0.9,"[[20825    26    71  3073 17235    18   382    57     3 14489    11    71
   3073 17235    18   382    18  4212   272   476     3 23758     1]]"
98b11f70239ef0e22511a3ecf6e413ecb726f954,100.0,No,[[  0 465   1]],No,[[465   1]]
00e9f088291fcf27956f32a791f87e4a1e311e41,0.0,general-purpose sentence representations,[[    0   879    18 19681  7142  6497     7     1]],"multi-lingual NMT, natural language inference, constituency parsing, skip-thought vectors","[[ 1249    18 25207   445  7323     6   793  1612    16 11788     6  6439
   4392   260     7    53     6 11202    18 11841    17 12938     7     1]]"
69ca609e86888c7e4e2e3d33435a0a36f77601b5,0.0,a highly efficient baseline,[[    0     3     9  1385  2918 20726     1]],a standard beam search decoder BIBREF5 with several straightforward performance optimizations,"[[    3     9  1068 11638   960    20  4978    52     3  5972 25582   371
    755    28   633 11753   821 11295     7     1]]"
e1f5531ed04d0aae1dfcb0559f1512a43134c43a,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
390aa2d733bd73699899a37e65c0dee4668d2cd8,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5e032de729ce9fc727b547e3064be04d30009324,6.6667,"perspectives should be identified as taking an opposing stance with respect to the claim, each perspective","[[    0 14013   225    36  4313    38   838    46 10720    53     3  8389
     28  1445    12     8  1988     6   284  3503]]","one needs to develop mechanisms to recognize valid argumentative structures, we ignore trustworthiness and credibility issues","[[   80   523    12  1344 12009    12  4206  3982  5464  1528  5278     6
     62  9751  2019  7048  6096    11 20327   807     1]]"
a61732774faf30bab15bf944b2360ec4710870c1,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
a939a53cabb4893b2fd82996f3dbe8688fdb7bbb,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5ba6f7f235d0f5d1d01fd97dd5e4d5b0544fd212,0.0,consistent large-scale evaluation,[[   0 4700  508   18 6649 5002    1]],"Class Membership Tests, Class Distinction Test, Word Equivalence Test","[[ 4501 19428  2300     7     6  4501  2678    17    77  4985  2300     6
   4467 21263  2165  1433  2300     1]]"
330f2cdeab689670b68583fc4125f5c0b26615a8,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"he proposed model outperforms all the baselines, being the svi version the one that performs best., the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm.","[[    3    88  4382   825    91   883  2032     7    66     8 20726     7
      6   271     8     3     7  2099   988     8    80    24  1912     7
    200     5     6     8     3     7  2099   988     3 28209     7   231
   3627    12  1146  2620    13     8  4303 18777 17902   116     3  2172
     12     8 11587   988     6    84     3 17441     8  3949    13     8
      3     7  2099 12628     5     1]]"
7011b26ffc54769897e4859e4932aeddfab82c9f,100.0,YouTube ASR system,[[   0 5343   71 6857  358    1]],YouTube ASR system ,[[5343   71 6857  358    1]]
1763a029daca7cab10f18634aba02a6bd1b6faa7,0.0,"Compared to the previous best results, the improvement is 0.86 points.","[[    0     3 25236    12     8  1767   200   772     6     8  4179    19
   4097  3840   979     5     1]]","AGDT improves the performance by 2.4% and 1.6% in the “DS” part of the two dataset, Our AGDT surpasses GCAE by a very large margin (+11.4% and +4.9% respectively) on both datasets, In the “HDS” part, the AGDT model obtains +3.6% higher accuracy than GCAE on the restaurant domain and +4.2% higher accuracy on the laptop domain","[[ 8859 12111  1172     7     8   821    57  1682  5988    11  1300  6370
     16     8   105  3592   153   294    13     8   192 17953     6   421
   8859 12111 25678    15     7     3 11055 14611    57     3     9   182
    508  6346 17134 10032  5988    11  1768  7984  7561  6898    61    30
    321 17953     7     6    86     8   105 11083   134   153   294     6
      8  8859 12111   825  3442     7  1768  5787  6370  1146  7452   145
      3 11055 14611    30     8  2062  3303    11  1768 19765  1454  1146
   7452    30     8  4544  3303     1]]"
f1214a05cc0e6d870c789aed24a8d4c768e1db2f,80.0,"German-English, English-German",[[    0  2968    18 26749     6  1566    18 24518     1]],"German-English, Turkish-English, English-German",[[ 2968    18 26749     6 15423    18 26749     6  1566    18 24518     1]]
3c16d4cf5dc23223980d9c0f924cb9e4e6943f13,100.0,AMS method.,[[    0     3 25346  1573     5     1]],AMS method.,[[    3 25346  1573     5     1]]
49b38189b8336ce41d0f0b4c5c9459722736e15b,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
40a45d59a2ef7a67c8ab0f2b2d5b43fc85b85498,100.0,No,[[  0 465   1]],No,[[465   1]]
b1f2db88a6f89d0f048803e38a0a568f5ba38fc5,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"cases of singular/plural, subject pronoun/object pronoun, etc.","[[ 1488    13 22166    87 12456  4900     6  1426   813 15358    29    87
  30536   813 15358    29     6   672     5     1]]"
60cb756d382b3594d9e1f4a5e2366db407e378ae,100.0,No,[[  0 465   1]],No,[[465   1]]
76ce9e02d97e2d77fe28c0fa78526809e7c195c6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]], MADAMIRA BIBREF6 system,[[  283 16759   329 19426     3  5972 25582   371   948   358     1]]
6baf5d7739758bdd79326ce8f50731c785029802,85.7143,"German, English, Chinese",[[   0 2968    6 1566    6 2830    1]],"German, English, Italian, Chinese",[[2968    6 1566    6 4338    6 2830    1]]
0b92fb692feb35d4b4bf4665f7754d283d6ad5f3,10.2564,BLEU scores of 82.0% and 82.0%,"[[    0     3  8775 12062  7586    13     3  4613     5  6932    11     3
   4613     5  6932     1]]","results that for the in-domain training case, our models BERT-ADA Lapt and BERT-ADA Rest achieve performance close to state-of-the-art on the laptops dataset, new state-of-the-art on the restaurants dataset with accuracies of $79.19\%$ and $87.14\%$, respectively.","[[  772    24    21     8    16    18 22999   761   495     6    69  2250
    272 24203    18 16759   301  6789    11   272 24203    18 16759  7870
   1984   821   885    12   538    18   858    18   532    18  1408    30
      8  4544     7 17953     6   126   538    18   858    18   532    18
   1408    30     8  3661 17953    28     3  6004  2414  6267     7    13
  11301  8797  2294     2  1454  3229    11 13155 25059   591     2  1454
   3229     6  6898     5     1]]"
3ee721c3531bf1b9a1356a40205d088c9a7a44fc,5.7143,Training set contains 26 services belonging to 16 domains.,"[[    0  4017   356  2579  2208   364 12770    12   898  3303     7     5
      1]]","simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers ","[[11108    18   390  7478  3381     6   213     8  1139    11   358  6270
     33     3 31126    12  3806     3     9   743  3634  2537     6    84
     54   258    36 12069    12   793  1612   338  4374  2765     1]]"
10045d7dac063013a8447b5a4bc3a3c2f18f9e82,100.0,No,[[  0 465   1]],No,[[465   1]]
14323046220b2aea8f15fba86819cbccc389ed8b,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
93f4ad6568207c9bd10d712a52f8de25b3ebadd4,80.0,the average uniqueness of all utterances,[[   0    8 1348  775  655   13   66    3 5108  663    7    1]], the average specificity of all utterances,[[   8 1348  806  485   13   66    3 5108  663    7    1]]
163a21c0701d5cda15be2d0eb4981a686e54a842,0.0,the rock genre,[[   0    8 2480 5349    1]], bossa-nova and jovem-guarda genres,"[[ 7930     9    18 14979    11     3  1927   162    51    18 11010     9
   5349     7     1]]"
1d941d390c0ee365aa7d7c58963e646eea74cbd6,100.0,No,[[  0 465   1]],No,[[465   1]]
1af4d56eeaf74460ca2c621a2ad8a5d8dbac491c,100.0,No,[[  0 465   1]],No,[[465   1]]
ed7985e733066cd067b399c36a3f5b09e532c844,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"They use a left-to-right attention mask so that the input tokens can only attend to other input tokens, and the target tokens can only attend to the input tokens and already generated target tokens.","[[  328   169     3     9   646    18   235    18  3535  1388  8181    78
     24     8  3785 14145     7    54   163  2467    12   119  3785 14145
      7     6    11     8  2387 14145     7    54   163  2467    12     8
   3785 14145     7    11   641  6126  2387 14145     7     5     1]]"
9d5df9022cc9eb04b9f5c5a9d8308a332ebdf50c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],They use a two-stage labeling strategy where in the first stage single annotators label a large number of short texts with relatively pure sentiment orientations and in the second stage multiple annotators label few text samples with mixed sentiment orientations,"[[  328   169     3     9   192    18 10705  3783    53  1998   213    16
      8   166  1726   712    46  2264  6230  3783     3     9   508   381
     13   710 14877    28  4352  4621  6493 12602     7    11    16     8
    511  1726  1317    46  2264  6230  3783   360  1499  5977    28  4838
   6493 12602     7     1]]"
12391aab31c899bac0ecd7238c111cb73723a6b7,0.0,LSTM,[[   0    3 7600 2305    1]],Our transference model extends the original transformer model to multi-encoder based transformer architecture. The transformer architecture BIBREF12 is built solely upon such attention mechanisms completely replacing recurrence and convolutions. ,"[[  421  2025  1433   825  4285     7     8   926 19903   825    12  1249
     18    35  4978    52     3   390 19903  4648     5    37 19903  4648
      3  5972 25582   371  2122    19  1192  4199   120  1286   224  1388
  12009  1551 11906     3    60  3663    52  1433    11   975 24817     7
      5     1]]"
56836afc57cae60210fa1e5294c88e40bb10cc0e,73.6842,"language identification, part-of-speech tagging, word segmentation, and","[[    0  1612 10356     6   294    18   858    18     7   855 10217     3
     17 15242     6  1448  5508   257     6    11]]","language identification, part-of-speech tagging, word segmentation, and preordering for statistical machine translation","[[ 1612 10356     6   294    18   858    18     7   855 10217     3    17
  15242     6  1448  5508   257     6    11   554  9397    53    21 11775
   1437  7314     1]]"
a4e66e842be1438e5cd8d7cb2a2c589f494aee27,0.0,Bi-LSTM with a 0.261 BLEU score,"[[    0  2106    18  7600  2305    28     3     9     3 18189  4241     3
   8775 12062  2604     1]]",Depeche + SVM,[[  374   855  1033  1768   180 12623     1]]
b99d100d17e2a121c3c8ff789971ce66d1d40a4d,5.2632,BERT,[[    0   272 24203     1]]," we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models)","[[   62   103    59 21119  4048    12  1767   585   437   167  1895   930
    893  9248  2755   331    41   232    78    34    56    59    36     3
      9  2725  4993   201   169  2254   554    18    26  1014   272 24203
     41   232    78    56   952    36    91   883 10816    57    69  2250
     61     1]]"
7b35593033e4c6b9dccba98f22a7eeaa3385df38,100.0,No,[[  0 465   1]],No,[[465   1]]
163c15da1aa0ba370a00c5a09294cd2ccdb4b96d,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
868c69c8f623e30b96df5b5c8336070994469f60,100.0,CoLA contains example sentences from linguistics publications labeled by experts,"[[    0   638  4569  2579   677 16513    45     3 24703     7 10142  3783
     15    26    57  2273     1]]", CoLA contains example sentences from linguistics publications labeled by experts,"[[  638  4569  2579   677 16513    45     3 24703     7 10142  3783    15
     26    57  2273     1]]"
6a566095e25cbb56330456d7a1f3471693817712,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
80f19be1cbe1f0ec89bbafb9c5f7a8ded37881fb,66.6667,CBOW and Skip-gram methods.,[[    0   205   279 15251    11 25378    18  5096  2254     5     1]],CBOW and Skip-gram methods in the FastText tool BIBREF9,"[[  205   279 15251    11 25378    18  5096  2254    16     8  6805 13598
     17  1464     3  5972 25582   371  1298     1]]"
14e259a312e653f8fc0d52ca5325b43c3bdfb968,0.0,No,[[  0 465   1]],"Yes, Transformer based seq2seq is evaluated with average BLEU 0.519, METEOR 0.388, ROUGE 0.631 CIDEr 2.531 and SER 2.55%.","[[ 2163     6 31220     3   390   142  1824   357     7    15  1824    19
  14434    28  1348     3  8775 12062     3 12100  2294     6  7934  3463
   2990     3 19997  4060     6   391 26260   427     3 22787  3341   205
  13162    52 10603  3341    11     3 18062 10603  2712     5     1]]"
d94ac550dfdb9e4bbe04392156065c072b9d75e1,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
38e2f07ba965b676a99be06e8872dade7c04722a,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
d9cbcaf8f0457b4be59178446f1a280d17a923fa,44.4444,by the number of parameters.,[[   0   57    8  381   13 8755    5    1]],Direct comparison of model parameters,[[7143 4993   13  825 8755    1]]
46bca122a87269b20e252838407a2f88f644ded8,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
4b624064332072102ea674254d7098038edad572,100.0,No,[[  0 465   1]],No,[[465   1]]
0a7ac8eccbc286e0ab55bc5949f3f8d2ea2d1a60,100.0,one,[[ 0 80  1]],one,[[80  1]]
e67d2266476abd157fc8c396b3dfb70cb343471e,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
4c88441f8a1b5fce0ca55a6fced34f97260206ae,16.6667,biaffine is a multi-task multi-task scoring system that uses a bi,"[[    0  2647 18581    15    19     3     9  1249    18 23615  1249    18
  23615 10389   358    24  2284     3     9  2647]]",biaffine attention BIBREF14,[[ 2647 18581    15  1388     3  5972 25582   371  2534     1]]
369b0a481a4b75439ade0ec4f12b44414c4e5164,11.1111,Turkish news dataset,[[    0 15423  1506 17953     1]],"Turkish news-web corpus,  TS TweetS by Sezer-2013 and 20M Turkish Tweets by Bolat and Amasyalı","[[15423  1506    18  8398 11736   302     6     3  4578 25335   134    57
    679  2558    18 11138    11   460   329 15423 25335     7    57  8166
    144    11   736     9     7    63   138     2     1]]"
9e391c8325b48f6119ca4f3d428b1b2b037f5c13,27.7778,WER is the metric used by a large number of machine learning models to evaluate the,"[[   0  549 3316   19    8    3 7959  261   57    3    9  508  381   13
  1437 1036 2250   12 6825    8]]","WER can reflect our model's effectiveness in generating questions that are similar to those of SQuAD, WER can be used for initial analyses","[[  549  3316    54  3548    69   825    31     7  9570    16     3 11600
    746    24    33  1126    12   273    13   180  5991  6762     6   549
   3316    54    36   261    21  2332 15282     1]]"
2b78052314cb730824836ea69bc968df7964b4e4,100.0,SQUAD,[[    0   180 16892  6762     1]],SQUAD,[[  180 16892  6762     1]]
d664054c8d1f8e84169d4ab790f2754274353685,0.0,WSJN-Ultrasound,[[    0     3  8439   683   567    18  1265    40  1313 17481     1]],the UBC database BIBREF14,[[    8   412  7645  3501     3  5972 25582   371  2534     1]]
9846f84747b89f5c692665c4ea7111671ad9839a,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
3e0c9469821cb01a75e1818f2acb668d071fcf40,100.0,"overall rating, mean number of turns",[[   0 1879 5773    6 1243  381   13 5050    1]],"overall rating, mean number of turns",[[1879 5773    6 1243  381   13 5050    1]]
43f86cd8aafe930ebb35ca919ada33b74b36c7dd,19.0476,"by proposing entity-centric ways of structuring input to the transformer networks, using the entity","[[    0    57  8028    53 10409    18 17456  1155    13 21055  1725  3785
     12     8 19903  5275     6   338     8 10409]]","In four entity-centric ways - entity-first, entity-last, document-level and sentence-level","[[   86   662 10409    18 17456  1155     3    18 10409    18 14672     6
  10409    18  5064     6  1708    18  4563    11  7142    18  4563     1]]"
9fe6339c7027a1a0caffa613adabe8b5bb6a7d4a,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
0e510d918456f3d2b390b501a145d92c4f125835,28.5714,"Using a multi-head self-attention layer, BIBREF10 demonstrates","[[    0     3  3626     3     9  1249    18  3313  1044    18 25615  3760
      6     3  5972 25582   371  1714     3 21275]]",constructively by selecting the parameters of the multi-head self-attention layer so that the latter acts like a convolutional layer,"[[21479   120    57  9581     8  8755    13     8  1249    18  3313  1044
     18 25615  3760    78    24     8  7722  6775   114     3     9   975
  24817   138  3760     1]]"
fbabde18ebec5852e3d46b1f8ce0afb42350ce62,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
6dc9960f046ec6bd280a721724458f66d5a9a585,15.3846,"LSTM, BLEU, BLEU, ROUGE, BLEU","[[    0     3  7600  2305     6     3  8775 12062     6     3  8775 12062
      6   391 26260   427     6     3  8775 12062]]","Text Overlap Metrics, including BLEU, Perplexity, Parameterized Metrics","[[ 5027  2035  8478  1212  3929     7     6   379     3  8775 12062     6
   1915  9247   485     6  4734  4401  1601  1212  3929     7     1]]"
bb169a0624aefe66d3b4b1116bbd152d54f9e31b,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
c515269b37cc186f6f82ab9ada5d9ca176335ded,41.8605,"We present that the model attends to shallow context clues, We suggest that the model attend","[[    0   101   915    24     8   825  2467     7    12 16906  2625 11354
      7     6   101  3130    24     8   825  2467]]",Using model gradients with respect to input features they presented that the most important model inputs are verbs associated with entities which shows that the model attends to shallow context clues,"[[    3  3626   825 26462     7    28  1445    12  3785   753    79  2569
     24     8   167   359   825  3785     7    33  7375     7  1968    28
  12311    84  1267    24     8   825  2467     7    12 16906  2625 11354
      7     1]]"
3d39e57e90903b776389f1b01ca238a6feb877f3,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
61fb982b2c67541725d6db76b9c710dd169b533d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],There are is a strong conjecture that it might be the reason but it is not proven.,"[[  290    33    19     3     9  1101   975 11827  1462    24    34   429
     36     8  1053    68    34    19    59  4162     5     1]]"
c1477a6c86bd1670dd17407590948000c9a6b7c6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"give more independence to the several learning methods (e.g. less human intervention) involved in the studies, increasing the size of the output images","[[  428    72 12315    12     8   633  1036  2254    41    15     5   122
      5   705   936  7897    61  1381    16     8  2116     6  3094     8
    812    13     8  3911  1383     1]]"
d0c636fa9ef99c4f44ab39e837a680217b140269,100.0,No,[[  0 465   1]],No,[[465   1]]
059acc270062921ad27ee40a77fd50de6f02840a,100.0,No,[[  0 465   1]],No,[[465   1]]
c45a160d31ca8eddbfea79907ec8e59f543aab86,0.0,WSJ 2014,[[   0    3 8439  683 1412    1]],Swissmetro dataset,[[11541    51 15252 17953     1]]
a22b900fcd76c3d36b5679691982dc6e9a3d34bf,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
7ee660927e2b202376849e489faa7341518adaf9,0.0,"high-dimensional vector space representations, low-dimensional vector space representations, and high-","[[    0   306    18 11619 12938   628  6497     7     6   731    18 11619
  12938   628  6497     7     6    11   306    18]]"," skip-gram, LDA",[[11202    18  5096     6   301  4296     1]]
735f58e28d84ee92024a36bc348cfac2ee114409,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
0d7de323fd191a793858386d7eb8692cc924b432,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"current news, historical news, free time, sports, juridical news pieces, personal adverts, editorials.","[[  750  1506     6  4332  1506     6   339    97     6  2100     6 14086
    138  1506  2161     6   525 24118     7     6 14829     7     5     1]]"
1959e0ebc21fafdf1dd20c6ea054161ba7446f61,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained., Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. ","[[14067  1499 21055  1725    41  6227   134    61    19     3     9  2404
   2491    21 27109    53  1035   585   331    45  3031   533  3187    41
    427 11120     7   201   213  8649  1868  1035   331     6   224    38
    823     8  1868    65   806  3976     6  6716     6    42   125     8
   8985   812    19     6   149   623    45     8  8985    19  1340    44
    383     8  3730     6    42   125     8   806 10343   794   741    19
      6    33  5105     5     6     3  8739     8  1435   205  4578  2491
      6    69     3 23008    18  6227   134  2491     3  8345    12  2928
      8   167  1341  1499    45   926  8986  1499     5     1]]"
a4fe5d182ddee24e5bbf222d6d6996b3925060c8,0.0,"WN18, WN18, WN18, WN18, WN18","[[    0     3 21170  2606     6     3 21170  2606     6     3 21170  2606
      6     3 21170  2606     6     3 21170  2606]]","CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0","[[  638   567 10376  3888     6  5744    51   427  2165  1412     6   638
    567 10376  4407     6   262   122  6513  6286     6   283  6463   940
      6  2142  2168 14910     6  7934  9156 15382     6     3 10466  4280
   2292     6   389  5715     9  6864     1]]"
a3c6acf900126bc9bd9c50ce99041ea00761da6a,4.878,using logical forms,[[   0  338    3 6207 2807    1]]," workers were given a seed qualitative relation, asked to enter two objects, people, or situations to compare, created a question, guided by a large number of examples, LFs were elicited using a novel technique of reverse-engineering them from a set of follow-up questions","[[ 2765   130   787     3     9  6677 19647  4689     6  1380    12  2058
    192  4820     6   151     6    42  4147    12  4048     6   990     3
      9   822     6 10995    57     3     9   508   381    13  4062     6
      3 18962     7   130     3    15 11192    15    26   338     3     9
   3714  3317    13  7211    18 20165    49    53   135    45     3     9
    356    13  1130    18   413   746     1]]"
86083a02cc9a80b31cac912c42c710de2ef4adfd,41.3793,by adding language IDs in the CS lexicon to the model,"[[    0    57  2651  1612  4699     7    16     8     3  4778     3 30949
    106    12     8   825     1]]","model is trained to predict language IDs as well as the subwords, we add language IDs in the CS point of transcriptio","[[  825    19  4252    12  9689  1612  4699     7    38   168    38     8
    769  6051     7     6    62   617  1612  4699     7    16     8     3
   4778   500    13 20146    23    32     1]]"
29e5e055e01fdbf7b90d5907158676dd3169732d,0.0,"BIBREF16, BIBREF17, BIBREF18","[[    0     3  5972 25582   371  2938     6     3  5972 25582   371  2517
      6     3  5972 25582   371  2606     1]]","merging, concatenating, or averaging the entity and its features to compute its embeddings, graph embedding approaches, matrix factorization to jointly embed KB and textual relations","[[12147    53     6   975  2138    35  1014     6    42     3     9 23980
      8 10409    11   165   753    12 29216   165 25078    26    53     7
      6  8373 25078    26    53  6315     6 16826  2945  1707    12 22801
  25078     3 17827    11  1499  3471  5836     1]]"
102a0439739428aac80ac11795e73ce751b93ea1,28.5714,Training data from the BIBREF13 and Training data from the BIBREF18,"[[    0  4017   331    45     8     3  5972 25582   371  2368    11  4017
    331    45     8     3  5972 25582   371  2606]]",KFTT BIBREF12 and BTEC BIBREF13,"[[  480  6245   382     3  5972 25582   371  2122    11   272 23933     3
   5972 25582   371  2368     1]]"
182c7919329bc5678cf0c79687a66c0f7782577e,0.0,bAbI BIBREF4 and bAbI BIBREF5,"[[    0     3   115  8952   196     3  5972 25582   371   591    11     3
    115  8952   196     3  5972 25582   371   755]]","gating function, Dynamic Memory",[[    3   122  1014  1681     6 13967  3113 19159     1]]
346f10ddb34503dfba72b0e49afcdf6a08ecacfa,0.0,"70,000",[[    0     3 28891     1]],46 documents makes up our base corpus,[[ 9668  2691   656    95    69  1247 11736   302     1]]
3b391cd58cf6a61fe8c8eff2095e33794e80f0e3,25.0,a dataset of stock quotes from the S&P 500 Index,"[[    0     3     9 17953    13  1519  7599    45     8   180   184   345
   2899 11507     1]]","historical S&P 500 component stocks
 306242 news articles","[[ 4332   180   184   345  2899  3876 10145   604  4056  4165  1506  2984
      1]]"
056fc821d1ec1e8ca5dc958d14ea389857b1a299,100.0,3 feature maps for a given tuple,[[   0  220 1451 8111   21    3    9  787    3   17  413  109    1]],3 feature maps for a given tuple,[[ 220 1451 8111   21    3    9  787    3   17  413  109    1]]
9174aded45bc36915f2e2adb6f352f3c7d9ada8b,0.0,"WSJ2V, WSJ3V, WSJ4V, ","[[   0    3 8439  683  357  553    6    3 8439  683  519  553    6    3
  8439  683  591  553    6    3]]","SST-2, Snips",[[  180  4209  4949     6   180    29 15432     1]]
279b633b90fa2fd69e84726090fadb42ebdf4c02,18.1818,"Hurricane Florence, Blizzard, Irma, El Nino, El Nin","[[    0 19927 18726     6   272 13287  7061    26     6    27    52    51
      9     6  1289 12776    32     6  1289 12776]]","the East Coast Bomb Cyclone,  the Mendocino, California wildfires, Hurricane Florence, Hurricane Michael, the California Camp Fires","[[    8  1932  5458 16327  6400  3903    29    15     6     8  3137  7171
     77    32     6  1826  3645  6608     7     6 19927 18726     6 19927
   2457     6     8  1826  4594  3655     7     1]]"
cf63a4f9fe0f71779cf5a014807ae4528279c25a,0.0,manually constructed text structures are extracted from the TArC and compared with the original text,"[[    0 12616  8520  1499  5278    33 21527    45     8     3  3221    52
    254    11     3  2172    28     8   926  1499]]",Automatic transcription of 5000 tokens through sequential neural models trained on the annotated part of the corpus,"[[19148 20267    13     3 12814 14145     7   190 29372 24228  2250  4252
     30     8    46  2264   920   294    13     8 11736   302     1]]"
d0bc782961567dc1dd7e074b621a6d6be44bb5b4,100.0,30 words,[[   0  604 1234    1]],30 words,[[ 604 1234    1]]
975085e3b6679cc644fdd6ad11b7c2d1261a2dc6,100.0,No,[[  0 465   1]],No,[[465   1]]
94e17980435aaa9fc3b5328f16f3368dc8a736bd,17.1429,"a vector of image and textual captions, with corresponding images and textual caption","[[    0     3     9 12938    13  1023    11  1499  3471 25012     7     6
     28     3  9921  1383    11  1499  3471 25012]]","The second method it to learn a common space for the two modalities before concatenation (project), The first method is concatenation of the text and image representation (concat)","[[   37   511  1573    34    12   669     3     9  1017   628    21     8
    192     3 20226  2197   274   975  2138    35   257    41 23574   201
     37   166  1573    19   975  2138    35   257    13     8  1499    11
   1023  6497    41  1018  2138    61     1]]"
ff1595a388769c6429423a75b6e1734ef88d3e46,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard. Predefined messages can also trigger other associated events such as pop-ups or follow-up non-verbal actions.","[[   37 23382    54  1738    80    13   633   554 17094  4175    12  1299
      6    42   686    70   293  1569     3    99   906     5  1443  1499
   4175   103    59   483     8  7478   538    16     8   377  4212     6
     78    34    19   359    12 25733    70   169    57  1260   631  7478
    931    12     8 23382     5  1266 17094  4175    54    92  7294   119
   1968   984   224    38  2783    18   413     7    42  1130    18   413
    529    18 11868   138  2874     5     1]]"
a2103e7fe613549a9db5e65008f33cf2ee0403bd,25.0,wealth,[[   0 5987    1]],"wealth , democracy , population, levels of ODA, conflict ","[[ 5987     3     6 14278     3     6  2074     6  1425    13   411  4296
      6  4129     1]]"
9ebb2adf92a0f8db99efddcade02a20a219ca7d9,17.6471,"The proficiency score is calculated according to 6 indicators: A1, A2, C1, C2, C","[[    0    37 27657  2604    19 11338  1315    12   431 15600    10    71
   4347    71  4482   205  4347   205  4482   205]]","They used 6 indicators for proficiency (same for written and spoken) each marked by bad, medium or good by one expert.","[[  328   261   431 15600    21 27657    41     7   265    15    21  1545
     11 11518    61   284  7027    57  1282     6  2768    42   207    57
     80  2205     5     1]]"
e8e00b4c0673af5ab02ec82563105e4157cc54bb,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],transformer model achieves higher BLEU score than both Attention encoder-decoder and sequence-sequence model,"[[19903   825  1984     7  1146     3  8775 12062  2604   145   321 20748
  23734    52    18   221  4978    52    11  5932    18     7    15   835
   3772   825     1]]"
828ce5faed7783297cf9ce202364f999b8d4a1f6,0.0,citation polarity,[[    0     3 13903     3  9618   485     1]],"F-score, micro-F, macro-F, weighted-F ","[[  377    18     7  9022     6  2179    18   371     6 11663    18   371
      6  1293    15    26    18   371     1]]"
04cab3325e20c61f19846674bf9a2c46ea60c449,0.0,"eval92, LibriSpeech",[[    0     3    15  2165  4508     6  1414  2160   134   855 10217     1]],"Wav2vec BIBREF22, a fully-supervised system using all labeled data","[[ 3129   208   357   162    75     3  5972 25582   371  2884     6     3
      9  1540    18 23313   358   338    66  3783    15    26   331     1]]"
e8c0fabae0d29491471e37dec34f652910302928,0.0,"DA labeling, DA labeling, DA labeling","[[   0    3 4296 3783   53    6    3 4296 3783   53    6    3 4296 3783
    53    1]]",beyond localized features and have access to the entire sequence,[[1909  415 1601  753   11   43  592   12    8 1297 5932    1]]
271019168ed3a2b0ef5e3780b48a1ebefc562b57,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Bi-LSTM: For low resource <17k clean data: Using distant supervision resulted in huge boost of F1 score (1k eg. ~9 to ~36 wit distant supervision)
BERT: <5k clean data boost of F1 (1k eg. ~32 to ~47 with distant supervision)","[[ 2106    18  7600  2305    10   242   731  3487     3     2  2517   157
   1349   331    10     3  3626 10382 15520   741    15    26    16  1450
   4888    13   377   536  2604  4077   157     3    15   122     5     3
      2  1298    12     3     2  3420     3  7820 10382 15520    61   272
  24203    10     3     2   755   157  1349   331  4888    13   377   536
   4077   157     3    15   122     5     3     2  2668    12     3     2
   4177    28 10382 15520    61     1]]"
a5dd569e6d641efa86d2c2b2e970ce5871e0963f,0.0,Dirichlet multinomial mixture model,[[   0 7454  362 1655 1249 3114   23  138 4989  825    1]],"K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods","[[  480    18   526  3247     6 25378    18 11841    17 29011     7     6
    419 15983   757  1484  9709  3426    11  4734  9413 29011     3   390
   9068    53  2254     1]]"
2d4d0735c50749aa8087d1502ab7499faa2f0dd8,4.2553,"Using the best state-of-the-art model, our model outperforms","[[   0    3 3626    8  200  538   18  858   18  532   18 1408  825    6
    69  825   91  883 2032    7]]","Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)","[[  749 12151  4674 17235    65     3 22426  3951     6     3 10917  3747
      6     3 22384  2606     6     3 22426  5865     3  2172    12     3
  22426  3940     6     3 10917  3628     6     3 22384  2368     6     3
  22426  4959    13   200   538    13     8   768   741    30 23045 26819
     15   848    52   127    41  4148   427   201 11663    18 28951    26
  23045 26819    15   848    52   127    41  4148  6037     3   201 14865
  13774  7452    41 19543     5    61    11  1293    15    26 14865 13774
   7452    41   518    17     5     3 19543     5    61     1]]"
3513682d4ee2e64725b956c489cd5b5995a6acf2,28.5714,a sampling method called INLINEFORM4,[[    0     3     9 17222  1573   718  3388 20006 24030   591     1]],"monte-carlo, sequential sampling",[[ 6278    15    18  1720    40    32     6 29372 17222     1]]
49aecc50823a60c852165e121dbc0ca54304e40f,17.7778,"The data is manually extracted from expert-crafted grammars, each containing 1000 minimal pairs and","[[    0    37   331    19 12616 21527    45  2205    18  8810 19519     7
      6   284     3  6443  5580  6211 14152    11]]"," The data generation scripts use a basic template to create each paradigm, pulling from a vocabulary of over 3000 words annotated for morphological, syntactic, and semantic features needed to create grammatical and semantically felicitous sentences.","[[   37   331  3381  4943     7   169     3     9  1857  3847    12   482
    284 20491     6 12473    45     3     9 19067    13   147   220  2313
   1234    46  2264   920    21     3  8886  4478     6  8953    17  2708
    447     6    11 27632   753   906    12   482     3  5096  4992   138
     11 27632  1427 27497  1162 16513     5     1]]"
6adec34d86095643e6b89cda5c7cd94f64381acc,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.,"[[  506   753    33     3  9942  1461    45     8  1448    11  4105     8
    879 17174    13     3     9  1448   271     3 28058    26    16  7295
      7     5     1]]"
18fbf9c08075e3b696237d22473c463237d153f5,0.0,Yes,[[   0 2163    1]],"For event types and participant types, there was a moderate to substantial level of agreement using the Fleiss' Kappa. For coreference chain annotation, there was average agreement of 90.5%.","[[  242   605  1308    11  8344  1308     6   132    47     3     9  8107
     12  7354   593    13  2791   338     8   377  4460     7     7    31
  12232   102     9     5   242  2583 11788  3741 30729     6   132    47
   1348  2791    13  2777     5  2712     5     1]]"
1c4cd22d6eaefffd47b93c2124f6779a06d2d9e1,0.0,"3,200 documents",[[   0 6180 3632 2691    1]],"3 million webpages processed with a CCG parser for training, 220 queries for development, and 307 queries for testing","[[  220   770 17652     7  8534    28     3     9   205 12150   260     7
     49    21   761     6   204  1755 13154    21   606     6    11   604
    940 13154    21  2505     1]]"
235c156d9c2adc895c9113f53c60f2dd8df45834,100.0,"Mandarin, English",[[    0 31057     6  1566     1]],"Mandarin, English",[[31057     6  1566     1]]
f6937199e4b06bfbaa22edacc7339410de9703db,40.0,"OpenSubtitles BIBREF7, OpenSubtitles BIBREF8,","[[    0  2384 25252 21869     7     3  5972 25582   371   940     6  2384
  25252 21869     7     3  5972 25582   371 11864]]","PersonaChat BIBREF12, DailyDialog BIBREF13, OpenSubtitles BIBREF7","[[ 5780     9   254   547     3  5972 25582   371  2122     6  8496 23770
   2152     3  5972 25582   371  2368     6  2384 25252 21869     7     3
   5972 25582   371   940     1]]"
0f2403fa77738bf05534d7f9d83c9dbb0a0d6140,22.2222,agreement was very low,[[   0 2791   47  182  731    1]],level of agreement (Krippendorff's INLINEFORM0 ),"[[  593    13  2791    41   439  5082   102 23033    89    31     7  3388
  20006 24030   632     3    61     1]]"
7aa8375cdf4690fc3b9b1799b0f5a9ec1c1736ed,11.7647,No,[[  0 465   1]],"No, other baseline metrics they use besides ROUGE-L are n-gram overlap, negative cross-entropy, perplexity, and BLEU.","[[  465     6   119 20726 15905    79   169     3 15262   391 26260   427
     18   434    33     3    29    18  5096 21655     6  2841  2269    18
     35 12395    63     6   399  9247   485     6    11     3  8775 12062
      5     1]]"
4ca0d52f655bb9b4bc25310f3a76c5d744830043,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation,"[[  209  6078  3783    15    26  7478     7    21   761    11     3 11944
     73  9339   400    26  7478     7    21  5002     1]]"
180047e1ccfc7c98f093b8d1e1d0479a4cca99cc,40.0,"Sequence-to-Sequence model, Attention-based model","[[    0 26859  1433    18   235    18   134    15   835  3772   825     6
  20748    18   390   825     1]]"," sequence-to-sequence model (denoted as S2S), HRAN","[[ 5932    18   235    18     7    15   835  3772   825    41   537    32
   1054    38   180   357   134   201   454 16375     1]]"
ab78f066144936444ecd164dc695bec1cb356762,100.0,jointly trained with slots,[[    0 22801  4252    28  9653     1]],jointly trained with slots,[[22801  4252    28  9653     1]]
d6e8b32048ff83c052e978ff3b8f1cb097377786,0.0,alternating dialogue acts,[[    0     3 30859  7478  6775     1]],By annotators on Amazon Mechanical Turk.,[[  938    46  2264  6230    30  2536 24483 23694     5     1]]
aa0d67c2a1bc222d1f2d9e5d51824352da5bb6dc,0.0,"BIBREF13, BIBREF16, BIBREF17","[[    0     3  5972 25582   371  2368     6     3  5972 25582   371  2938
      6     3  5972 25582   371  2517     1]]","TransE, TransR and TransH, PTransE, and ALL-PATHS, R-GCN BIBREF24 and KR-EAR BIBREF26","[[ 4946   427     6  4946   448    11  4946   566     6   276 18474   427
      6    11  8229    18   345 24786   134     6   391    18   517 10077
      3  5972 25582   371  2266    11   480   448    18 19356     3  5972
  25582   371  2688     1]]"
09a993756d2781a89f7ec5d7992f812d60e24232,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
a8e4a67dd67ae4a9ebf983a90b0d256f4b9ff6c6,0.0,MGNC-CNN,[[    0     3  9306  8137    18   254 17235     1]]," SST-1, SST-2, Subj , TREC , Irony ","[[  180  4209  2292     6   180  4209  4949     6  3325   354     3     6
    332 20921     3     6  9046    63     1]]"
2d536961c6e1aec9f8491e41e383dc0aac700e0a,0.0,"a number of grammatical, temporal, and grammatical modifications","[[    0     3     9   381    13     3  5096  4992   138     6 10301  8563
      6    11     3  5096  4992   138 14172     1]]","- paraphrase 1
- paraphrase 2
- different meaning
- opposite meaning
- nonsense
- minimal change
- generalization
- gossip
- formal sentence
- non-standard sentence
- simple sentence
- possibility
- ban
- future
- past","[[    3    18  3856 27111   209     3    18  3856 27111   204     3    18
    315  2530     3    18  6401  2530     3    18     3 25143     3    18
   6211   483     3    18   879  1707     3    18 29517     3    18  4727
   7142     3    18   529    18 16020  7142     3    18   650  7142     3
     18  5113     3    18  4514     3    18   647     3    18   657     1]]"
c6aa8a02597fea802890945f0b4be8d631e4d5cd,22.2222,"semantic structure is a vector of an entity and relation embedding space, and contains ","[[    0 27632  1809    19     3     9 12938    13    46 10409    11  4689
  25078    26    53   628     6    11  2579     3]]","Semantic similarity structure, Semantic direction structure",[[ 679  348 1225 1126  485 1809    6  679  348 1225 2212 1809    1]]
ce6201435cc1196ad72b742db92abd709e0f9e8d,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
b4f5bf3b7b37e2f22d13b724ca8fe7d0888e04a2,0.0,speakers' dictionaries,[[    0  7215    31     3 12472  5414     1]],speaker systems in the real world,[[5873 1002   16    8  490  296    1]]
85d1831c28d3c19c84472589a252e28e9884500f,50.0,"BERT-Base, QRE",[[    0   272 24203    18 14885    15     6  1593  4386     1]],"BERT-Base, QANet",[[  272 24203    18 14885    15     6     3 23008  9688     1]]
ad362365656b0b218ba324ae60701eb25fe664c1,0.0,latent variables are modeled in the PCFG model.,"[[    0    50  4669 11445    33     3 25125    16     8  2104 22807   825
      5     1]]","syntactic information, semantic and topical information",[[ 8953    17  2708   447   251     6 27632    11  2859   138   251     1]]
30db81df46474363d5749d7f6a94b7ef95cd3e01,100.0,"Twitter, Yelp reviews and movie reviews",[[   0 3046    6 7271   40  102 2456   11 1974 2456    1]],"Twitter, Yelp reviews and movie reviews",[[3046    6 7271   40  102 2456   11 1974 2456    1]]
cf8edc6e8c4d578e2bd9965579f0ee81f4bf35a9,0.0,"WSJ15, WSJ15, WSJ15","[[   0    3 8439  683 1808    6    3 8439  683 1808    6    3 8439  683
  1808    1]]","WMT2014, WMT2016 and IWSLT-2014","[[  549  7323 10218     6   549  7323 11505    11    27  8439  9012    18
  10218     1]]"
20df24165b881f97dc1ac32f343939554dd68011,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],linear logistic regression to a set of stock technical signals,[[13080 28820 26625    12     3     9   356    13  1519  2268  9650     1]]
0716b481b78d80b012bca17c897c62efbe7f3731,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
12c7d79d2a26af2d445229d0c8ba3ba1aab3f5b5,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
2c20426c003f7e3053f8e6c333f8bb744f6f31f8,0.0,"BERT-Segmentation, BERT-XLNet, BERT-","[[    0   272 24203    18   134    15   122   297   257     6   272 24203
     18     4   434  9688     6   272 24203    18]]","Emotion Classification (EC), Named Entity Recognition (NER), Sentence Pair Matching (SPM), Natural Language Inference (NLI)","[[  262  7259  4501  2420    41  3073   201  5570    26  4443   485 31110
     41 18206   201  4892    17  1433 25072 12296    53    41   134  6218
    201  6869 10509    86 11788    41 18207   196    61     1]]"
aaed6e30cf16727df0075b364873df2a4ec7605b,100.0,Efficiency task aimed at reducing the number of parameters while minimizing drop in performance,"[[    0 31624  2491     3  8287    44     3  5503     8   381    13  8755
    298     3 28807  2328    16   821     1]]",efficiency task aimed  at reducing the number of parameters while minimizing drop in performance,"[[ 3949  2491     3  8287    44     3  5503     8   381    13  8755   298
      3 28807  2328    16   821     1]]"
ef04182b6ae73a83d52cb694cdf4d414c81bf1dc,23.0769,Disaster data from the United States from October 2007 to October 2015,"[[    0 29798   331    45     8   907  1323    45  1797  4101    12  1797
   1230     1]]"," disaster data from BIBREF5, Queensland flood in Queensland, Australia and Alberta flood in Alberta, Canada","[[ 6912   331    45     3  5972 25582   371 11116 18838  8347    16 18838
      6  2051    11 19014  8347    16 19014     6  1894     1]]"
b0b1ff2d6515fb40d74a4538614a0db537e020ea,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
be6971827707afcd13af3085d0a775a0bd61c5dd,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
9c4ed8ca59ba6d240f031393b01f634a9dc3615d,0.0,KVRET,[[    0   480   553 27514     1]],"VecMap, Muse, Barista",[[ 3901    75 25760     6  6887    15     6  1386   343     9     1]]
a12a08099e8193ff2833f79ecf70acf132eda646,100.0,No,[[  0 465   1]],No,[[465   1]]
544b68f6f729e5a62c2461189682f9e4307a05c6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Google's Neural Machine Translation,[[ 1163    31     7  1484  9709  5879 24527     1]]
cfffc94518d64cb3c8789395707e4336676e0345,100.0,"classification, regression, neural methods",[[    0 13774     6 26625     6 24228  2254     1]],"classification, regression, neural methods",[[13774     6 26625     6 24228  2254     1]]
8568c82078495ab421ecbae38ddd692c867eac09,0.0,3,[[  0 220   1]],"1, 4, 8, 16, 32, 64",[[ 1914  6464  9478 11940  3538     6  6687     1]]
10ddac87daf153cf674589cc1c64a795907d5d9a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],significantly improves the accuracy and F1 score of aspect polarity classification,"[[ 4019  1172     7     8  7452    11   377   536  2604    13  2663     3
   9618   485 13774     1]]"
4a61260d6edfb0f93100d92e01cf655812243724,33.3333,"Machine translation, Text Mining, Web Analytics",[[    0  5879  7314     6  5027 13280     6  1620 13926     1]],"machine translation, statistical machine, sentiment analysis",[[ 1437  7314     6 11775  1437     6  6493  1693     1]]
18288c7b0f8bd7839ae92f9c293e7fb85c7e146a,66.6667,strong correlation with p-value of 0.98,[[    0  1101 18712    28     3   102    18 12097    13  4097  3916     1]],weak correlation with p-value of 0.08,[[ 5676 18712    28     3   102    18 12097    13  4097  4018     1]]
62736ad71c76a20aee8e003c462869bab4ab4b1e,6.8966,the results are then compared with the mean number of binomials in each order,"[[    0     8   772    33   258     3  2172    28     8  1243   381    13
   2701 30019    40     7    16   284   455     1]]","draw our data from news publications, wine reviews, and Reddit, develop new metrics for the agreement of binomial orderings across communities and the movement of binomial orderings over time,  develop a null model to determine how much variation in binomial orderings we might expect across communities and across time","[[ 3314    69   331    45  1506 10142     6  2013  2456     6    11  1624
     26   155     6  1344   126 15905    21     8  2791    13  2701 30019
     40 12320     7   640  2597    11     8  2426    13  2701 30019    40
  12320     7   147    97     6  1344     3     9   206   195   825    12
   2082   149   231 12338    16  2701 30019    40 12320     7    62   429
   1672   640  2597    11   640    97     1]]"
42a61773aa494f7b12838f71a949034c12084de1,0.0,"bAbI, Recursive Networks, Convolutional Neural Networks,","[[    0     3   115  8952   196     6   419 15983   757  3426     7     6
   1193 24817   138  1484  9709  3426     7     6]]","MemN2N BIBREF12, Attentive and Impatient Readers BIBREF6","[[ 1212    51   567   357   567     3  5972 25582   371  2122     6   486
    324  3268    11  1318 10061 16120     7     3  5972 25582   371   948
      1]]"
7cc22fd8c9d0e1ce5e86d0cbe90bf3a177f22a68,66.6667,"3030 conversations composed of 106,432 sentences and 2,714 tokens","[[    0   604  1458  9029 10431    13     3 16431     6   591  2668 16513
     11  3547   940  2534 14145     7     1]]",1000 conversations composed of 6833 sentences and 88047 tokens,"[[ 5580  9029 10431    13     3  3651  4201 16513    11   505  2079  4177
  14145     7     1]]"
f858031ebe57b6139af46ee0f25c10870bb00c3c,100.0,SParC BIBREF2 and CoSQL BIBREF6,"[[    0  6760   291   254     3  5972 25582   371   357    11   638   134
   2247   434     3  5972 25582   371   948     1]]",SParC BIBREF2 and CoSQL BIBREF6,"[[ 6760   291   254     3  5972 25582   371   357    11   638   134  2247
    434     3  5972 25582   371   948     1]]"
3460393d6888dd34113fa0813a1b3a1514c66aa6,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
45a2ce68b4a9fd4f04738085865fbefa36dd0727,0.0,four different datasets were available for the IJCNLP 2017 shared task,"[[    0   662   315 17953     7   130   347    21     8    27   683 10077
   6892  1233  2471  2491     1]]",The dataset from a joint ADAPT-Microsoft project,"[[   37 17953    45     3     9  4494     3 16759  6383    18   329    23
   2771 12369   516     1]]"
129c03acb0963ede3915415953317556a55f34ee,3.6364,supporting facts are labeled during training.,[[   0 3956 6688   33 3783   15   26  383  761    5    1]],"First, the GRU only allows sentences to have context from sentences before them, but not after them. This prevents information propagation from future sentences. Second, the supporting sentences may be too far away from each other on a word level to allow for these distant sentences to interact through the word level GRU.","[[ 1485     6     8   350  8503   163  1250 16513    12    43  2625    45
  16513   274   135     6    68    59   227   135     5   100  1709     7
    251 17554   257    45   647 16513     5  5212     6     8  3956 16513
    164    36   396   623   550    45   284   119    30     3     9  1448
    593    12   995    21   175 10382 16513    12  6815   190     8  1448
    593   350  8503     5     1]]"
1024f22110c436aa7a62a1022819bfe62dc0d336,0.0,statistically validated by BIBREF16,[[    0 11775   120 16742    26    57     3  5972 25582   371  2938     1]],"To verify the reliability of the transformed semantic space, we propose an evaluation benchmark on the basis of word similarity datasets. Given an enriched space INLINEFORM0 and a similarity dataset INLINEFORM1 , we compute the similarity of each word pair INLINEFORM2 as the cosine similarity of their corresponding transformed vectors INLINEFORM3 and INLINEFORM4 from the two spaces, where INLINEFORM5 and INLINEFORM6 for LS and INLINEFORM7 and INLINEFORM8 for CCA. ","[[  304 10446     8 10581    13     8 13421 27632   628     6    62  4230
     46  5002 15705    30     8  1873    13  1448  1126   485 17953     7
      5  9246    46     3 27307   628  3388 20006 24030   632    11     3
      9  1126   485 17953  3388 20006 24030   536     3     6    62 29216
      8  1126   485    13   284  1448  3116  3388 20006 24030   357    38
      8   576     7   630  1126   485    13    70     3  9921 13421 12938
      7  3388 20006 24030   519    11  3388 20006 24030   591    45     8
    192  4856     6   213  3388 20006 24030   755    11  3388 20006 24030
    948    21     3  7600    11  3388 20006 24030   940    11  3388 20006
  24030   927    21   205  4490     5     1]]"
45bd22f2cfb62a5f79ec3c771c8324b963567cc0,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
ca7e71131219252d1fab69865804b8f89a2c0a8f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods.,"[[  465 10581  6423     7    33   937    11   150 17623  4993    19   263
    344  3410  7586    42  2254     5     1]]"
03ce42ff53aa3f1775bc57e50012f6eb1998c480,0.0,English-Italian,[[   0 1566   18  196   17    9 9928    1]],"EN<->ES
EN<->DE
EN<->IT
EN<->EO
EN<->MS
EN<->FI","[[13209     2 13114  3205 13209     2 13114  5596 13209     2 13114  3177
  13209     2 13114 13113 13209     2 13114  4211 13209     2 13114  4936
      1]]"
2e6b7afaf14871ed6db674782b93709910020b06,11.7647,"Compared to baselines, our model achieves better performance on transfer tasks.","[[    0     3 25236    12 20726     7     6    69   825  1984     7   394
    821    30  2025  4145     5     1]]","original models were better in some tasks (CR, MPQA, MRPC), utilizing self-attentive sentence representation further improves performances in 5 out of 8 tasks","[[  926  2250   130   394    16   128  4145    41  4545     6  5220 23008
      6     3  9320  4051   201     3 11182  1044    18   144   324  3268
   7142  6497   856  1172     7  7357    16   305    91    13   505  4145
      1]]"
479fc9e6d6d80e69f425d9e82e618e6b7cd12764,0.0,"idioms, idioms, idioms, idioms","[[    0     3 19916    51     7     6     3 19916    51     7     6     3
  19916    51     7     6     3 19916    51     7]]",intra-sequential and intra-word,[[6344   18    7   15 2436 7220   11 6344   18 6051    1]]
42af0472e6895eaf7b9392674b0d956e64e86b03,33.3333,"English, German$leftrightarrow $French, Chinese$leftrightarrow","[[    0  1566     6  2968  3229     2 17068  3535  6770  1514   371    60
   5457     6  2830  3229     2 17068  3535  6770]]","German$\leftrightarrow $English, German$\leftrightarrow $French, Chinese$\leftrightarrow $English, English$\rightarrow $Lithuanian, English$\rightarrow $Finnish, and Russian$\rightarrow $English, Lithuanian$\rightarrow $English, Finnish$\rightarrow $English, and English$\rightarrow $Kazakh","[[ 2968  3229     2 17068  3535  6770  1514 26749     6  2968  3229     2
  17068  3535  6770  1514   371    60  5457     6  2830  3229     2 17068
   3535  6770  1514 26749     6  1566  3229     2  3535  6770  1514   434
     23   189    76     9 15710     6  1566  3229     2  3535  6770  1514
    371    77    29  1273     6    11  4263  3229     2  3535  6770  1514
  26749     6 27620    29  3229     2  3535  6770  1514 26749     6 28124
   3229     2  3535  6770  1514 26749     6    11  1566  3229     2  3535
   6770  1514   439     9   172 18965     1]]"
d3bb06d730efbedd30ec226fe8cf828a4773bf5c,41.6667,"RDF stands for Resource Description Framework (RDF), FHIR is a set of","[[    0   391 10665  5024    21 16154  7726 20589    41 10255   371   201
    377  7094   448    19     3     9   356    13]]","Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR), Resource Description Framework (RDF)","[[ 1685  7166 13065  6805 16254  3037    32   883  2020 10179    41 13641
    940   377  7094   448   201 16154  7726 20589    41 10255   371    61
      1]]"
539eb559744641e6a4aefe267cbc4c79e2bcceae,100.0,Reddit,[[   0 1624   26  155    1]],Reddit,[[1624   26  155    1]]
587885bc86543b8f8b134c20e2c62f6251195571,100.0,"English, Spanish and Zulu",[[   0 1566    6 5093   11 1811   40   76    1]],"English, Spanish and Zulu",[[1566    6 5093   11 1811   40   76    1]]
a9610cbcca813f4376fbfbf21cc14689c7fbd677,0.0,BLEU scores,[[    0     3  8775 12062  7586     1]],"In the overall available data there are 40,071 training, 4,988 validation, and 5,050 usable testing stories.","[[   86     8  1879   347   331   132    33  1283     6  4560   536   761
      6  6464  3916   927 16148     6    11  7836   632  1752   178   179
   2505  1937     5     1]]"
e96adf8466e67bd19f345578d5a6dc68fd0279a1,5.0,unsupervised,[[    0    73 23313     1]],"Even though natural language and image synthesis were part of several contributions on the supervised side of deep learning, unsupervised learning saw recently a tremendous rise in input from the research community specially on two subproblems: text-based natural language and image synthesis","[[ 1441   713   793  1612    11  1023     3 17282   130   294    13   633
   7548    30     8     3 23313   596    13  1659  1036     6    73 23313
   1036  1509  1310     3     9  9425  3098    16  3785    45     8   585
    573 12273    30   192   769 19307     7    10  1499    18   390   793
   1612    11  1023     3 17282     1]]"
66bf0d61ffc321f15e7347aaed191223f4ce4b4a,0.0,2,[[  0 204   1]],"2,060 workers",[[3547  632 3328 2765    1]]
d4ce220dcdabcf9ebf4da9bdd3ad778e2d79fc07,0.0,F1 score of 82.0%,[[   0  377  536 2604   13    3 4613    5 6932    1]],increases F1-score by 10.2% and 3% compared with the existing best systems $LS \cup KLD \cup CONN$ and $KLD \cup LS \cup LS_{inter}$,"[[5386  377  536   18    7 9022   57 5477 5406   11    3 5170    3 2172
    28    8 1895  200 1002 1514 7600    3    2 4658  480 9815    3    2
  4658 8472  567 3229   11 1514  439 9815    3    2 4658    3 7600    3
     2 4658    3 7600  834    2 3870    2 3229    1]]"
cdf1bf4b202576c39e063921f6b63dc9e4d6b1ff,13.3333,accuracy,[[   0 7452    1]],"Accuracy and F1 score for supervised tasks, Pearson's and Spearman's correlation for unsupervised tasks","[[ 4292  3663  4710    11   377   536  2604    21     3 23313  4145     6
  29300    31     7    11  8974   291   348    31     7 18712    21    73
  23313  4145     1]]"
0fc17e51a17efce17577e2db89a24abd6607bb2b,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
a979749e59e6e300a453d8a8b1627f97101799de,24.0,"Because of this, monolingual embedding spaces are not isomorphic","[[    0  2070    13    48     6  7414 25207 25078    26    53  4856    33
     59    19    32  8886   447     1]]",because word pair similarity increases if the two words translate to similar parts of the cross-lingual embedding space,"[[  250  1448  3116  1126   485  5386     3    99     8   192  1234 13959
     12  1126  1467    13     8  2269    18 25207 25078    26    53   628
      1]]"
feedddb7ae4998b6a3eaa2d6323017ba278748cc,5.4795,search space that reflects the recent advances in feed-forward seq2seq models,"[[    0   960   628    24     3 17441     8  1100 15895    16  3305    18
  26338   142  1824   357     7    15  1824  2250]]","Our search space consists of two stackable cells, one for the model encoder and one for the decoder , Each cell contains NASNet-style blocks, which receive two hidden state inputs and produce new hidden states as outputs, Our search space contains five branch-level search fields (input, normalization, layer, output dimension and activation), one block-level search field (combiner function) and one cell-level search field (number of cells).","[[  421   960   628     3  6848    13   192  9013   179  2640     6    80
     21     8   825 23734    52    11    80    21     8    20  4978    52
      3     6  1698  2358  2579     3 21277  9688    18  4084  6438     6
     84   911   192  5697   538  3785     7    11  1759   126  5697  2315
     38  3911     7     6   421   960   628  2579   874  6421    18  4563
    960  4120    41    77  2562     6  1389  1707     6  3760     6  3911
   9340    11  5817   257   201    80  2463    18  4563   960  1057    41
   9763  4899  1681    61    11    80  2358    18  4563   960  1057    41
   5525  1152    13  2640   137     1]]"
1bc8904118eb87fa5949ad7ce5b28ad3b3082bd0,100.0,Twitter,[[   0 3046    1]],Twitter,[[3046    1]]
33d1f53cf25a7701db605b6b7ac36946af588bb7,0.0,No,[[  0 465   1]],local businesses (i.e. restaurants),[[ 415 1623   41   23    5   15    5 3661   61    1]]
1bdf7e9f3f804930b2933ebd9207a3e000b27742,100.0,No,[[  0 465   1]],No,[[465   1]]
347e86893e8002024c2d10f618ca98e14689675f,40.0,high-quality,[[   0  306   18 4497    1]],only high-quality data helps,[[ 163  306   18 4497  331 1691    1]]
ac3c88ace59bf75788370062db139f60499c2056,0.0,PV-DM,[[    0 16303    18  7407     1]],"The D2V model has been rated 80 times as ""bad relevance"" while the pmra returned only 24 times badly relevant documents.","[[   37   309   357   553   825    65   118     3  4094  2775   648    38
     96  5514 20208   121   298     8  6366    52     9  3666   163   997
    648 14621  2193  2691     5     1]]"
63d9b12dc3ff3ceb1aed83ce11371bca8aac4e8f,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
093039f974805952636c19c12af3549aa422ec43,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],It uses deep learning framework (pytorch),[[  94 2284 1659 1036 4732   41  102   63   17  127  524   61    1]]
88f8ab2a417eae497338514142ac12c3cec20876,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
ae17066634bd2731a07cd60e9ca79fc171692585,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
753990d0b621d390ed58f20c4d9e4f065f0dc672,47.619,seed lexicon consists of positive and negative predicates,"[[    0  6677     3 30949   106     3  6848    13  1465    11  2841   554
  11346  1422     1]]",a vocabulary of positive and negative predicates that helps determine the polarity score of an event,"[[    3     9 19067    13  1465    11  2841   554 11346  1422    24  1691
   2082     8     3  9618   485  2604    13    46   605     1]]"
4c50f75b1302f749c1351de0782f2d658d4bea70,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Annotators went through various phases to make sure their annotations did not deviate from the mean.,"[[  389  2264  6230   877   190   796 17258    12   143   417    70 30729
      7   410    59    20  2099   342    45     8  1243     5     1]]"
d491ee69db39ec65f0f6da9ec03450520389699a,13.1579,Use of emojis is significantly different between gang members and the rest of the,"[[    0  2048    13     3    15    51 21892     7    19  4019   315   344
      3  3810   724    11     8   880    13     8]]","32.25% of gang members in our dataset have chained together the police and the pistol emoji, compared to just 1.14% of non-gang members, only 1.71% of non-gang members have used the hundred points emoji and pistol emoji together in tweets while 53% of gang members have used them, gang members have a penchant for using just a small set of emoji symbols that convey their anger and violent behavior","[[  220 15300  2712    13     3  3810   724    16    69 17953    43  3741
     15    26   544     8  2095    11     8 24183     3    15    51 21892
      6     3  2172    12   131     3 11039  5988    13   529    18  3810
    724     6   163     3 18596  4704    13   529    18  3810   724    43
    261     8  6189   979     3    15    51 21892    11 24183     3    15
     51 21892   544    16 10657     7   298   305  5170    13     3  3810
    724    43   261   135     6     3  3810   724    43     3     9  4550
   5367    21   338   131     3     9   422   356    13     3    15    51
  21892 13619    24 11770    70 11213    11  9674  3889     1]]"
2e1660405bde64fb6c211e8753e52299e269998f,100.0,"645, 600000",[[   0  431 2128    6 7366 2313    1]],"645, 600000",[[ 431 2128    6 7366 2313    1]]
47b00652ac66039aafe886780e86961bfc5b466e,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
0bffc3d82d02910d4816c16b390125e5df55fd01,100.0,No,[[  0 465   1]],No,[[465   1]]
827464c79f33e69959de619958ade2df6f65fdee,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5cb3d69607f60e1c5be2120462726a477ead9570,100.0,classification,[[    0 13774     1]],classification,[[13774     1]]
78102422a5dc99812739b8dd2541e4fdb5fe3c7a,100.0,current model,[[  0 750 825   1]], current model,[[750 825   1]]
ebae0cd1fe0e7ba877d4b3055190e8b1dfcaeb53,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"User location (uloc), User language (ulang), Timezone (tz), Tweet language (tlang), Offset (offset), User name (name), User description (description), Tweet content (content)","[[ 6674  1128    41    76  5133   201  6674  1612    41    83  1468   201
   2900  9431    41    17   172   201 25335  1612    41    17  4612   201
   4395  2244    41  1647  2244   201  6674   564    41  4350   201  6674
   4210    41   221 11830   201 25335   738    41 14819    61     1]]"
11d2f0d913d6e5f5695f8febe2b03c6c125b667c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],using the BLEU score as a quantitative metric and human evaluation for quality,"[[  338     8     3  8775 12062  2604    38     3     9 18906     3  7959
     11   936  5002    21   463     1]]"
3e3f5254b729beb657310a5561950085fa690e83,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"We define the Effective Word Score of score x as

EFWS(x) = N(+x) - N(-x),

where N(x) is the number of words in the tweet with polarity score x.","[[  101  6634     8 18652  4467 17763    13  2604     3   226    38     3
   9976  8439   599   226    61  3274   445   599  1220   226    61     3
     18   445   599    18   226   201   213   445   599   226    61    19
      8   381    13  1234    16     8 10657    28     3  9618   485  2604
      3   226     5     1]]"
cb20aebfedad1a306e82966d6e9e979129fcd9f9,100.0,weighted F1-score,[[   0 1293   15   26  377  536   18    7 9022    1]],weighted F1-score,[[1293   15   26  377  536   18    7 9022    1]]
a592498ba2fac994cd6fad7372836f0adb37e22a,100.0,100 million sentences,[[    0   910   770 16513     1]],100 million sentences,[[  910   770 16513     1]]
3758669426e8fb55a4102564cf05f2864275041b,15.0,incorrect mentions are annotated in different ways.,"[[    0 12153  2652     7    33    46  2264   920    16   315  1155     5
      1]]","allows the annotator to define each markable as a certain mention type (pronoun, NP, VP or clause), The mentions referring to the same discourse item are linked between each other., chain members are annotated for their correctness","[[ 1250     8    46  2264  1016    12  6634   284  3946   179    38     3
      9   824  2652   686    41  1409 15358    29     6     3  9082     6
      3 11527    42 14442   201    37  2652     7     3 13215    12     8
    337 22739  2118    33  5229   344   284   119     5     6  3741   724
     33    46  2264   920    21    70  2024   655     1]]"
d2804ac0f068e9c498e33582af9c66906b26cac3,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"we extract sentences from Wikipedia in languages for which public multilingual is pretrained. For each sentence, we use the open-source BERT wordpiece tokenizer BIBREF4 , BIBREF1 and compute cross-entropy loss for each wordpiece: INLINEFORM0","[[   62  5819 16513    45 16885    16  8024    21    84   452  1249 25207
     19  7140 10761     5   242   284  7142     6    62   169     8   539
     18  7928   272 24203  1448  8424 14145  8585     3  5972 25582   371
    591     3     6     3  5972 25582   371   536    11 29216  2269    18
     35 12395    63  1453    21   284  1448  8424    10  3388 20006 24030
    632     1]]"
02417455c05f09d89c2658f39705ac1df1daa0cd,100.0,"$1,728",[[   0 1970    6  940 2577    1]],"$1,728",[[1970    6  940 2577    1]]
fbd094918b493122b3bba99cefe5da80cf88959c,100.0,No,[[  0 465   1]],No,[[465   1]]
1afd550cbee15b753db45d7db2c969fc3d12a7d9,100.0,No,[[  0 465   1]],No,[[465   1]]
f68bd65b5251f86e1ed89f0c858a8bb2a02b233a,85.7143,Randomly from Twitter,[[    0 25942   120    45  3046     1]],Randomly from a Twitter dump,[[25942   120    45     3     9  3046 11986     1]]
0c08af6e4feaf801185f2ec97c4da04c8b767ad6,100.0,No,[[  0 465   1]],No,[[465   1]]
0d7f514f04150468b2d1de9174c12c28e52c5511,75.0,agreement of 0.88 and Kappa value of 0.88,"[[    0  2791    13  4097  4060    11 12232   102     9   701    13  4097
   4060     1]]",agreement of 0.85 and Kappa value of 0.83,"[[ 2791    13  4097  4433    11 12232   102     9   701    13  4097  4591
      1]]"
68edb6a483cdec669c9130c928994654f1c19839,14.2857,"NDCG, recall, recall, recall, recall, recall, recall, recall, recall","[[    0     3 10604 12150     6  7881     6  7881     6  7881     6  7881
      6  7881     6  7881     6  7881     6  7881]]","NDCG, MRR, recall@k, mean rank","[[    3 10604 12150     6   283 12224     6  7881  1741   157     6  1243
  11003     1]]"
445e792ce7e699e960e2cb4fe217aeacdd88d392,17.3913,By exploiting multimodal big data to discern depressed individuals using a wide variety of features,"[[    0   938  9248    53  1249 20226   600   331    12 22997    20  8918
   1742   338     3     9  1148  1196    13   753]]",Demographic information is predicted using weighted lexicon of terms.,"[[10007 16587   251    19 15439   338  1293    15    26     3 30949   106
     13  1353     5     1]]"
11dde2be9a69a025f2fc29ce647201fb5a4df580,10.5263,"Compared to the current state-of-the-art, this variant outperforms","[[    0     3 25236    12     8   750   538    18   858    18   532    18
   1408     6    48  6826    91   883  2032     7]]",Proposed method achieves 94.5 UAS and 92.4 LAS  compared to 94.3 and 92.2 of best state-of-the -art greedy based parser. Best state-of-the art parser overall achieves 95.8 UAS and 94.6 LAS.,"[[  749 12151  1573  1984     7   668 12451   412  3291    11   668 17638
      3 20245     3  2172    12   668 21841    11   668 15300    13   200
    538    18   858    18   532     3    18  1408 30337    63     3   390
    260     7    49     5  1648   538    18   858    18   532   768   260
      7    49  1879  1984     7 11923     5   927   412  3291    11   668
  25652     3 20245     5     1]]"
b85ab5f862221fac819cf2fef239bcb08b9cafc6,100.0,localization accuracy,[[   0  415 1707 7452    1]],localization accuracy,[[ 415 1707 7452    1]]
a85698f19a91ecd3cd3a90a93a453d2acebae1b7,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
a8e0796c1ac353d428d84f4506a92b51bce51b87,0.0,"On a dataset of 256,432 documents","[[    0   461     3     9 17953    13     3 19337     6   591  2668  2691
      1]]","D-IMDB (derived from large scale IMDB data snapshot), D-FB (derived from large scale Freebase data snapshot)","[[  309    18  5166  9213    41  9942    45   508  2643     3  5166  9213
    331 23052   201   309    18 15586    41  9942    45   508  2643  1443
  10925   331 23052    61     1]]"
83f567489da49966af3dc5df2d9d20232bb8cb1e,100.0,No,[[  0 465   1]],No,[[465   1]]
9f89bff89cea722debc991363f0826de945bc582,0.0,"WSJ14, WSJ15, WSJ15, WSJ15","[[   0    3 8439  683 2534    6    3 8439  683 1808    6    3 8439  683
  1808    6    3 8439  683 1808]]","WS353S, SimLex999, SimVerb3500","[[    3  8439  2469   519   134     6  6619   434   994 19446     6  6619
   5000   115   519  2560     1]]"
02ce4c288df14a90a210cb39973c6ac0fb4cec59,100.0,English,[[   0 1566    1]],English,[[1566    1]]
83251fd4a641cea8b180b49027e74920bca2699a,11.7647,"frequency of pronouns, prepositions, and subordinate clause constructions","[[    0  7321    13   813 15358    29     7     6   554  4718     7     6
     11   769 21122 14442  1449     7     1]]","style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style","[[  869    13     3     9  7142    19  7283    38     3     9 12938    13
  12052    13  3168  1448  2287    41  2376   525   813 15358    29     7
     61    38   168    38 12052    13  8953    17  2708   447   753   114
      8   381    13 16757  4280   529    18  6544  5405    16   165  6439
   4392   260     7    15     6   437 14442  1809    65   118  2008    12
     36 28618    13   869     1]]"
6a31bd676054222faf46229fc1d283322478a020,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"[error, correction] pairs",[[  784 17262     6 11698   908 14152     1]]
edb2d24d6d10af13931b3a47a6543bd469752f0c,17.7778,They selected 300 communities from Reddit for comparison.,[[   0  328 2639 3147 2597   45 1624   26  155   21 4993    5    1]],They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language.,"[[  328  2639    66     8   769  1271    26  7085    45  1762  2038    12
   1882  1412    28    44   709  2899  1234    16     8 19067    11    44
    709   314   767    13     8   769  1271    26   155    31     7   892
      5   328    92  3641  2597    28     8  7942    13     8  7548    33
     16  2959  1612     5     1]]"
fc65f19a30150a0e981fb69c1f5720f0136325b0,100.0,No,[[  0 465   1]],No,[[465   1]]
a9def7958eac7b9a780403d4f136927f756bab83,0.0,QANET BIBREF5,[[    0     3 23008  9978     3  5972 25582   371   755     1]],MTMSN BIBREF4,[[  283  2305  8544     3  5972 25582   371   591     1]]
a8f1029f6766bffee38a627477f61457b2d6ed5c,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
ac54a9c30c968e5225978a37032158a6ffd4ddb8,0.0,paragraph level,[[   0 8986  593    1]],This seems to indicate that the downstream QA module relies more on the upstream paragraph-level retrieval whereas the verification module relies more on the upstream sentence-level retrieval.,"[[  100  1330    12  6360    24     8 26804     3 23008  6008 11455     7
     72    30     8    95  8103  8986    18  4563 24515   138     3 10339
      8 17549  6008 11455     7    72    30     8    95  8103  7142    18
   4563 24515   138     5     1]]"
9d578ddccc27dd849244d632dd0f6bf27348ad81,0.0,a logically simple and effective method for learning affective events that only requires a,"[[   0    3    9    3 6207  120  650   11 1231 1573   21 1036 2603  757
   984   24  163 2311    3    9]]","Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. 
Using a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.","[[    3  3626    66   331    12  2412    10  8761  1636  2106   517  8503
   5153     3 22384  4906  7452     6  8761  1636   272 24203  5153     3
  22384  3891  7452     6  8761  1220  4490  1220  5911  1636  2106   517
   8503  5153  4097 26750  7452     6  8761  1220  4490  1220  5911  1636
    272 24203  5153     3 22384  2469     6  7452     6    71  4184  1636
   2106   517  8503  5153     3 23758  2294  7452     6    71  4184  1636
    272 24203     3  1836   757    26     3 23758  4201     6  7452     6
     71  4184  1220  4090  1220  4490  1220  5911  1636  2106   517  8503
   5153     3 23758  2517  7452     6    71  4184  1220  4090  1220  4490
   1220  5911  1636   272 24203  5153     3 23758  2368  7452     5     3
   3626     3     9   769  2244    12  2412    10   272 24203  5153     3
  22384  3959  7452   338    71  4184 11372   439   201   272 24203  5153
      3 22384  3840  7452   338    71  4184 11372   439    61  1768  8761
      6  2106   517  8503  5153     3 22384  1458  7452   338    71  4184
  11372   439   201  2106   517  8503  5153     3 22384  4440  7452   338
     71  4184 11372   439    61  1768  8761  1768  3087  1768  2847     5
      1]]"
9c423e3b44e3acc2d4b0606688d4ac9d6285ed0f,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
ae95a7d286cb7a0d5bc1a8283ecbf803e9305951,50.0,sequence-to-sequence (Seq2Seq) models,"[[   0 5932   18  235   18    7   15  835 3772   41  134   15 1824  357
   134   15 1824   61 2250    1]]", recurrent neural network (RNN)-based sequence-to-sequence (Seq2Seq) models for NATS,"[[    3    60 14907 24228  1229    41 14151   567    61    18   390  5932
     18   235    18     7    15   835  3772    41   134    15  1824   357
    134    15  1824    61  2250    21   445 23377     1]]"
e215fa142102f7f9eeda9c9eb8d2aeff7f2a33ed,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"for each multiple-choice question $(q,A) \in Q_\mathit {tr}$ and each choice $a \in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S, take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \in A$ and over all questions in $Q_\mathit {tr}$","[[   21   284  1317    18  3995   867   822  1514   599  1824     6   188
     61     3     2    77  1593   834     2  3357 10536     3     2    17
     52     2  3229    11   284  1160  1514     9     3     2    77    71
   3229     3     6    62   169    66   529    18  7618  6051 14145     7
     16  1514  1824  3229    11  1514     9  3229    38    46  1289 10057
  25001 11417   581   180     6   240     8   420  2382  8046     6   661
   2384     3  5091     3   208  8525    11 12955     8     3  5490     3
     17   413   965   147    66  1514     9     3     2    77    71  3229
     11   147    66   746    16  1514  2247   834     2  3357 10536     3
      2    17    52     2  3229     1]]"
1a43df221a567869964ad3b275de30af2ac35598,100.0,Yelp Challenge dataset BIBREF2,[[    0  7271    40   102  7729 17953     3  5972 25582   371   357     1]],Yelp Challenge dataset BIBREF2,[[ 7271    40   102  7729 17953     3  5972 25582   371   357     1]]
87159024d4b6dac8c456bb74a91044df292f6b99,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
78c010db6413202b4063dc3fb6e3cc59ec16e7e3,11.1111,Improved QA-SRL crowdsourcing protocol allows for significantly more reliable performance evaluation of existing,"[[    0 20825    26     3 23008    18   134 12831  4374 19035 10015  1250
     21  4019    72  3468   821  5002    13  1895]]",a trained worker consolidates existing annotations ,[[    3     9  4252 10416 13250  6203  1895 30729     7     1]]
b5484a0f03d63d091398d3ce4f841a45062438a7,14.8148,a multi-step process to generate a new embedding for each input word,"[[    0     3     9  1249    18  7910   433    12  3806     3     9   126
  25078    26    53    21   284  3785  1448     1]]","proposed method comprises of two steps: a neighbourhood reconstruction step (Section ""Nearest Neighbour Reconstruction"" ), and a projection step (Section ""Projection to Meta-Embedding Space"" ). In the reconstruction step, we represent the embedding of a word by the linearly weighted combination of the embeddings of its nearest neighbours in each source embedding space. ","[[ 4382  1573 13009    13   192  2245    10     3     9 19704 20532  1147
     41   134    15  4985    96   567  2741   222  1484  9031   115  1211
    419 26471   121     3   201    11     3     9 13440  1147    41   134
     15  4985    96  3174 21440    12 14204    18 17467    15  7249  5844
    121     3   137    86     8 20532  1147     6    62  4221     8 25078
     26    53    13     3     9  1448    57     8 13080   120  1293    15
     26  2711    13     8 25078    26    53     7    13   165 13012 14245
      7    16   284  1391 25078    26    53   628     5     1]]"
954c4756e293fd5c26dc50dc74f505cc94b3f8cc,7.1429,dilated convolution layers are used to create a stack of convolution layers,"[[    0     3    26   173   920   975 24817  7500    33   261    12   482
      3     9  9013    13   975 24817  7500     1]]",Similar to standard convolutional networks but instead they skip some input values effectively operating on a broader scale.,"[[18347    12  1068   975 24817   138  5275    68  1446    79 11202   128
   3785  2620  3762  2699    30     3     9     3 13627  2643     5     1]]"
e8e6986365f899dead0768ecf7b1eca8a2699f2f,100.0,No,[[  0 465   1]],No,[[465   1]]
c38a48d65bb21c314194090d0cc3f1a45c549dd6,0.0,"Connexus, Chinese",[[   0 1193   29  994  302    6 2830    1]],"Conll, Weblogs, Newsgroups, Reviews, Answers","[[ 1193   195     6   101 12139     7     6  3529 10739     7     6 16305
      6 11801     7     1]]"
2376c170c343e2305dac08ba5f5bda47c370357f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"They crawled travel information from the Web to build a database, created a multi-domain goal generator from the database, collected dialogue between workers an automatically annotated dialogue acts. ","[[  328 18639    15    26  1111   251    45     8  1620    12   918     3
      9  3501     6   990     3     9  1249    18 22999  1288  9877    45
      8  3501     6  4759  7478   344  2765    46  3269    46  2264   920
   7478  6775     5     1]]"
89b9a2389166b992c42ca19939d750d88c5fa79b,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
4bf4374135c39d10dafece4bed8ef547dc3bf9f0,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
f52b2ca49d98a37a6949288ec5f281a3217e5ae8,6.0606,"By generating a token representing a given length-rati class, we augment the","[[    0   938     3 11600     3     9 14145  9085     3     9   787  2475
     18  6850 32134   853     6    62 15189     8]]","They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group.","[[  328   169   386  1637   710    87 12110    87  2961  7314  2287    12
    669  2475 14145     6    84    19    16    16 11788   261    12 14387
   1229    12  3806  5327  2475   563     5     1]]"
63496705fff20c55d4b3d8cdf4786f93e742dd3d,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
003f884d3893532f8c302431c9f70be6f64d9be8,100.0,No,[[  0 465   1]],No,[[465   1]]
e42fbf6c183abf1c6c2321957359c7683122b48e,8.3333,accuracy of 82.0%,[[   0 7452   13    3 4613    5 6932    1]],"BiLSTM-XR-Dev Estimation accuracy is 83.31 for SemEval-15 and 87.68 for SemEval-16.
BiLSTM-XR accuracy is 83.31 for SemEval-15 and 88.12 for SemEval-16.
","[[ 2106  7600  2305    18     4   448    18  2962   208 23621    23   106
   7452    19     3  4591     5  3341    21   679    51   427  2165 10106
     11     3  4225     5  3651    21   679    51   427  2165 10892     5
   2106  7600  2305    18     4   448  7452    19     3  4591     5  3341
     21   679    51   427  2165 10106    11     3  4060     5  2122    21
    679    51   427  2165 10892     5     1]]"
b0376a7f67f1568a7926eff8ff557a93f434a253,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Comparing with the highest performing baseline: 1.3 points on ACE2004 dataset, 0.6 points on CWEB dataset, and 0.86 points in the average of all scores.","[[20959    53    28     8  2030  5505 20726    10     3 13606   979    30
      3 11539 21653 17953     6     3 22787   979    30     3 18105 15658
  17953     6    11  4097  3840   979    16     8  1348    13    66  7586
      5     1]]"
5e057e115f8976bf9fe70ab5321af72eb4b4c0fc,100.0,No,[[  0 465   1]],No,[[465   1]]
ca8e023d142d89557714d67739e1df54d7e5ce4b,5.4054,they were asked to define their own classes by comparing their own classes to the ones from the,"[[    0    79   130  1380    12  6634    70   293  2287    57     3 14622
     70   293  2287    12     8  2102    45     8]]",inspired by the OntoNotes5 corpus BIBREF7 as well as the ACE (Automatic Content Extraction) English Annotation Guidelines for Entities Version 6.6 2008.06.13 BIBREF8,"[[ 3555    57     8   461   235 10358    15     7   755 11736   302     3
   5972 25582   371   940    38   168    38     8     3 11539    41 16204
   4992  7185  1881 10559    61  1566   389  2264   257 25090    21  4443
   2197  8011     3 28833 10986 13821  2368     3  5972 25582   371   927
      1]]"
14e78db206a8180ea637774aa572b073e3ffa219,100.0,RNN encoders,[[    0   391 17235 23734    52     7     1]],RNN encoders,[[  391 17235 23734    52     7     1]]
a3783e42c2bf616c8a07bd3b3d503886660e4344,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
722e9b6f55971b4c48a60f7a9fe37372f5bf3742,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The proposed system consists of a Bi-directional Long Short-Term Memory (BiLSTM) BIBREF16, a two-level attention mechanism BIBREF29, BIBREF30 and a shared representation for emotion and sentiment analysis tasks., Each of the shared representations is then fed to the primary attention mechanism","[[   37  4382   358     3  6848    13     3     9  2106    18 26352  3230
   7110    18 11679 19159    41   279    23  7600  2305    61     3  5972
  25582   371  2938     6     3     9   192    18  4563  1388  8557     3
   5972 25582   371  3166     6     3  5972 25582   371  1458    11     3
      9  2471  6497    21 13868    11  6493  1693  4145     5     6  1698
     13     8  2471  6497     7    19   258 13126    12     8  2329  1388
   8557     1]]"
38a5cc790f66a7362f91d338f2f1d78f48c1e252,100.0,SVM,[[    0   180 12623     1]],SVM,[[  180 12623     1]]
b68d2549431c524a86a46c63960b3b283f61f445,62.069,fragments are interchangeable if they occur in the same environment as the sentences in the previous,"[[    0 12071     7    33 24177   179     3    99    79  4093    16     8
    337  1164    38     8 16513    16     8  1767]]",fragments are interchangeable if they occur in at least one lexical environment that is exactly the same,"[[12071     7    33 24177   179     3    99    79  4093    16    44   709
     80     3 30949   138  1164    24    19  1776     8   337     1]]"
4547818a3bbb727c4bb4a76554b5a5a7b5c5fedb,0.0,10k sentences,[[    0   335   157 16513     1]],ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words),"[[ 6173    18  3216   331  1706    41  2915   157  1234    13   761   331
     61    11     8   423    27  8439  9012   968   761 11736   302    41
  19162   329  1234    61     1]]"
2480dfe2d996afef840a81bd920aeb9c26e5b31d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"exact string matching, inflectional string matching",[[ 2883  6108  8150     6    16 26303  6318  6108  8150     1]]
62613aca3d7c7d534c9f6d8cb91ff55626bb8695,66.6667,"Argus Dataset, Argus Dataset, Argus Dataset","[[    0     3 16627   302  2747  2244     6     3 16627   302  2747  2244
      6     3 16627   302  2747  2244     1]]","Argus Dataset, AI2-8grade/CK12 Dataset, MCTest Dataset","[[    3 16627   302  2747  2244     6  7833   357  6039  6801    87 10459
   2122  2747  2244     6   283  6227   222  2747  2244     1]]"
0ee20a3a343e1e251b74a804e9aa1393d17b46d6,15.0,MH17 classifier can be used to automatically classify information from multiple sources,"[[    0     3 20131  2517   853  7903    54    36   261    12  3269   853
   4921   251    45  1317  2836     1]]","quality of the classifier predictions is too low to be integrated into the network analysis right away, the classifier drastically facilitates the annotation process for human annotators compared to annotating unfiltered tweets","[[  463    13     8   853  7903 20099    19   396   731    12    36  4580
    139     8  1229  1693   269   550     6     8   853  7903 24173  6758
      7     8 30729   433    21   936    46  2264  6230     3  2172    12
     46  2264  1014    73 23161 10657     7     1]]"
6adde6bc3e27a32eac5daa57d30ab373f77690be,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
8ad815b29cc32c1861b77de938c7269c9259a064,0.0,"German, English, Chinese",[[   0 2968    6 1566    6 2830    1]],"EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO","[[13209     6     3 13853     6     3  3205     6 11155     6     3  6383
      6     3 12725     6     3  4611     6     3  7422     6 11466     6
      3  8503     6  2344     6  3396     6     3  5329     6     3 18207
      6     3  3577     6     3  7416     6  8536     6  9766     6     3
   4936     6     3  4778     6  1270     6     3  7094     6     3  4296
      6     3 17861     6  5693     6 10264     6     3  6857     6     3
  15086     6     3 19179     6     3  5905     6     3  3221     6     3
   9320     6     3 19174     6  3388     6   480   567     6 10104     6
      3  5629     6     3 13961     6     3 17063     6  1027   566     6
    205 17827     6  6827     6     3  9012     6     3  6858     6  7933
      6    27   518     6     3  4171     6     3 23815     6   283   476
      6     3 12733     6     3 12048     6 10842     1]]"
0a8bc204a76041a25cee7e9f8e2af332a17da67a,0.0,"accuracy, recall, F1",[[   0 7452    6 7881    6  377  536    1]],"BLEU, Self-BLEU, n-gram based score, probability score","[[    3  8775 12062     6  8662    18  8775 12062     6     3    29    18
   5096     3   390  2604     6 15834  2604     1]]"
da1994421934082439e8fe5071a01d3d17b56601,100.0,No,[[  0 465   1]],No,[[465   1]]
9b1d789398f1f1a603e4741a5eee63ccaf0d4a4f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"confusion matrices, $\text{F}_1$ score","[[12413     3 20705    15     7     6  1514     2  6327     2   371     2
    834   536  3229  2604     1]]"
e97545f4a5e7bc96515e60f2f9b23d8023d1eed9,12.9032,cao2018retrieve simply concatenated template encoding after the source,"[[   0  212   32 9457   60 1788   15  162  914  975 2138   35  920 3847
     3   35 9886  227    8 1391]]","For each source article, Retrieve aims to return a few candidate templates from the training corpus. Then, the Fast Rerank module quickly identifies a best template from the candidates.","[[  242   284  1391  1108     6  8298     3  8345    12  1205     3     9
    360  4775  7405    45     8   761 11736   302     5    37    29     6
      8  6805   419  6254  6008  1224     3  8826     7     3     9   200
   3847    45     8  4341     5     1]]"
8ec2ca6c7f60c46eedac1fe0530b5c4448800fec,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
e07df8f613dbd567a35318cd6f6f4cb959f5c82d,100.0,perplexity,[[   0  399 9247  485    1]],perplexity,[[ 399 9247  485    1]]
bed527bcb0dd5424e69563fba4ae7e6ea1fca26a,66.6667,GermEval 2019 shared task,[[   0 5744   51  427 2165 1360 2471 2491    1]],2019 GermEval shared task on hierarchical text classification,"[[ 1360  5744    51   427  2165  2471  2491    30  1382  7064  1950  1499
  13774     1]]"
eddabb24bc6de6451bcdaa7940f708e925010912,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Answer with content missing: (Data and pre-processing section) The data is suited for our experiments because the annotators were explicitly asked to provide annotations on a clausal level.,"[[11801    28   738  3586    10    41 20367    11   554    18 15056    53
   1375    61    37   331    19     3 10967    21    69 12341   250     8
     46  2264  6230   130 21119  1380    12   370 30729     7    30     3
      9     3  4651   302   138   593     5     1]]"
13149342ccbb7a6df9b4b1bed890cfbdc1331c1f,69.5652,"2,560 pseudo-tweets in three different languages: Japanese (ja),","[[    0  3547   755  3328 22726    18    17  1123    15    17     7    16
    386   315  8024    10  4318    41  1191   201]]","a total of 2,560 pseudo-tweets in three different languages: Japanese (ja), English (en) and Chinese (zh)","[[    3     9   792    13  3547   755  3328 22726    18    17  1123    15
     17     7    16   386   315  8024    10  4318    41  1191   201  1566
     41    35    61    11  2830    41   172   107    61     1]]"
7aa8375cdf4690fc3b9b1799b0f5a9ec1c1736ed,100.0,No,[[  0 465   1]],No,[[465   1]]
3be8859103016ce2afe4c0a8552b9d980f7958bf,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
0b54032508c96ff3320c3db613aeb25d42d00490,63.4146,Tweets related to a Bank of America DDos attack were used as training data,"[[    0 25335     7  1341    12     3     9  1925    13  1371   309  4135
      7  3211   130   261    38   761   331     1]]","Tweets related to a Bank of America DDos attack were used as training data. The test datasets contain tweets related to attacks to Bank of America, PNC and Wells Fargo.","[[25335     7  1341    12     3     9  1925    13  1371   309  4135     7
   3211   130   261    38   761   331     5    37   794 17953     7  3480
  10657     7  1341    12  6032    12  1925    13  1371     6   276  8137
     11  1548     7  5186   839     5     1]]"
47ecaca8adc7306e3014e8c4358e306a5f0e1716,10.0,a low-dimensional entity model called $v_is_capital_,"[[    0     3     9   731    18 11619 10409   825   718  1514   208   834
      2   159     2   834  4010  9538     2   834]]",This article presented a brief overview of embedding models of entity and relationships for KB completion. ,"[[  100  1108  2569     3     9  4456  8650    13 25078    26    53  2250
     13 10409    11  3079    21     3 17827  6929     5     1]]"
9a5d02062fa7eec7097f1dc1c38b5e6d5c82acdf,0.0,relative accuracy,[[   0 5237 7452    1]],"the CIDEr-D BIBREF22 , SPICE BIBREF23 , BLEU BIBREF24 , METEOR BIBREF25 , and ROUGE-L BIBREF26 metrics","[[    8   205 13162    52    18   308     3  5972 25582   371  2884     3
      6  6760  8906     3  5972 25582   371  2773     3     6     3  8775
  12062     3  5972 25582   371  2266     3     6  7934  3463  2990     3
   5972 25582   371  1828     3     6    11   391 26260   427    18   434
      3  5972 25582   371  2688 15905     1]]"
6cdd61ebf84aa742155f4554456cc3233b6ae2bf,75.0,BiLSTM with RBF kernel,[[    0  2106  7600  2305    28   391 19780 20563     1]],SVM with RBF kernel,[[  180 12623    28   391 19780 20563     1]]
b13902af1bcf0e199a3ea42bbc8fcd8e696a381a,100.0,parallel data available for the WMT 2016,[[   0 8449  331  347   21    8  549 7323 1421    1]],parallel data available for the WMT 2016,[[8449  331  347   21    8  549 7323 1421    1]]
fb381a59732474dc71a413e25cac37e239547b55,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
827b5bd215599623a3125afe331b56b89b42bf09,100.0,The De7 database,[[   0   37  374  940 3501    1]],The De7 database,[[  37  374  940 3501    1]]
19225e460fff2ac3aebc7fe31fcb4648eda813fb,0.0,Simple Word Embeddings,[[    0  9415  4467     3 17467    15  7249     7     1]],Common Crawl ,[[ 7155   205 10936    40     1]]
480e10e5a1b9c0ae9f7763b7611eeae9e925096b,100.0,No,[[  0 465   1]],No,[[465   1]]
4cbc56d0d53c4c03e459ac43e3c374b75fd48efe,40.0,"Cochrane-Lexis, LSTM, LSTM+E,","[[   0  638  524 2002   15   18  434  994  159    6    3 7600 2305    6
     3 7600 2305 1220  427    6]]","LSTM, SCIBERT",[[    3  7600  2305     6   180  3597 12920   382     1]]
39c78924df095c92e058ffa5a779de597e8c43f4,0.0,linear regression,[[    0 13080 26625     1]],Using Latent Dirichlet Allocation on TF-IDF transformed from the corpus,"[[    3  3626   325  4669  7454   362  1655   901 14836    30     3  9164
     18  4309   371 13421    45     8 11736   302     1]]"
e5ae8ac51946db7475bb20b96e0a22083b366a6d,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
93ac147765ee2573923f68aa47741d4bcbf88fa8,11.7647,"feature that connects the persuasive comment and the original post, feature that connects the original post","[[    0  1451    24  1979     7     8 29535  1670    11     8   926   442
      6  1451    24  1979     7     8   926   442]]","Non-contextual properties of a word, Word usage in an OP or PC (two groups), How a word connects an OP and PC., General OP/PC properties","[[5388   18 1018 6327 3471 2605   13    3    9 1448    6 4467 4742   16
    46    3 4652   42 2104   41 8264 1637  201  571    3    9 1448 1979
     7   46    3 4652   11 2104    5    6 2146    3 4652   87 4051 2605
     1]]"
68794289ed6078b49760dc5fdf88618290e94993,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],A sequence of logical statements represented in a computational graph,"[[   71  5932    13     3  6207  6643  7283    16     3     9 25850  8373
      1]]"
d42031893fd4ba5721c7d37e1acb1c8d229ffc21,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
d325a3c21660dbc481b4e839ff1a2d37dcc7ca46,25.0,"Detection, Inclusion, Distinctness, Distinctness","[[    0     3 31636    23   106     6    86 11593     6  2678 19827   655
      6  2678 19827   655     1]]","Detection, Direction, Graded Entailment","[[    3 31636    23   106     6 19436     6 13027    26   695  5756   297
      1]]"
ee279ace5bc69d15e640da967bd4214fe264aa1a,0.0,accuracy,[[   0 7452    1]],"mean rank (MR), mean reciprocal rank (MRR), as well as Hits@1, Hits@3, and Hits@10","[[ 1243 11003    41  9320   201  1243 22882   138 11003    41   329 12224
    201    38   168    38 11436     7  1741  4347 11436     7  1741  6355
     11 11436     7  1741  1714     1]]"
7f2fd7ab968de720082133c42c2052d351589a67,20.0,"word2vec, 256",[[    0  1448   357   162    75     6     3 19337     1]],"word2vec, 200 as the dimension of the obtained word vectors","[[ 1448   357   162    75     6  2382    38     8  9340    13     8  5105
   1448 12938     7     1]]"
bdc1f37c8b5e96e3c29cc02dae4ce80087d83284,0.0,BLEU score,[[    0     3  8775 12062  2604     1]],unweighted average recall (UAR) metric,[[  73 9378   15   26 1348 7881   41 1265 4280   61    3 7959    1]]
5367f8979488aaa420d8a69fec656851095ecacb,5.7143,a multi-task task with a focus on the underlying structure of the text,"[[    0     3     9  1249    18 23615  2491    28     3     9   992    30
      8     3 10067  1809    13     8  1499     1]]","attention probabilities learned tend to respect the conditions of Lemma UNKREF15, corroborating our hypothesis, validate that our model learns a meaningful classifier we compare it to the standard ResNet18","[[ 1388  9551  2197  2525  2134    12  1445     8  1124    13   312   635
      9  4417   439  4386   371  1808     6     3 26391  6693  1014    69
  22455     6 16742    24    69   825   669     7     3     9  7892   853
   7903    62  4048    34    12     8  1068  7127  9688  2606     1]]"
616c205142c7f37b3f4e81c0d1c52c79f926bcdc,9.5238,"a multi-domain DST model BIBREF7, a multi-domain","[[    0     3     9  1249    18 22999   309  4209   825     3  5972 25582
    371   940     6     3     9  1249    18 22999]]","SUMBT BIBREF17 is the current state-of-the-art model on WOZ 2.0, TRADE BIBREF3 is the current published state-of-the-art model","[[  180  6122  9021     3  5972 25582   371  2517    19     8   750   538
     18   858    18   532    18  1408   825    30     3 16733   956  6864
      6 11466 20458     3  5972 25582   371   519    19     8   750  1790
    538    18   858    18   532    18  1408   825     1]]"
084fb7c80a24b341093d4bf968120e3aff56f693,100.0,represent the state using natural language,[[   0 4221    8  538  338  793 1612    1]], represent the state using natural language,[[4221    8  538  338  793 1612    1]]
cc680cb8f45aeece10823a3f8778cf215ccc8af0,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"number of epochs is an important parameter and its increase leads to results that rank our two worst models almost equal, or even better than others","[[  381    13     3    15   102  6322     7    19    46   359 15577    11
    165   993  3433    12   772    24 11003    69   192  6025  2250   966
   4081     6    42   237   394   145   717     1]]"
f60629c01f99de3f68365833ee115b95a3388699,14.2857,"classification, regression, neural methods",[[    0 13774     6 26625     6 24228  2254     1]],"NNC SU4 F1, NNC top 5, Support Vector Classification (SVC)","[[  445  8137     3  4138   591   377  4347   445  8137   420  7836  4224
  29011  4501  2420    41   134  7431    61     1]]"
e8647f9dc0986048694c34ab9ce763b3167c3deb,100.0,No,[[  0 465   1]],No,[[465   1]]
8112d18681e266426cf7432ac4928b87f5ce8311,100.0,"English, Hindi",[[    0  1566     6 25763     1]],"English, Hindi",[[ 1566     6 25763     1]]
ccb3d21885250bdbfc4c320e99f25923896e70fa,100.0,"calendar, weather, navigation",[[   0 4793    6 1969    6 8789    1]],"calendar, weather, navigation",[[4793    6 1969    6 8789    1]]
2ec9c1590c96f17a66c7d4eb95dc5d3a447cb973,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"sampled all papers published in the Computer Science subcategories of Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Social and Information Networks (cs.SI), Computational Linguistics (cs.CL), Computers and Society (cs.CY), Information Retrieval (cs.IR), and Computer Vision (CS.CV), the Statistics subcategory of Machine Learning (stat.ML), and Social Physics (physics.soc-ph), filtered for papers in which the title or abstract included at least one of the words “machine learning”, “classif*”, or “supervi*” (case insensitive), filtered to papers in which the title or abstract included at least “twitter” or “tweet” (case insensitive)","[[ 3106    26    66  5778  1790    16     8  5491  2854   769  8367   839
   2593    13 24714  5869  2825  1433    41    75     7     5  9822   201
   5879  6630    41    75     7     5 24214   201  2730    11  2784  3426
      7    41    75     7     5   134   196   201   638 31148   138  6741
   1744  3040     7    41    75     7     5  8440   201  5491     7    11
   3467    41    75     7     5 17063   201  2784   419  1788    15  2165
     41    75     7     5  5705   201    11  5491 10886    41  4778     5
  20410   201     8 19258   769  8367   839   651    13  5879  6630    41
   8547     5  6858   201    11  2730 22139    41 11599     5     7    32
     75    18   102   107   201     3 23161    21  5778    16    84     8
   2233    42  9838  1285    44   709    80    13     8  1234   105  8276
    630  1036  1241   105  4057    99  1935  1241    42   105 21771  2099
   1935   153    41  6701    16 22118   201     3 23161    12  5778    16
     84     8  2233    42  9838  1285    44   709   105    17  7820   449
    153    42   105    17  1123    15    17   153    41  6701    16 22118
     61     1]]"
1fdcc650c65c11908f6bde67d5052087245f3dde,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
dccc3b182861fd19ccce5bd00ce9c3f40451ed6e,100.0,No,[[  0 465   1]],No,[[465   1]]
a1e07c7563ad038ee2a7de5093ea08efdd6077d4,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"(about 4 million sentences, 138 million word tokens), one trained on the Billion Word benchmark","[[   41  7932   314   770 16513     6     3 22744   770  1448 14145     7
    201    80  4252    30     8   272 14916  4467 15705     1]]"
e06e1b103483e1e58201075c03e610202968c877,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
544e29937e0c972abcdd27c953dc494b2376dd76,100.0,two different BERT models were developed,[[    0   192   315   272 24203  2250   130  1597     1]],Two different BERT models were developed,[[ 2759   315   272 24203  2250   130  1597     1]]
96526a14820b7debfd6f7c5beeade0a854b93d1a,100.0,"trained annotators BIBREF4, crowdsourcing BIBREF5","[[    0  4252    46  2264  6230     3  5972 25582   371  8525  4374 19035
      3  5972 25582   371   755     1]]"," trained annotators BIBREF4, crowdsourcing BIBREF5 ","[[ 4252    46  2264  6230     3  5972 25582   371  8525  4374 19035     3
   5972 25582   371   755     1]]"
6bc45d4f908672945192390642da5a2760971c40,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
ccf7415b515fe5c59fa92d4a8af5d2437c591615,0.0,"This allows the encoder to become independent of $x$, an issue known as posterior collapse","[[    0   100  1250     8 23734    52    12   582  2547    13  1514   226
   3229     6    46   962   801    38 29928 11612]]",by setting a non-zero positive constraint ($C\ge 0$) on the KL term ($|D_{KL}\big (q_\phi ({z}|{x}) || p({z})\big )-C|$),"[[   57  1898     3     9   529    18  2558    32  1465 27354    17  8785
    254     2   397     3   632  3229    61    30     8   480   434  1657
   8785  9175   308   834     2   439   434     2 12911    41  1824   834
      2 11692    41     2   172     2  9175     2   226     2    61  1820
   9175     3   102   599     2   172     2    61     2 12911     3    61
     18   254  9175  3229    61     1]]"
7ce7edd06925a943e32b59f3e7b5159ccb7acaf6,9.5238,consistent distribution of the results is significantly worse than that of the control dataset,"[[    0  4700  3438    13     8   772    19  4019  4131   145    24    13
      8   610 17953     1]]",consistent increase in the validation loss after about 15 epochs,"[[ 4700   993    16     8 16148  1453   227    81   627     3    15   102
   6322     7     1]]"
b5e883b15e63029eb07d6ff42df703a64613a18a,22.2222,topic identification was done using a combination of questionnaire and questionnaire.,"[[    0  2859 10356    47   612   338     3     9  2711    13 19144    11
  19144     5     1]]",using topic modeling model Latent Dirichlet Allocation (LDA),"[[  338  2859 15309   825   325  4669  7454   362  1655   901 14836    41
   9815   188    61     1]]"
ad4658c64056b6eddda00d3cbc55944ae01eb437,46.6667,final hidden layer of the neural network as a task-specific embedding of the claim,"[[    0   804  5697  3760    13     8 24228  1229    38     3     9  2491
     18  9500 25078    26    53    13     8  1988]]"," task-specific embedding of the claim together with all the above evidence about it, which comes from the last hidden layer of the NN","[[ 2491    18  9500 25078    26    53    13     8  1988   544    28    66
      8   756  2084    81    34     6    84   639    45     8   336  5697
   3760    13     8     3 17235     1]]"
eea089baedc0ce80731c8fdcb064b82f584f483a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members, within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers ","[[ 2597    24    33     3 16730    57     3  8689     6  4259    18   413
     26  1014   738    43  1146  1139 14344  1917     6    68    92  6981
   2186     3 24703 15853    24  2450   126   287   277    45  2127   724
      6   441 11562  2597     6  2127  1105    43    46  1936   813  3801
    485    12  4082    28     8   573    31     7     3  8689   738     6
      3  2172    12   126   287   277     1]]"
12ac76b77f22ed3bcb6430bcd0b909441d79751b,44.4444,"scheduled sampling, a supervised learning method, and a teacher forcing method","[[    0  2023    26 17222     6     3     9     3 23313  1036  1573     6
     11     3     9  3145 19060  1573     1]]","TEACHER FORCING (TF), SCHEDULED SAMPLING (SS),  SEQGAN, RANKGAN, LEAKGAN.","[[  332 29027  3316  5652   254  2365    41  9164   201   180 13717  7874
  17717   180 15837   434  2365    41  4256   201   180 23346   517  5033
      6     3 16375 18256  5033     6  9132   188 18256  5033     5     1]]"
4d844c9453203069363173243e409698782bac3f,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
828615a874512844ede9d7f7d92bdc48bb48b18d,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
a71ebd8dc907d470f6bd3829fa949b15b29a0631,23.0769,"if it contains negative or neutral terms, it is considered racist","[[    0     3    99    34  2579  2841    42  7163  1353     6    34    19
   1702 23713     1]]","if it includes  negative utterances, negative generalizations and insults concerning ethnicity, nationality, religion and culture.","[[    3    99    34   963  2841     3  5108   663     7     6  2841   879
   1707     7    11 21548     7  6238 11655   485     6  1157   485     6
   5562    11  1543     5     1]]"
04f72eddb1fc73dd11135a80ca1cf31e9db75578,100.0,278 more annotations,[[    0   204  3940    72 30729     7     1]],278 more annotations,[[  204  3940    72 30729     7     1]]
2210178facc0e7b3b6341eec665f3c098abef5ac,100.0,GRU,[[   0  350 8503    1]],GRU,[[ 350 8503    1]]
3a6559dc6eba7f5abddf3ac27376ba0b9643a908,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
d6d29040e7fafceb188e62afba566016b119b23c,100.0,"PDP-60, WSC-273",[[    0   276  7410 19516     6   549  4112  4949  4552     1]],"PDP-60, WSC-273",[[  276  7410 19516     6   549  4112  4949  4552     1]]
8051927f914d730dfc61b2dc7a8580707b462e56,42.1053,"a sentence-level prediction algorithm, a multimodal neural network model, and a","[[    0     3     9  7142    18  4563 21332 12628     6     3     9  1249
  20226 24228  1229   825     6    11     3     9]]","a sentence-level prediction algorithm, a segment retrieval algorithm and a pipeline segment retrieval algorithm","[[    3     9  7142    18  4563 21332 12628     6     3     9  5508 24515
    138 12628    11     3     9 12045  5508 24515   138 12628     1]]"
2255c36c8c7ed6084da577b480eb01d349f52943,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Such a system would benefit educators by saving time to generate quizzes and tests.,"[[ 3900     3     9   358   133  1656 16530    57  4380    97    12  3806
  20967   776     7    11  3830     5     1]]"
fa2a384a23f5d0fe114ef6a39dced139bddac20e,100.0,903019 references,[[   0 2777 1458 2294 9811    1]],903019 references,[[2777 1458 2294 9811    1]]
95abda842c4df95b4c5e84ac7d04942f1250b571,100.0,"German-English, French-English, and Japanese-English","[[    0  2968    18 26749     6  2379    18 26749     6    11  4318    18
  26749     1]]","German-English, French-English, and Japanese-English","[[ 2968    18 26749     6  2379    18 26749     6    11  4318    18 26749
      1]]"
894c086a2cbfe64aa094c1edabbb1932a3d7c38a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"For sentiment analysis UWB, INF-UFRGS-OPINION-MINING, LitisMind, pkudblab and SVM + n-grams + sentiment and for emotion analysis MaxEnt, SVM, LSTM, BiLSTM and CNN","[[  242  6493  1693   412   518   279     6    27 12619    18 18899 12912
    134    18  4652  3162  9215    18 17684  2365     6 15507   159 12858
     26     6     3   102  2729    26  4605   115    11   180 12623  1768
      3    29    18  5096     7  1768  6493    11    21 13868  1693  5370
  16924     6   180 12623     6     3  7600  2305     6  2106  7600  2305
     11 19602     1]]"
d92f1c15537b33b32bfc436e6d017ae7d9d6c29a,0.0,eight,[[   0 2641    1]],"four different languages: English, Portuguese, Spanish and French",[[  662   315  8024    10  1566     6 21076     6  5093    11  2379     1]]
938cf30c4f1d14fa182e82919e16072fdbcf2a82,12.5,by the number of active users and by the number of new users.,"[[   0   57    8  381   13 1676 1105   11   57    8  381   13  126 1105
     5    1]]",the average volatility of all utterances,[[    8  1348 24868    13    66     3  5108   663     7     1]]
5e997d4499b18f1ee1ef6fa145cadbc018b8dd87,0.0,social media,[[  0 569 783   1]],"Google Images, Reddit Memes Dataset",[[ 1163 15180     6  1624    26   155  1212  2687  2747  2244     1]]
e020677261d739c35c6f075cde6937d0098ace7f,9.6386,The proposed variation of the GAN is the most interesting idea in the last 10 years in ,"[[    0    37  4382 12338    13     8   350  5033    19     8   167  1477
    800    16     8   336   335   203    16     3]]","HDGAN produced relatively better visual results on the CUB and Oxford datasets while AttnGAN produced far more impressive results than the rest on the more complex COCO dataset, In terms of inception score (IS), which is the metric that was applied to majority models except DC-GAN, the results in Table TABREF48 show that StackGAN++ only showed slight improvement over its predecessor, text to image synthesis is continuously improving the results for better visual perception and interception","[[ 3726   517  5033  2546  4352   394  3176   772    30     8   205 10134
     11 10274 17953     7   298   486    17    29   517  5033  2546   623
     72  4423   772   145     8   880    30     8    72  1561  2847  5911
  17953     6    86  1353    13    16  7239  2604    41  4555   201    84
     19     8     3  7959    24    47  2930    12  2942  2250  3578  5795
     18   517  5033     6     8   772    16  4398     3  3221 25582   371
   3707   504    24     3 19814   517  5033 16702   163  3217  9927  4179
    147   165 21654     6  1499    12  1023     3 17282    19 11721  4863
      8   772    21   394  3176  8136    11  1413  7239     1]]"
de12e059088e4800d7d89e4214a3997994dbc0d9,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The system is compared to baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM","[[   37   358    19     3  2172    12 20726  2250    10     3  7600  2305
      6     3 12831    18  4274  3162   567    11  2846    51  2370  7552
     18  7600  2305     1]]"
97757a69d9fc28b260e68284fd300726fbe358d0,0.0,a lexical identifier that consists of a number of n-,"[[    0     3     9     3 30949   138     3  8826    52    24     3  6848
     13     3     9   381    13     3    29    18]]","Bias feature, Token feature, Uppercase feature (y/n), Titlecase feature (y/n), Character trigram feature, Quotation feature (y/n), Word suffix feature (last three characters), POS tag (provided by spaCy utilities), Word shape (provided by spaCy utilities), Word embedding (see Table TABREF26)","[[ 2106     9     7  1451     6   304  2217  1451     6 16147  6701  1451
     41    63    87    29   201 11029  6701  1451    41    63    87    29
    201 20087  6467  5096  1451     6  2415    32  6821  1451    41    63
     87    29   201  4467 10286   226  1451    41  5064   386  2850   201
      3 16034  7860    41 29189    26    57  4174   254    63 17930   201
   4467  2346    41 29189    26    57  4174   254    63 17930   201  4467
  25078    26    53    41  2338  4398     3  3221 25582   371  2688    61
      1]]"
f229069bcb05c2e811e4786c89b0208af90d9a25,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
a93196fb0fb5f8202912971e14552fd7828976db,0.0,WN18 and FB15k,[[    0     3 21170  2606    11     3 15586  1808   157     1]],"Penn Treebank (PTB), end-to-end (E2E) text generation corpus","[[11358  7552  4739    41  6383   279   201   414    18   235    18   989
     41   427   357   427    61  1499  3381 11736   302     1]]"
92d09654011d424cfef5691eec28ee934f88d954,0.0,accuracy,[[   0 7452    1]],"clear mapping between plans and text helps to reduce these issues greatly, the system in BIBREF0 still has 2% errors, work in neural text generation and summarization attempt to address these issues","[[  964 14670   344  1390    11  1499  1691    12  1428   175   807  6891
      6     8   358    16     3  5972 25582   371   632   341    65     3
   5406  6854     6   161    16 24228  1499  3381    11  4505  1635  1707
   3332    12  1115   175   807     1]]"
2555ca85ff6b56bd09c3919aa6b277eb7a4d4631,100.0,Stanford Sentiment Treebank,[[    0 19796  4892  2998   295  7552  4739     1]],Stanford Sentiment Treebank,[[19796  4892  2998   295  7552  4739     1]]
7835d8f578386834c02e2c9aba78a345059d56ca,100.0,No,[[  0 465   1]],No,[[465   1]]
b10632eaa0ca48f86522d8ec38b1d702cb0b8c01,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
13b36644357870008d70e5601f394ec3c6c07048,100.0,No,[[  0 465   1]],No,[[465   1]]
fbe149bd76863575b98fafb3679f411d3d21b4a3,28.5714,translations that were optimal were those that were better in both languages,"[[   0 7314    7   24  130 6624  130  273   24  130  394   16  321 8024
     1]]",translations that were reasonable but not consistent with the labels,[[ 7314     7    24   130  4360    68    59  4700    28     8 11241     1]]
8a0a51382d186e8d92bf7e78277a1d48958758da,9.5238,F1 score of gCAS approach is 0.82,"[[    0   377   536  2604    13     3   122 18678  1295    19  4097  4613
      1]]","For entity  F1 in the movie, taxi and restaurant domain it results in scores of 50.86, 64, and 60.35. For success, it results it outperforms in the movie and restaurant domain with scores of 77.95 and 71.52","[[  242 10409   377   536    16     8  1974     6  9256    11  2062  3303
     34   772    16  7586    13   943     5  3840     6  6687     6    11
   1640     5  2469     5   242  1269     6    34   772    34    91   883
   2032     7    16     8  1974    11  2062  3303    28  7586    13     3
   4013     5  3301    11     3  4450     5  5373     1]]"
c8b9b962e4d40c50150b2f8873a4004f25398464,6.6667,"BLEU does not correlate with human judgment on adequacy, meaning preservation and","[[    0     3  8775 12062   405    59 30575    28   936  7661    30     3
      9   221  2436  4710     6  2530 19368    11]]","High scores to semantically opposite translations/summaries, Low scores to semantically related translations/summaries and High scores to unintelligible translations/summaries.","[[ 1592  7586    12 27632  1427  6401  7314     7    87  4078    51  5414
      6  5586  7586    12 27632  1427  1341  7314     7    87  4078    51
   5414    11  1592  7586    12    73  2429    40  2825  2317  7314     7
     87  4078    51  5414     5     1]]"
74eb363ce30c44d318078cc1a46f8decf7db3ade,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
f62c78be58983ef1d77049738785ec7ab9f2a3ee,0.0,BIBREF0,[[    0     3  5972 25582   371   632     1]],"Kaggle
Subversive Kaggle
Wikipedia
Subversive Wikipedia
Reddit
Subversive Reddit ","[[ 2209   122  3537  3325  2660   757  2209   122  3537 16885  3325  2660
    757 16885  1624    26   155  3325  2660   757  1624    26   155     1]]"
be7f52c4f2bad20e728785a357c383853d885d94,38.0952,"3000 citation instances from two different sources, including 106 papers from various journals, including","[[    0   220  2313     3 13903 10316    45   192   315  2836     6   379
      3 16431  5778    45   796 18178     6   379]]","includes 1,941 citation instances from 186 papers",[[  963  1914  4240   536     3 13903 10316    45     3 25398  5778     1]]
212977344f4bf2ae8f060bdac0317db2d1801724,100.0,No,[[  0 465   1]],No,[[465   1]]
47a30eb4d0d6f5f2ff4cdf6487265a25c1b18fd8,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
4e2e19a58e1f2cc5a7b1bc666c1577922454d8c8,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
d5498d16e8350c9785782b57b1e5a82212dbdaad,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Relative error is less than 5%,[[ 419   40 1528 3505   19  705  145    3 2712    1]]
39cf0b3974e8a19f3745ad0bcd1e916bf20eeab8,11.7647,Psychiatric EHRs of 61 psychiatrists and 58 psychologists from,"[[    0     3 21513    23     9  3929   262 11120     7    13     3  4241
  31925     7    11     3  3449 21782     7    45]]"," a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital, an additional data set for training our vector space model, comprised of EHR texts queried from the Research Patient Data Registry (RPDR)","[[    3     9 11736   302    13 12445  4505    51  5414     6  7209  3358
      6   928  6326  3358     6    11   119  3739  3358    45   204  1755
   1221    16     8   461   382 16729  2305   478    44  3038  2796   152
   4457     6    46  1151   331   356    21   761    69 12938   628   825
      6  9418    26    13   262 11120 14877   238  9889    45     8  2200
  17656  2747 25656    41  6294  3913    61     1]]"
a48cc6d3d322a7b159ff40ec162a541bf74321eb,100.0,Word Sense Induction & Disambiguation,[[    0  4467     3 19003    86  8291     3   184  2678 24621   257     1]],Word Sense Induction & Disambiguation,[[ 4467     3 19003    86  8291     3   184  2678 24621   257     1]]
6d9fbd42b54313cfdc2665809886330f209e9286,0.0,CNMT-CNMT-CNMT-CNMT-CNMT-CNMT-,"[[    0     3 10077  7323    18 10077  7323    18 10077  7323    18 10077
   7323    18 10077  7323    18 10077  7323    18]]","IWSLT16, WMT15, NIST",[[   27  8439  9012  2938     6   549  7323  1808     6   445 13582     1]]
82596190560dc2e2ced2131779730f40a3f3eb8c,47.619,EHRs of 73 psychosis patients from the University of Bath,"[[    0   262 11120     7    13     3  4552  8423     7   159  1221    45
      8   636    13 11247     1]]","EHRs of 183 psychosis patients from McLean Psychiatric Hospital in Belmont, MA","[[  262 11120     7    13     3 24361  8423     7   159  1221    45  3038
   2796   152     3 21513    23     9  3929  4457    16  5622  4662     6
   4800     1]]"
8a1d4ed00d31c1f1cb05bc9d5e4f05fe87b0e5a4,100.0,Authors,[[    0 10236     7     1]],Authors,[[10236     7     1]]
8720c096c8b990c7b19f956ee4930d5f2c019e2b,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
b799936d6580c0e95102027175d3fe184f0ee253,5.0633,Using a combination of different graph models,[[   0    3 3626    3    9 2711   13  315 8373 2250    1]],"The given corpus is traversed, and for each element INLINEFORM6 , its successor INLINEFORM7 , together with a given element, forms a directed edge INLINEFORM8 . Finally, such edges are weighted according to the number of times they appear in a given corpus. Thus the graph, constructed after traversing a given corpus, consists of all local neighborhoods (order one), merged into a single joint structure. Global contextual information is potentially kept intact (via weights), even though it needs to be detected via network analysis","[[   37   787 11736   302    19  5187    15    26     6    11    21   284
   3282  3388 20006 24030   948     3     6   165 22261  3388 20006 24030
    940     3     6   544    28     3     9   787  3282     6  2807     3
      9  6640  3023  3388 20006 24030   927     3     5  4213     6   224
   9804    33  1293    15    26  1315    12     8   381    13   648    79
   2385    16     3     9   787 11736   302     5  5309     8  8373     6
   8520   227  5187    53     3     9   787 11736   302     6     3  6848
     13    66   415 17383    41  9397    80   201     3 21726   139     3
      9   712  4494  1809     5  3699 28131   251    19  6149  2697 17722
     41  5907  1293     7   201   237   713    34   523    12    36 14619
   1009  1229  1693     1]]"
2d274c93901c193cf7ad227ab28b1436c5f410af,0.0,"MARCO 2.1, MARCO 4.0","[[    0     3 13845  5911     3 14489     6     3 13845  5911     3 15021
      1]]","BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D","[[ 2106  4296   371     6  9509  1336     7  6615     3 23008     6   180
     18  9688  1220 17664   357   134     6   272 24203  1220 31922    18
   7861  9688     6  6185   127  1220  2823   517     6   584  9978     6
   3396 16986  4630   345     6   283 13201  7381  1220  7400  4666     6
   1193   956  9688     6   391  9320  1220   188   357   308     1]]"
c06b5623c35b6fa7938340fa340269dc81d061e1,0.0,WN18 and FB15k,[[    0     3 21170  2606    11     3 15586  1808   157     1]],"noun-noun subset of bless, leds BIBREF13, bless, wbless, bibless, hyperlex BIBREF20","[[  150   202    18 15358    29   769  2244    13 12606     6  2237     7
      3  5972 25582   371  2368     6 12606     6     3   210   115   924
      6 26996     7     7     6  6676   109   226     3  5972 25582   371
   1755     1]]"
0d6d5b6c00551dd0d2519f117ea81d1e9e8785ec,100.0,Google's machine translation system (GMT),[[   0 1163   31    7 1437 7314  358   41 7381  382   61    1]],Google's machine translation system (GMT),[[1163   31    7 1437 7314  358   41 7381  382   61    1]]
8fa7011e7beaa9fb4083bf7dd75d1216f9c7b2eb,100.0,No,[[  0 465   1]],No,[[465   1]]
6333845facb22f862ffc684293eccc03002a4830,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
9f5507a8c835c4671020d7d310fff2930d44e75a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Danish/Swedish (da/sv), Russian/Bulgarian (ru/bg), Finnish/Hungarian (fi/hu), Spanish/Portuguese (es/pt)","[[23124    87   134  1123    26  1273    41    26     9    87     7   208
    201  4263    87   279    83   122  6855    41    52    76    87   115
    122   201 28124    87   566   425  6855    41    89    23    87   107
     76   201  5093    87 14714    76 15991    15    41    15     7    87
    102    17    61     1]]"
bab8c69e183bae6e30fc362009db9b46e720225e,11.7647,LSTM with a corresponding structure and a corresponding semantic role,"[[    0     3  7600  2305    28     3     9     3  9921  1809    11     3
      9     3  9921 27632  1075     1]]",Marcheggiani and Titov (2017) and Cai et al. (2018),"[[ 1332    15 15406  2738    11  2262   235   208     3 26224    11  1336
     23     3    15    17   491     5 28068     1]]"
7fbbe191f4d877cc6af89c00fcfd5b5774d2a2bb,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
32c149574edf07b1a96d7f6bc49b95081de1abd2,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
be074c880263f56e0d4a8f42d9a95d2d77ac2280,0.0,ObamaCare' tweets,"[[    0     3     2   667   115   265     9  6936    15    31 10657     7
      1]]",landing pages of URLs,[[9501 1688   13 8889    7    1]]
3371d586a3a81de1552d90459709c57c0b1a2594,8.3333,"a graded criteria for faithfulness, a graded one for plausibility and","[[    0     3     9  2769    26  6683    21 13855   655     6     3     9
   2769    26    80    21  9564   302 11102    11]]","Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks., Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves.","[[    3 25087  2250    11  4145    10    37  1952    41     9     7  9954
   6649    61    13 13855   655    44     8   593    13   806  2250    11
   4145     5     6     3 25087  3785   628    10    37  1952    13 13855
    655    44     8   593    13   769  6633     7    13     8  3785   628
      6   224    38 17383    13  1126  3785     7     6    42 22166  3785
      7  1452     5     1]]"
e93b4a15b54d139b768d5913fb5fd1aed8ab25da,100.0,manually cleaned human-produced utterances,[[    0 12616 12631   936    18 29462     3  5108   663     7     1]],manually cleaned human-produced utterances,[[12616 12631   936    18 29462     3  5108   663     7     1]]
aa54e12ff71c25b7cff1e44783d07806e89f8e54,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The health benefits of alcohol consumption are more limited than previously thought, researchers say","[[  37  533 1393   13 4798 5962   33   72 1643  145 3150  816    6 4768
   497    1]]"
b8f711179a468fec9a0d8a961fb0f51894af4b31,100.0,CNN,[[    0 19602     1]],CNN,[[19602     1]]
25b2ae2d86b74ea69b09c140a41593c00c47a82b,100.0,using Amazon Mechanical Turk using simulated environments with topological maps,"[[    0   338  2536 24483 23694   338     3 31126  8258    28   420  4478
   8111     1]]",using Amazon Mechanical Turk using simulated environments with topological maps,"[[  338  2536 24483 23694   338     3 31126  8258    28   420  4478  8111
      1]]"
8d1f9d3aa2cc2e2e58d3da0f5edfc3047978f3ee,0.0,F1 score,[[   0  377  536 2604    1]],"To have an estimation about human performance in each metric, we iteratively treat every reference sentence in dev/test data as the prediction to be compared with all references (including itself).","[[  304    43    46 22781    81   936   821    16   284     3  7959     6
     62    34    49  1528   120  2665   334  2848  7142    16    20   208
     87  4377   331    38     8 21332    12    36     3  2172    28    66
   9811    41  5751  1402   137     1]]"
5a230fe4f0204bf2eebc0e944cf8defaf33d165c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"One of the several formats into which FHIR can be serialized is RDF, there is the potential for a slight mismatch between the models","[[  555    13     8   633 10874   139    84   377  7094   448    54    36
  10501  1601    19   391 10665     6   132    19     8  1055    21     3
      9  9927  1817 19515   344     8  2250     1]]"
7aba5e4483293f5847caad144ee0791c77164917,100.0,WikiHop,[[   0 2142 2168 4489  102    1]],WikiHop,[[2142 2168 4489  102    1]]
ed11b4ff7ca72dd80a792a6028e16ba20fccff66,100.0,they are available in the Visual Genome dataset,[[    0    79    33   347    16     8 10893 21849   526 17953     1]],they are available in the Visual Genome dataset,[[   79    33   347    16     8 10893 21849   526 17953     1]]
eeb6e0caa4cf5fdd887e1930e22c816b99306473,11.1111,"Grave:18, annotated on the back of the fastText word embedd","[[    0 15199    15    10  2606     6    46  2264   920    30     8   223
     13     8  1006 13598    17  1448 25078    26]]",The contexts are manually labelled with WordNet senses of the target words,"[[   37  2625     7    33 12616     3 29506    28  4467  9688  1254     7
     13     8  2387  1234     1]]"
8958465d1eaf81c8b781ba4d764a4f5329f026aa,28.5714,"neighborhood mean square error (MD), WEAT statistic (WEAT), WEAT cosine","[[    0  5353  1243  2812  3505    41 11731   201   549 21613 19720    41
    518 21613   201   549 21613   576     7   630]]","RIPA, Neighborhood Metric, WEAT",[[  391 25981     6 29852  1212  3929     6   549 21613     1]]
944d5dbe0cfc64bf41ea36c11b1d378c408d40b8,100.0,x-vector,[[   0    3  226   18  162 5317    1]],x-vector,[[   3  226   18  162 5317    1]]
061682beb3dbd7c76cfa26f7ae650e548503d977,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
537b2d7799124d633892a1ef1a485b3b071b303d,100.0,WNLaMPro dataset,[[    0     3 21170  3612   329  3174 17953     1]],WNLaMPro dataset,[[    3 21170  3612   329  3174 17953     1]]
02e4bf719b1a504e385c35c6186742e720bcb281,0.0,using discourse relations,[[    0   338 22739  5836     1]],cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity,"[[ 1137  4689    10   321   984    16     8  4689   225    43     8   337
      3  9618   485   117 19093  4689    10   984   225    43  6401     3
   9618   485     1]]"
21c104d14ba3db7fe2cd804a191f9e6258208235,100.0,PAR score,[[    0 17917  2604     1]],PAR score,[[17917  2604     1]]
2376c170c343e2305dac08ba5f5bda47c370357f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. , Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context., Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states., Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. ","[[20230  8663    10    62 18639    15    26  1111   251    16 14465    45
      8  1620     6   379  2282     6   486 10559     6    11  6233  3303
      7    41    88    60 10245    62   564     8   386  3303     7    38
      3 25430  3303     7   137    37    29     6    62   261     8 12810
    251    13 12311    16     3 25430  3303     7    12   918     8 12810
   3501     5     3     6 17916 11946    10     3     9  1249    18 22999
   1288  9877    47   876     3   390    30     8  3501     5    37  4689
    640  3303     7    19  9534    16   192  1155     5   555    19    12
  27354   192  8874    24  9586  1084   284   119     5    37   119    19
     12   169     3     9  9256    42 12810    12 16606   344   192  8874
     16     3 25430  3303     7  2799    16     8  2625     5     6  5267
  10384  6767    10   274     8  4727   331  1232  3511     6    62   831
      8  2765    12   143     3     9   422   381    13  7478     7    11
   1891   135  3160    81     8  7478   463     5    37    29     6   168
     18    17 10761  2765   130     3 13804    12 20379    15  1315    12
      8   787  1766     5    37  2765   130    92  1380    12    46  2264
    342   321  1139  2315    11   358  2315     5     6  5267 10384   389
   2264   257    10    62   261   128  2219    12  3269    46  2264   342
   7478  6775  1315    12  1139  2315     6   358  2315     6    11  7478
  27771     5     1]]"
5fb348b2d7b012123de93e79fd46a7182fd062bd,0.0,"WN18, FB15k",[[    0     3 21170  2606     6     3 15586  1808   157     1]],"NELL-One, Wiki-One",[[  445 12735    18 10723     6  2142  2168    18 10723     1]]
516626825e51ca1e8a3e0ac896c538c9d8a747c8,100.0,No,[[  0 465   1]],No,[[465   1]]
4547818a3bbb727c4bb4a76554b5a5a7b5c5fedb,11.1111,10k sentences,[[    0   335   157 16513     1]],"Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development","[[ 4017   331    28     3 27904  2313     6  2775  2313     6  1283  2313
      6   460  2313     6   335  2313    11     3 12814 16513     6    11
   6374  4608 16513    21   606     1]]"
17de58c17580350c9da9c2f3612784b432154d11,0.0,SVM,[[    0   180 12623     1]],multi-class Naive Bayes,[[1249   18 4057 1823  757 2474   15    7    1]]
4cab33c8dd46002e0ccafda3916b37366a24a394,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
af073d84b8a7c968e5822c79bef34a28655886de,10.5263,a corresponding improvement over the SOTA was in the accuracy of prediction.,"[[    0     3     9     3  9921  4179   147     8   180 27976    47    16
      8  7452    13 21332     5     1]]","1.34 and 1.12 BLEU score on top of the strong baselines in BIBREF3, perplexity scores are also better, On the Google Production dataset, our model achieved 1.01 higher test BLEU score","[[ 1300  3710    11  1300  2122     3  8775 12062  2604    30   420    13
      8  1101 20726     7    16     3  5972 25582   371  6355   399  9247
    485  7586    33    92   394     6   461     8  1163 11114 17953     6
     69   825  5153  1300  4542  1146   794     3  8775 12062  2604     1]]"
6c8bd7fa1cfb1b2bbeb011cc9c712dceac0c8f06,0.0,SQuAD,[[   0  180 5991 6762    1]],"Our baseline model is composed of the following typical components: word embedding, input encoder, alignment, aggregation, and prediction.","[[  421 20726   825    19 10431    13     8   826  4541  3379    10  1448
  25078    26    53     6  3785 23734    52     6 14632     6     3 31761
     23   106     6    11 21332     5     1]]"
f3d0e6452b8d24b7f9db1fd898d1fbe6cd23f166,100.0,No,[[  0 465   1]],No,[[465   1]]
4d062673b714998800e61f66b6ccbf7eef5be2ac,19.3548,Moral Choice Machine (MC) is a machine learning tool that automatically learns moral choices from,"[[    0 28466 13745  5879    41  3698    61    19     3     9  1437  1036
   1464    24  3269   669     7  4854  3703    45]]",Moral Choice Machine computes the cosine similarity in a sentence embedding space of an arbitrary action embedded in question/answer pairs,"[[28466 13745  5879 29216     7     8   576     7   630  1126   485    16
      3     9  7142 25078    26    53   628    13    46     3 26968  1041
  13612    16   822    87  3247  3321 14152     1]]"
6583e8bfa7bcc3a792a90b30abb316e6d423f49b,4.878,multilingual,[[    0  1249 25207     1]],"Direct source$\rightarrow $target: A standard NMT model trained on given source$\rightarrow $target, Multilingual: A single, shared NMT model for multiple translation directions, Many-to-many: Trained for all possible directions among source, target, and pivot languages, Many-to-one: Trained for only the directions to target language","[[ 7143  1391  3229     2  3535  6770  1514 24315    10    71  1068   445
   7323   825  4252    30   787  1391  3229     2  3535  6770  1514 24315
      6  4908 25207    10    71   712     6  2471   445  7323   825    21
   1317  7314  7943     6  1404    18   235    18   348    63    10   332
  10761    21    66   487  7943   859  1391     6  2387     6    11 16959
   8024     6  1404    18   235    18   782    10   332 10761    21   163
      8  7943    12  2387  1612     1]]"
e74ba39c35af53d3960be5a6c86eddd62cef859f,0.0,LSTM,[[   0    3 7600 2305    1]],IR methods perform better than the best neural models,[[    3  5705  2254  1912   394   145     8   200 24228  2250     1]]
626873982852ec83c59193dd2cf73769bf77b3ed,100.0,"document-level accuracy, precision, recall, F-score","[[    0  1708    18  4563  7452     6 11723     6  7881     6   377    18
      7  9022     1]]","document-level accuracy, precision, recall, F-score","[[ 1708    18  4563  7452     6 11723     6  7881     6   377    18     7
   9022     1]]"
082bc58e1a2a65fc1afec4064a51e4c785674fd7,73.6842,Long-short Term Hybrid Memory (LSTHM) is a memory,"[[    0  3230    18     7 14184     3 11679  5555  2160    26 19159    41
   7600  4611   329    61    19     3     9  2594]]",Long-short Term Hybrid Memory (LSTHM) is an extension of the Long-short Term Memory (LSTM) ,"[[ 3230    18     7 14184     3 11679  5555  2160    26 19159    41  7600
   4611   329    61    19    46  4924    13     8  3230    18     7 14184
      3 11679 19159    41  7600  2305    61     1]]"
ef7b62a705f887326b7ebacbd62567ee1f2129b3,0.0,"Representative baselines are: BIBREF0, BIBREF3","[[    0   419 12640  1528 20726     7    33    10     3  5972 25582   371
    632     6     3  5972 25582   371   519     1]]","Siamese neural network consisting of an embedding layer, a LSTM layer and a feed-forward layer with ReLU activations","[[  925     9  2687    15 24228  1229  5608    53    13    46 25078    26
     53  3760     6     3     9     3  7600  2305  3760    11     3     9
   3305    18 26338  3760    28   419  9138  5817  1628     1]]"
f5eac66c08ebec507c582a2445e99317a83e9ebe,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
922f1b740f8b13fdc8371e2a275269a44c86195e,100.0,No,[[  0 465   1]],No,[[465   1]]
2df2f6e4efd19023434c84f5b4f29a2f00bfc9fb,0.0,TDSM,[[    0     3 10494  4212     1]],"bag of words, tf-idf, bag-of-means","[[2182   13 1234    6    3   17   89   18   23   26   89    6 2182   18
   858   18  526 3247    1]]"
56a3c9bd74c6573abce3805177cdebf941db0b71,100.0,manually reviewed,[[    0 12616  9112     1]],manually reviewed,[[12616  9112     1]]
ffa4d4bfb226382ca4ecde65ecdc44a3d9e0ce81,100.0,Paraphrase Identification,[[    0  4734 27111 31474     1]],Paraphrase Identification,[[ 4734 27111 31474     1]]
5b1cd21936aeec85233c978ba8d7282931522a3a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The number of fake news that have a duplicate in the training dataset are 1018 whereas, the number of articles with genuine content that have a duplicate article in the training set is 322.","[[   37   381    13  9901  1506    24    43     3     9 19197    16     8
    761 17953    33   335  2606     3 10339     6     8   381    13  2984
     28  7746   738    24    43     3     9 19197  1108    16     8   761
    356    19  3538  4416     1]]"
97159b8b1ab360c34a1114cd81e8037474bd37db,100.0,No,[[  0 465   1]],No,[[465   1]]
e9f868f22ae70c7681c28228b6019e155013f8d6,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
de2d33760dc05f9d28e9dabc13bab2b3264cadb7,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
8e9de181fa7d96df9686d0eb2a5c43841e6400fa,2.9851,Yes,[[   0 2163    1]],"Yes, CRWIZ has been used for data collection and its initial use resulted in 145 dialogues. The average time taken for the task was close to the estimate of 10 minutes, 14 dialogues (9.66%) resolved the emergency in the scenario, and these dialogues rated consistently higher in subjective and objective ratings than those which did not resolve the emergency. Qualitative results showed that participants believed that they were interacting with an automated assistant.","[[ 2163     6     3  4545   518 20091    65   118   261    21   331  1232
     11   165  2332   169   741    15    26    16     3 20987  7478     7
      5    37  1348    97  1026    21     8  2491    47   885    12     8
   7037    13   335   676     6   968  7478     7 14156     5  3539  6210
  13803     8  3583    16     8  8616     6    11   175  7478     7     3
   4094  8182  1146    16 24242    11  5997  9712   145   273    84   410
     59  7785     8  3583     5 25388    17  1528   772  3217    24  3008
   6141    24    79   130     3 23396    28    46 10069  6165     5     1]]"
471d624498ab48549ce492ada9e6129da05debac,0.0,"LSTM, LSTM+CNN, LSTM+CNN, ","[[    0     3  7600  2305     6     3  7600  2305  1220   254 17235     6
      3  7600  2305  1220   254 17235     6     3]]","Concat
Turn
Gate
Action Copy
Tree Copy
SQL Attn
Concat + Action Copy
Concat + Tree Copy
Concat + SQL Attn
Turn + Action Copy
Turn + Tree Copy
Turn + SQL Attn
Turn + SQL Attn + Action Copy","[[ 1193  2138  5527 11118  6776 20255  7552 20255 12558   486    17    29
   1193  2138  1768  6776 20255  1193  2138  1768  7552 20255  1193  2138
   1768 12558   486    17    29  5527  1768  6776 20255  5527  1768  7552
  20255  5527  1768 12558   486    17    29  5527  1768 12558   486    17
     29  1768  6776 20255     1]]"
3f9ef59ac06db3f99b8b6f082308610eb2d3626a,0.0,"SVM, JavaScript, and JavaScript",[[    0   180 12623     6 10318 18255     6    11 10318 18255     1]],"langid.py library, encoder-decoder EquiLID system, GRU neural network LanideNN system, CLD2, CLD3","[[12142    23    26     5   102    63  3595     6 23734    52    18   221
   4978    52 21263   434  4309   358     6   350  8503 24228  1229  9144
   1599 17235   358     6   205  9815  4482   205  9815   519     1]]"
c33d0bc5484c38de0119c8738ffa985d1bd64424,100.0,monolingual,[[    0  7414 25207     1]],monolingual,[[ 7414 25207     1]]
0c29d08f766b06ceb2421aa402e71a2d65a5a381,0.0,SVM,[[    0   180 12623     1]],Convolutional Neural Network (CNN),[[ 1193 24817   138  1484  9709  3426    41   254 17235    61     1]]
a49832c89a2d7f95c1fe6132902d74e4e7a3f2d0,0.0,Parallel English tutorials collected by BIBREF15 and BIBREF16,"[[    0 27535  1566  7114     7  4759    57     3  5972 25582   371  1808
     11     3  5972 25582   371  2938     1]]",CoNLL 2014,[[  638   567 10376  1412     1]]
f6f8054f327a2c084a73faca16cf24a180c094ae,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
b6ffa18d49e188c454188669987b0a4807ca3018,0.0,LSTM,[[   0    3 7600 2305    1]],SPARQL,[[6760 4280 2247  434    1]]
aceac4ad16ffe1af0f01b465919b1d4422941a6b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],we provide an extensive analysis of the state-of-the-art model,"[[  62  370   46 3616 1693   13    8  538   18  858   18  532   18 1408
   825    1]]"
938688871913862c9f8a28b42165237b7324e0de,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
3c362bfa11c60bad6c7ea83f8753d427cda77de0,100.0,They think it will help human TCM practitioners make prescriptions.,"[[    0   328   317    34    56   199   936   332  5518 14194   143  7744
      7     5     1]]",They think it will help human TCM practitioners make prescriptions.,"[[  328   317    34    56   199   936   332  5518 14194   143  7744     7
      5     1]]"
ad67ca844c63bf8ac9fdd0fa5f58c5a438f16211,66.6667,WSJ audio data,[[   0    3 8439  683 2931  331    1]],1000 hours of WSJ audio data,[[5580  716   13    3 8439  683 2931  331    1]]
1083ec9a2a33f7fe2b6b51bbcebd2d9aec4b4de2,23.5294,Unanswerable questions are more likely to be answerable if answerable if answer,"[[   0  597 3247 3321  179  746   33   72  952   12   36 1525  179    3
    99 1525  179    3   99 1525]]",is to minimize the negative likelihood of the aligned unanswerable question $\tilde{q}$ given the answerable question $q$ and its corresponding paragraph $p$ that contains the answer ,"[[   19    12 10558     8  2841 17902    13     8  7901    15    26    73
   3247  3321   179   822  1514     2    17   173   221     2  1824     2
   3229   787     8  1525   179   822  1514  1824  3229    11   165     3
   9921  8986  1514   102  3229    24  2579     8  1525     1]]"
923b12c0a50b0ee22237929559fad0903a098b7b,100.0,Plackett-Luce Model for SMT Reranking,"[[    0  8422    75 12922    18 11748    15  5154    21   180  7323   419
   6254    53     1]]",Plackett-Luce Model for SMT Reranking,"[[ 8422    75 12922    18 11748    15  5154    21   180  7323   419  6254
     53     1]]"
c1f4d632da78714308dc502fe4e7b16ea6f76f81,100.0,French-English,[[    0  2379    18 26749     1]],French-English,[[ 2379    18 26749     1]]
13d92cbc2c77134626e26166c64ca5c00aec0bf5,0.0,"LSTMs, LSTMs with a corresponding sentence-level LS","[[   0    3 7600 2305    7    6    3 7600 2305    7   28    3    9    3
  9921 7142   18 4563    3 7600]]","HotspotQA: Yang, Ding, Muppet
Fever: Hanselowski, Yoneda, Nie","[[ 5396     7  3013 23008    10 21078     6   309    53     6  4159  6811
     17   377  3258    10 10001    15    40 21180     6     3   476   782
     26     9     6 17584     1]]"
010e3793eb1342225857d3f95e147d8f8467192a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The Dutch section consists of 2,333,816 sentences and 53,487,257 words., The SONAR500 corpus consists of more than 500 million words obtained from different domains.","[[   37 10098  1375     3  6848    13  3547 23360     6   927  2938 16513
     11 12210     6   591  4225     6   357  3436  1234     5     6    37
      3 10466  4280  2560 11736   302     3  6848    13    72   145  2899
    770  1234  5105    45   315  3303     7     5     1]]"
14421b7ae4459b647033b3ccba635d4ba7bb114b,0.0,by analyzing the sentiments of the users over social media.,"[[    0    57     3 19175     8  6493     7    13     8  1105   147   569
    783     5     1]]",experts in Washington Post,[[2273   16 2386 1844    1]]
d7aed39c359fd381495b12996c4dfc1d3da38ed5,0.0,using pseudo-parallel examples,[[    0   338 22726    18  1893 13701    40  4062     1]]," applying the rule INLINEFORM4 to a set of natural language questions INLINEFORM5, both models are improved following the back-translation protocol that target sequences should follow the real data distribution","[[ 6247     8  3356  3388 20006 24030   591    12     3     9   356    13
    793  1612   746  3388 20006 24030 11116   321  2250    33  3798   826
      8   223    18  7031  6105 10015    24  2387  5932     7   225  1130
      8   490   331  3438     1]]"
ba56afe426906c4cfc414bca4c66ceb4a0a68121,11.1111,"Festival, TTS-R, and Open-End",[[   0 3397    6  332 4578   18  448    6   11 2384   18 8532   26    1]],"Datasets used are Celex (English, Dutch), Festival (Italian), OpenLexuque (French), IIT-Guwahati (Manipuri), E-Hitz (Basque)","[[ 2747  2244     7   261    33 13136   226    41 26749     6 10098   201
   3397    41   196    17     9  9928   201  2384   434   994    76   835
     41   371    60  5457   201    27  3177    18  9105 17771   144    23
     41  7296    23   102   459   201   262    18   566  5615    41 14885
    835    61     1]]"
fd5412e2784acefb50afc3bfae1e087580b90ab9,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
08b87a90139968095433f27fc88f571d939cd433,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"As the baseline, we simply judge the input token as IOCs on the basis of the spelling features described in BIBREF12","[[  282     8 20726     6    62   914  5191     8  3785 14145    38    27
   5618     7    30     8  1873    13     8 19590   753  3028    16     3
   5972 25582   371  2122     1]]"
f8d32088d17b32b0c877d59965b35c4f51f0ceea,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
2f4acd34eb2d09db9b5ad9b1eb82cb4a88c13f5b,23.5294,a news dataset from the S&P 500 Index Funds (S&P 500 Index,"[[    0     3     9  1506 17953    45     8   180   184   345  2899 11507
   3563     7    41   134   184   345  2899 11507]]",the public financial news dataset released by BIBREF4,"[[    8   452   981  1506 17953  1883    57     3  5972 25582   371   591
      1]]"
f809fd0d3acfaccbe6c8abb4a9d951a83eec9a32,100.0,labeled by experts,[[   0 3783   15   26   57 2273    1]],labeled by experts,[[3783   15   26   57 2273    1]]
cd1034c183edf630018f47ff70b48d74d2bb1649,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
579941de2838502027716bae88e33e79e69997a6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.
For number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. 
For date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1.","[[  242   712    18     7  2837   746     6     8  4382     3 22492  5042
     18   134 16892  6762  1172   821    13     8   283  2305  8544 15599
  20726    21     3 14489     3  6037    11  1300  3769   377  5411   242
    381   686   822     6   283  2305  8544 15599 20726    43  4179   147
      3 22492  5042    18   134 16892  6762    21  6180  2596     3  6037
     11  3547  3916   377  5411   242   833   822     6     3 22492  5042
     18   134 16892  6762    43  6867    16  3547  4305     3  6037    68
    283  2305  8544 15599    43  4179    13  6464  3288   377  5411     1]]"
fb56743e942883d7e74a73c70bd11016acddc348,100.0,BABEL speech corpus,[[    0   272  5359  3577  5023 11736   302     1]], BABEL speech corpus ,[[  272  5359  3577  5023 11736   302     1]]
a5418e4af99a2cbd6b7a2b8041388a2d01b8efb2,8.5106,by comparing the generated text quality with the reconstruction and the prior.,"[[    0    57     3 14622     8  6126  1499   463    28     8 20532    11
      8  1884     5     1]]","Loss analysis. To conduct a more thorough evaluation, we further investigate model behaviours in terms of both reconstruction loss and KL loss, as shown in Figure FIGREF14. These plots were obtained based on the E2E training set using the inputless setting.","[[ 3144     7  1693     5   304  3498     3     9    72  9517  5002     6
     62   856  9127   825  7916     7    16  1353    13   321 20532  1453
     11   480   434  1453     6    38  2008    16  7996 11376  4386   371
   2534     5   506  5944     7   130  5105     3   390    30     8   262
    357   427   761   356   338     8  3785   924  1898     5     1]]"
f10325d022e3f95223f79ab00f8b42e3bb7ca040,0.0,"Using a set of RST-based features, we can achieve state-of-","[[   0    3 3626    3    9  356   13  391 4209   18  390  753    6   62
    54 1984  538   18  858   18]]",They derive entity grid with grammatical relations and RST discourse relations and concatenate them with pooling vector for the char-bigrams before feeding to the resulting vector to the softmax layer.,"[[  328    74   757 10409  8634    28     3  5096  4992   138  5836    11
    391  4209 22739  5836    11   975  2138    35   342   135    28  2201
     53 12938    21     8     3  4059    18 12911  2375     7   274 10795
     12     8     3  5490 12938    12     8  1835  9128  3760     5     1]]"
69ef007fc131b04b5b71b0b446db2f77f434f1b3,12.5,Twitter users' tweets,[[    0  3046  1105    31 10657     7     1]],Tesla and Ford are investigated on how Twitter sentiment could impact the stock price,"[[19939    11  5222    33 18277    30   149  3046  6493   228  1113     8
   1519   594     1]]"
4db3c2ca6ddc87209c31b20763b7a3c1c33387bc,0.0,Unanswerable,[[   0  597 3247 3321  179    1]], from a collection of advanced persistent threats (APT) reports which are published from 2008 to 2018,"[[   45     3     9  1232    13  2496 15803 11262    41  2965   382    61
   2279    84    33  1790    45  2628    12   846     1]]"
b3238158392684a5a6b62a7eabaa2a10fbecf3e6,0.0,predicting the word given its context,[[    0     3 29856     8  1448   787   165  2625     1]],"most relevant content of the website, including all subsites",[[ 167 2193  738   13    8  475    6  379   66  769 3585    7    1]]
2cfcc5864a30259fd35f1cc035fab956802c1c5b,0.0,vocabulary prediction task,[[    0 19067 21332  2491     1]],"Language Modeling (LM), PTB BIBREF25 , WikiText-103 BIBREF26 and One-Billion Word benchmark BIBREF27 datasets, neural machine translation (NMT), WMT 2016 English-German dataset","[[10509  5154    53    41 11160   201   276  9041     3  5972 25582   371
   1828     3     6  2142  2168 13598    17    18 17864     3  5972 25582
    371  2688    11   555    18   279 14916  4467 15705     3  5972 25582
    371  2555 17953     7     6 24228  1437  7314    41   567  7323   201
    549  7323  1421  1566    18 24518 17953     1]]"
bd7a95b961af7caebf0430a7c9f675816c9c527f,0.0,"WSJ15, WSJ15, WSJ15 and WSJ15","[[   0    3 8439  683 1808    6    3 8439  683 1808    6    3 8439  683
  1808   11    3 8439  683 1808]]","DSTC2, M2M-sim-M, M2M-sim-R","[[   3 3592 3838 4482  283  357  329   18    7  603   18  329    6  283
   357  329   18    7  603   18  448    1]]"
4d4550533edb19c38cb876b1640e62e34e2b88e0,100.0,hatespeechdata.com,[[    0  5591     7   855 10217  6757     5   287     1]],hatespeechdata.com,[[ 5591     7   855 10217  6757     5   287     1]]
160e6d2fc6e04bb0b4ee8d59c06715355dec4a17,18.1818,F1 score of 0.83,[[   0  377  536 2604   13 4097 4591    1]],the best performing model obtained an accuracy of 0.86,[[   8  200 5505  825 5105   46 7452   13 4097 3840    1]]
6f2f304ef292d8bcd521936f93afeec917cbe28a,14.8148,"Compared to the previous best, the proposed approaches achieve a corresponding improvement of 0.8","[[    0     3 25236    12     8  1767   200     6     8  4382  6315  1984
      3     9     3  9921  4179    13     3 22384]]",It eliminates non-termination in some models fixing for some models up to 6% of non-termination ratio.,"[[   94  6312     7   529    18  6544   257    16   128  2250 16618    21
    128  2250    95    12     3  6370    13   529    18  6544   257  5688
      5     1]]"
ff2bcf2d8ffee586751ce91cf15176301267b779,0.0,a city dialect is defined as a continuum that gradually connect places with slightly different,"[[    0     3     9   690 28461    19  4802    38     3     9 23945    51
     24 11556  1979 32134  1747    28  3300   315]]",Lexicon of the cities tend to use most forms of a particular concept,"[[17546    23  1018    13     8  3119  2134    12   169   167  2807    13
      3     9  1090  2077     1]]"
ef8099e2bc0ac4abc4f8216740e80f2fa22f41f6,80.0,"Arabic, German, Portuguese, Russian and Swedish",[[    0 19248     6  2968     6 21076     6  4263    11 16531     1]],"Spanish, English, Italian, Arabic, German, Portuguese, Russian and Swedish","[[ 5093     6  1566     6  4338     6 19248     6  2968     6 21076     6
   4263    11 16531     1]]"
f1b738a7f118438663f9d77b4ccd3a2c4fd97c01,0.0,accuracy,[[   0 7452    1]],"precision , recall , Hamming loss, micro averaged precision and recall ","[[11723     3     6  7881     3     6  5845    51    53  1453     6  2179
   1348    26 11723    11  7881     1]]"
4ade72bfa28bd1f6b75cc7fa687fa634717782f2,12.5,"Compared to only training QA, the two models achieve significantly better QA performance.","[[    0     3 25236    12   163   761     3 23008     6     8   192  2250
   1984  4019   394     3 23008   821     5     1]]",We see that A-gen performance improves significantly with the joint model: both F1 and EM increase by about 10 percentage points. ,"[[ 101  217   24   71   18  729  821 1172    7 4019   28    8 4494  825
    10  321  377  536   11    3 6037  993   57   81  335 5294  979    5
     1]]"
83f14af3ccca4ab9deb4c6d208f624d1e79dc7eb,10.0,the ensemble that performs best is the one who uses the entity embeddings most often,"[[    0     8  8784    24  1912     7   200    19     8    80   113  2284
      8 10409 25078    26    53     7   167   557]]",Answer with content missing: (Table 2) CONCAT ensemble,[[11801    28   738  3586    10    41 20354  9266  8472 18911  8784     1]]
f7d67d6c6fbc62b2953ab74db6871b122b3c92cc,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"It is an order of magnitude more efficient in terms of training time., his model requires pre-training and mutual-learning and requires days of training time, whereas the simple architecture we propose requires on the order of an hour","[[   94    19    46   455    13 20722    72  2918    16  1353    13   761
     97     5     6   112   825  2311   554    18 13023    11  8543    18
  20779    11  2311   477    13   761    97     6     3 10339     8   650
   4648    62  4230  2311    30     8   455    13    46  1781     1]]"
18ad60f97f53af64cb9db2123c0d8846c57bfa4a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"word embeddings to generate a new feature, i.e., summarizing a local context","[[ 1448 25078    26    53     7    12  3806     3     9   126  1451     6
      3    23     5    15     5     6  4505  1635  2610     3     9   415
   2625     1]]"
312417675b3dc431eb7e7b16a917b7fed98d4376,35.2941,"causal mapping method, a combination of statistical and symbolic methods, a combination of statistical and","[[    0 31161 14670  1573     6     3     9  2711    13 11775    11 24981
   2254     6     3     9  2711    13 11775    11]]",Axelrod's causal mapping method,[[   71   226    15    40  9488    31     7 31161 14670  1573     1]]
a83a351539fb0b6acb5bdee32323dd924f4fd1e7,100.0,100,[[  0 910   1]],100 ,[[910   1]]
fa9df782d743ce0ce1a7a5de6a3de226a7e423df,33.3333,"English, Chinese",[[   0 1566    6 2830    1]],"The languages considered were English, Chinese, German, Russian, Arabic, Spanish, French","[[   37  8024  1702   130  1566     6  2830     6  2968     6  4263     6
  19248     6  5093     6  2379     1]]"
b42323d60827ecf0d9e478c9a31f90940cfae975,0.0,5700 sentences,[[    0   305  9295 16513     1]],"contains thousands of XML files, each of which are constructed by several records","[[2579 2909   13    3    4 6858 2073    6  284   13   84   33 8520   57
   633 3187    1]]"
458dbf217218fcab9153e33045aac08a2c8a38c6,0.0,82,[[   0    3 4613    1]],"Total number of annotated data:
Semeval'15: 10712
Semeval'16: 28632
Tass'15: 69000
Sentipol'14: 6428","[[ 9273   381    13    46  2264   920   331    10   679   526  2165    31
   1808    10     3 18057  2122   679   526  2165    31  2938    10   204
   3840  2668  2067     7     7    31  1808    10     3  3951  2313  4892
     17    23  3233    31  2534    10  6687  2577     1]]"
e015d033d4ee1c83fe6f192d3310fb820354a553,0.0,"BIBREF16, BIBREF23, BIBREF24","[[    0     3  5972 25582   371  2938     6     3  5972 25582   371  2773
      6     3  5972 25582   371  2266     1]]",BIBREF8 a refined collection of tweets gathered from twitter,"[[    3  5972 25582   371   927     3     9 16097  1232    13 10657     7
      3  9094    45 19010     1]]"
44a2a8e187f8adbd7d63a51cd2f9d2d324d0c98d,100.0,HEOT,[[   0    3 6021 6951    1]],HEOT,[[   3 6021 6951    1]]
6608f171b3e0dcdcd51b3e0c697d6e5003ab5f02,0.0,sarcastic cues from tweets,[[    0     3     7  4667 10057   123    15     7    45 10657     7     1]],"adjective and adverb patterns, verb, subject, and object arguments, verbal patterns","[[31268    11     3     9    26 11868  4264     6  7375     6  1426     6
     11  3735 12874     6  7375   138  4264     1]]"
5679fabeadf680e35a4f7b092d39e8638dca6b4d,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
045dbdbda5d96a672e5c69442e30dbf21917a1ee,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
2348d68e065443f701d8052018c18daa4ecc120e,100.0,"highly data-inefficient, underperform phrase-based statistical machine translation","[[    0  1385   331    18    77 16995     6   365   883  2032  9261    18
    390 11775  1437  7314     1]]","highly data-inefficient, underperform phrase-based statistical machine translation","[[ 1385   331    18    77 16995     6   365   883  2032  9261    18   390
  11775  1437  7314     1]]"
efe49829725cfe54de01405c76149a4fe4d18747,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"For example, in QuasarT, it improves 16.8% in EM score and 20.4% in F1 score. , For example, in QuasarT, it improves 4.6% in EM score and 3.5% in F1 score.","[[  242   677     6    16  2415     9     7   291   382     6    34  1172
      7 10128  5953    16     3  6037  2604    11   460     5  5988    16
    377   536  2604     5     3     6   242   677     6    16  2415     9
      7   291   382     6    34  1172     7  2853  6370    16     3  6037
   2604    11  1877  2712    16   377   536  2604     5     1]]"
11e6b79f1f48ddc6c580c4d0a3cb9bcb42decb17,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"40 mel-scaled log filterbank enegries (FBanks) computed every 10 ms with 25 ms window, deltas and delta-deltas (120 features in vector), spectrogram","[[ 1283     3  2341    18  6649    26  4303  4191  4739     3    35    15
    122  2593    41   371 21347     7    61 29216    26   334   335     3
     51     7    28   944     3    51     7  2034     6 25110     7    11
  25110    18 24876     9     7    41 15518   753    16 12938   201     3
   5628    52 16275     1]]"
caf9819be516d2c5a7bfafc80882b07517752dfa,100.0,They evaluate quantitatively.,[[    0   328  6825 18906   120     5     1]],They evaluate quantitatively.,[[  328  6825 18906   120     5     1]]
0aa46c132515d8830a72f263812cdf7cbd5627c6,0.0,"BIBREF2, BIBREF3",[[    0     3  5972 25582   371  4482     3  5972 25582   371   519     1]],"RS-Average , RS-Linear, RS-Item, RS-MF, Sum-Opinosis, Sum-LSTM-Att","[[    3  5249    18   188   624   545     3     6     3  5249    18 21022
    291     6     3  5249    18   196  3524     6     3  5249    18 13286
      6 12198    18   667  3180    32     7   159     6 12198    18  7600
   2305    18   188    17    17     1]]"
1546356a8c5893dc2d298dcbd96d0307731dd54d,0.0,"Gender Prediction Treebank, Universal Dependencies Treebank BIBREF0","[[    0   350  3868  1266 12472  7552  4739     6 12489 30718 11573  7552
   4739     3  5972 25582   371   632     1]]",The baseline model BIBREF5 we compare with regards the output space of the model as a subset INLINEFORM2 where INLINEFORM3 is the set of all tag sets seen in this training data.,"[[   37 20726   825     3  5972 25582   371   755    62  4048    28  9544
      8  3911   628    13     8   825    38     3     9   769  2244  3388
  20006 24030   357   213  3388 20006 24030   519    19     8   356    13
     66  7860  3369   894    16    48   761   331     5     1]]"
9e805020132d950b54531b1a2620f61552f06114,0.0,linearly map source word embedding into target word embedding space BIBREF,"[[    0 13080   120  2828  1391  1448 25078    26    53   139  2387  1448
  25078    26    53   628     3  5972 25582   371]]","CNN-mean, CNN-avgmax",[[19602    18   526   152     6 19602    18     9   208   122  9128     1]]
fcd0bd2db39898ee4f444ae970b80ea4d1d9b054,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
06776b8dfd1fe27b5376ae44436b367a71ff9912,100.0,"Mandarin dataset, Cantonese dataset",[[    0 31057 17953     6  1072  6948     7    15 17953     1]],"Mandarin dataset, Cantonese dataset",[[31057 17953     6  1072  6948     7    15 17953     1]]
b0799e26152197aeb3aa3b11687a6cc9f6c31011,0.0,multimodal detection,[[    0  1249 20226 10664     1]],"Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), Textual Kernels Model (TKM)","[[    3 16772  1193  2138    35   257  5154    41  5390   329   201  5641
  10646  1193  2138    35   257  5154    41   134  5518   201  5027  3471
  16469  3573  5154    41 22110   329    61     1]]"
3d34a02ceebcc93ee79dc073c408651d25e538bc,0.0,LSTM,[[   0    3 7600 2305    1]],Support Vector Machines (SVM) classifier,[[ 4224 29011  5879     7    41   134 12623    61   853  7903     1]]
e79a5e435fcf5587535f06c9215d19a66caadaff,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
66125cfdf11d3bf8e59728428e02021177142c3a,4.1667,high-accuracy word-alignment,[[   0  306   18 6004  450 4710 1448   18  138 3191  297    1]],"Table TABREF15 shows that word-alignment based on mBERT representations surpasses the outputs of the standard FastAlign tool even if it was provided large parallel corpus. This suggests that word-level semantics are well captured by mBERT contextual embeddings. For this task, learning an explicit projection had a negligible effect on the performance.","[[ 4398     3  3221 25582   371  1808  1267    24  1448    18   138  3191
    297     3   390    30     3    51 12920   382  6497     7 25678    15
      7     8  3911     7    13     8  1068  6805   188  2825    29  1464
    237     3    99    34    47   937   508  8449 11736   302     5   100
   6490    24  1448    18  4563 27632     7    33   168  9534    57     3
     51 12920   382 28131 25078    26    53     7     5   242    48  2491
      6  1036    46 17623 13440   141     3     9 14261  2825  2317  1504
     30     8   821     5     1]]"
2815bac42db32d8f988b380fed997af31601f129,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],It had the highest accuracy comparing to all datasets 0.986% and It had the highest improvement comparing to previous methods on the same dataset by 8%,"[[   94   141     8  2030  7452     3 14622    12    66 17953     7  4097
   3916  6370    11    94   141     8  2030  4179     3 14622    12  1767
   2254    30     8   337 17953    57     3  5953     1]]"
4e9684fd68a242cb354fa6961b0e3b5c35aae4b6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Unimodal LSTM vs Best Multimodal (FCM)
- F score: 0.703 vs 0.704
- AUC: 0.732 vs 0.734 
- Mean Accuracy: 68.3 vs 68.4 ","[[ 5645 20226     3  7600  2305     3   208     7  1648  4908 20226    41
   5390   329    61     3    18   377  2604    10     3 22426  4928     3
    208     7     3 22426  6348     3    18    71  6463    10     3 22426
   2668     3   208     7     3 22426  3710     3    18 23045  4292  3663
   4710    10     3  3651     5   519     3   208     7     3  3651     5
    591     1]]"
93b1b94b301a46251695db8194a2536639a22a88,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
bb8f62950acbd4051774f1bfc50e3d424dd33b7c,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
9646fa1abbe3102a0364f84e0a55d107d45c97f0,25.0,jokes generated by users of virtual assistants such as Siri and Alexa,"[[    0 10802     7  6126    57  1105    13  4291  6165     7   224    38
  22438    11  5104     9     1]]"," jokes of different categories (sci-fi, sports, etc) and types (puns, limerick, etc)","[[10802     7    13   315  5897    41     7    75    23    18    89    23
      6  2100     6   672    61    11  1308    41  6225     7     6 13222
   5206     6   672    61     1]]"
ff814793387c8f3b61f09b88c73c00360a22a60e,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
3554ac92d4f2d00dbf58f7b4ff2b36a852854e95,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The sentence we want to generate from the target-aspect pair is a question, and the format needs to be the same., For the NLI task, the conditions we set when generating sentences are less strict, and the form is much simpler., For QA-B, we add the label information and temporarily convert TABSA into a binary classification problem ( INLINEFORM0 ) to obtain the probability distribution, auxiliary sentence changes from a question to a pseudo-sentence","[[   37  7142    62   241    12  3806    45     8  2387    18     9  5628
   3116    19     3     9   822     6    11     8  1910   523    12    36
      8   337     5     6   242     8   445  8159  2491     6     8  1124
     62   356   116     3 11600 16513    33   705  6926     6    11     8
    607    19   231 15673     5     6   242     3 23008    18   279     6
     62   617     8  3783   251    11 18223  5755   332  5359  4507   139
      3     9 14865 13774   682    41  3388 20006 24030   632     3    61
     12  3442     8 15834  3438     6     3 31086  7142  1112    45     3
      9   822    12     3     9 22726    18  5277  1433     1]]"
4ec538e114356f72ef82f001549accefaf85e99c,66.6667,"all caps, quotation marks, emoticons, intensifiers and hyperbole","[[    0    66 16753     6 20222  6784     6 25993  8056     6  9608  7903
      7    11  6676  4243    15     1]]","all caps, quotation marks, emoticons, emojis, hashtags","[[   66 16753     6 20222  6784     6 25993  8056     6     3    15    51
  21892     7     6 25354     7     1]]"
acc512c57aef4d5a15c15e3593f0a9b3e7e7e8b8,0.0,"Bi-LSTM, BERT",[[    0  2106    18  7600  2305     6   272 24203     1]],"1) Connectionist Temporal Classification (CTC), 2) Attention-based methods, 3) RNN-tranducer","[[ 8925 19466   343 27695  4900  4501  2420    41   254  3838   201  9266
  20748    18   390  2254     6     3  5268   391 17235    18 11665  4817
     49     1]]"
6b7d76c1e1a2490beb69609ba5652476dde8831b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],adding context causes speakers to focus on broader semantic and pragmatic issues of discourse coherence,"[[ 2651  2625  4110  7215    12   992    30     3 13627 27632    11 29810
    807    13 22739   576   760  1433     1]]"
052d19b456f1795acbb8463312251869cc5b38da,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
3748787379b3a7d222c3a6254def3f5bfb93a60e,0.0,linguistic quality,[[    0     3 24703   463     1]],"Grammaticality, non-redundancy, referential clarity, focus, structure & coherence","[[20278  4992 10355     6   529    18  1271  1106  6833     6  2401  7220
  12428     6   992     6  1809     3   184   576   760  1433     1]]"
be73a88d5b695200e2ead4c2c24e2a977692970e,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
e5a965e7a109ae17a42dd22eddbf167be47fca75,8.0,"ambiguity in labelled PIO data, integration of training data from different tasks and sources","[[    0     3 24621   485    16     3 29506   276  7550   331     6  5660
     13   761   331    45   315  4145    11  2836]]",Some sentences are associated to ambiguous dimensions in the hidden state output,"[[  886 16513    33  1968    12     3 24621  1162  8393    16     8  5697
    538  3911     1]]"
3f46d8082a753265ec2a88ae8f1beb6651e281b6,0.0,WSJ-2013 Sentiment Classification Task,"[[    0     3  8439   683    18 11138  4892  2998   295  4501  2420 16107
      1]]","CBT NE/CN, MR Movie reviews, IMDB Movie reviews, SUBJ","[[  205  9021     3  4171    87 10077     6     3  9320 10743  2456     6
      3  5166  9213 10743  2456     6   180 10134   683     1]]"
da495e2f99ee2d5db9cc17eca5517ddaa5ea8e42,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],LDC corpus,[[  301  6338 11736   302     1]]
710fa8b3e74ee63d2acc20af19f95f7702b7ce5e,0.0,LSTM-based encoder-decoder models,"[[    0     3  7600  2305    18   390 23734    52    18   221  4978    52
   2250     1]]",WordDecoding (WDec) model,[[ 4467  2962  9886    41 17698    15    75    61   825     1]]
28e7711f94e093137eb8828f0b1eff1b05e4fa38,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
e659ceb184777015c12db2da5ae396635192f0b0,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
00ef9cc1d1d60f875969094bb246be529373cb1d,14.2857,a novel method to intuit binary sentiments of large numbers of tweets for training purposes,"[[    0     3     9  3714  1573    12     3 26145 14865  6493     7    13
    508  2302    13 10657     7    21   761  3659]]",Influential tweeters ( who they define as individuals certain to have a classifiable sentiment regarding the topic at hand) is used to label tweets in bulk in the absence of manually-labeled tweets.,"[[23636 10646 10657   277    41   113    79  6634    38  1742   824    12
     43     3     9   853    99    23   179  6493  1918     8  2859    44
    609    61    19   261    12  3783 10657     7    16  7942    16     8
   8605    13 12616    18  9339   400    26 10657     7     5     1]]"
7975c3e1f61344e3da3b38bb12e1ac6dcb153a18,10.5263,diverse versions of word embeddings can have different effects on the performance,"[[    0  2399  5204    13  1448 25078    26    53     7    54    43   315
   1951    30     8   821     1]]",each embedding version is crucial for good performance,[[  284 25078    26    53   988    19  4462    21   207   821     1]]
eb95af36347ed0e0808e19963fe4d058e2ce3c9f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],51.7 and 51.6 on 4th and 8th grade question sets with no curated knowledge. 47.5 and 48.0 on 4th and 8th grade question sets when both solvers are given the same knowledge,"[[  305 18596    11   305 15062    30   314   189    11   505   189  2769
    822  3369    28   150     3 22579  1103     5   314 15731    11   314
  27376    30   314   189    11   505   189  2769   822  3369   116   321
   4602    52     7    33   787     8   337  1103     1]]"
3f85cc5be84479ba668db6d9f614fedbff6d77f1,0.0,"70,000",[[    0     3 28891     1]],eight GB,[[2641    3 3443    1]]
8ec94313ea908b6462e1f5ee809a977a7b6bdf01,100.0,No,[[  0 465   1]],No,[[465   1]]
5e41516a27c587aa2f80dba8cf4c3f616174099b,27.2727,"Character level perturbations are eliminated, character level projections are replaced with character level LSH projection","[[    0 20087   593 29404  1628    33 17809     6  1848   593 13440     7
     33  5821    28  1848   593   301  9122 13440]]",same sentences after applying character level perturbations,[[  337 16513   227  6247  1848   593 29404  1628     1]]
fa2ffc6b4b046e17bc41e199855c4941673e2caf,100.0,Parallel monolingual corpus in English and Mandarin,[[    0 27535  7414 25207 11736   302    16  1566    11 31057     1]],Parallel monolingual corpus in English and Mandarin,[[27535  7414 25207 11736   302    16  1566    11 31057     1]]
2159062595f24ec29826d517429e1b809ba068b3,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
f42d470384ca63a8e106c7caf1cb59c7b92dbc27,36.3636,F1 score,[[   0  377  536 2604    1]],"exact match, f1 score, edit distance and goal match",[[2883 1588    6    3   89  536 2604    6 4777 2357   11 1288 1588    1]]
bf00808353eec22b4801c922cce7b1ec0ff3b777,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
67d8e50ddcc870db71c94ad0ad7f8a59a6c67ca6,100.0,3,[[  0 220   1]],3 ,[[220   1]]
c418deef9e44bc8448d9296c6517824cb95bd554,100.0,"F1-score, BLEU score",[[    0   377   536    18     7  9022     6     3  8775 12062  2604     1]],"F1-score, BLEU score",[[  377   536    18     7  9022     6     3  8775 12062  2604     1]]
6da2cb3187d3f28b75ac0a61f6562a8adf716109,50.0,"Pointer-Generator, SPNet",[[    0  4564    49    18 13714    49  1016     6  6760  9688     1]],"Pointer-Generator, Transformer",[[ 4564    49    18 13714    49  1016     6 31220     1]]
fc3f0eb297b2308b99eb4661a510c9cdbb6ffba2,100.0,3029,[[   0  604 3166    1]],3029,[[ 604 3166    1]]
79443bf3123170da44396b0481364552186abb91,0.0,BLEU and DISPLAYFORM5,[[    0     3  8775 12062    11     3 15438   345 29002 24030   755     1]],"sequence classification, sequence labeling",[[ 5932 13774     6  5932  3783    53     1]]
1ebd6f703458eb6690421398c79abf3fc114148f,0.0,"Transformator, MT-34, MT-34, and MT-34","[[   0 4946 8995  127    6    3 7323   18 3710    6    3 7323   18 3710
     6   11    3 7323   18 3710]]","first two systems are transformer models trained on different amounts of data, The third system includes a modification to consider the information of full coreference chains","[[  166   192  1002    33 19903  2250  4252    30   315  6201    13   331
      6    37  1025   358   963     3     9 12767    12  1099     8   251
     13   423  2583 11788 16534     1]]"
2fa0b9d0cb26e1be8eae7e782ada6820bc2c037f,100.0,97.32%,[[    0   668 27914  5406     1]],97.32%,[[  668 27914  5406     1]]
81e8d42dad08a58fe27eea838f060ec8f314465e,0.0,Bi-Directorial RNNs,[[    0  2106    18 23620 11929   391 17235     7     1]],neural attention model with a convolutional encoder with an RNN decoder and RNN encoder-decoder,"[[24228  1388   825    28     3     9   975 24817   138 23734    52    28
     46   391 17235    20  4978    52    11   391 17235 23734    52    18
    221  4978    52     1]]"
928828544e38fe26c53d81d1b9c70a9fb1cc3feb,50.0,"29,500 documents",[[    0 14405  2560  2691     1]],"29,500 documents in the CORD-19 corpus (2020-03-13)","[[14405  2560  2691    16     8   205 18400  4481 11736   302    41 22224
     18  4928 13056    61     1]]"
07c9863e1e86c31b740b5b5a77fe8000be00c273,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
9714cb7203c18a0c53805f6c889f2e20b4cab5dd,11.7647,video of a talking face,[[   0  671   13    3    9 2508  522    1]],video sequence is first fed into the VGG model BIBREF9 to extract visual feature,"[[  671  5932    19   166 13126   139     8   584 21320   825     3  5972
  25582   371  1298    12  5819  3176  1451     1]]"
d47c074012eae27426cd700f841fd8bf490dcc7b,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
a56fbe90d5d349336f94ef034ba0d46450525d19,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Author's own DCG rules are defined from scratch.,"[[10236    31     7   293  5795   517  2219    33  4802    45  8629     5
      1]]"
dee7383a92c78ea49859a2d5ff2a9d0a794c1f0f,35.2941,variational dropout is a technique used to tune parameters of a LSTM to,"[[    0 12338   138  2328   670    19     3     9  3317   261    12  7368
   8755    13     3     9     3  7600  2305    12]]",the dropout technique of Gal & Ghahramani gal,"[[   8 2328  670 3317   13 6084    3  184  350 1024  107 2375 2738 7466
     1]]"
b9f2a30f5ef664ff845d860cf4bfc2afb0a46e5a,13.7931,by the divergence between the conditional probability distributions given these relations,"[[    0    57     8 12355   122  1433   344     8  1706   138 15834  3438
      7   787   175  5836     1]]",By assessing similarity of 360 pairs of relations from a subset of Wikidata using an integer similarity score from 0 to 4,"[[  938     3 20861  1126   485    13  9181 14152    13  5836    45     3
      9   769  2244    13  2142  2168  6757   338    46 30278  1126   485
   2604    45     3   632    12   314     1]]"
6cd01609c8afb425fbed941441a2528123352940,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
32537fdf0d4f76f641086944b413b2f756097e5e,0.0,122344444,"[[  0   3   2 536   2 357   2 357   2 519   2 591   2 591   2 591   2 591
    2 591]]",improving the score for WNLaMPro-medium by 50% compared to BERT$_\text{base}$ and 31% compared to Attentive Mimicking,"[[ 4863     8  2604    21     3 21170  3612   329  3174    18  5700   440
     57  5743     3  2172    12   272 24203  3229   834     2  6327     2
  10925     2  3229    11   220  4704     3  2172    12   486   324  3268
   2133  3113  1765     1]]"
7b89515d731d04dd5cbfe9c2ace2eb905c119cbc,8.6957,LSTM with a lexical based on phonetic and phonotact,"[[    0     3  7600  2305    28     3     9     3 30949   138     3   390
     30   951  1225    11     3  9553  2264  2708]]","The three baseline models are the i-vector model, a standard RNN LID system and a multi-task RNN LID system. ","[[   37   386 20726  2250    33     8     3    23    18   162  5317   825
      6     3     9  1068   391 17235   301  4309   358    11     3     9
   1249    18 23615   391 17235   301  4309   358     5     1]]"
18d8b52b4409c718bf1cc90ce9e013206034bbd9,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],average 12.8 min per recording,[[ 1348   209 19419  3519   399  5592     1]]
f12a282571f842b818d4bee86442751422b52337,100.0,TF.IDF-based features,[[   0    3 9164    5 4309  371   18  390  753    1]],TF.IDF-based features,[[   3 9164    5 4309  371   18  390  753    1]]
6ce057d3b88addf97a30cb188795806239491154,0.0,"XLNet-Large, XLNet-Large","[[   0    3    4  434 9688   18  434 8240   15    6    3    4  434 9688
    18  434 8240   15    1]]","BERT, XLNET RoBERTa, ALBERT, DistilBERT","[[  272 24203     6     3     4   434  9978  2158 12920   382     9     6
     71 16976 24203     6  2043 17153 12920   382     1]]"
3ccd337f77c5d2f7294eb459ccc1770796c2eaef,0.0,"quality, recall, recall, recall, recall, recall, recall, recall, recall, recall","[[   0  463    6 7881    6 7881    6 7881    6 7881    6 7881    6 7881
     6 7881    6 7881    6 7881]]","Model Confidence, Continuity, Query-relatedness, Repetitiveness, Specificity","[[ 5154 11847  1599  3772     6     3 16798    76   485     6     3 27569
     18  3897   655     6   419 19427   757   655     6 18070   485     1]]"
efb52bda7366d2b96545cf927f38de27de3b5b77,100.0,No,[[  0 465   1]],No,[[465   1]]
e3981a11d3d6a8ab31e1b0aa2de96f253653cfb2,100.0,English,[[   0 1566    1]],English,[[1566    1]]
dfbab3cd991f86d998223726617d61113caa6193,16.0,reviews are used as bridge between consumers and companies to polish the quality of their services,"[[    0  2456    33   261    38  4716   344  3674    11   688    12 12535
      8   463    13    70   364     1]]",reviews under distinct product categories are considered specific domain knowledge,[[2456  365 6746  556 5897   33 1702  806 3303 1103    1]]
0153f563f5e2680c2de1a5f6d0e443454dc1ef2a,100.0,No,[[  0 465   1]],No,[[465   1]]
6ee27ab55b1f64783a9e72e3f83b7c9ec5cc8073,0.0,"WSJ13, WSJ18, WSJ19, WSJ20","[[   0    3 8439  683 2368    6    3 8439  683 2606    6    3 8439  683
  2294    6    3 8439  683 1755]]","the CMU ARCTIC database BIBREF33,  the M-AILABS speech dataset BIBREF34 ","[[    8   205  9696     3 18971 18679  3501     3  5972 25582   371  4201
      6     8   283    18 22862  5359   134  5023 17953     3  5972 25582
    371  3710     1]]"
a60030cfd95d0c10b1f5116c594d50cb96c87ae6,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
fbee81a9d90ff23603ee4f5986f9e8c0eb035b52,5.7143,F1 score of 0.82,[[   0  377  536 2604   13 4097 4613    1]],"Achieved the highest per-domain scores on Substance (F1 ≈ 0.8) and the lowest scores on Interpersonal and Mood (F1 ≈ 0.5), and show consistency in per-domain performance rankings between MLP and RBF models.","[[   71  9781   162    26     8  2030   399    18 22999  7586    30  3325
   8389    41   371   536     3     2     3 22384    61    11     8  7402
   7586    30  3037 17108    11     3 16334    26    41   371   536     3
      2     3 12100   201    11   504 12866    16   399    18 22999   821
  16890   344   283  6892    11   391 19780  2250     5     1]]"
195611926760d1ceec00bd043dfdc8eba2df5ad1,100.0,Random Forest classifier,[[    0 25942  6944   853  7903     1]],Random Forest classifier,[[25942  6944   853  7903     1]]
b53efdbb9e53a65cd3828a3eb485c70f782a06e5,8.8889,"first, we design a graph structure to hold information in the HotpotQA dataset by assign","[[    0   166     6    62   408     3     9  8373  1809    12  1520   251
     16     8  5396  3013 23008 17953    57 12317]]","we fully connect nodes that represent sentences from the same passage, we fully connect nodes that represent the first sentence of each passage, we add an edge between the question and every node for each passage","[[   62  1540  1979   150  1395    24  4221 16513    45     8   337  5454
      6    62  1540  1979   150  1395    24  4221     8   166  7142    13
    284  5454     6    62   617    46  3023   344     8   822    11   334
    150   221    21   284  5454     1]]"
aecb485ea7d501094e50ad022ade4f0c93088d80,100.0,No,[[  0 465   1]],No,[[465   1]]
c8031c1629d270dedc3b0c16dcb7410524ff1bab,0.0,Logician is a multi-model system that uses a set of supervised,"[[    0     3 20641    23   152    19     3     9  1249    18 21770   358
     24  2284     3     9   356    13     3 23313]]","restricted copy mechanism to ensure literally honestness, coverage mechanism to alleviate the under extraction and over extraction problem, and gated dependency attention mechanism to incorporate dependency information","[[12103  2405  8557    12   766  6672  5057   655     6  2591  8557    12
  18807    15     8   365 16629    11   147 16629   682     6    11 10530
     26 27804  1388  8557    12  6300 27804   251     1]]"
7e328cc3cffa521e73f111d6796aaa9661c8eb07,100.0,predicting the word given its context,[[    0     3 29856     8  1448   787   165  2625     1]],predicting the word given its context,[[    3 29856     8  1448   787   165  2625     1]]
4196d329061f5a9d147e1e77aeed6a6bd9b35d18,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],seq2seq translation,[[ 142 1824  357    7   15 1824 7314    1]]
c165ea43256d7ee1b1fb6f5c0c8af5f7b585e60d,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"most of the models have similar performance on BPRA: DSTC2 (+0.0015), Maluuba (+0.0729)
GDP achieves the best performance in APRA: DSTC2 (+0.2893), Maluuba (+0.2896)
GDP significantly outperforms the baselines on BLEU: DSTC2 (+0.0791), Maluuba (+0.0492)","[[  167    13     8  2250    43  1126   821    30     3 11165  4763    10
      3  3592  3838   357 17134 10667  1808   201  2148    76 17309 17134
  11739  5865 11728 11284  1984     7     8   200   821    16     3  2965
   4763    10     3  3592  3838   357 17134 18189  3914  5268     6  2148
     76 17309 17134 18189  3914 10938 11284  4019    91   883  2032     7
      8 20726     7    30     3  8775 12062    10     3  3592  3838   357
  17134 11739  4440  6982     6  2148    76 17309 17134 11739  3647  7318
      1]]"
076928bebde4dffcb404be216846d9d680310622,17.6471,a well-known graph-based model called a co-occurrence network with a,"[[    0     3     9   168    18  5661  8373    18   390   825   718     3
      9   576    18 16526  1229    28     3     9]]","in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window, connects only adjacent words in the so called word adjacency networks","[[   16     3     9   576    18 16526  1229   284   315  1448  2992     3
      9   150   221    11  9804    33  2127  1009   576    18 16526    16
      3     9  5327  2034     6  1979     7   163 12487  1234    16     8
     78   718  1448 19181     9    75  4392  5275     1]]"
63403ffc0232ff041f3da8fa6c30827cfd6404b7,100.0,Accuracy,[[   0 4292 3663 4710    1]],Accuracy,[[4292 3663 4710    1]]
37753fbffc06ce7de6ada80c89f1bf5f190bbd88,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Preceding and following sentence of each metaphor and paraphrase are added as document context,"[[ 1266   565    26    53    11   826  7142    13   284 21253    11  3856
  27111    33   974    38  1708  2625     1]]"
6bcff3ef61aad6bf1280ea26ed79585e1b838e64,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
8de64483ae96c0a03a8e527950582f127b43dceb,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
46ee1cbbfbf0067747b28bdf4c8c2f7dc8955650,100.0,LSTMs,[[   0    3 7600 2305    7    1]],LSTMs,[[   3 7600 2305    7    1]]
79620a2b4b121b6d3edd0f7b1d4a8cc7ada0b516,0.0,"LSTM, LSTM-Vec, LSTM-Vec","[[   0    3 7600 2305    6    3 7600 2305   18  553   15   75    6    3
  7600 2305   18  553   15   75]]","To the best of our knowledge, our method achieves state-of-the-art results in weighted-accuracy and standard accuracy on the dataset","[[  304     8   200    13    69  1103     6    69  1573  1984     7   538
     18   858    18   532    18  1408   772    16  1293    15    26    18
   6004   450  4710    11  1068  7452    30     8 17953     1]]"
a1885f807753cff7a59f69b5cf6d0fdef8484057,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"English wikipedia dataset has more than 18 million, a dump of 15 million English news articles ","[[ 1566     3 29474 17953    65    72   145   507   770     6     3     9
  11986    13   627   770  1566  1506  2984     1]]"
d5256d684b5f1b1ec648d996c358e66fe51f4904,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Improve existing NLP methods. Improve linguistic analysis. Measure impact of word normalization tools.,"[[20825  1895   445  6892  2254     5 20825     3 24703  1693     5 16317
   1113    13  1448  1389  1707  1339     5     1]]"
b8fdc600f9e930133bb3ec8fbcc9c600d60d24b0,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
88d9d32fb7a22943e1f4868263246731a1726e6e,31.25,exemplars are images that are used by a bot to generate questions for a human,"[[    0     3 26710     7    33  1383    24    33   261    57     3     9
  14761    12  3806   746    21     3     9   936]]","Exemplars aim to provide appropriate context., joint image-caption embedding for the supporting exemplar are closer to that of the target image-caption","[[31402     7  2674    12   370  2016  2625     5     6  4494  1023    18
   4010  1575 25078    26    53    21     8  3956     3 26710    33  4645
     12    24    13     8  2387  1023    18  4010  1575     1]]"
710c1f8d4c137c8dad9972f5ceacdbf8004db208,100.0,No,[[  0 465   1]],No,[[465   1]]
4d5f112874250d48eb49649c4abe31d6c9236700,100.0,GPT-2,[[   0  350 6383 4949    1]],GPT-2,[[ 350 6383 4949    1]]
53377f1c5eda961e438424d71d16150e669f7072,100.0,pure summarization model NHG,[[    0  4621  4505  1635  1707   825   445 21855     1]],pure summarization model NHG,[[ 4621  4505  1635  1707   825   445 21855     1]]
65e32f73357bb26a29a58596e1ac314f7e9c6c91,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The lack of background, Non-cursing aggressions and insults, the presence of controversial topic words ,  shallow meaning representation, directly ask the suspected troll if he/she is trolling or not, a blurry line between “Frustrate” and “Neutralize”, distinction between the classes “Troll” and “Engage”","[[   37  2136    13  2458     6  5388    18 15983    53 28986     7    11
  21548     7     6     8  3053    13 15202  2859  1234     3     6 16906
   2530  6497     6  1461   987     8 18024     3    17  4046     3    99
      3    88    87     7    88    19 10968 12013    42    59     6     3
      9 16810   651   689   344   105   371  9277  2206   153    11   105
  26288  8792  1737  1241 13005   344     8  2287   105   382  4046   153
     11   105   427  1725   545   153     1]]"
1f6180bba0bc657c773bd3e4269f87540a520ead,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
b1e90a546dc92e96b657fff5dad8e89f4ac6ed5e,100.0,No,[[  0 465   1]],No,[[465   1]]
e097c2ec6021b1c1195b953bf3e930374b74d8eb,10.1695,octaves are extended to multiple resolutions,"[[    0     3    32    75    17 16001    33  4760    12  1317  3161     7
      1]]","The resolution of the low-frequency feature maps is reduced by an octave – height and width dimensions are divided by 2. In this work, we explore spatial reduction by up to 3 octaves – dividing by $2^t$, where $t=1,2,3$ – and for up to 4 groups. We refer to such a layer as a multi-octave convolutional (MultiOctConv) layer,","[[   37  3161    13     8   731    18 30989  1451  8111    19  3915    57
     46     3    32    75    17     9   162     3   104  3902    11  9400
   8393    33  8807    57  1682    86    48   161     6    62  2075 15208
   4709    57    95    12   220     3    32    75    17 16001     3   104
      3 29111    57  3771     2    17  3229     6   213  1514    17  2423
   4347  4482   519  3229     3   104    11    21    95    12   314  1637
      5   101  2401    12   224     3     9  3760    38     3     9  1249
     18    32    75    17     9   162   975 24817   138    41 31922   667
     75    17  4302   208    61  3760     6     1]]"
da55878d048e4dca3ca3cec192015317b0d630b1,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
3f7a7e81908a763e5ca720f90570c5f224ac64f6,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
def3d623578bf84139d920886aa3bd6cdaaa7c41,100.0,"Arabic, Czech and Turkish",[[    0 19248     6 16870    11 15423     1]],"Arabic, Czech and Turkish",[[19248     6 16870    11 15423     1]]
25e4dbc7e211a1ebe02ee8dff675b846fb18fdc5,0.0,CRF,[[   0  205 8556    1]],"Raw data from Gigaword, Automatically segmented text from Gigaword, Heterogenous training data from People's Daily, POS data from People's Daily","[[19401   331    45     3 20640     9  6051     6 19148  1427  5508    15
     26  1499    45     3 20640     9  6051     6   216   449  5255  1162
    761   331    45  2449    31     7  8496     6     3 16034   331    45
   2449    31     7  8496     1]]"
f33a21c6a9c75f0479ffdbb006c40e0739134716,0.0,No,[[  0 465   1]],syntax-based system may generate correct syntactic analyses for partial grammatical fragments,"[[28230    18   390   358   164  3806  2024  8953    17  2708   447 15282
     21 11807     3  5096  4992   138 12071     7     1]]"
82a28c1ed7988513d5984f6dcacecb7e90f64792,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],The negative effects were insignificant.,[[   37  2841  1951   130    16 26251     5     1]]
68ba5bf18f351e8c83fae7b444cc50bef7437f13,0.0,"language modeling, sentence length prediction",[[    0  1612 15309     6  7142  2475 21332     1]],"De-En, En-Fr and En-Vi translation tasks","[[ 374   18 8532    6  695   18  371   52   11  695   18  553   23 7314
  4145    1]]"
85e45b37408bb353c6068ba62c18e516d4f67fe9,0.0,Language Modeling,[[    0 10509  5154    53     1]],The baseline is a multi-task architecture inspired by another paper.,"[[   37 20726    19     3     9  1249    18 23615  4648  3555    57   430
   1040     5     1]]"
749a307c3736c5b06d7b605dc228d80de36cbabe,0.0,WN15 and WSJ15,[[    0     3 21170  1808    11     3  8439   683  1808     1]],"WMT 2019 parallel dataset, a restricted dataset containing the full TED corpus from MUST-C BIBREF10, sampled sentences from WMT 2019 dataset","[[  549  7323  1360  8449 17953     6     3     9 12103 17953     3  6443
      8   423     3 11430 11736   302    45 23716    18   254     3  5972
  25582   371  1714     6  3106    26 16513    45   549  7323  1360 17953
      1]]"
4170ed011b02663f5b1b1a3c1f0415b7abfaa85d,12.1212,relationship is found between the co-voting and the coalition behavior,"[[    0  1675    19   435   344     8   576    18  1621  1222    11     8
  17952  3889     1]]","we observe a positive correlation between retweeting and co-voting, strongest positive correlations are in the areas Area of freedom, security and justice, External relations of the Union, and Internal markets, Weaker, but still positive, correlations are observed in the areas Economic, social and territorial cohesion, European citizenship, and State and evolution of the Union, significantly negative coefficient, is the area Economic and monetary system","[[   62  7743     3     9  1465 18712   344     3    60    17  1123    15
   1222    11   576    18  1621  1222     6 19095  1465 18712     7    33
     16     8   844  5690    13  4333     6  1034    11  4831     6 26241
   5836    13     8  3545     6    11 18524  3212     6   101     9  2304
      6    68   341  1465     6 18712     7    33  6970    16     8   844
   9071     6   569    11 23871   576    88  1938     6  1611 22214     6
     11  1015    11  9009    13     8  3545     6  4019  2841 27742     6
     19     8   616  9071    11     3 14356   358     1]]"
281cd4e78b27a62713ec43249df5000812522a89,100.0,Average claim length is 8.9 tokens.,[[    0 23836  1988  2475    19  4848  1298 14145     7     5     1]],Average claim length is 8.9 tokens.,[[23836  1988  2475    19  4848  1298 14145     7     5     1]]
af8d3ee6a282aaa885e9126aa4bcb08ac68837e0,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"over 41,250 videos and 825,000 captions in both English and Chinese., over 206,000 English-Chinese parallel translation pairs","[[  147  8798     6 11434  3075    11   505 24338 25012     7    16   321
   1566    11  2830     5     6   147   460 14835  1566    18  3541  4477
     15  8449  7314 14152     1]]"
1df24849e50fcf22f0855e0c0937c1288450ed5c,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
50be9e6203c40ed3db48ed37103f967ef0ea946c,9.0909,BIBREF18 evaluates sentence representations against a set of supervised metric,"[[    0     3  5972 25582   371  2606  6825     7  7142  6497     7   581
      3     9   356    13     3 23313     3  7959]]","standard benchmarks BIBREF36 , BIBREF37, to use our learned representations as features for a low complexity classifier (typically linear) on a novel supervised task/domain unseen during training without updating the parameters, transfer learning evaluation in an artificially constructed low-resource setting","[[ 1068 15705     7     3  5972 25582   371  3420     3     6     3  5972
  25582   371  4118     6    12   169    69  2525  6497     7    38   753
     21     3     9   731 11641   853  7903    41 21888   120 13080    61
     30     3     9  3714     3 23313  2491    87 22999  1149    15    35
    383   761   406 14809     8  8755     6  2025  1036  5002    16    46
   7353   120  8520   731    18    60  7928  1898     1]]"
2f9d30e10323cf3a6c9804ecdc7d5872d8ae35e4,0.0,WSJ5-Sim-Sim-Sim-Sim-Sim,"[[   0    3 8439  683  755   18  134  603   18  134  603   18  134  603
    18  134  603   18  134  603]]",SNAP (Stanford Network Analysis Project),"[[    3  8544  2965    41   134    17   152  2590  3426 10582  2786    61
      1]]"
191d4fe8a37611b2485e715bb55ff1a30038ad6a,100.0,No,[[  0 465   1]],No,[[465   1]]
7994b4001925798dfb381f9aa5c0545cdbd77220,17.3913,they manually retrain sentences with grammatical errors,"[[    0    79 12616     3    60  9719 16513    28     3  5096  4992   138
   6854     1]]",They randomly sample sentences from Wikipedia that contains an object RC and add them to training data,"[[  328 21306  3106 16513    45 16885    24  2579    46  3735     3  4902
     11   617   135    12   761   331     1]]"
915cf3d481164217290d7b1eb9d48ed3e249196d,1.6529,overlapping dialogue acts,[[    0     3 31212  7478  6775     1]],"A request for information act should be issued early in a conversation, followed by an answer, informative statement, or apology towards the end of the conversation,  offering extra help at the end of a conversation, or thanking the customer yields more satisfied customers, and more resolved problems , asking yes-no questions early-on in a conversation is highly associated with problem resolution (ratio 3:1), but asking them at the end of a conversation has as similarly strong association with unsatisfied customers, Giving elaborate answers that are not a simple affirmative, negative, or response acknowledgement (i.e. Answer (Other)) towards the middle of a conversation leads to satisfied customers that are not frustrated, requesting information towards the end of a conversation (implying that more information is still necessary at the termination of the dialogue) leads to unsatisfied and unresolved customers","[[   71  1690    21   251  1810   225    36  4683   778    16     3     9
   3634     6  2348    57    46  1525     6 11152  2493     6    42 30430
   1587     8   414    13     8  3634     6  1772   996   199    44     8
    414    13     3     9  3634     6    42  2763    53     8   884  6339
      7    72  7035   722     6    11    72 13803   982     3     6  3558
   4273    18    29    32   746   778    18   106    16     3     9  3634
     19  1385  1968    28   682  3161    41  6850    32 29760   201    68
   3558   135    44     8   414    13     3     9  3634    65    38 19467
   1101  6028    28    73  9275     7  8549    26   722     6 22643 16224
   4269    24    33    59     3     9   650 11153  1528     6  2841     6
     42  1773  8406   297    41    23     5    15     5 11801    41   667
    189    49    61    61  1587     8  2214    13     3     9  3634  3433
     12  7035   722    24    33    59 17144     6     3 22686   251  1587
      8   414    13     3     9  3634    41 18531    53    24    72   251
     19   341  1316    44     8 18739    13     8  7478    61  3433    12
     73  9275     7  8549    26    11    73    60  6065    15    26   722
      1]]"
6e2ad9ad88cceabb6977222f5e090ece36aa84ea,0.0,"BERT-Base, LSTM-Base, LSTM-Base","[[    0   272 24203    18 14885    15     6     3  7600  2305    18 14885
     15     6     3  7600  2305    18 14885    15]]",The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254.,"[[   37 20726   825    19     3     9  1659  5932    18   235    18     7
     15   835  3772 23734    52    87   221  4978    52   825    28  1388
      5    37 23734    52    19     3     9  2647 26352  3230    18   134
  14184     3 11679 19159   599  7600  2305    61  2358     3  5972 25582
    371  2534    11     8    20  4978    52     3     9   712     3  7600
   2305  2358    28  1388  8557     5    37  1388  8557    19 29216    26
     38    16     3  5972 25582   371  1298    11    62   169     3     9
  30337    63   960    21    20  9886     5   101  2412   414    18   235
     18   989   379     8  1234 25078    26    53     7     5    37 25078
     26    53   812   261    19    13   209  2577    11     8  5697   538
    812    13     8     3  7600  2305  2640    19    13   944  7984     1]]"
ed522090941f61e97ec3a39f52d7599b573492dd,2.4691,cross-lingual representations in a shared continuous space,"[[    0  2269    18 25207  6497     7    16     3     9  2471  7558   628
      1]]","Answer with content missing: (Chapter 3) The concept can be easily explained with an example, visualized in Figure 1. Consider the Portuguese (Pt) word trabalho which, according to the MUSE Pt–En dictionary, has the words job and work as possible En translations. In turn, these two En words can be translated to 4 and 5 Czech (Cs) words respectively. By utilizing the transitive property (which translation should exhibit) we can identify the set of 7 possible Cs translations for the Pt word trabalho.","[[11801    28   738  3586    10    41  3541  6789    49     3  5268    37
   2077    54    36  1153  5243    28    46   677     6 25086    26    16
   7996  1300  9151     8 21076    41   345    17    61  1448     3  1313
   3849   107    32    84     6  1315    12     8   283 11927   276    17
    104  8532 24297     6    65     8  1234   613    11   161    38   487
    695  7314     7     5    86   919     6   175   192   695  1234    54
     36 15459    12   314    11   305 16870    41   254     7    61  1234
   6898     5   938     3 11182     8 11811   757   785    41  3339  7314
    225  6981    61    62    54  2862     8   356    13   489   487   205
      7  7314     7    21     8   276    17  1448     3  1313  3849   107
     32     5     1]]"
9cc0fd3721881bd8e246d20fff5d15bd32365655,100.0,image,[[   0 1023    1]],image,[[1023    1]]
191107cd112f7ee6d19c1dc43177e6899452a2c7,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
0c09ffb337be0feb25e2fd14164b35a0969d7b4c,100.0,word2vec BIBREF0,[[    0  1448   357   162    75     3  5972 25582   371   632     1]],word2vec BIBREF0,[[ 1448   357   162    75     3  5972 25582   371   632     1]]
fd80a7162fde83077ed82ae41d521d774f74340a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Burckhardt et al. BIBREF22, Liu et al. BIBREF18, Dernoncourt et al. BIBREF9, Yang et al. BIBREF10","[[ 4152  2406  5651    17     3    15    17   491     5     3  5972 25582
    371  2884     6  1414    76     3    15    17   491     5     3  5972
  25582   371  2606     6   660    29   106 14492     3    15    17   491
      5     3  5972 25582   371  1298     6 21078     3    15    17   491
      5     3  5972 25582   371  1714     1]]"
564dcaf8d0bcc274ab64c784e4c0f50d7a2c17ee,10.5263,"German-English, French-English, and Japanese-English","[[    0  2968    18 26749     6  2379    18 26749     6    11  4318    18
  26749     1]]","We apply this conversion to the 31 languages, Arabic, Hindi, Lithuanian, Persian, and Russian. , Dutch, Spanish","[[  101  1581    48  6113    12     8  2664  8024     6 19248     6 25763
      6 27620    29     6 25518     6    11  4263     5     3     6 10098
      6  5093     1]]"
cc5d8e12f6aecf6a5f305e2f8b3a0c67f49801a9,100.0,36%,[[   0  220 6370    1]],36%,[[ 220 6370    1]]
199bdb3a6b1f7c89d95ea6c6ddbbb5eff484fa1f,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
5bcc12680cf2eda2dd13ab763c42314a26f2d993,0.0,F1 score,[[   0  377  536 2604    1]],"For sentence-level prediction they used tolerance accuracy, for segment retrieval accuracy and MRR and for the pipeline approach they used overall accuracy","[[  242  7142    18  4563 21332    79   261 17313  7452     6    21  5508
  24515   138  7452    11   283 12224    11    21     8 12045  1295    79
    261  1879  7452     1]]"
9ac923be6ada1ba2aa20ad62b0a3e593bb94e085,11.1111,Using the metric of interpretability,[[   0    3 3626    8    3 7959   13 7280 2020    1]],"For evaluating the interpretability, we use $Coherence@k$ (Equation 6 ) , automated and manual word intrusion tests.","[[  242     3 17768     8  7280  2020     6    62   169  1514  3881   760
   1433  1741   157  3229    41   427  2436   257   431     3    61     3
      6 10069    11  3354  1448  2155 11733  3830     5     1]]"
70afd28b0ecc02eb8e404e7ff9f89879bf71a670,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
9776156fc93daa36f4613df591e2b49827d25ad2,19.0476,"Using the best existing approach, we achieved better results by improving the DCN and BiDA","[[    0     3  3626     8   200  1895  1295     6    62  5153   394   772
     57  4863     8   309 10077    11  2106  4296]]","In terms of F1 score, the Hybrid approach improved by 23.47% and 1.39% on BiDAF and DCN respectively. The DCA approach improved by 23.2% and 1.12% on BiDAF and DCN respectively.","[[   86  1353    13   377   536  2604     6     8  5555  2160    26  1295
   3798    57   204 23204  6170    11     3 13606  7561    30  2106  4296
    371    11   309 10077  6898     5    37  5795   188  1295  3798    57
   1902     5  5406    11  1300 26821    30  2106  4296   371    11   309
  10077  6898     5     1]]"
3070d6d6a52aa070f0c0a7b4de8abddd3da4f056,100.0,"BPC, Perplexity",[[   0  272 4051    6 1915 9247  485    1]],"BPC, Perplexity",[[ 272 4051    6 1915 9247  485    1]]
6e76f114209f59b027ec3b3c8c9cdfc3e682589f,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
99ef97336c0112d9f60df108f58c8b04b519a854,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
79b174d20ea5dd4f35e25c9425fb97f40e27cd6f,100.0,No,[[  0 465   1]],No,[[465   1]]
52f7e42fe8f27d800d1189251dfec7446f0e1d3b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively.","[[ 4292  3663  4710    13   200  4382  1573     3 12048  4171    41  7600
   2305  1220  4302  2138    35   257    61    33  4097  2079  2596     6
   4097  4433  4508     6  4097  3840  3076     3  2172    12   200   538
     18   858    18   532   768  1573   391    18   517 10077  1768     3
  12564  4097  4013  2658     6     3 22384  2294  6355  4097  4613  3166
     30   386 17953     7  6898     5     1]]"
508580af51483b5fb0df2630e8ea726ff08d537b,3.8462,Using crowdsourcing,[[    0     3  3626  4374 19035     1]],"We experiment with three different pretrained representations: ELMo BIBREF5 , BERT BIBREF6 , and GloVe BIBREF18 . To produce a single city embedding, we compute the TF-IDF weighted element-wise mean of the token-level representations. For all pretrained methods, we additionally reduce the dimensionality of the city embeddings to 40 using PCA for increased compatibility with our clustering algorithm.","[[  101  5016    28   386   315  7140 10761  6497     7    10   262 11160
     32     3  5972 25582   371   755     3     6   272 24203     3  5972
  25582   371   948     3     6    11  9840   553    15     3  5972 25582
    371  2606     3     5   304  1759     3     9   712   690 25078    26
     53     6    62 29216     8     3  9164    18  4309   371  1293    15
     26  3282    18 10684  1243    13     8 14145    18  4563  6497     7
      5   242    66  7140 10761  2254     6    62 11700  1428     8     3
  11619   485    13     8   690 25078    26    53     7    12  1283   338
   2104   188    21  1936 14235   485    28    69  9068    53 12628     5
      1]]"
d0444cbf01efdcc247b313c7487120a2f047f421,100.0,No,[[  0 465   1]],No,[[465   1]]
09f0dce416a1e40cc6a24a8b42a802747d2c9363,0.0,a public word2vec embeddings model pre-trained to analyze and,"[[    0     3     9   452  1448   357   162    75 25078    26    53     7
    825   554    18    17 10761    12  8341    11]]",Continuous Bag-of-Words (CBOW),"[[31295  8055    18   858    18   518   127    26     7    41   254   279
  15251    61     1]]"
08cbc9b8a8df56ec7be626f89285a621e1350f63,10.0,some,[[  0 128   1]],"the annotated dataset reported by degen2015investigating, a dataset of the utterances from the Switchboard corpus of telephone dialogues BIBREF21 that contain the word some","[[    8    46  2264   920 17953  2196    57    20   729  8651 15601 12581
   1222     6     3     9 17953    13     8     3  5108   663     7    45
      8 13218  1976 11736   302    13  6596  7478     7     3  5972 25582
    371  2658    24  3480     8  1448   128     1]]"
8d989490c5392492ad66e6a5047b7d74cc719f30,0.0,"ensemble tuning, hyperparameter tuning",[[    0  8784 23668     6  6676  6583  4401 23668     1]],choosing the answer from the network that had the highest probability and choosing no answer if any of the networks predicted no answer,"[[ 4622     8  1525    45     8  1229    24   141     8  2030 15834    11
   4622   150  1525     3    99   136    13     8  5275 15439   150  1525
      1]]"
f6556d2a8b42b133eaa361f562745edbe56c0b51,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
5d70c32137e82943526911ebdf78694899b3c28a,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
3604c4fba0a82d7139efd5ced47612c90bd10601,100.0,No,[[  0 465   1]],No,[[465   1]]
cb8a6f5c29715619a137e21b54b29e9dd48dad7d,100.0,well-formed sentences vs concise answers,[[    0   168    18 10816 16513     3   208     7 22874  4269     1]],well-formed sentences vs concise answers,[[  168    18 10816 16513     3   208     7 22874  4269     1]]
6b91fe29175be8cd8f22abf27fb3460e43b9889a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Gospel, Sertanejo, MPB, Forró, Pagode, Rock, Samba, Pop, Axé, Funk-carioca, Infantil, Velha-guarda, Bossa-nova and Jovem-guarda","[[15654     6  5631    17   152    15  1927     6  5220   279     6   242
     52  4922     6  2709   839   221     6  3120     6   180 14303     6
   5777     6    71   226   154     6 21867    18    75 14414   658     6
  28405   173     6 11670  1024    18 11010     9     6  1491     7     7
      9    18 14979    11  2194   162    51    18 11010     9     1]]"
6916596253d67f74dba9222f48b9e8799581bad9,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
8e8097cada29d89ca07166641c725e0f8fed6676,30.303,the context of the argument is added to the dataset.,"[[    0     8  2625    13     8  5464    19   974    12     8 17953     5
      1]]","While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument.","[[  818     3 17768     8  1113    13     3     9  1988     6  1105    43
    592    12     8   423  5464  2625    11  2459     6    79    54  6570
    149  1113  1329     3     9  1988    19    16     8   787  2625    13
     46  5464     5     1]]"
4ef3bfebabda83a6d5ca55d30de0e05893f241e3,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
e09dcb6fc163bba7d704178e7edba2e630b573c2,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
07c79edd4c29635dbc1c2c32b8df68193b7701c6,25.0,HEOT,[[   0    3 6021 6951    1]],"HEOT , A labelled dataset for a corresponding english tweets ","[[    3  6021  6951     3     6    71     3 29506 17953    21     3     9
      3  9921 22269 10657     7     1]]"
d5ff8fc4d3996db2c96cb8af5a6d215484991e62,7.4074,DBCA is the best method to measure the compositional generalization of a given set,"[[   0    3 9213 4490   19    8  200 1573   12 3613    8 5761  138  879
  1707   13    3    9  787  356]]",The MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments,"[[   37   283  6931  5679     7  1984     3     9  4019  1146 12771 12355
    122  1433    44     3     9  1126     3 10432 12355   122  1433   116
      3  2172    12     8   119 12341     1]]"
0a4e82dc3728be0bd0325bfe944e7e7de0b98b22,100.0,human representative to review the IVA chat history and resume the failed task,"[[   0  936 6978   12 1132    8   27 8230 3582  892   11 4258    8 4567
  2491    1]]",human representative to review the IVA chat history and resume the failed task,"[[ 936 6978   12 1132    8   27 8230 3582  892   11 4258    8 4567 2491
     1]]"
5b029ad0d20b516ec11967baaf7d2006e8d7199f,100.0,two labels,[[    0   192 11241     1]], two labels ,[[  192 11241     1]]
44c4bd6decc86f1091b5fc0728873d9324cdde4e,0.0,100 million sentences,[[    0   910   770 16513     1]],The ACP corpus has around 700k events split into positive and negative polarity ,"[[   37    71  4184 11736   302    65   300 12283   157   984  5679   139
   1465    11  2841     3  9618   485     1]]"
a91878129583fcb6d16067ba8ba3600e39d70021,0.0,one-hot encoding or weighting by Term Frequency-Inverse Document,"[[    0    80    18 10718     3    35  9886    42  1293    53    57     3
  11679  5532   835 11298    18  1570  7583 11167]]","k-means, hierarchical clustering with Ward's method for merging clusters BIBREF23","[[    3   157    18   526  3247     6  1382  7064  1950  9068    53    28
  15811    31     7  1573    21 12147    53  9068     7     3  5972 25582
    371  2773     1]]"
05671d068679be259493df638d27c106e7dd36d0,0.0,TP-N2F achieves a BLEU score of 89.9,"[[    0     3  7150    18   567   357   371  1984     7     3     9     3
   8775 12062  2604    13     3  3914     5  1298]]","Operation accuracy: 71.89
Execution accuracy: 55.95","[[ 6411  1575  7452    10     3  4450     5  3914 25183    23   106  7452
     10  6897     5  3301     1]]"
adae0c32a69928929101d0ba37d36c0a45298ad6,0.0,"LSTM, LSTM+Seq, LSTM+Seq","[[   0    3 7600 2305    6    3 7600 2305 1220  134   15 1824    6    3
  7600 2305 1220  134   15 1824]]","Support Vector Machines (SVMs), logistic regression, Naïve Bayes, Perceptron, and decision trees, a two-layer bidirectional Gated Recurrent Neural Network (GRNN) BIBREF20 and Convolutional Neural Network (CNN) (as designed in BIBREF21)","[[ 4224 29011  5879     7    41   134 12623     7   201 28820 26625     6
   1823     2   162  2474    15     7     6  1915  6873    52   106     6
     11  1357  3124     6     3     9   192    18 18270  2647 26352 11118
     26   419 14907  1484  9709  3426    41  8727 17235    61     3  5972
  25582   371  1755    11  1193 24817   138  1484  9709  3426    41   254
  17235    61    41     9     7   876    16     3  5972 25582   371  2658
     61     1]]"
859e0bed084f47796417656d7a68849eb9cb324f,100.0,low-frequency words,[[    0   731    18 30989  1234     1]],low-frequency words,[[  731    18 30989  1234     1]]
6367877c05beebfdbb31e83c1f25dfddf925b6b6,50.0,"Cora, Hepth, Hepth, Hepth, Hepth,","[[   0 2487    9    6  216  102  189    6  216  102  189    6  216  102
   189    6  216  102  189    6]]","Cora, Hepth, Zhihu",[[2487    9    6  216  102  189    6 1027  107   23  107   76    1]]
b3dc6d95d1570ad9a58274539ff1def12df8f474,0.0,QA/QC,[[    0     3 23008    87 26265     1]],"Through the experiments, we empirically studied our analysis on DIRL and the effectiveness of our proposed solution in dealing with the problem it suffered from.","[[ 4582     8 12341     6    62 23941   120  7463    69  1693    30 10826
  12831    11     8  9570    13    69  4382  1127    16  4945    28     8
    682    34  8151    45     5     1]]"
eecf62e18a790bcfdd8a56f0c4f498927ff2fb47,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"softly augments a randomly chosen word in a sentence by its contextual mixture of multiple related words, replacing the one-hot representation of a word by a distribution provided by a language model over the vocabulary","[[ 1835   120 15189     7     3     9 21306  3934  1448    16     3     9
   7142    57   165 28131  4989    13  1317  1341  1234     6 11906     8
     80    18 10718  6497    13     3     9  1448    57     3     9  3438
    937    57     3     9  1612   825   147     8 19067     1]]"
5efa19058f815494b72c44d746c157e9403f726e,100.0,micro-averaged F1 score,[[    0  2179    18 28951    26   377   536  2604     1]],micro-averaged F1 score,[[ 2179    18 28951    26   377   536  2604     1]]
b9f852256113ef468d60e95912800fab604966f6,0.0,"TABREF15, TABREF16","[[    0     3  3221 25582   371  1808     6     3  3221 25582   371  2938
      1]]","Camrest, InCar Assistant",[[5184 6216    6   86 6936 9255    1]]
04e90c93d046cd89acef5a7c58952f54de689103,0.0,"BIBREF11, BIBREF12, BIBREF13","[[    0     3  5972 25582   371  2596     6     3  5972 25582   371  2122
      6     3  5972 25582   371  2368     1]]","CMRC-2017, People's Daily (PD), Children Fairy Tales (CFT) , Children's Book Test (CBT)","[[  205 27396    18  9887     6  2449    31     7  8496    41  6251   201
   4351  4506    63 19098     7    41   254  6245    61     3     6  4351
     31     7  3086  2300    41   254  9021    61     1]]"
529dabe7b4a8a01b20ee099701834b60fb0c43b0,16.0,"from a wide range of sources, including radio, television, movie, and movie sets","[[   0   45    3    9 1148  620   13 2836    6  379 2252    6 4390    6
  1974    6   11 1974 3369    1]]","entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement","[[ 4527     6  2772     6  8782     6   577     6  1974     6     3   208
   2152     6   619  6878     6  5023     6  6616     6     3    60 13903
     11 21592     1]]"
288613077787159e512e46b79190c91cd4e5b04d,100.0,"Bi-LSTM, BERT",[[    0  2106    18  7600  2305     6   272 24203     1]],"Bi-LSTM, BERT",[[ 2106    18  7600  2305     6   272 24203     1]]
b67420da975689e47d3ea1c12b601851018c4071,0.0,their model is different from BERT.,[[    0    70   825    19   315    45   272 24203     5     1]],"overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer.","[[ 1879  4648    13     8  4382 20748   138   695  4978    52  3426    41
    188  5332   201    84     3  4894     3  6848    13    46 25078    26
     53  3760     6    46  1388   138 23734    52  3760     6     3     9
   2387    18  9500  1388  3760     6    11    46  3911  3760     5     1]]"
39cfb8473c8be4e5d8ecc3227b800a10477c5f80,23.0769,a correlation of 0.35 and 0.88 between the two datasets,"[[    0     3     9 18712    13  4097  2469    11  4097  4060   344     8
    192 17953     7     1]]",the extent to which the text obtained from the two platforms of Yahoo! Answers and Twitter reflect the true attributes of neighbourhoods,"[[    8  5996    12    84     8  1499  5105    45     8   192  5357    13
  15670    55 11801     7    11  3046  3548     8  1176 12978    13 19704
      7     1]]"
b124137e62178a2bd3b5570d73b1652dfefa2457,0.0,"Graph completion, semantic analysis",[[    0     3 21094  6929     6 27632  1693     1]]," analogy query, analogy browsing",[[10552    63 11417     6 10552    63 14139     1]]
441886f0497dc84f46ed8c32e8fa32983b5db42e,100.0,partisan news detector,[[    0     3 18237  1506 19199     1]],partisan news detector,[[    3 18237  1506 19199     1]]
228425783a4830e576fb98696f76f4c7c0a1b906,0.0,"German, English, Chinese",[[   0 2968    6 1566    6 2830    1]],two translation directions (En-It and En-De),[[ 192 7314 7943   41 8532   18  196   17   11  695   18 2962   61    1]]
f9c5799091e7e35a8133eee4d95004e1b35aea00,100.0,Exp. 5.1,[[    0     3 12882     5     3 20519     1]],Exp. 5.1,[[    3 12882     5     3 20519     1]]
6bff681f1f6743ef7aa6c29cc00eac26fafdabc2,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
e0b7acf4292b71725b140f089c6850aebf2828d2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Word alignments are generated for parallel text, and aligned words are assumed to also share AMR node alignments.","[[ 4467 14632     7    33  6126    21  8449  1499     6    11  7901    15
     26  1234    33 14176    12    92   698    71  9320   150   221 14632
      7     5     1]]"
a4d8fdcaa8adf99bdd1d7224f1a85c610659a9d3,32.0,"performance of new methods is comparable, performance drops of 0.29%","[[    0   821    13   126  2254    19 13289     6   821 11784    13     3
  18189  7561     1]]","Performance was comparable, with the proposed method quite close and sometimes exceeding performance of baseline method.","[[ 8233    47 13289     6    28     8  4382  1573   882   885    11  1664
  19829   821    13 20726  1573     5     1]]"
42bc4e0cd0f3e238a4891142f1b84ebcd6594bf1,0.0,CWVAE,[[    0     3 18105  8230   427     1]],"RNN-based Seq2Seq, Variational Seq2Seq, VRNMT , CWVAE-Unpretrained","[[  391 17235    18   390   679  1824   357   134    15  1824     6 12928
    257   138   679  1824   357   134    15  1824     6 13893   567  7323
      3     6     3 18105  8230   427    18  5110  2026    17 10761     1]]"
e78a47aec37d9a3bec5a18706b0a462c148c118b,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
8740c3000e740ac5c0bc8f329d908309f7ffeff6,0.0,"WSJ, TEDx, TEDx4000","[[    0     3  8439   683     6     3 11430   226     6     3 11430   226
    591  2313     1]]","MojiTalk , PersonaChat , Empathetic-Dialogues","[[  283 21892 27387     3     6  5780     9   254   547     3     6  3967
  27826    18 23770 10384     7     1]]"
dba3d05c495e2c8ca476139e78f65059db2eb72d,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
a7510ec34eaec2c7ac2869962b69cc41031221e5,100.0,52.0%,[[   0 9065    5 6932    1]],52.0%,[[9065    5 6932    1]]
225a567eeb2698a9d3f1024a8b270313a6d15f82,0.0,"LSTM, LSTM-Network, LSTM-Network","[[   0    3 7600 2305    6    3 7600 2305   18 9688 1981    6    3 7600
  2305   18 9688 1981    1]]","RNN model, CNN model , RNN-CNN model, attn1511 model, Deep Averaging Network model, avg mean of word embeddings in the sentence with projection matrix","[[  391 17235   825     6 19602   825     3     6   391 17235    18   254
  17235   825     6    44    17    29  1808  2596   825     6  9509    71
  23980  3426   825     6     3     9   208   122  1243    13  1448 25078
     26    53     7    16     8  7142    28 13440 16826     1]]"
9bffc9a9c527e938b2a95ba60c483a916dbd1f6b,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
ad1f230f10235413d1fe501e414358245b415476,33.3333,"BERT, BIBREF2, and BERT-M","[[    0   272 24203     6     3  5972 25582   371  4482    11   272 24203
     18   329     1]]","BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT","[[ 2106  7600  2305    18  9128     6     3 19990  5244     6     3  3205
   5166     6   480  5166     6     3  3205  5166  1768   262 11160    32
      6    11   272 24203     1]]"
869aaf397c9b4da7ab52d6dd0961887ae08da9ae,44.4444,"Bengali, English, Spanish, Spanish, Arabic, Arabic, Spanish, Arabic, Arabic,","[[    0 20008    23     6  1566     6  5093     6  5093     6 19248     6
  19248     6  5093     6 19248     6 19248     6]]","Bengali, English, German, Spanish, Dutch, Amharic, Arabic, Hindi, Somali ","[[20008    23     6  1566     6  2968     6  5093     6 10098     6   736
  16250    75     6 19248     6 25763     6 26991     1]]"
66125cfdf11d3bf8e59728428e02021177142c3a,0.0,high-accuracy word-alignment,[[   0  306   18 6004  450 4710 1448   18  138 3191  297    1]],explicit projection had a negligible effect on the performance,"[[17623 13440   141     3     9 14261  2825  2317  1504    30     8   821
      1]]"
7d3c036ec514d9c09c612a214498fc99bf163752,0.0,WikiText-TL-39,[[    0  2142  2168 13598    17    18 12733    18  3288     1]],"Online sites tagged as fake news site by Verafiles and NUJP and news website in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera","[[ 1777  1471     3  6153    38  9901  1506   353    57   781     9 11966
      7    11 13046   683   345    11  1506   475    16     8 12729     6
    379 16536    23  3180    32  2042   445   122     9    63   106     6
    891  1841     6    11  4483  1498     1]]"
71fca845edd33f6e227eccde10db73b99a7e157b,46.1538,the baseline provided by BIBREF16,[[    0     8 20726   937    57     3  5972 25582   371  2938     1]],"the baseline provided by BIBREF8, the baselines provided by the ABSA organizers","[[    8 20726   937    57     3  5972 25582   371 11864     8 20726     7
    937    57     8 20798   188 14250     7     1]]"
5c4a2a3d6e02bcbeae784e439441524535916e85,100.0,No,[[  0 465   1]],No,[[465   1]]
c5171daf82107fce0f285fa18f19e91fbd1215c5,0.0,F1 score,[[   0  377  536 2604    1]],"the evaluation metrics include BLEU and ROUGE (1, 2, L) scores","[[    8  5002 15905   560     3  8775 12062    11   391 26260   427  4077
      6  3547   301    61  7586     1]]"
cf0085c1d7bd9bc9932424e4aba4e6812d27f727,66.6667,"Freebase BIBREF0, DBpedia BIBREF1 and YA","[[    0  1443 10925     3  5972 25582   371   632     6     3  9213 24477
      3  5972 25582   371   536    11     3 17419]]","Freebase BIBREF0, DBpedia BIBREF1 and a self-construction game knowledge graph","[[ 1443 10925     3  5972 25582   371   632     6     3  9213 24477     3
   5972 25582   371   536    11     3     9  1044    18 26471   467  1103
   8373     1]]"
1d860d7f615b9ca404c504f9df4231a702f840ef,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
37bc8763eb604c14871af71cba904b7b77b6e089,10.0,using a supervised behavioral state analysis model,[[    0   338     3     9     3 23313 17340   538  1693   825     1]],pre-trained to identify the presence of behavior from a sequence of word using the Couples Therapy Corpus,"[[  554    18    17 10761    12  2862     8  3053    13  3889    45     3
      9  5932    13  1448   338     8 25185     7 13587 10052   302     1]]"
73d657d6faed0c11c65b1ab60e553db57f4971ca,100.0,No,[[  0 465   1]],No,[[465   1]]
bb71a638668a21c2d446b44cbf51676c839658f7,0.0,"validation takes place using the Common Voice website, iPhone app, and microphone","[[    0 16148  1217   286   338     8  7155 12347   475     6  3146  1120
      6    11 18701     1]]","A maximum of three contributors will listen to any audio clip. If an $<$audio,transcript$>$ pair first receives two up-votes, then the clip is marked as valid. If instead the clip first receives two down-votes, then it is marked as invalid.","[[   71  2411    13   386 13932     7    56  3011    12   136  2931  5516
      5   156    46  1514     2  3229 28696     6 11665 11815  3229  3155
   3229  3116   166   911     7   192    95    18  1621  1422     6   258
      8  5516    19  7027    38  3982     5   156  1446     8  5516   166
    911     7   192   323    18  1621  1422     6   258    34    19  7027
     38 17070     5     1]]"
9b2b063e8a9938da195c9c0d6caa3e37a4a615a8,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5d85d7d4d013293b4405beb4b53fa79ac7c03401,33.3333,human preference annotation is added to fine-tuning process,"[[    0   936 11633 30729    19   974    12  1399    18    17   202    53
    433     1]]","human preference annotation is available, $Q(x_1, x_2) \in \lbrace >,<,\approx \rbrace $ is the true label for the pair","[[  936 11633 30729    19   347     6  1514  2247   599   226   834  4347
      3   226   834  7318     3     2    77     3     2    40  1939   565
   2490     6     2     6     2 12497   226     3     2    52  1939   565
   1514    19     8  1176  3783    21     8  3116     1]]"
4b0ba460ae3ba7a813f204abd16cf631b871baca,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],text clustering on the embeddings of texts,[[ 1499  9068    53    30     8 25078    26    53     7    13 14877     1]]
d20d6c8ecd7cb0126479305d27deb0c8b642b09f,0.0,"LSTM-Attention, LSTM-Seq, LSTM","[[   0    3 7600 2305   18  188   17 9174    6    3 7600 2305   18  134
    15 1824    6    3 7600 2305]]","FBanks with cepstral mean normalization (CMN), variance with mean normalization (CMVN)","[[  377 21347     7    28   197   102     7  8792  1243  1389  1707    41
   5518   567   201 27154    28  1243  1389  1707    41  5518   553   567
     61     1]]"
db62d5d83ec187063b57425affe73fef8733dd28,0.0,Laplace-based model,[[   0  325 4687   18  390  825    1]],Markov random field with an optional neural parameterization,[[ 1571  9789  6504  1057    28    46  9042 24228 15577  1707     1]]
5be62428f973a08c303c66018b081ad140c559c8,0.0,F1 score of 0.88,[[   0  377  536 2604   13 4097 4060    1]],"Overall, our AMRAN outperforms all baselines, achieving 0.657 HR@10 and 0.410 NDCG@10.","[[ 9126     6    69  5422 16375    91   883  2032     7    66 20726     7
      6     3  9582     3 22787  3436  8383  1741  1714    11  4097 24175
      3 10604 12150  1741 10415     1]]"
38854255dbdf2f36eebefc0d9826aa76df9637c6,100.0,FarsNet,[[   0 5186    7 9688    1]],FarsNet,[[5186    7 9688    1]]
e70236c876c94dbecd9a665d9ba8cefe7301dcfd,100.0,No,[[  0 465   1]],No,[[465   1]]
07c59824f5e7c5399d15491da3543905cfa5f751,0.0,1700,[[    0     3 26774     1]],"4,261  days for France and 4,748 for the UK",[[6464  357 4241  477   21 1410   11 6464  940 3707   21    8 1270    1]]
d0c79f4a5d5c45fe673d9fcb3cd0b7dd65df7636,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"New best results of accuracy (P@1) on Vecmap:
Ours-GeoMMsemi: EN-IT 50.00 IT-EN 42.67 EN-DE 51.60 DE-EN 47.22 FI-EN 39.62 EN-ES 39.47 ES-EN 36.43","[[  368   200   772    13  7452    41   345  1741  6982    30  3901    75
  11576    10   421     7    18   517    15    32  8257     7    15    51
     23    10 13209    18  3177   305 10667  2344    18  5332  6426     5
   3708 13209    18  5596 11696     5  3328  3396    18  5332 10635     5
   2884     3  4936    18  5332  6352     5  4056 13209    18  3205  6352
      5  4177     3  3205    18  5332  4475     5  4906     1]]"
82fa2b99daa981fc42a882bb6db8481bdbbb9675,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5c4c8e91d28935e1655a582568cc9d94149da2b2,0.0,BERT-based attention,[[    0   272 24203    18   390  1388     1]],About the same performance,[[4504    8  337  821    1]]
45306b26447ea4b120655d6bb2e3636079d3d6e0,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
ed7a3e7fc1672f85a768613e7d1b419475950ab4,100.0,single-domain setting,[[    0   712    18 22999  1898     1]],single-domain setting,[[  712    18 22999  1898     1]]
b7e419d2c4e24c40b8ad0fae87036110297d6752,0.0,"feature identification, question and comment feature, word embeddings, word embeddings","[[    0  1451 10356     6   822    11  1670  1451     6  1448 25078    26
     53     7     6  1448 25078    26    53     7]]","Text Similarity to Source Tweet, Text Similarity to Replied Tweet, Tweet Depth","[[ 5027 18347   485    12  9149 25335     6  5027 18347   485    12  7144
  15310 25335     6 25335 25734   107     1]]"
82bcacad668351c0f81bd841becb2dbf115f000e,86.6667,"a type of reasoning based on word replacement, requires the ability to capture the interaction between","[[    0     3     9   686    13 20893     3   390    30  1448  3709     6
   2311     8  1418    12  4105     8  6565   344]]","a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures","[[    3     9   686    13 20893     3   390    30  1448  3709     6  2311
      8  1418    12  4105     8  6565   344     3 30949   138    11  8953
     17  2708   447  5278     1]]"
d557752c4706b65dcdb7718272180c59d77fb7a7,0.0,LSTM,[[   0    3 7600 2305    1]],unsupervised word segmentation method latticelm,[[   73 23313  1448  5508   257  1573     3    40 15817  7125    51     1]]
b3ac67232c8c7d5a759ae025aee85e9c838584eb,100.0,No,[[  0 465   1]],No,[[465   1]]
bd7039f81a5417474efa36f703ebddcf51835254,0.0,"BiLSTM, BERT",[[    0  2106  7600  2305     6   272 24203     1]],"Reasoner model, also implemented with the MatchLSTM architecture, Ranker model","[[21272    49   825     6    92  6960    28     8 12296  7600  2305  4648
      6     3 22557    49   825     1]]"
bf6c14e9c5f476062cbaaf9179b0c9b751222c8f,22.2222,Basic Questions and Image-Questions,[[    0 11909 14218    11  6298    18  5991   222  2865     1]],the basic question generation module (Module 1) and co-attention visual question answering module (Module 2),"[[    8  1857   822  3381  6008    41 15594    83    15  8925    11   576
     18 25615  3176   822 18243  6008    41 15594    83    15  9266     1]]"
2173809eb117570d289cefada6971e946b902bd6,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
bc9c31b3ce8126d1d148b1025c66f270581fde10,100.0,WN18 and YAGO3-10,[[    0     3 21170  2606    11     3 17419  5577   519  4536     1]],WN18 and YAGO3-10,[[    3 21170  2606    11     3 17419  5577   519  4536     1]]
cb1126992a39555e154bedec388465b249a02ded,85.7143,using a mixture of manual and semi-automatic methods,"[[    0   338     3     9  4989    13  3354    11  4772    18 18698   447
   2254     1]]",using a mixture of manual and semi-automatic techniques,"[[  338     3     9  4989    13  3354    11  4772    18 18698   447  2097
      1]]"
f01a88e15ef518a68d8ca2bec992f27e7a3a6add,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"DISPLAYFORM0, DISPLAYFORM0 DISPLAYFORM1","[[    3 15438   345 29002 24030   632     6     3 15438   345 29002 24030
    632     3 15438   345 29002 24030   536     1]]"
d3839c7acee4f9c8db0a4a475214a8dcbd0bc26f,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"76.58% of the shared links are related to hip-hop music, gangster rap, and the culture that surrounds this music genre","[[  489 17255  5953    13     8  2471  2416    33  1341    12  5436    18
  10776   723     6     3  3810  1370     3  5846     6    11     8  1543
     24 14227     7    48   723  5349     1]]"
974868e4e22f14766bcc76dc4927a7f2795dcd5e,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
022e5c996a72aeab890401a7fdb925ecd0570529,0.0,cooperative method.,[[    0 20270  1573     5     1]],"Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards","[[21272    49   669     7    12  5819     8 17988 10409    45 16534  2639
     57     3     9   168    18    17 10761     3 22557    49     6    11
     34  1393     8     3 22557    49   761    57  1260   996 11157     1]]"
1f63ccc379f01ecdccaa02ed0912970610c84b72,33.3333,between using the mixed objective and using only cross-entropy,"[[    0   344   338     8  4838  5997    11   338   163  2269    18    35
  12395    63     1]]",The mixed objective improves EM by 2.5% and F1 by 2.2%,"[[  37 4838 5997 1172    7    3 6037   57 1682 2712   11  377  536   57
  1682 5406    1]]"
a0ef0633d8b4040bf7cdc5e254d8adf82c8eed5e,0.0,multimodal detection,[[    0  1249 20226 10664     1]], single layer LSTM with a 150-dimensional hidden state for hate / not hate classification,"[[  712  3760     3  7600  2305    28     3     9  4261    18 11619  5697
    538    21  5591     3    87    59  5591 13774     1]]"
2e3265d83d2a595293ed458152d3ee76ad19e244,13.3333,Wikipedea Corpus and WikiPedia,"[[    0  2142  2168  3138    15     9 10052   302    11  2142  2168   345
  18999     1]]",collection of headlines published by HuffPost BIBREF12 between 2012 and 2018,"[[ 1232    13 12392     7  1790    57   454  2999 22507     3  5972 25582
    371  2122   344  1673    11   846     1]]"
cdc5a998cb73262594cdae1dda49576044da3d3d,0.0,BLEU scores,[[    0     3  8775 12062  7586     1]],We evaluate the false-reject (FR) and false-accept (FA) tradeoff across several end-to-end models of distinct sizes and computational complexities.,"[[  101  6825     8  6136    18    60 11827    41  7422    61    11  6136
     18 18693    41  4795    61  1668  1647   640   633   414    18   235
     18   989  2250    13  6746  4342    11 25850     3 30556     5     1]]"
6e8c587b6562fafb43a7823637b84cd01487059a,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Ranges from 44.22 to 100.00 depending on K and the sequence length.,"[[10971     7    45  8537     5  2884    12   335 10667  3345    30   480
     11     8  5932  2475     5     1]]"
cd1792929b9fa5dd5b1df0ae06fc6aece4c97424,100.0,No,[[  0 465   1]],No,[[465   1]]
d8cecea477dfc5163dca6e2078a2fe6bc94ce09f,100.0,accuracy,[[   0 7452    1]],accuracy,[[7452    1]]
37eba8c3cfe23778498d95a7dfddf8dfb725f8e2,0.0,"BiLSTMs, attention-based models",[[   0 2106 7600 2305    7    6 1388   18  390 2250    1]],"Sequential (Denoising) Autoencoder, TF-IDF BOW, SkipThought, FastSent, Siamese C-BOW, C-BOW, C-PHRASE, ParagraphVector","[[26859  7220    41   308    35    32  4890    61  2040    35  4978    52
      6     3  9164    18  4309   371   272 15251     6 25378  8991  4607
     17     6  6805   134   295     6   925     9  2687    15   205    18
    279 15251     6   205    18   279 15251     6   205    18  8023   448
  17892     6  4734  9413   553    15  5317     1]]"
ef081d78be17ef2af792e7e919d15a235b8d7275,0.0,"WNeWPro dataset, WNeWPro dataset","[[    0     3 21170    15   518  3174 17953     6     3 21170    15   518
   3174 17953     1]]","MNLI BIBREF21, AG's News BIBREF22, DBPedia BIBREF23","[[  283 18207   196     3  5972 25582   371  2658     6  8859    31     7
   3529     3  5972 25582   371  2884     6   309 11165 18999     3  5972
  25582   371  2773     1]]"
5633d93ef356aca02592bae3dfc1b3ec8fce27dc,100.0,No,[[  0 465   1]],No,[[465   1]]
c80669cb444a6ec6249b971213b0226f59940a82,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
a33ab5ce8497ff63ca575a80b03e0ed9c6acd273,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
601e58a3d2c03a0b4cd627c81c6228a714e43903,100.0,CAEVO,[[   0 3087  427 8040    1]],CAEVO,[[3087  427 8040    1]]
77af93200138f46bb178c02f710944a01ed86481,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
4a4b7c0d3e7365440b49e9e6b67908ea5cea687d,0.0,"LSTMs, LSTMs",[[   0    3 7600 2305    7    6    3 7600 2305    7    1]],"Majority, ESA, Word2Vec , Binary-BERT","[[ 9236   485     6   262  4507     6  4467   357   553    15    75     3
      6 29221    18 12920   382     1]]"
892e42137b14d9fabd34084b3016cf3f12cac68a,100.0,No,[[  0 465   1]],No,[[465   1]]
d028dcef22cdf0e86f62455d083581d025db1955,0.0,a strong baseline is the attention-based encoder-decoder model BIBRE,"[[    0     3     9  1101 20726    19     8  1388    18   390 23734    52
     18   221  4978    52   825     3  5972 25582]]",optimize single task with no synthetic data,[[13436   712  2491    28   150 13699   331     1]]
e28a6e3d8f3aa303e1e0daff26b659a842aba97b,100.0,No,[[  0 465   1]],No,[[465   1]]
5699996a7a2bb62c68c1e62e730cabf1e3186eef,100.0,No,[[  0 465   1]],No,[[465   1]]
4670e1be9d6a260140d055c7685bce365781d82b,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
6976296126e4a5c518e6b57de70f8dc8d8fde292,0.0,multimodal tweet detection,[[    0  1249 20226 10657 10664     1]],"Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), Textual Kernels Model (TKM)","[[    3 16772  1193  2138    35   257  5154    41  5390   329   201  5641
  10646  1193  2138    35   257  5154    41   134  5518   201  5027  3471
  16469  3573  5154    41 22110   329    61     1]]"
5fa431b14732b3c47ab6eec373f51f2bca04f614,0.0,LSTMs,[[   0    3 7600 2305    7    1]],"TF-IDF, NVDM",[[    3  9164    18  4309   371     6     3 17058  7407     1]]
da2b43d7d048f3f59adf26a67ce66bd2d8a06326,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Training and testing are done in alternating steps: In each epoch, for training, we first present to an LSTM network 1000 samples in a given language, which are generated according to a certain discrete probability distribution supported on a closed finite interval. We then freeze all the weights in our model, exhaustively enumerate all the sequences in the language by their lengths, and determine the first $k$ shortest sequences whose outputs the model produces inaccurately. , experimented with 1, 2, 3, and 36 hidden units for $a^n b^n$ ; 2, 3, 4, and 36 hidden units for $a^n b^n c^n$ ; and 3, 4, 5, and 36 hidden units for $a^n b^n c^n d^n$ . , Following the traditional approach adopted by BIBREF7 , BIBREF12 , BIBREF9 and many other studies, we train our neural network as follows. At each time step, we present one input character to our model and then ask it to predict the set of next possible characters, based on the current character and the prior hidden states. Given a vocabulary $\mathcal {V}^{(i)}$ of size $d$ , we use a one-hot representation to encode the input values; therefore, all the input vectors are $d$ -dimensional binary vectors. The output values are $(d+1)$ -dimensional though, since they may further contain the termination symbol $\dashv $ , in addition to the symbols in $\mathcal {V}^{(i)}$ . The output values are not always one-hot encoded, because there can be multiple possibilities for the next character in the sequence, therefore we instead use a $k$ -hot representation to encode the output values. Our objective is to minimize the mean-squared error (MSE) of the sequence predictions.","[[ 4017    11  2505    33   612    16     3 30859  2245    10    86   284
      3    15   102  6322     6    21   761     6    62   166   915    12
     46     3  7600  2305  1229  5580  5977    16     3     9   787  1612
      6    84    33  6126  1315    12     3     9   824 19217    15 15834
   3438  3510    30     3     9  3168   361  7980  8572     5   101   258
  14038    66     8  1293     7    16    69   825     6 29124   120     3
     35   440    49   342    66     8  5932     7    16     8  1612    57
     70  2475     7     6    11  2082     8   166  1514   157  3229     3
  31115  5932     7     3  2544  3911     7     8   825  9560 27801   120
      5     3     6  5016    15    26    28  1914  3547  6180    11  4475
   5697  3173    21  1514     9     2    29     3   115     2    29  3229
      3   117  3547  6180  6464    11  4475  5697  3173    21  1514     9
      2    29     3   115     2    29     3    75     2    29  3229     3
    117    11  6180  6464  7836    11  4475  5697  3173    21  1514     9
      2    29     3   115     2    29     3    75     2    29     3    26
      2    29  3229     3     5     3     6  6851     8  1435  1295  7546
     57     3  5972 25582   371   940     3     6     3  5972 25582   371
   2122     3     6     3  5972 25582   371  1298    11   186   119  2116
      6    62  2412    69 24228  1229    38  6963     5   486   284    97
   1147     6    62   915    80  3785  1848    12    69   825    11   258
    987    34    12  9689     8   356    13   416   487  2850     6     3
    390    30     8   750  1848    11     8  1884  5697  2315     5  9246
      3     9 19067  1514     2  3357   107  1489     3     2   553     2
    599    23    61     2  3229    13   812  1514    26  3229     3     6
     62   169     3     9    80    18 10718  6497    12 23734     8  3785
   2620   117  2459     6    66     8  3785 12938     7    33  1514    26
   3229     3    18 11619 14865 12938     7     5    37  3911  2620    33
   1514   599    26 18446    61  3229     3    18 11619   713     6   437
     79   164   856  3480     8 18739  6083  1514     2    26  3198   208
   1514     3     6    16   811    12     8 13619    16  1514     2  3357
    107  1489     3     2   553     2   599    23    61     2  3229     3
      5    37  3911  2620    33    59   373    80    18 10718 23734    26
      6   250   132    54    36  1317  7839    21     8   416  1848    16
      8  5932     6  2459    62  1446   169     3     9  1514   157  3229
      3    18 10718  6497    12 23734     8  3911  2620     5   421  5997
     19    12 10558     8  1243    18 19687    26  3505    41   329  4132
     61    13     8  5932 20099     5     1]]"
981443fce6167b3f6cadf44f9f108d68c1a3f4ab,100.0,german,[[    0 13692     1]],german ,[[13692     1]]
d58c264068d8ca04bb98038b4894560b571bab3e,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
221e9189a9d2431902d8ea833f486a38a76cbd8e,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],The average number of utterances per dialog is about 23 ,"[[   37  1348   381    13     3  5108   663     7   399 13463    19    81
   1902     1]]"
36d892460eb863220cd0881d5823d73bbfda172c,18.1818,"MultiRC BIBREF4, MCTest BIBREF5","[[    0  4908  4902     3  5972 25582   371  8525   283  6227   222     3
   5972 25582   371   755     1]]","DREAM, MCTest, TOEFL, and SemEval-2018 Task 11","[[    3  3913 22684     6   283  6227   222     6  3001  9976   434     6
     11   679    51   427  2165 20259 16107   850     1]]"
a1064307a19cd7add32163a70b6623278a557946,16.6667,100 million uni words,[[   0  910  770   73   23 1234    1]],908456 unique words are available in collected corpus.,"[[ 2777  4608  4834   775  1234    33   347    16  4759 11736   302     5
      1]]"
69a88b6be3b34acc95c5e36acbe069c0a0bc67d6,0.0,"70,000",[[    0     3 28891     1]],"8.1 million scientific documents, 154K computer science articles, 622K citing sentences","[[    3 20677   770  4290  2691     6     3 27308   439  1218  2056  2984
      6   431  2884   439     3 17994 16513     1]]"
dcd8138f0cba0dcd109ccb21c228da5c110a68eb,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
ed6462da17c553bda112ef35917fefe6942fce3c,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Feature selection, Random forest, XGBoost, Hierarchical Model","[[    3 16772  1801     6 25942  5827     6     3     4   517 16481     6
   3204  7064  1950  5154     1]]"
100cf8b72d46da39fedfe77ec939fb44f25de77f,10.0,Chinese dataset BIBREF0,[[    0  2830 17953     3  5972 25582   371   632     1]],"dataset that contains article-comment parallel contents INLINEFORM0 , and an unpaired dataset that contains the documents (articles or comments) INLINEFORM1","[[17953    24  2579  1108    18   287   297  8449 10223  3388 20006 24030
    632     3     6    11    46    73 13804 17953    24  2579     8  2691
     41  8372     7    42  2622    61  3388 20006 24030   536     1]]"
a3d83c2a1b98060d609e7ff63e00112d36ce2607,0.0,one,[[ 0 80  1]],27.41 transformation on average of single seed sentence is available in dataset.,"[[ 2307     5  4853  6586    30  1348    13   712  6677  7142    19   347
     16 17953     5     1]]"
5065ff56d3c295b8165cb20d8bcfcf3babe9b1b8,0.0,BLEU and TER scores,[[    0     3  8775 12062    11     3  5946  7586     1]],"BLEU-3/4, ROUGE-2/L, CIDEr, SPICE, BERTScore","[[    3  8775 12062  3486 13572     6   391 26260   427    18 15896   434
      6   205 13162    52     6  6760  8906     6     3 12920  4578  9022
      1]]"
18dab362ae4587408a291a55299f347f8870e9f1,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
ce18c50dadab7b9f28141fe615fd7de69355d9dd,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"RDF was designed as an abstract information model and FHIR was designed for operational use in a healthcare setting, RDF makes statements of fact, whereas FHIR makes records of events, RDF is intended to have the property of monotonicity, meaning that previous facts cannot be invalidated by new facts","[[  391 10665    47   876    38    46  9838   251   825    11   377  7094
    448    47   876    21  7763   169    16     3     9  4640  1898     6
    391 10665   656  6643    13   685     6     3 10339   377  7094   448
    656  3187    13   984     6   391 10665    19  3855    12    43     8
    785    13 30378    23  6726     6  2530    24  1767  6688  1178    36
  17070   920    57   126  6688     1]]"
b6a6bdca6dee70f8fe6dd1cfe3bb2c5ff03b1605,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
9a65cfff4d99e4f9546c72dece2520cae6231810,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev","[[  37 4382  825 1984    7    3 6037    3 4013    6 3891   11  377  536
  2775    6 4552   30    8  794   11    3 6037    3 3959    6 3301   11
   377  536 2775    6 1828   30    8   20  208    1]]"
d1909ce77d09983aa1b3ab5c56e2458caefbd442,0.0,accuracy,[[   0 7452    1]],"entity match rate, BLEU score, Success F1 score","[[10409  1588  1080     6     3  8775 12062  2604     6 16581   377   536
   2604     1]]"
74cd51a5528c6c8e0b634f3ad7a9ce366dfa5706,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],it is less expensive and quantifies interpretability using continuous values rather than binary evaluations,"[[   34    19   705  2881    11 13500 15821  7280  2020   338  7558  2620
   1066   145 14865  5002     7     1]]"
d2fbf34cf4b5b1fd82394124728b03003884409c,100.0,IDEA,[[    0    27 25473     1]],IDEA,[[   27 25473     1]]
b48cd91219429f910b1ea6fcd6f4bd143ddf096f,20.0,"F1 score, BLEU score, Distinct tokens score","[[    0   377   536  2604     6     3  8775 12062  2604     6  2678 19827
  14145     7  2604     1]]","BLEU, Distinct-1 & distinct-2",[[    3  8775 12062     6  2678 19827  2292     3   184  6746  4949     1]]
2a57fdc7e985311989b6829c1ceb201096e5c809,0.0,"spelling mistakes, emoticons",[[    0 19590  8176     6 25993  8056     1]],"Parts of Speech (POS) tags, Prior polarity of the words, Capitalization, Negation, Text Feature","[[ 2733     7    13 26351    41 16034    61 12391     6  6783     3  9618
    485    13     8  1234     6  5826  1707     6 17141   257     6  5027
      3 16772     1]]"
1835f65694698a9153857e33cd9b86a96772fff5,100.0,No,[[  0 465   1]],No,[[465   1]]
586b7470be91efe246c3507b05e30651ea6b9832,53.8462,"By capturing both high-order structural information of KGs, we can achieve the desired","[[    0   938     3 18147   321   306    18  9397  8649   251    13     3
  18256     7     6    62    54  1984     8  5327]]","To capture both high-order structural information of KGs, we used an attention-based embedding propagation method.","[[  304  4105   321   306    18  9397  8649   251    13     3 18256     7
      6    62   261    46  1388    18   390 25078    26    53 17554   257
   1573     5     1]]"
f319f2c3f9339b0ce47478f5aa0c32da387a156e,100.0,"Penn Treebank, Text8",[[    0 11358  7552  4739     6  5027   927     1]],"Penn Treebank, Text8",[[11358  7552  4739     6  5027   927     1]]
0da6cfbc8cb134dc3d247e91262f5050a2200664,0.0,SVM,[[    0   180 12623     1]],"Clusters of Twitter user ids from accounts of American or German political actors, musicians, media websites or sports club","[[28552     7    13  3046  1139     3    23    26     7    45  3744    13
    797    42  2968  1827 10485     6 11424     6   783  3395    42  2100
   1886     1]]"
096f5c59f43f49cab1ef37126341c78f272c0e26,0.0,210,[[    0     3 15239     1]],"51,104",[[11696     6 15442     1]]
dc28ac845602904c2522f5349374153f378c42d3,100.0,"44,000 tweets",[[    0   314 13161 10657     7     1]],"44,000 tweets",[[  314 13161 10657     7     1]]
fbe22e133fa919f06abd8afbed3395af51d2bfef,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
6fd07f4dc037a82c8fa0ed80469eb4171dcebf12,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
ad7b13579823cbc7825421c84d16f23ed863f6ee,0.0,"BIBREF13, BIBREF26","[[    0     3  5972 25582   371  2368     6     3  5972 25582   371  2688
      1]]","VATEX, WMT 2014 English-to-German, and VQA-v2 datasets","[[  584  6048     4     6   549  7323  1412  1566    18   235    18 24518
      6    11   584 23008    18   208   357 17953     7     1]]"
68e3f3908687505cb63b538e521756390c321a1c,100.0,2.7 accuracy points,[[    0     3 21280  7452   979     1]],2.7 accuracy points,[[    3 21280  7452   979     1]]
1231934db6adda87c1b15e571468b8e9d225d6fe,0.0,3,[[  0 220   1]],"Excluding the embedding weights, our model requires 100k parameters","[[ 1881 10562    26    53     8 25078    26    53  1293     7     6    69
    825  2311   910   157  8755     1]]"
a09633584df1e4b9577876f35e38b37fdd83fa63,0.0,diversity,[[   0 7322    1]],Through Amazon MTurk annotators to determine plausibility and content richness of the response,"[[ 4582  2536     3  7323   450   157    46  2264  6230    12  2082  9564
    302 11102    11   738  2354   655    13     8  1773     1]]"
e8fcfb1412c3b30da6cbc0766152b6e11e17196c,0.0,a corresponding improvement over the SOTA on language modelling,"[[    0     3     9     3  9921  4179   147     8   180 27976    30  1612
  24716     1]]",Perpexity is improved from 34.7 to 28.0.,"[[ 1915   102   994   485    19  3798    45   220 25211    12   204 27376
      5     1]]"
b34c60eb4738e0439523bcc679fe0fe70ceb8bde,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"in the OpenBookQA setup the open book part is much larger, the open book part is much larger (than a small paragraph) and is not complete as additional common knowledge may be required","[[   16     8  2384 13355 23008  5818     8   539   484   294    19   231
   2186     6     8   539   484   294    19   231  2186    41  6736     3
      9   422  8986    61    11    19    59   743    38  1151  1017  1103
    164    36   831     1]]"
2965c86467d12b79abc16e1457d848cb6ca88973,33.3333,Dialogue Act Labeling,[[    0  5267 10384  1983 16229    53     1]],Dialogue Act Markup in Several Layers (DAMSL) tag set,"[[ 5267 10384  1983  2185   413    16     3  8656 22697     7    41   308
  25346   434    61  7860   356     1]]"
896e99d7f8f957f6217185ff787e94f84c136087,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Unsupervised CLWEs. These methods first induce a seed dictionary $D^{(1)}$ leveraging only two unaligned monolingual spaces (C1). While the algorithms for unsupervised seed dictionary induction differ, they all strongly rely on the assumption of similar topological structure between the two pretrained monolingual spaces. Once the seed dictionary is obtained, the two-step iterative self-learning procedure (C2) takes place: 1) a dictionary $D^{(k)}$ is first used to learn the joint space $\mathbf {Y}^{(k)} = \mathbf {X{W}}^{(k)}_x \cup \mathbf {Z{W}}^{(k)}_z$ ; 2) the nearest neighbours in $\mathbf {Y}^{(k)}$ then form the new dictionary $D^{(k+1)}$ . We illustrate the general structure in Figure 1 .","[[  597 23313 11175 15713     7     5   506  2254   166 21151     3     9
   6677 24297  1514   308     2 14296     2  3229     3 26072   163   192
     73   138  6962    26  7414 25207  4856    41   254 13883   818     8
  16783    21    73 23313  6677 24297    16  8291  7641     6    79    66
   7157     3  4610    30     8 20662    13  1126   420  4478  1809   344
      8   192  7140 10761  7414 25207  4856     5  1447     8  6677 24297
     19  5105     6     8   192    18  7910    34    49  1528  1044    18
  20779  3979    41   254  7318  1217   286    10  8925     3     9 24297
   1514   308     2   599   157    61     2  3229    19   166   261    12
    669     8  4494   628  1514     2  3357   107   115    89     3     2
    476     2   599   157    61     2  3274     3     2  3357   107   115
     89     3     2     4     2   518     2   599   157    61     2   834
    226     3     2  4658     3     2  3357   107   115    89     3     2
    956     2   518     2   599   157    61     2   834   172  3229     3
    117  9266     8 13012 14245     7    16  1514     2  3357   107   115
     89     3     2   476     2   599   157    61     2  3229   258   607
      8   126 24297  1514   308     2   599   157 18446    61     2  3229
      3     5   101 11485     8   879  1809    16  7996   209     3     5
      1]]"
5067e5eb2cddbb34b71e8b74ab9210cd46bb09c5,87.5,Matching features from matching sentences from multiple perspectives.,[[    0 12296    53   753    45  8150 16513    45  1317 14013     5     1]],Matching features from matching sentences from various perspectives.,[[12296    53   753    45  8150 16513    45   796 14013     5     1]]
54415efa91566d5d7135fa23bce3840d41a6389e,100.0,300-dimensional vectors,[[    0  3147    18 11619 12938     7     1]],300-dimensional vectors,[[ 3147    18 11619 12938     7     1]]
352a1bf734b2d7f0618e9e2b0dbed4a3f1787160,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
c47f593a5b92abc2e3c536fe2baaca226913688b,0.0,"atomic transformation, sequence of transformations, LSTM","[[    0     3 20844  6586     6  5932    13  6586     7     6     3  7600
   2305     1]]",See Figure FIGREF3,[[ 1610  7996 11376  4386   371   519     1]]
b6c235d5986914b380c084d9535a7b01310c0278,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"correct class can be directly inferred from the text content easily, even without background knowledge, correct class can be inferred from the text content, given that event-specific knowledge is provided, orrect class can be inferred from the text content if the text is interpreted correctly","[[ 2024   853    54    36  1461    16  1010  1271    45     8  1499   738
   1153     6   237   406  2458  1103     6  2024   853    54    36    16
   1010  1271    45     8  1499   738     6   787    24   605    18  9500
   1103    19   937     6    42 12621   853    54    36    16  1010  1271
     45     8  1499   738     3    99     8  1499    19     3 20603  6549
      1]]"
53bf6238baa29a10f4ff91656c470609c16320e1,100.0,Users' tweets,[[    0 13504    31 10657     7     1]],Users' tweets,[[13504    31 10657     7     1]]
5758ebff49807a51d080b0ce10ba3f86dcf71925,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],low-rank approximation of the co-occurrence matrix,"[[  731    18  6254  1120 12907   603   257    13     8   576    18 16526
  16826     1]]"
9a94dcee17cdb9a39d39977191e643adece58dfc,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
2b52d481b30185d2c6e7b403d37277f70337d6ca,13.3333,type-logical grammars,[[    0   686    18  6207 19519     7     1]],"a family of grammar formalisms built on a foundation of logic and type theory. Type-logical grammars originated when BIBREF4 introduced his Syntactic Calculus (called the Lambek calculus, L, by later authors).","[[    3     9   384    13 19519   607  6835     7  1192    30     3     9
   3361    13  9769    11   686  4516     5  6632    18  6207 19519     7
  23809   116     3  5972 25582   371   591  3665   112  8951    17  2708
    447 18555   302    41  9341     8 19211    15   157  7779   302     6
    301     6    57   865  5921   137     1]]"
61fba3ab10f7b6906e27b028fb1d42ec601c3fb8,0.0,No,[[  0 465   1]],Unanswerable,[[ 597 3247 3321  179    1]]
a996b6aee9be88a3db3f4127f9f77a18ed10caba,0.0,accuracy of 0.88,[[   0 7452   13 4097 4060    1]],"0.8320 on semantic typing, 0.7194 on entity matching","[[    3 22384 15003    30 27632 21321     6  4097  4450  4240    30 10409
   8150     1]]"
c4a6b727769328333bb48d59d3fc4036a084875d,22.2222,FastQA,[[    0  6805 23008     1]],"Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN","[[ 3892     6  6805 23008     6  2106  4296   371     6  9020    89    18
    517  8503     6   283 13201  7381     6   101     9   624     3    87
   9101   122     9     6     3 20131 23008    18  8727   567     1]]"
cca3301f20db16f82b5d65a102436bebc88a2026,35.0,122 rows of text messages from a labelled dataset were collected from a corresponding,"[[    0     3 20889 17918    13  1499  4175    45     3     9     3 29506
  17953   130  4759    45     3     9     3  9921]]","A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al, HEOT obtained from one of the past studies done by Mathur et al","[[   71     3 29506 17953    21     3     9     3  9921 22269 10657     7
    130    92  5105    45     3     9   810  4468    57     3 23268     3
     15    17   491     6     3  6021  6951  5105    45    80    13     8
    657  2116   612    57  9762   450     3    15    17   491     1]]"
6c4cd8da5b4b298f29af3123b58d9a5d4b02180b,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
c4b5cc2988a2b91534394a3a0665b0c769b598bb,31.5789,a global variance loss to encourage the model to put most of the attention on just a,"[[    0     3     9  1252 27154  1453    12  2454     8   825    12   474
    167    13     8  1388    30   131     3     9]]",The reciprocal of the variance of the attention distribution,[[   37 22882   138    13     8 27154    13     8  1388  3438     1]]
a997fc1a62442fd80d1873cd29a9092043f025ad,0.0,Adaptive schedules,[[    0     3 14808   757  2023     7     1]],"Transformer models in their base configuration BIBREF11, using 6 encoder and decoder layers, with model and hidden dimensions of 512 and 2048 respectively, and 8 heads for all attention layers","[[31220  2250    16    70  1247  5298     3  5972 25582   371  2596     6
    338   431 23734    52    11    20  4978    52  7500     6    28   825
     11  5697  8393    13     3 24163    11   460  3707  6898     6    11
    505  7701    21    66  1388  7500     1]]"
4d223225dbf84a80e2235448a4d7ba67bfb12490,10.5263,"removing redundancy in the annotation scheme, removing redundancy in the annotation scheme","[[    0     3  8499  1131  1106  6833    16     8 30729  5336     6     3
   8499  1131  1106  6833    16     8 30729  5336]]",removing AltLexC and adding Progression into our sense hierarchy,"[[    3  8499  4588   434   994   254    11  2651   749 22430   139    69
   1254 25515     1]]"
7b3d207ed47ae58286029b62fd0c160a0145e73d,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
d70ba6053e245ee4179c26a5dabcad37561c6af0,0.0,ACL-HLT 2019,[[   0   71 8440   18  566 9012 1360    1]],ConciergeQA and AmazonQA,[[ 1193  9186   397 23008    11  2536 23008     1]]
447eb98e602616c01187960c9c3011c62afd7c27,16.6667,"gloomy, dark, satanic, or harsh topics","[[    0     3   122 14351    63     6  2164     6     3     7   144     9
   2532     6    42 11955  4064     1]]",Table TABREF10 displays the twenty resulting topics,"[[ 4398     3  3221 25582   371  1714  8397     8  6786     3  5490  4064
      1]]"
a883bb41449794e0a63b716d9766faea034eb359,37.5,images and text,[[   0 1383   11 1499    1]],"context is a procedural text, the question and the multiple choice answers are composed of images","[[ 2625    19     3     9 19291    40  1499     6     8   822    11     8
   1317  1160  4269    33 10431    13  1383     1]]"
1eeabfde99594b8d9c6a007f50b97f7f527b0a17,100.0,validation data,[[    0 16148   331     1]],validation data,[[16148   331     1]]
decb07f9be715de024236e50dc7011a132363480,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"CNN model can be trained in a purely online setting. We first initialize the model parameters $\theta _0$ (line 1), which can be a trained model from other disaster events or it can be initialized randomly to start from scratch.

As a new batch of labeled tweets $B_t= \lbrace \mathbf {s}_1 \ldots \mathbf {s}_n \rbrace $ arrives, we first compute the log-loss (cross entropy) in Equation 11 for $B_t$ with respect to the current parameters $\theta _t$ (line 2a). Then, we use backpropagation to compute the gradients $f^{\prime }(\theta _{t})$ of the loss with respect to the current parameters (line 2b). Finally, we update the parameters with the learning rate $\eta _t$ and the mean of the gradients (line 2c). We take the mean of the gradients to deal with minibatches of different sizes. Notice that we take only the current minibatch into account to get an updated model. ","[[19602   825    54    36  4252    16     3     9     3 18760   367  1898
      5   101   166  2332  1737     8   825  8755  1514     2   532    17
      9     3   834   632  3229    41   747  8925     6    84    54    36
      3     9  4252   825    45   119  6912   984    42    34    54    36
   2332  1601 21306    12   456    45  8629     5   282     3     9   126
  11587    13  3783    15    26 10657     7  1514   279   834    17  2423
      3     2    40  1939   565     3     2  3357   107   115    89     3
      2     7     2   834   536     3     2    40    26    32    17     7
      3     2  3357   107   115    89     3     2     7     2   834    29
      3     2    52  1939   565  1514 16732     6    62   166 29216     8
   4303    18  2298     7    41 11465     3    35 12395    63    61    16
  25262   257   850    21  1514   279   834    17  3229    28  1445    12
      8   750  8755  1514     2   532    17     9     3   834    17  3229
     41   747   204     9   137    37    29     6    62   169   223  1409
  11057   257    12 29216     8 26462     7  1514    89     2  8234    15
      3     2   599     2   532    17     9     3   834     2    17     2
     61  3229    13     8  1453    28  1445    12     8   750  8755    41
    747   204   115   137  4213     6    62  2270     8  8755    28     8
   1036  1080  1514     2    15    17     9     3   834    17  3229    11
      8  1243    13     8 26462     7    41   747   204    75   137   101
    240     8  1243    13     8 26462     7    12  1154    28  3016  3697
   2951    13   315  4342     5 13946    24    62   240   163     8   750
   3016   115 14547   139   905    12   129    46  3250   825     5     1]]"
3f5f74c39a560b5d916496e05641783c58af2c5d,28.5714,using a mixture of manual and synthetic methods,[[    0   338     3     9  4989    13  3354    11 13699  2254     1]],"Random perturbation of Wikipedia sentences using mask-filling with BERT, backtranslation and randomly drop out","[[25942 29404   257    13 16885 16513   338  8181    18 16326    53    28
    272 24203     6   223  7031  6105    11 21306  2328    91     1]]"
2a46db1b91de4b583d4a5302b2784c091f9478cc,0.0,5,[[  0 305   1]],"Around 388k examples, 194k from tst2013 (in-domain) and 194k from newstest2014 (out-of-domain)","[[16140   220  4060   157  4062     6   957   591   157    45     3    17
      7    17 11138    41    77    18 22999    61    11   957   591   157
     45  1506  4377 10218    41   670    18   858    18 22999    61     1]]"
d86c7faf5a61d73a19397a4afa2d53206839b8ad,100.0,"Language, Vision, Acoustic",[[    0 10509     6 10886     6 31614     1]],"Language, Vision, Acoustic",[[10509     6 10886     6 31614     1]]
212495af630c16745d0fcb614119d75327952271,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
018ef092ffc356a2c0e970ae64ad3c2cf8443288,66.6667,8772 news records,[[   0    3 4225 5865 1506 3187    1]],8757 news records,[[   3 4225 3436 1506 3187    1]]
5872279c5165cc8a0c58cf1f89838b7c43217b0e,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
36b25021464a9574bf449e52ae50810c4ac7b642,100.0,From Twitter profile descriptions of the users.,[[    0  1029  3046  3278 15293    13     8  1105     5     1]],From Twitter profile descriptions of the users.,[[ 1029  3046  3278 15293    13     8  1105     5     1]]
4e379d6d5f87554fabf6f7f7b6ed92d2025e7280,100.0,CSKS task,[[    0     3  4778 13383  2491     1]],CSKS task,[[    3  4778 13383  2491     1]]
1898f999626f9a6da637bd8b4857e5eddf2fc729,13.9535,F1 score of 82.0% compared to previous work.,"[[   0  377  536 2604   13    3 4613    5 6932    3 2172   12 1767  161
     5    1]]","Our WordDecoding (WDec) model achieves F1 scores that are $3.9\%$ and $4.1\%$ higher than HRL on the NYT29 and NYT24 datasets respectively, In the ensemble scenario, compared to HRL, WDec achieves $4.2\%$ and $3.5\%$ higher F1 scores","[[  421  4467  2962  9886    41 17698    15    75    61   825  1984     7
    377   536  7586    24    33  1514 28640     2  1454  3229    11  1514
  19708     2  1454  3229  1146   145  8383   434    30     8  5825   382
   3166    11  5825   382  2266 17953     7  6898     6    86     8  8784
   8616     6     3  2172    12  8383   434     6     3 17698    15    75
   1984     7  1514 19765     2  1454  3229    11  1514  9285     2  1454
   3229  1146   377   536  7586     1]]"
d7d611f622552142723e064f330d071f985e805c,0.0,6,[[  0 431   1]],Total number of utterances available is: 70607 (37344 ENG + 33263 GER),"[[ 9273   381    13     3  5108   663     7   347    19    10  2861  3328
    940  6918  4552  3628     3 23182  1768   220  2668  3891     3 17966
     61     1]]"
36383971a852d1542e720d3ea1f5adeae0dbff18,0.0,"DAQUAR BIBREF0, COCO-QA BIBREF","[[    0     3  4296 16892  4280     3  5972 25582   371   632     3     6
   2847  5911    18 23008     3  5972 25582   371]]","holistic, TraAtt, RegAtt, ConAtt, ConAtt, iBOWIMG , VQA, VQA, WTL , NMN , SAN , AMA , FDA , D-NMN, DMN+","[[16676     6  3083   188    17    17     6  7777   188    17    17     6
   1193   188    17    17     6  1193   188    17    17     6     3    23
   8471 16785  9306     3     6   584 23008     6   584 23008     6   549
  12733     3     6     3 16568   567     3     6     3 19976     3     6
      3 21250     3     6 13648     3     6   309    18 16568   567     6
      3  7407   567  1220     1]]"
55c840a2f1f663ab2bff984ae71501b17429d0c0,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
acf278679c584ae4f332f6134711602af26edfb4,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
7772cb23b7609f1d4cfd6511ac3fcdc20f8481ba,0.0,BERT and Flair,[[    0   272 24203    11  5766  2256     1]],"Table TABREF44, Table TABREF44, Table TABREF47, Table TABREF47","[[ 4398     3  3221 25582   371  3628     6  4398     3  3221 25582   371
   3628     6  4398     3  3221 25582   371  4177     6  4398     3  3221
  25582   371  4177     1]]"
7cf726db952c12b1534cd6c29d8e7dfa78215f9e,11.7647,word confusion network (DNN),[[    0  1448 12413  1229    41   308 17235    61     1]],It is a network used to encode speech lattices to maintain a rich hypothesis space.,"[[   94    19     3     9  1229   261    12 23734  5023    50    17  1225
     15     7    12  1961     3     9  2354 22455   628     5     1]]"
819d2e97f54afcc7cdb3d894a072bcadfba9b747,0.0,"MNIST, Toronto Face Database (TFD), CIFAR-10, SVH","[[    0   283   567 13582     6  7030  8881 20230    41  9164   308   201
    205  6058  4280  4536     6     3  7416   566]]","CNN, TIME, 20 Newsgroups, and Reuters-21578","[[19602     6   332 15382     6   460  3529 10739     7     6    11     3
  18844  4949  1808  3940     1]]"
3b7798a6bce1a5faf411bb12e2e011dbab1e279d,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
edb068df4ffbd73b379590762125990fcd317862,55.1724,Stanford Sentiment Treebank BIBREF7 and AG English news corpus BI,"[[    0 19796  4892  2998   295  7552  4739     3  5972 25582   371   940
     11  8859  1566  1506 11736   302     3  5972]]", They used Stanford Sentiment Treebank benchmark for sentiment classification task and   AG English news corpus for the text classification task.,"[[  328   261 19796  4892  2998   295  7552  4739 15705    21  6493 13774
   2491    11  8859  1566  1506 11736   302    21     8  1499 13774  2491
      5     1]]"
43761478c26ad65bec4f0fd511ec3181a100681c,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
f398587b9a0008628278a5ea858e01d3f5559f65,9.0909,SPNet outperforms by 0.86 on evaluation and 0.88 on validation,"[[    0  6760  9688    91   883  2032     7    57  4097  3840    30  5002
     11  4097  4060    30 16148     1]]","SPNet vs best baseline:
ROUGE-1: 90.97 vs 90.68
CIC: 70.45 vs 70.25","[[ 6760  9688     3   208     7   200 20726    10   391 26260   427  2292
     10  2777     5  4327     3   208     7  2777     5  3651   205  4666
     10  2861     5  2128     3   208     7  2861     5  1828     1]]"
1ef8d1cb1199e1504b6b0daea52f2e4bd2ef7023,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
fed0785d24375ebbde51fb0503b93f14da1d8583,0.0,No,[[  0 465   1]],"These models are likely to be deficient in encoding morphological features is that they are word level models, and do not have direct access sub-word information like inflectional endings, which indicates that these features are difficult to learn effectively purely from lexical distributions.","[[  506  2250    33   952    12    36    20 17397    16     3    35  9886
      3  8886  4478   753    19    24    79    33  1448   593  2250     6
     11   103    59    43  1223   592   769    18  6051   251   114    16
  26303  6318  7784     7     6    84  9379    24   175   753    33  1256
     12   669  3762     3 18760    45     3 30949   138  3438     7     5
      1]]"
4bd894c365d85e20753d9d2cb6edebb8d6f422e9,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"we devise a test set consisting of ‘adversarial’ examples, i.e, perturbed examples that can potentially change the base model's prediction. , We use two approaches described in literature: back-translation and noisy sequence autoencoder.","[[   62 13282    15     3     9   794   356  5608    53    13   458     9
     26  2660     9 12042    22  4062     6     3    23     5    15     6
  29404    15    26  4062    24    54  6149   483     8  1247   825    31
      7 21332     5     3     6   101   169   192  6315  3028    16  6678
     10   223    18  7031  6105    11 26847  5932  1510    35  4978    52
      5     1]]"
dee116df92f9f92d9a67ac4d30e32822c22158a6,100.0,No,[[  0 465   1]],No,[[465   1]]
66f0dee89f084fe0565539a73f5bbe65f3677814,100.0,No,[[  0 465   1]],No,[[465   1]]
45893f31ef07f0cca5783bd39c4e60630d6b93b3,12.5,They use a random selection procedure to select the facts that are monotonicity.,"[[    0   328   169     3     9  6504  1801  3979    12  1738     8  6688
     24    33 30378    23  6726     5     1]]",They derive it from Wordnet,[[ 328   74  757   34   45 4467 1582    1]]
9190c56006ba84bf41246a32a3981d38adaf422c,0.0,GloVe BIBREF6,[[    0  9840   553    15     3  5972 25582   371   948     1]],"extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila)","[[21527    45     3     9 11986    13     8  4338 16885    41 14134  4144
  14161  4542   201    45     8   711  5897    13  4338  1163  3529    41
  30944     6   445  8015     6     3 31855     6     3 25036  7400 20151
    476     6     3  6431 24203 13570 11810     6   180 14536   134     6
    180  3597 22614     6     3  6021  4090  4611    61    11    45   128
  19581  1601  3582     7   344  1105    11     3     9   884   124  3582
   4045    41  3612   173     9    61     1]]"
abad9beb7295d809d7e5e1407cbf673c9ffffd19,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
e63bde5c7b154fbe990c3185e2626d13a1bad171,0.0,82.0%,[[   0    3 4613    5 6932    1]],"Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87","[[11805    76  2292    10 10630     5  2596     6 11805    76  4278    10
    604     5  4906     6  7934  3463  2990    10  2208     5  2368     6
    391 26260   427    18   434    10     3  3390     5  4225     1]]"
931a2a13a1f6a8d9107d26811089bdccc39b0800,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"simply split the image into two parts. One for the text input, and the other for the tabular data","[[ 914 5679    8 1023  139  192 1467    5  555   21    8 1499 3785    6
    11    8  119   21    8 3808 4885  331    1]]"
85a7dbf6c2e21bfb7a3a938381890ac0ec2a19e0,0.0,WSJT-T1,[[   0    3 8439  683  382   18  382  536    1]],"English$\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)","[[ 1566  3229     2  3535  6770  1514   196    17     9  9928    87 24518
  17622    13     8  4159  4209    18   254 11736   302     6   282  1151
    331     6    62   169     3     9  2153    13   452    11 16950   331
     21    81   898   770  7142 14152    21  1566    18   196    17     9
   9928    41  8532    18   196    17    61    11  1514 23444  3229   770
    549  7323  2534  7142 14152    21     8  1566    18 24518    41  8532
     18  2962    61     1]]"
16f33de90b76975a99572e0684632d5aedbd957c,0.0,Wiktionary,[[    0  2142 12696  1208     1]],"a reference corpus of 21,093 tokens and their correct lemmas","[[    3     9  2848 11736   302    13 12026  4198   519 14145     7    11
     70  2024    90    51  2754     1]]"
4f9a8b50903deb1850aee09c95d1b6204a7410b4,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
f129c97a81d81d32633c94111018880a7ffe16d1,23.5294,"attention, regional features, global visual features, regional visual features, attention","[[   0 1388    6 3518  753    6 1252 3176  753    6 3518 3176  753    6
  1388    1]]","Soft attention, Hard Stochastic attention, Local Attention",[[11759  1388     6  6424   472  6322 10057  1388     6  4593 20748     1]]
ce14b87dacfd5206d2a5af7c0ed1cfeb7b181922,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"does not just consider the question tokens, but also the relationship between those tokens and the properties","[[  405    59   131  1099     8   822 14145     7     6    68    92     8
   1675   344   273 14145     7    11     8  2605     1]]"
537a786794604ecc473fb3ef6222e0c3cb81f772,100.0,How2,[[  0 571 357   1]],How2,[[571 357   1]]
9dd65dca9dffd2bf78ecc22b17824edc885d1fa2,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
b8ffb81e74c1c1ad552051aca8741b0141ae6e97,68.75,"The task aims at extracting information about persons, organizations or geo-political entities","[[    0    37  2491     3  8345    44  5819    53   251    81  7609     6
   2371    42  9502    18  3003    17  1950 12311]]","The task aims at extracting information about persons, organizations or geo-political entities from a large collection of news, web and discussion forum documents.","[[   37  2491     3  8345    44  5819    53   251    81  7609     6  2371
     42  9502    18  3003    17  1950 12311    45     3     9   508  1232
     13  1506     6   765    11  3071  5130  2691     5     1]]"
4d8b3928f89d73895a7655850a227fbac08cdae9,16.6667,vSTS has better sentence representations when having access to the corresponding images,"[[   0    3  208 4209  134   65  394 7142 6497    7  116  578  592   12
     8    3 9921 1383    1]]", largest improvement ($22-26\%$ E.R) when text-based unsupervised models are combined with image representations,"[[ 2015  4179  8785  2884    18  2688     2  1454  3229   262     5   448
     61   116  1499    18   390    73 23313  2250    33  3334    28  1023
   6497     7     1]]"
3d583a0675ad34eb7a46767ef5eba5f0ea898aa9,100.0,LSTM,[[   0    3 7600 2305    1]],LSTM,[[   3 7600 2305    1]]
118ff1d7000ea0d12289d46430154cc15601fd8e,100.0,logistic regression,[[    0 28820 26625     1]],logistic regression,[[28820 26625     1]]
3de27c81af3030eb2d9de1df5ec9bfacdef281a9,25.0,"3,200 sentences",[[    0  6180  3632 16513     1]],"$421\,829\,960$ words divided into $17\,305\,401$ sentences","[[ 7514  2658     2     6   927  3166     2     6 27699  3229  1234  8807
    139  1970   940     2     6 26724     2     6 20016  3229 16513     1]]"
983c2fe7bdbf471bb8b15db858fd2cbec86b96a5,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
74b0d3ee0cc9b0a3d9b264aba9901ff97048a897,11.4286,they induce the CFG by using a combination of manual and semi-automatic methods.,"[[    0    79 21151     8   205 22807    57   338     3     9  2711    13
   3354    11  4772    18 18698   447  2254     5]]","the parser first learns to parse simple sentences, then proceeds to learn more complex ones. The induction method is iterative, semi-automatic and based on frequent patterns","[[    8   260     7    49   166   669     7    12   260     7    15   650
  16513     6   258 14942    12   669    72  1561  2102     5    37    16
   8291  1573    19    34    49  1528     6  4772    18 18698   447    11
      3   390    30  8325  4264     1]]"
57586358dd01633aa2ebeef892e96a549b1d1930,100.0,"DSTC2, Maluuba",[[    0     3  3592  3838  4482  2148    76 17309     1]],"DSTC2, Maluuba",[[    3  3592  3838  4482  2148    76 17309     1]]
3bfb8c12f151dada259fbd511358914c4b4e1b0e,100.0,"precision, recall, F-measure",[[    0 11723     6  7881     6   377    18 31038     1]],"precision, recall, F-measure",[[11723     6  7881     6   377    18 31038     1]]
f92ee3c5fce819db540bded3cfcc191e21799cb1,20.0,a speech recognition tool called a t-snapper,"[[   0    3    9 5023 5786 1464  718    3    9    3   17   18    7   29
     9 8153    1]]",We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions),"[[  101  1581    46   326    18   532    18     7   107 10386  1464    21
  13868  5786    41   532  4818  1178    36 19972   788    12 16604  9076
     61     1]]"
8c8a32592184c88f61fac1eef12c7d233dbec9dc,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Both supervised and unsupervised, depending on the task that needs to be solved.","[[ 2867     3 23313    11    73 23313     6  3345    30     8  2491    24
    523    12    36 16384     5     1]]"
892c346617a3391c7dafc9da1b65e5ea3890294d,0.0,LSTM,[[   0    3 7600 2305    1]],convolutional neural networks (CNN),[[  975 24817   138 24228  5275    41   254 17235    61     1]]
6e51af9088c390829703c6fa966e98c3a53114c1,17.3913,dialect of Arabic,[[    0 28461    13 19248     1]],"Modern Standard Arabic (MSA), MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine","[[ 5070  5150 19248    41  4211   188   201  5266   188    38   168    38
  28461     7    44   796  4526    13     3  7662  4885   485   224    38
  16341     6 13566     6    11   312  6451   630     1]]"
26c64edbc5fa4cdded69ace66fdba64a9648b78e,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
4e50e9965059899d15d3c3a0c0a2d73e0c5802a0,0.0,9,[[  0 668   1]],"9710 passages, with an average of 6.24 sentences per passage, 16.16 words per sentence, and an average length of 86 words","[[    3  4327  1714  5454     7     6    28    46  1348    13  4357  2266
  16513   399  5454     6 10128  2938  1234   399  7142     6    11    46
   1348  2475    13     3  3840  1234     1]]"
c65b6470b7ed0a035548cc08e0bc541c2c4a95a7,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"the latest CLWE developments almost exclusively focus on fully unsupervised approaches BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 , BIBREF30 : they fully abandon any source of (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces","[[    8  1251 11175 15713 11336   966  9829   992    30  1540    73 23313
   6315     3  5972 25582   371  2773     3     6     3  5972 25582   371
   2266     3     6     3  5972 25582   371  1828     3     6     3  5972
  25582   371  2688     3     6     3  5972 25582   371  2555     3     6
      3  5972 25582   371  2577     3     6     3  5972 25582   371  3166
      3     6     3  5972 25582   371  1458     3    10    79  1540 10755
    136  1391    13    41  6190  5676    61 15520    11  5819     8  2332
   6677 24297    57  9248    53   420  4478 25758   344   554    18    17
  10761  7414 25207 25078    26    53  4856     1]]"
a3b1520e3da29d64af2b6e22ff15d330026d0b36,0.0,Individual-level demographics,[[    0 10963    18  4563 14798     7     1]],"facial presence, Facial Expression, General Image Features,  textual content, analytical thinking, clout, authenticity, emotional tone, Sixltr,  informal language markers, 1st person singular pronouns","[[10203  3053     6  1699  4703 26376     6  2146  6298 13977     6  1499
   3471   738     6 18355  1631     6     3    75    40   670     6 23833
      6  3973  5739     6  7643    40    17    52     6 15347  1612 17334
      6   209     7    17   568 22166   813 15358    29     7     1]]"
9544cc0244db480217ce9174aa13f1bf09ba0d94,100.0,"English, German",[[   0 1566    6 2968    1]],"English, German",[[1566    6 2968    1]]
521a7042b6308e721a7c8046be5084bc5e8ca246,29.6296,graph-like structures that are surrounded by multiple hypothesized objects,"[[    0  8373    18  2376  5278    24    33     3  8623    57  1317 10950
    532  5120  4820     1]]","graph-like structures where arcs connect nodes representing multiple hypothesized words, thus allowing multiple incoming arcs unlike 1-best sequences","[[ 8373    18  2376  5278   213     3  4667     7  1979   150  1395  9085
   1317 10950   532  5120  1234     6  2932     3  3232  1317     3 19583
      3  4667     7  9770  8218  9606  5932     7     1]]"
090f2b941b9c5b6b7c34ae18c2cc97e9650f1f0b,0.0,LSTM,[[   0    3 7600 2305    1]],Information Retrieval,[[2784  419 1788   15 2165    1]]
d3341eefe4188ee8a68914a2e8c9047334997e84,22.2222,concatenation provides no competitive advantage against gated-attention,"[[    0   975  2138    35   257   795   150  3265  2337   581 10530    26
     18 25615     1]]",concatenation consistently outperforms the gated-attention mechanism for both training and testing instructions,"[[  975  2138    35   257  8182    91   883  2032     7     8 10530    26
     18 25615  8557    21   321   761    11  2505  3909     1]]"
1f6666c2c1d1d5f66208a6fa7da3b3442a577dbc,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"unigram, bigram and trigram",[[  73   23 5096    6  600 2375   11 6467 5096    1]]
af60462881b2d723adeb4acb5fbc07ea27b6bde2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"we demonstrate that harassment occurred more frequently during the night time than the day time, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) , We also found that the majority of young perpetrators engaged in harassment behaviors on the streets, we found that adult perpetrators of sexual harassment are more likely to act alone, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location , commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers.","[[   62  5970    24 23556  6935    72  4344   383     8   706    97   145
      8   239    97     6    34  1267    24     3 15262    73  7576  3676
  13037     7    41  2264  2008    16     8  2320   201  3498   127     7
     11  3863    33   420     8   570    13  4313  1308    13 30139   277
      6  2348    57   803    11 12867     6    62     3 24530    24   132
   3223  1101 18712     7   344     8  1246    13 28998   127     7    11
      8  1128    13 23556     6   344     8   712    87 23829  4788 30139
     49   599     7    61    11  1128     6    11   344  1246    11   712
     87 23829  4788 30139    49   599     7    61     3     6   101    92
    435    24     8  2942    13  1021 28998   127     7  5908    16 23556
  15400    30     8  6162     6    62   435    24  3165 28998   127     7
     13  6949 23556    33    72   952    12  1810  2238     6    62    92
    435    24     8 18712     7   344     8  2807    13 23556    28     8
   1246     6   712    87 23829  4788 30139    49     6   686    13 30139
     49     6    11  1128     3     6  1670    53  2817    72  4344   116
  30139   277   130    16  1637     5  2506    68    59   709     6   452
   5127    19   213   151   530    16   221  3728   120 15880   167  4344
    321    57  4999  9234    11    57  3498   127     7    11  3863     5
      1]]"
8c0a0747a970f6ea607ff9b18cfeb738502d9a95,15.3846,F1 score of 0.83,[[   0  377  536 2604   13 4097 4591    1]],ERR of 19.05 with i-vectors and 15.52 with x-vectors,"[[  262 12224    13  9997  3076    28     3    23    18   162  5317     7
     11  9996  5373    28     3   226    18   162  5317     7     1]]"
4ecb6674bcb4162bf71aea8d8b82759255875df3,100.0,BIBREF5,[[    0     3  5972 25582   371   755     1]],BIBREF5,[[    3  5972 25582   371   755     1]]
d1747b1b56fddb05bb1225e98fd3c4c043d74592,0.0,LSTM+SemEval 2014-2016,"[[    0     3  7600  2305  1220   134    15    51   427  2165  1412    18
  11505     1]]","Convolutional Neural Network , bidirectional Recurrent Neural Network model with attention mechanism","[[ 1193 24817   138  1484  9709  3426     3     6  2647 26352   419 14907
   1484  9709  3426   825    28  1388  8557     1]]"
c0035fb1c2b3de15146a7ce186ccd2e366fb4da2,9.7561,"MGNC-CNN outperforms by 0.86, 0.86, 0.86","[[    0     3  9306  8137    18   254 17235    91   883  2032     7    57
   4097  3840     6  4097  3840     6  4097  3840]]","In terms of Subj the Average MGNC-CNN is better than the average score of baselines by 0.5.  Similarly, Scores of SST-1, SST-2, and TREC where MGNC-CNN has similar improvements. 
In case of Irony the difference is about 2.0. 
","[[   86  1353    13  3325   354     8 23836     3  9306  8137    18   254
  17235    19   394   145     8  1348  2604    13 20726     7    57     3
  12100     5     3 15209     6 17763     7    13   180  4209  2292     6
    180  4209  4949     6    11   332 20921   213     3  9306  8137    18
    254 17235    65  1126  6867     5    86   495    13  9046    63     8
   1750    19    81  6864     5     1]]"
d5f8707ddc21741d52b3c2a9ab1af2871dc6c90b,0.0,"Pointer-Generator score, SPNet accuracy","[[    0  4564    49    18 13714    49  1016  2604     6  6760  9688  7452
      1]]","ROUGE and CIC, relevance, conciseness and readability on a 1 to 5 scale, and rank the summary pair","[[  391 26260   427    11   205  4666     6 20208     6 22874   655    11
    608  2020    30     3     9   209    12   305  2643     6    11 11003
      8  9251  3116     1]]"
fdd9dea06550a2fd0df7a1e6a5109facf3601d76,25.0,78 meetings and 740 discussion groups,[[   0    3 3940 4677   11  489 2445 3071 1637    1]], 75 meetings and about 70 hours of real-time audio duration,[[6374 4677   11   81 2861  716   13  490   18  715 2931 8659    1]]
bdc93ac1b8643617c966e91d09c01766f7503872,11.7647,"2 datasets, containing 370 dialogues",[[    0   204 17953     7     6     3  6443     3 22520  7478     7     1]],1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation,"[[  209  6078  3783    15    26  7478     7    21   761    11     3 11944
     73  9339   400    26  7478     7    21  5002     1]]"
67ee7a53aa57ce0d0bc1a20d41b64cb20303f4b7,50.0,1080 utterances,[[   0  335 2079    3 5108  663    7    1]],"163,110,000 utterances",[[  898  6355   536 29573     3  5108   663     7     1]]
150af1f5f4ce0ec94a7114397cffc59c4798441e,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
479d334b79c1eae3f2aa3701d28aa0d8dd46036a,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
f399d5a8dbeec777a858f81dc4dd33a83ba341a2,84.2105,"QnAMaker Portal, QnaMaker Management APIs, Azure Search Index","[[    0  1593    29  4815     9  2304 16290     6  1593    29     9 22638
     52  2159  6429     7     6 21808  4769 11507]]","QnAMaker Portal, QnaMaker Management APIs, Azure Search Index, QnaMaker WebApp, Bot","[[ 1593    29  4815     9  2304 16290     6  1593    29     9 22638    52
   2159  6429     7     6 21808  4769 11507     6  1593    29     9 22638
     52  1620  9648     6 10091     1]]"
dfb0351e8fa62ceb51ce77b0f607885523d1b8e8,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
c78f18606524539e4c573481e5bf1e0a242cc33c,100.0,1001,[[  0 910 536   1]],1001,[[910 536   1]]
b54fc86dc2cc6994e10c1819b6405de08c496c7b,0.0,"agreement, answer, agreement, retweet, answer, retwe","[[   0 2791    6 1525    6 2791    6    3   60   17 1123   15   17    6
  1525    6    3   60   17 1123]]",time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred,"[[   97     3    15 16543    26   344     8   798     8  1309    42   738
     47  1694    87    17  1123    15  1054    11     8   798    24     8
   6363  1670    42 10657  6935     1]]"
f011d6d5287339a35d00cd9ce1dfeabb1f3c0563,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
05bb75a1e1202850efa9191d6901de0a34744af0,0.0,LSTMs,[[   0    3 7600 2305    7    1]],documents from the CommonCrawl dataset that has the most overlapping n-grams with the question,"[[ 2691    45     8  7155   254 10936    40 17953    24    65     8   167
      3 31212     3    29    18  5096     7    28     8   822     1]]"
acc8d9918d19c212ec256181e51292f2957b37d7,0.0,The network can be used to model a combination of image and text similarity,"[[   0   37 1229   54   36  261   12  825    3    9 2711   13 1023   11
  1499 1126  485    1]]",This approach considers related images,[[ 100 1295 1099    7 1341 1383    1]]
df79d04cc10a01d433bb558d5f8a51bfad29f46b,0.0,Chinese,[[   0 2830    1]],"Answer with content missing: (Applications section) We use Wikipedia articles
in five languages
(Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English) as well as the Na dataset of Adams
et al. (2017).
Select:
Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English","[[11801    28   738  3586    10    41   188   102 13555     7  1375    61
    101   169 16885  2984    16   874  8024    41   439    77    63   291
    210   232     9     6   325    32     6  6156   107   235     6   304
    157  2745     7    77     6    11     3     9   769  2244    13  1566
     61    38   168    38     8  1823 17953    13 14274     3    15    17
    491     5     3 26224     5  6185    10 13070    63   291   210   232
      9     6   325    32     6  6156   107   235     6   304   157  2745
      7    77     6    11     3     9   769  2244    13  1566     1]]"
a25c1883f0a99d2b6471fed48c5121baccbbae82,0.0,"78.9, 79.9 on FastQA and 78.9 on","[[    0     3  3940     5  1298     3     6     3  4440     5  1298    30
   6805 23008    11     3  3940     5  1298    30]]","During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models","[[    3  2092  2505    10     3  3708     5   948    21   712   825   406
   2583 11788     6   431 27869    21   712   825    28  2583 11788     6
    489 10917    21  8784    13   305  2250     1]]"
aee1af55d39145f609da95116ab1b154adb5fa7e,0.0,PDH,[[    0   276 15538     1]],allows models that are consistently performing well to train for more steps,[[1250 2250   24   33 8182 5505  168   12 2412   21   72 2245    1]]
96c09ece36a992762860cde4c110f1653c110d96,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"For task 1 best F1 score was 0.9391 on closed and 0.9414 on open test.
For task2 best result had: Ratio 0.3175 , Satisfaction 64.53, Fluency 0, Turns -1 and Guide 2","[[ 242 2491  209  200  377  536 2604   47 4097 4271 4729   30 3168   11
  4097 4240 2534   30  539  794    5  242 2491  357  200  741  141   10
  6455   23   32 4097 3341 3072    3    6 8821  159   89 4787 6687    5
  4867    6 9507 4392 8014 5527    7    3 2292   11 4637  204    1]]"
6e4505609a280acc45b0a821755afb1b3b518ffd,100.0,BLEU metric,[[    0     3  8775 12062     3  7959     1]],The BLEU metric ,[[   37     3  8775 12062     3  7959     1]]
4b8a0e99bf3f2f6c80c57c0e474c47a5ee842b2c,20.0,LSTMs with a higher diversity (AvgOut) score,"[[    0     3  7600  2305     7    28     3     9  1146  7322    41   188
    208   122 15767    61  2604     1]]","LSTMs with and without attention, HRED, VHRED with and without attention, MMI and Reranking-RL","[[    3  7600  2305     7    28    11   406  1388     6   454 13729     6
    584   566 13729    28    11   406  1388     6   283  7075    11   419
   6254    53    18 12831     1]]"
11ed8c4d98a4e8994990edba54319efe9c6745f2,100.0,NELL,[[    0   445 12735     1]],NELL,[[  445 12735     1]]
8e5e03a5f35f0820a3a1651e148dd6faf646fb67,4.2553,a LSTM-based IQ estimator,"[[    0     3     9     3  7600  2305    18   390     3 20835 12182  1016
      1]]","a BiLSTM without attention (BiLSTM) as well as a single forward-LSTM layer with attention (LSTM+att) and without attention (LSTM), baselines are defined by BIBREF32 who already proposed an LSTM-based architecture that only uses non-temporal features, and the SVM-based estimation model as originally used for reward estimation by BIBREF24","[[    3     9  2106  7600  2305   406  1388    41   279    23  7600  2305
     61    38   168    38     3     9   712  1039    18  7600  2305  3760
     28  1388    41  7600  2305  1220   144    17    61    11   406  1388
     41  7600  2305   201 20726     7    33  4802    57     3  5972 25582
    371  2668   113   641  4382    46     3  7600  2305    18   390  4648
     24   163  2284   529    18 13089  4900   753     6    11     8   180
  12623    18   390 22781   825    38  5330   261    21  9676 22781    57
      3  5972 25582   371  2266     1]]"
ab0bb4d0a9796416d3d7ceba0ba9ab50c964e9d6,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
de015276dcde4e7d1d648c6e31100ec80f61960f,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
2a0a44f169ad61774d77df65f8846bd57685bfcf,9.0909,The New York Times Annotated Corpus BIBREF1 contains 1.8 million news,"[[    0    37   368  1060  5324   389  2264   920 10052   302     3  5972
  25582   371   536  2579     3 16253   770  1506]]",hree years of online news articles from June 2016 to June 2019,"[[   3  107   60   15  203   13  367 1506 2984   45 1515 1421   12 1515
  1360    1]]"
85aa125b3a15bbb6f99f91656ca2763e8fbdb0ff,20.0,"precision, recall, F-measure",[[    0 11723     6  7881     6   377    18 31038     1]],"Precision@1, Mean Average Precision, Mean Reciprocal Rank","[[28464  1741  4347 23045 23836 28464     6 23045  7136    23  1409  1489
      3 22557     1]]"
95d8368b1055d97250df38d1e8c4a2b283d2b57e,100.0,pipeline that is used at Microsoft for production data,[[    0 12045    24    19   261    44  2803    21   999   331     1]],pipeline that is used at Microsoft for production data,[[12045    24    19   261    44  2803    21   999   331     1]]
17f5f4a5d943c91d46552fb75940b67a72144697,19.3548,"Compared to baseline model, our model outperforms by 0.25% in accuracy.","[[    0     3 25236    12 20726   825     6    69   825    91   883  2032
      7    57     3 18189  2712    16  7452     5]]",the rank-correlation for MFH model increases by 36.4% when is evaluated in VQA-HAT dataset and 7.7% when is evaluated in VQA-X,"[[    8 11003    18  5715    60  6105    21     3 13286   566   825  5386
     57  4475     5  5988   116    19 14434    16   584 23008    18  5478
    382 17953    11  4306  6170   116    19 14434    16   584 23008    18
      4     1]]"
b6b5f92a1d9fa623b25c70c1ac67d59d84d9eec8,28.5714,"Compared to previous best results by 0.6 points on the t-SNE dataset,","[[    0     3 25236    12  1767   200   772    57     3 22787   979    30
      8     3    17    18   134  4171 17953     6]]",Their best average precision tops previous best result by 0.202,"[[ 2940   200  1348 11723   420     7  1767   200   741    57  4097 19818
      1]]"
8b11bc3a23932afe7d52c19deffd9dec4830f2e9,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
6c8bd7fa1cfb1b2bbeb011cc9c712dceac0c8f06,0.0,SQuAD,[[   0  180 5991 6762    1]],"word embedding, input encoder, alignment, aggregation, and prediction.","[[ 1448 25078    26    53     6  3785 23734    52     6 14632     6     3
  31761    23   106     6    11 21332     5     1]]"
9dc844f82f520daf986e83466de0c84d93953754,0.0,"MultiRC BIBREF4, MCTest BIBREF5","[[    0  4908  4902     3  5972 25582   371  8525   283  6227   222     3
   5972 25582   371   755     1]]",MultiNLI BIBREF15 and SNLI BIBREF16 ,"[[ 4908 18207   196     3  5972 25582   371  1808    11     3  8544  8159
      3  5972 25582   371  2938     1]]"
c9c85eee41556c6993f40e428fa607af4abe80a9,60.0,$sim $ 8.7M utterances,[[   0 1514    2    7  603 1514 4848  940  329    3 5108  663    7    1]],on $\sim $ 8.7M annotated anonymised user utterances,"[[   30  1514     2     7   603  1514  4848   940   329    46  2264   920
  19581  3375  1139     3  5108   663     7     1]]"
08a5f8d36298b57f6a4fcb4b6ae5796dc5d944a4,47.0588,they incorporate domain-specific features into pre-trained language model,"[[    0    79  6300  3303    18  9500   753   139   554    18    17 10761
   1612   825     1]]",integrate clinical named entity information into pre-trained language model,"[[ 9162  3739  2650 10409   251   139   554    18    17 10761  1612   825
      1]]"
dad8cc543a87534751f9f9e308787e1af06f0627,0.0,WN18KO,[[    0     3 21170  2606 12725     1]],"AIDA-CoNLL, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI, OURSELF-WIKI","[[   71 26483    18  3881   567 10376     6     3 11539 21653     6  5266
  15829     6    71 16892   188 13777     6     3 21170  2326    18 18105
  15658     6     3 21170  2326    18 16785 14108     6     3  9131 23143
    371    18 16785 14108     1]]"
98785bf06e60fcf0a6fe8921edab6190d0c2cec1,15.3846,they use their own unique set of information sources to determine which words are informative,"[[    0    79   169    70   293   775   356    13   251  2836    12  2082
     84  1234    33 11152     1]]",Informative are those that will not be suppressed by regularization performed.,"[[11710  1528    33   273    24    56    59    36 18513    15    26    57
   1646  1707  3032     5     1]]"
37db7ba2c155c2f89fc7fb51fffd7f193c103a34,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF), gradient boosting (XGB)","[[ 4224 29011  5879    41   134 12623   201  7736  3040 26625    41 22084
      5 17748   201 25942  6944    41  8556   201 26462     3 24220    41
      4  3443    61     1]]"
3e839783d8a4f2fe50ece4a9b476546f0842b193,50.0,F1 score of 82.56 on Stance Sentiment Corpus,"[[    0   377   536  2604    13     3  4613     5  4834    30   472   663
   4892  2998   295 10052   302     1]]",F1 score of 66.66%,[[  377   536  2604    13   431 28833  6370     1]]
13f7d50b3b8b0b97d90401eeb0a4e97c9eab3a76,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
bb97537a0a7c8f12a3f65eba73cefa6abcd2f2b2,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.
","[[13967  3113  2597    43 15284  1146  1917    13  3718  1139 14344   145
     72  5711  2597     5  1537 11562  2597  6981  8107   120  1146  3718
  14344  1917   145    72  8165  2597     5   290    19    92     3     9
   1101  1465  1675   344     3     9   573    31     7  4896   485    11
      8  1348   381    13   767    24     3     9  1139    56  1049    16
     24   573     3    18     3     9   710    18  1987  4166  6970    21
   3718 14344     3 29213   139  1200    18  1987  3813    11  6490    24
    307    18  1987  1139 14344   429    36  7157  6737    57     8  5996
     12    84     3     9   573 11967   795  3714   738     5     1]]"
18482658e0756d69e39a77f8fcb5912545a72b9b,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
e6469135e0273481cf11a6c737923630bc7ccfca,0.0,"WNQ2M, QANet",[[    0     3 21170  2247   357   329     6     3 23008  9688     1]],"WikiSQL, SimpleQuestions, SequentialQA","[[ 2142  2168   134  2247   434     6  9415  5991   222  2865     6 26859
   7220 23008     1]]"
0d4aa05eb00d9dee74000ea5b21b08f693ba1e62,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Although cursing is frequent in tweets, they represent just 1.15% of all words used BIBREF21 . In contrast, we found 5.72% of all words posted by gang member accounts to be classified as a curse word, gang members talk about material things with terms such as got, money, make, real, need whereas ordinary users tend to vocalize their feelings with terms such as new, like, love, know, want, look, make, us","[[ 1875  8385    53    19  8325    16 10657     7     6    79  4221   131
      3 11039  2712    13    66  1234   261     3  5972 25582   371  2658
      3     5    86  4656     6    62   435     3 27220  5406    13    66
   1234  1694    57     3  3810  1144  3744    12    36 12910    38     3
      9 19375  1448     6     3  3810   724  1350    81  1037   378    28
   1353   224    38   530     6   540     6   143     6   490     6   174
      3 10339  9495  1105  2134    12  6721  1737    70  6382    28  1353
    224    38   126     6   114     6   333     6   214     6   241     6
    320     6   143     6   178     1]]"
ca26cfcc755f9d0641db0e4d88b4109b903dbb26,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61.,"[[  377   536  2604    13   200  5921    31   825    19  6897     5  3916
      3  2172    12  2106  7600  2305    11  6805 13598    17    24    43
    377   536  2604     3     7  2825   107    40    17    63  1146   145
   9668     5  4241     5     1]]"
3f6610d1d68c62eddc2150c460bf1b48a064e5e6,100.0,No,[[  0 465   1]],No,[[465   1]]
d2473c039ab85f8e9e99066894658381ae852e16,36.3636,"image feature, question feature, caption feature, question feature, text feature, question feature, image","[[    0  1023  1451     6   822  1451     6 25012  1451     6   822  1451
      6  1499  1451     6   822  1451     6  1023]]","image feature, question feature, label vector for the user's answer","[[ 1023  1451     6   822  1451     6  3783 12938    21     8  1139    31
      7  1525     1]]"
2b5dc3595dfc3d52a1525783d943b3dd0cc62473,7.6923,PDH allows models that are consistently performing well to train for more steps,"[[    0   276 15538  1250  2250    24    33  8182  5505   168    12  2412
     21    72  2245     1]]","It begins as ordinary tournament selection evolutionary architecture search with early stopping, with each child model training for a relatively small $s_0$ number of steps before being evaluated for fitness. However, after a predetermined number of child models, $m$ , have been evaluated, a hurdle, $h_0$ , is created by calculating the the mean fitness of the current population. For the next $m$ child models produced, models that achieve a fitness greater than $h_0$ after $s_0$ train steps are granted an additional $s_1$ steps of training and then are evaluated again to determine their final fitness. Once another $m$ models have been considered this way, another hurdle, $h_1$ , is constructed by calculating the mean fitness of all members of the current population that were trained for the maximum number of steps. For the next $m$ child models, training and evaluation continues in the same fashion, except models with fitness greater than $m$0 after $m$1 steps of training are granted an additional $m$2 number of train steps, before being evaluated for their final fitness. This process is repeated until a satisfactory number of maximum training steps is reached.","[[   94  4396    38  9495  5892  1801 27168  4648   960    28   778 10847
      6    28   284   861   825   761    21     3     9  4352   422  1514
      7   834   632  3229   381    13  2245   274   271 14434    21  4639
      5   611     6   227     3     9   554 22755   381    13   861  2250
      6  1514    51  3229     3     6    43   118 14434     6     3     9
  23463     6  1514   107   834   632  3229     3     6    19   990    57
      3 25956     8     8  1243  4639    13     8   750  2074     5   242
      8   416  1514    51  3229   861  2250  2546     6  2250    24  1984
      3     9  4639  2123   145  1514   107   834   632  3229   227  1514
      7   834   632  3229  2412  2245    33  7020    46  1151  1514     7
    834   536  3229  2245    13   761    11   258    33 14434   541    12
   2082    70   804  4639     5  1447   430  1514    51  3229  2250    43
    118  1702    48   194     6   430 23463     6  1514   107   834   536
   3229     3     6    19  8520    57     3 25956     8  1243  4639    13
     66   724    13     8   750  2074    24   130  4252    21     8  2411
    381    13  2245     5   242     8   416  1514    51  3229   861  2250
      6   761    11  5002  3256    16     8   337  2934     6  3578  2250
     28  4639  2123   145  1514    51  3229   632   227  1514    51  3229
    536  2245    13   761    33  7020    46  1151  1514    51  3229   357
    381    13  2412  2245     6   274   271 14434    21    70   804  4639
      5   100   433    19 12171   552     3     9 25140   381    13  2411
    761  2245    19  3495     5     1]]"
545ff2f76913866304bfacdb4cc10d31dbbd2f37,0.0,"Word2Vec, GloVe, etc",[[   0 4467  357  553   15   75    6 9840  553   15    6  672    1]],WMT 2014 En-Fr parallel corpus,[[  549  7323  1412   695    18   371    52  8449 11736   302     1]]
994ac7aa662d16ea64b86510fcf9efa13d17b478,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
d484a71e23d128f146182dccc30001df35cdf93f,41.1765,Perplexity of the best model is 0.81 vs 0.81 of best original translation,"[[   0 1915 9247  485   13    8  200  825   19 4097 4959    3  208    7
  4097 4959   13  200  926 7314]]","Perplexity of the best model is 65.58 compared to best baseline 105.79.
Bleu of the best model is 6.57 compared to best baseline 5.50.","[[ 1915  9247   485    13     8   200   825    19  7123     5  3449     3
   2172    12   200 20726     3 12869     5  4440     5 11805    76    13
      8   200   825    19  4357  3436     3  2172    12   200 20726  3594
   1752     5     1]]"
b5bfa6effdeae8ee864d7d11bc5f3e1766171c2d,100.0,all regions except those that are colored black,[[    0    66  6266  3578   273    24    33 11999  1001     1]],all regions except those that are colored black,[[   66  6266  3578   273    24    33 11999  1001     1]]
347e86893e8002024c2d10f618ca98e14689675f,100.0,high-quality,[[   0  306   18 4497    1]],high-quality,[[ 306   18 4497    1]]
766e2e35968ef7434b56330aa41957c5d5f8d0ee,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],970 hours of audio data with corresponding text transcripts (around 10M word tokens) and an additional 800M word token text only dataset,"[[  668  2518   716    13  2931   331    28     3  9921  1499 20146     7
     41 15590   335   329  1448 14145     7    61    11    46  1151  8640
    329  1448 14145  1499   163 17953     1]]"
dcd6f18922ac5c00c22cef33c53ff5ae08b42298,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"First preference is given to the labels that are perfectly matching in all the neural annotators., In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models., When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one. , Finally, when none the above conditions are fulfilled, we leave out the label with an unknown category.","[[ 1485 11633    19   787    12     8 11241    24    33  3923  8150    16
     66     8 24228    46  2264  6230     5     6    86   495   192    91
     13   386  2625  2250    33  2024     6   258    34    19   271  7122
      3    99    24  3783    19    92  2546    57    44   709    80    13
      8   529    18  1018  6327  2250     5     6   366    62   217    24
   5839    13     8  2625  2250    19  5874     8   337   772     6   258
     62 11003     8 11241    28    70  6477  3410  2620  2546    38     3
      9 15834  3438   338     8  1514 12369  9128  3229  1681     5    37
  11241    33     3 14504    16     3 30960   455  1315    12  3410  2620
      5    37    29    62   691     3    99     8   166   386    41  6701
    116    80  2625   825    11   321   529    18  1018  6327  2250  1759
      8   337  3783    61    42    44   709   192 11241    33  8150     6
    258    62   995    12  1432    24    80     5     3     6  4213     6
    116  5839     8   756  1124    33 20795     6    62  1175    91     8
   3783    28    46  7752  3295     5     1]]"
9bcc1df7ad103c7a21d69761c452ad3cd2951bda,66.6667,Variety prediction task,[[    0 31884 21332  2491     1]],Gender prediction task,[[  350  3868 21332  2491     1]]
bbeb74731b9ac7f61e2d74a7d9ea74caa85e62ef,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
7dce1b64c0040500951c864fce93d1ad7a1809bc,0.0,a simulated acoustic event using a simulated acous,"[[    0     3     9     3 31126     3     9  3422     7  1225   605   338
      3     9     3 31126     3     9  3422     7]]","a masking speech enhancement model BIBREF11, BIBREF12, BIBREF13","[[    3     9  8181    53  5023 15220   825     3  5972 25582   371  2596
      6     3  5972 25582   371  2122     6     3  5972 25582   371  2368
      1]]"
ffb7a12dfe069ab7263bb7dd366817a9d22b8ef2,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
964705a100e53a9181d1a5ac8150696de12ecaf0,26.087,"Training dataset contains 1, 2, 3, 4, 8, 12, 16, 32, 64, 64, 64, 64","[[    0  4017 17953  2579  1914  3547  6180  6464  9478 10440 11940  3538
      6  6687     6  6687     6  6687     6  6687]]"," training dataset contains 2,815 examples, 761 testing examples","[[  761 17953  2579  3547   927  1808  4062     6   489  4241  2505  4062
      1]]"
3b995a7358cefb271b986e8fc6efe807f25d60dc,50.0,"GloVE, XML",[[   0 9840 8575    6    3    4 6858    1]],GloVE; SGNS,[[9840 8575  117    3 9945 7369    1]]
d71937fa5da853f7529f767730547ccfb70e5908,100.0,"News Articles, Twitter",[[   0 3529 7491    7    6 3046    1]],"News Articles, Twitter",[[3529 7491    7    6 3046    1]]
8a871b136ccef78391922377f89491c923a77730,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Stanford NER, BiLSTM+CRF, LSTM+CNN+CRF, T-NER and BiLSTM+CNN+Co-Attention","[[19796     3 18206     6  2106  7600  2305  1220  4545   371     6     3
   7600  2305  1220   254 17235  1220  4545   371     6   332    18 18206
     11  2106  7600  2305  1220   254 17235  1220  3881    18   188    17
   9174     1]]"
0e57a0983b4731eba9470ba964d131045c8c7ea7,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
d94ac550dfdb9e4bbe04392156065c072b9d75e1,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
b29b5c39575454da9566b3dd27707fced8c6f4a1,0.0,No,[[  0 465   1]],"As the question has integrated previous utterances, the model needs to directly relate previously mentioned concept with the current question. This is helpful for concept carry-over and coreference resolution.","[[  282     8   822    65  4580  1767     3  5108   663     7     6     8
    825   523    12  1461  9098  3150  2799  2077    28     8   750   822
      5   100    19  2690    21  2077  2331    18  1890    11  2583 11788
   3161     5     1]]"
9555aa8de322396a16a07a5423e6a79dcd76816a,43.4783,their model outperforms by 0.86 and 0.88 the state-of-the-,"[[   0   70  825   91  883 2032    7   57 4097 3840   11 4097 4060    8
   538   18  858   18  532   18]]",w.r.t Rouge-1 their model outperforms by 0.98% and w.r.t Rouge-L their model outperforms by 0.45%,"[[    3   210     5    52     5    17 23777  2292    70   825    91   883
   2032     7    57     3 23758  5953    11     3   210     5    52     5
     17 23777    18   434    70   825    91   883  2032     7    57  4097
  27516     1]]"
8db45a8217f6be30c31f9b9a3146bf267de68389,0.0,a novel distribution-based compositionality assessment method to quantitatively measure this aspect of learning,"[[    0     3     9  3714  3438    18   390  5761 10355  4193  1573    12
  18906   120  3613    48  2663    13  1036     1]]","random , Output length, Input length, Output pattern, Input pattern","[[6504    3    6 3387 2562 2475    6   86 2562 2475    6 3387 2562 3275
     6   86 2562 3275    1]]"
4519afe91b1042876d7c021487d98e2d72a09861,21.0526,"Using distance supervision, we obtain temporal relations from clinical documents.","[[    0     3  3626  2357 15520     6    62  3442 10301  8563  5836    45
   3739  2691     5     1]]",dominant temporal associations can be learned from training data,[[12613 10301  8563 10906    54    36  2525    45   761   331     1]]
53640834d68cf3b86cf735ca31f1c70aa0006b72,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
f1bd66bb354e3dabf5dc4a71e6f08b17d472ecc9,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],by adding extra supervision to generate the slots that will be present in the response,"[[   57  2651   996 15520    12  3806     8  9653    24    56    36   915
     16     8  1773     1]]"
30803eefd7cdeb721f47c9ca72a5b1d750b8e03b,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"EER 16.04, Cmindet 0.6012, Cdet 0.6107","[[  262  3316 10128  6348     6   205 14481    15    17  4097  3328  2122
      6   205   221    17     3 22787 18057     1]]"
609fbe627309775de415682f48588937d5dd8748,76.9231,large-scale document classification datasets from BIBREF14,"[[    0   508    18  6649  1708 13774 17953     7    45     3  5972 25582
    371  2534     1]]",large-scale document classification datasets introduced by BIBREF14,"[[  508    18  6649  1708 13774 17953     7  3665    57     3  5972 25582
    371  2534     1]]"
5d9b088bb066750b60debfb0b9439049b5a5c0ce,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],Remove numbers and interjections,[[10002  2302    11  1413 21440     7     1]]
d13efa7dee280c7c2f6dc451c4fbbf0240fc2efa,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
43ee69902a5fc1e3c7bacc4456d3f779c45a911d,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
3ee976add83e37339715d4ae9d8aa328dd54d052,13.3333,F1 score of 82.86 on flood data and 82.86 on tornado data,"[[    0   377   536  2604    13     3  4613     5  3840    30  8347   331
     11     3  4613     5  3840    30 29578   331]]","Queensland flood which provided 96% accuracy, Alberta flood with the same configuration of train-test split which provided 95% accuracy","[[18838  8347    84   937   668  6370  7452     6 19014  8347    28     8
    337  5298    13  2412    18  4377  5679    84   937   668  2712  7452
      1]]"
802687121a98ba4d7df1f8040ea0dc1cc9565b69,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
a7cb4f8e29fd2f3d1787df64cd981a6318b65896,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
b8dea4a98b4da4ef1b9c98a211210e31d6630cf3,7.1429,"generating multiple acts per turn (called multi-act), generating a sequence of ","[[    0     3 11600  1317  6775   399   919    41  9341  1249    18  2708
    201     3 11600     3     9  5932    13     3]]","It has three sequentially connected units to output continue, act and slots generating multi-acts in a doble recurrent manner.","[[   94    65   386 29372   120  2895  3173    12  3911   916     6  1810
     11  9653     3 11600  1249    18  2708     7    16     3     9   103
   2296     3    60 14907  3107     5     1]]"
18e915b917c81056ceaaad5d6581781c0168dac9,0.0,the most common error type,[[   0    8  167 1017 3505  686    1]],all annotators that a triple extraction was incorrect,[[   66    46  2264  6230    24     3     9 12063 16629    47 12153     1]]
1d1ab5d8a24dfd15d95a5a7506ac0456d1192209,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
5e65bb0481f3f5826291c7cc3e30436ab4314c61,100.0,entity grid with grammatical relations and RST discourse relations,"[[    0 10409  8634    28     3  5096  4992   138  5836    11   391  4209
  22739  5836     1]]",Entity grid with grammatical relations and RST discourse relations.,"[[ 4443   485  8634    28     3  5096  4992   138  5836    11   391  4209
  22739  5836     5     1]]"
ae8354e67978b7c333094c36bf9d561ca0c2d286,42.1053,NIST shared task datasets for Work Summarization and Reference (WSH),"[[    0   445 13582  2471  2491 17953     7    21  3118 12198  1635  1707
     11 17881    41   518  9122    61     1]]","datasets from the NIST DUC-05, DUC-06 and DUC-07 shared tasks","[[17953     7    45     8   445 13582   309  6463    18  3076     6   309
   6463    18  5176    11   309  6463    18  4560  2471  4145     1]]"
5fa464a158dc8abf7cef8ca7d42a7080670c1edd,100.0,No,[[  0 465   1]],No,[[465   1]]
04012650a45d56c0013cf45fd9792f43916eaf83,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"comparing to the results from reducing the number of layers in the decoder, the BLEU score was 69.93 which is less than 1% in case of test2016 and in case of test2017 it was less by 0.2 %. In terms of TER it had higher score by 0.7 in case of test2016 and 0.1 in case of test2017. ","[[    3 14622    12     8   772    45     3  5503     8   381    13  7500
     16     8    20  4978    52     6     8     3  8775 12062  2604    47
      3  3951     5  4271    84    19   705   145     3  4704    16   495
     13   794 11505    11    16   495    13   794  9887    34    47   705
     57     3 18189     3  1454     5    86  1353    13     3  5946    34
    141  1146  2604    57     3 22426    16   495    13   794 11505    11
      3 16029    16   495    13   794  9887     5     1]]"
04a4b0c6c8bd4c170c93ea7ea1bf693965ef38f4,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
a8545f145d5ea2202cb321c8f93e75ad26fcf4aa,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
7182f6ed12fa990835317c57ad1ff486282594ee,7.1429,compositional generalization is evaluated in a sequence-to-sequence setting by,"[[    0  5761   138   879  1707    19 14434    16     3     9  5932    18
    235    18     7    15   835  3772  1898    57]]","it systematically holds out inputs in the training set containing basic primitive verb, ""jump"", and tests on sequences containing that verb.","[[   34     3 28657  4532    91  3785     7    16     8   761   356     3
   6443  1857 26322  7375     6    96  2047  1167  1686    11  3830    30
   5932     7     3  6443    24  7375     5     1]]"
13ca4bf76565564c8ec3238c0cbfacb0b41e14d2,0.0,TDXL,[[    0     3 10494     4   434     1]],"14 TDs, BIBREF15",[[  968     3 10494     7     6     3  5972 25582   371  1808     1]]
b69f0438c1af4b9ed89e531c056d9812d4994016,100.0,3600 user-generated comments,[[    0   220  6007  1139    18 29955  2622     1]],3600 user-generated comments,[[  220  6007  1139    18 29955  2622     1]]
5e5460ea955d8bce89526647dd7c4f19b173ab34,0.0,6,[[  0 431   1]],Total number of transcribed utterances including Train and Test for both Eng and Ger language is 5562 (2188 cleaned),"[[ 9273   381    13     3 11665 22573     3  5108   663     7   379 15059
     11  2300    21   321 19650    11  5744  1612    19  6897  4056  4743
  25794 12631    61     1]]"
9f2634c142dc4ad2c68135dbb393ecdfd23af13f,22.2222,22k dialogues,[[   0 1630  157 7478    7    1]],"we obtain 52,053 dialogues and 460,358 utterances","[[   62  3442  9065     6  3076   519  7478     7    11     3 25991     6
    519  3449     3  5108   663     7     1]]"
c2553166463b7b5ae4d9786f0446eb06a90af458,0.0,"BIBREF4, BIBREF9",[[    0     3  5972 25582   371  8525     3  5972 25582   371  1298     1]],"the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL).","[[    8     3  5972 25582   371   755   680   162   358    45     8  3356
     18   390 20491    41  4822    12    38     3  8503  3765   201     3
   5972 25582   371   948    45     8 11775 20491    41  4209  5767   201
     11     8     3  5972 25582   371  2596  1659 28050   358    45     8
  24228 20491    41   567 26296  4090   137     1]]"
6aed1122050b2d508dc1790c13cdbe38ff126089,100.0,"attention-based sequence-to-sequence model, CVAE","[[    0  1388    18   390  5932    18   235    18     7    15   835  3772
    825     3     6 10430 14611     1]]","attention-based sequence-to-sequence model , CVAE","[[ 1388    18   390  5932    18   235    18     7    15   835  3772   825
      3     6 10430 14611     1]]"
774ead7c642f9a6c59cfbf6994c07ce9c6789a35,100.0,Amazon reviews,[[   0 2536 2456    1]],Amazon reviews,[[2536 2456    1]]
1771a55236823ed44d3ee537de2e85465bf03eaf,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Between the model and Stanford, Spacy and Flair the differences are 42.91, 25.03, 69.8 with Traditional NERs as reference and  49.88, 43.36, 62.43 with Wikipedia titles as reference.","[[13095     8   825    11 19796     6  2526  4710    11  5766  2256     8
   5859    33  6426     5  4729     6   944     5  4928     6     3  3951
      5   927    28 16505     3 18206     7    38  2848    11  9526     5
   4060     6  8838     5  3420     6     3  4056     5  4906    28 16885
   8342    38  2848     5     1]]"
48b12eb53e2d507343f19b8a667696a39b719807,32.0,"a set of aesthetic emotions that are predictive of aesthetic appreciation in the reader, and allow the","[[    0     3     9   356    13  9386  7848    24    33 27875    13  9386
  11746    16     8  5471     6    11   995     8]]","feelings of suspense experienced in narratives not only respond to the trajectory of the plot's content, but are also directly predictive of aesthetic liking (or disliking), Emotions that exhibit this dual capacity have been defined as “aesthetic emotions”","[[ 6382    13  4324  3801    15  1906    16  8109     7    59   163  3531
     12     8 29912    13     8  5944    31     7   738     6    68    33
     92  1461 27875    13  9386 23340    41   127  1028  8654    53   201
    262  7259     7    24  6981    48  7013  2614    43   118  4802    38
    105     9    15     7 17194  7848   153     1]]"
6dcbe941a3b0d5193f950acbdc574f1cfb007845,7.4074,"services, hotels, restaurants, chat rooms, weather, weather, weather, weather, weather,","[[   0  364    6 5694    6 3661    6 3582 2801    6 1969    6 1969    6
  1969    6 1969    6 1969    6]]","Alarm
Bank
Bus
Calendar
Event
Flight
Home
Hotel
Media
Movie
Music
RentalCar
Restaurant
RideShare
Service
Travel
Weather","[[24920  1925  5703 18783  8042 16736  1210  2282  3159 10743  3057 14157
   6936  6233 17392 24501  1387  4983 17709     1]]"
d216d715ec27ee2d4949f9e908895a18fb3238e2,0.0,"close to random,",[[   0  885   12 6504    6    1]],"word length, number of phonemes, number of syllables, alphabetical order, and frequency","[[ 1448  2475     6   381    13   951  2687     6   381    13     3     7
     63   195   179     7     6 20688  1950   455     6    11  7321     1]]"
22c802872b556996dd7d09eb1e15989d003f30c0,7.6923,Pearson correlation,[[    0 29300 18712     1]],They compute Pearson’s correlation between NED measure for patient-to-therapist and patient-perceived emotional bond rating and NED measure for therapist-to-patient and patient-perceived emotional bond rating,"[[  328 29216 29300    22     7 18712   344   445  2326  3613    21  1868
     18   235    18  9962    11  1868    18   883   565   757    26  3973
   6235  5773    11   445  2326  3613    21     3  9962    18   235    18
  10061    11  1868    18   883   565   757    26  3973  6235  5773     1]]"
da8bda963f179f5517a864943dc0ee71249ee1ce,50.0,3 layers,[[   0  220 7500    1]],4 layers,[[ 314 7500    1]]
14684ad200915ff1e3fc2a89cb614e472a1a2854,100.0,No,[[  0 465   1]],No,[[465   1]]
9bfa46ad55136f2a365e090ce585fc012495393c,20.0,Joint Goals dataset,[[    0 16761 17916     7 17953     1]],"the single domain dataset, WoZ2.0 , the multi-domain dataset, MultiWoZ","[[    8   712  3303 17953     6  3488   956 24273     3     6     8  1249
     18 22999 17953     6  4908   518    32   956     1]]"
71a7153e12879defa186bfb6dbafe79c74265e10,0.0,Chinese general corpus,[[    0  2830   879 11736   302     1]],Unanswerable,[[ 597 3247 3321  179    1]]
7c9c73508da628d58aaadb258f3a9d4cc2a8a9b3,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
a37e4a21ba98b0259c36deca0d298194fa611d2f,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
09cd7ae01fe97bba230c109d0234fee80a1f013b,0.0,Mboshi UL,[[   0  283  115   32 5605    3 4254    1]],French-Mboshi 5K corpus,[[ 2379    18   329   115    32  5605   305   439 11736   302     1]]
dc49746fc98647445599da9d17bc004bafdc4579,100.0,Yes,[[   0 2163    1]],Yes,[[2163    1]]
5d83b073635f5fd8cd1bdb1895d3f13406583fbd,0.0,"LSTMs, LSTMs with a corresponding entity model","[[    0     3  7600  2305     7     6     3  7600  2305     7    28     3
      9     3  9921 10409   825     1]]","Hasty Student, Impatient Reader, BiDAF, BiDAF w/ static memory","[[ 4498    17    63  6341     6  1318 10061 16120     6  2106  4296   371
      6  2106  4296   371     3   210    87 14491  2594     1]]"
b093b440ae3cd03555237791550f3224d159d85b,0.0,BIBREF21,[[    0     3  5972 25582   371  2658     1]],"EMPATHETICDIALOGUES dataset, a dataset containing 1.5 million Twitter conversation, gathered by using Twitter API from customer care account of 62 brands across several industries, SEMAINE corpus BIBREF30","[[  262  5244 24786   427 18679 24605 20151  5078   134 17953     6     3
      9 17953     3  6443  8613   770  3046  3634     6     3  9094    57
    338  3046  6429    45   884   124   905    13     3  4056  3635   640
    633  5238     6   180 20211  9730 11736   302     3  5972 25582   371
   1458     1]]"
70f84c73172211186de1a27b98f5f5ae25a94e55,94.1176,Stanford Sentiment Treebank (SST) BIBREF15 and AG News ,"[[    0 19796  4892  2998   295  7552  4739    41   134  4209    61     3
   5972 25582   371  1808    11  8859  3529     3]]",Stanford Sentiment Treebank (SST) BIBREF15 and AG News BIBREF16,"[[19796  4892  2998   295  7552  4739    41   134  4209    61     3  5972
  25582   371  1808    11  8859  3529     3  5972 25582   371  2938     1]]"
7b2bf0c1a24a2aa01d49f3c7e1bdc7401162c116,5.5556,using random selection,[[   0  338 6504 1801    1]],"By using a Bayesian approach  and by using word-pairs, where they extract all the pairs of co-occurring words within each tweet.  They search for the words that achieve the highest number of spikes matching the days of events.","[[  938   338     3     9  2474    15 10488  1295    11    57   338  1448
     18   102  2256     7     6   213    79  5819    66     8 14152    13
    576    18 13377   450  1007  1234   441   284 10657     5   328   960
     21     8  1234    24  1984     8  2030   381    13 22440     7  8150
      8   477    13   984     5     1]]"
96c20af8bbef435d0d534d10c42ae15ff2f926f8,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"potentially indicating a shining through effect, explicitation effect","[[ 6149     3 15716     3     9 23215   190  1504     6 17623   257  1504
      1]]"
476d0b5579deb9199423bb843e584e606d606bc7,40.0,"BIBREF13, BIBREF14","[[    0     3  5972 25582   371  2368     6     3  5972 25582   371  2534
      1]]","BIBREF13, majority baseline",[[    3  5972 25582   371  2368     6  2942 20726     1]]
351510da69ab6879df5ff5c7c5f49a8a7aea4632,100.0,Unanswerable,[[   0  597 3247 3321  179    1]],Unanswerable,[[ 597 3247 3321  179    1]]
b25e7137f49f77e7e67ee2f40ca585d3a377f8b5,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"spellchecking mammography reports and tweets BIBREF7 , BIBREF4","[[10783 10031    53   954   635  5984  2279    11 10657     7     3  5972
  25582   371   940     3     6     3  5972 25582   371   591     1]]"
54c7fc08598b8b91a8c0399f6ab018c45e259f79,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],"Proposed method vs best baseline result on Vecmap (Accuracy P@1):
EN-IT: 50 vs 50
IT-EN: 42.67 vs 42.67
EN-DE: 51.6 vs 51.47
DE-EN: 47.22 vs 46.96
EN-FI: 35.88 vs 36.24
FI-EN: 39.62 vs 39.57
EN-ES: 39.47 vs 39.30
ES-EN: 36.43 vs 36.06","[[  749 12151  1573     3   208     7   200 20726   741    30  3901    75
  11576    41 19543   450  4710   276  1741  6982    10 13209    18  3177
     10   943     3   208     7   943  2344    18  5332    10  6426     5
   3708     3   208     7  6426     5  3708 13209    18  5596    10   305
  15062     3   208     7 11696     5  4177  3396    18  5332    10 10635
      5  2884     3   208     7  9668     5  4314 13209    18  4936    10
   3097     5  4060     3   208     7  4475     5  2266     3  4936    18
   5332    10  6352     5  4056     3   208     7  6352     5  3436 13209
     18  3205    10  6352     5  4177     3   208     7  6352     5  1458
      3  3205    18  5332    10  4475     5  4906     3   208     7  4475
      5  5176     1]]"
097ab15f58cb1fce5b5ffb5082b8d7bbee720659,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],thai,[[  3 189   9  23   1]]
ba61ed892b4f7930430389e80a0c8e3b701c8e5d,0.0,accuracy,[[   0 7452    1]]," functional dissimilarity score, nearest neighbours experiment",[[ 5014  1028 26714   485  2604     6 13012 14245     7  5016     1]]
5dfa59c116e0ceb428efd99bab19731aa3df4bbd,0.0,2,[[  0 204   1]],Totally 6980 validation and test image-sentence pairs have been corrected.,"[[    3 30115     3  3951  2079 16148    11   794  1023    18  5277  1433
  14152    43   118 23006     5     1]]"
33d864153822bd378a98a732ace720e2c06a6bc6,0.0,Unanswerable,[[   0  597 3247 3321  179    1]],In closed setting 84.22 F1 and in open 87.35 F1.,"[[  86 3168 1898    3 4608    5 2884  377  536   11   16  539    3 4225
     5 2469  377 5411    1]]"
