ids,f1_seg_0,preds_seg_0,preds_seg_0_tokens,target_text,labels
195611926760d1ceec00bd043dfdc8eba2df5ad1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Random Forest classifier,[[    0 45134  5761  1380 24072     2]]
72ceeb58e783e3981055c70a3483ea706511fac3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],joint goal accuracy,[[    0   267 15494   724  8611     2]]
1244cf6d75e3aa6d605a0f4b141c015923a3f2e7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The first one is that it should be highly correlated with human judgement of similarity. The second one is that it should be able to distinguish sentences which are in logical contradiction, logically unrelated or in logical agreement. The third one is that a robust evaluator should also be able to identify unintelligible sentences. The last criteria is that a good evaluation metric should not give high scores to semantically distant sentences and low scores to semantically related sentences.","[[    0   133    78    65    16    14    24   197    28  2200 34852    19
   1050 17219     9 37015     4    20   200    65    16    14    24   197
     28   441     7 22929 11305    61    32    11 16437 34920     6 42531
  16354    50    11 16437  1288     4    20   371    65    16    14    10
   6295 37131   257  2630   197    67    28   441     7  3058 45467 37448
   4748 11305     4    20    94  8608    16    14    10   205 10437 14823
    197    45   492   239  4391     7  9031 38600 13258 11305     8   614
   4391     7  9031 38600  1330 11305     4     2]]"
10210d5c31dc937e765051ee066b971b6f04d3af,0.0,Entailment,[[    2     0 30495  3760  1757     2]], 16k questions,[[   0  545  330 1142    2]]
b8ffb81e74c1c1ad552051aca8741b0141ae6e97,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The task aims at extracting information about persons, organizations or geo-political entities from a large collection of news, web and discussion forum documents.","[[    0   133  3685  5026    23 37213   335    59  5151     6  2665    50
  32816    12 17522  8866    31    10   739  2783     9   340     6  3748
      8  3221  7900  2339     4     2]]"
cbbcafffda7107358fa5bf02409a01e17ee56bfd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],It is observed some variability - but not significant. Bert does not seem to gain much more syntax information than with type level information.,"[[    0   243    16  6373   103 36049   111    53    45  1233     4 12975
    473    45  2045     7  2364   203    55 45362   335    87    19  1907
    672   335     4     2]]"
46f175e1322d648ab2c0258a9609fe6f43d3b44e,0.0,Entailment,[[    2     0 30495  3760  1757     2]], inclusion of longer parts of the conversation,[[   0 9290    9 1181 1667    9    5 1607    2]]
54be3541cfff6574dba067f1e581444537a417db,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Compared with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively.","[[    0 44891    19     5  1298  1743     9 11202   717  6486  6266  3603
      6    84  7208 35499   195     4   288  4234   112     4   401  4234
    112     4   306  4234   112     4   246   207  7833  3077    15  2808
  28302 38036   288  2156  2808 28302 38036   134  2156  2808 28302 38036
    176     8  2808 28302 38036   246  4067     4     2]]"
0428e06f0550e1063a64d181210795053a8e6436,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
8051927f914d730dfc61b2dc7a8580707b462e56,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a sentence-level prediction algorithm, a segment retrieval algorithm and a pipeline segment retrieval algorithm","[[    0   102  3645    12  4483 16782 17194     6    10  2835 43372 17194
      8    10  4116  2835 43372 17194     2]]"
003f884d3893532f8c302431c9f70be6f64d9be8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
8a1d4ed00d31c1f1cb05bc9d5e4f05fe87b0e5a4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Authors,[[    0 44298   994     2]]
ccb3d21885250bdbfc4c320e99f25923896e70fa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"calendar, weather, navigation",[[    0 11762 29702     6  1650     6 14461     2]]
12ac76b77f22ed3bcb6430bcd0b909441d79751b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"TEACHER FORCING (TF), SCHEDULED SAMPLING (SS),  SEQGAN, RANKGAN, LEAKGAN.","[[    0  6433 11083  2076  5089   347  1862    36 20249   238 26744  1691
   6597  1691 23978  7205  1862    36  8108   238  1437  6324  1864 38416
      6   248 10227 38416     6 10611  7140 38416     4     2]]"
cbf1137912a47262314c94d36ced3232d5fa1926,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"fastText, CWE-LP",[[    0 16963 39645     6   230  9112    12 21992     2]]
7955dbd79ded8ef4ae9fc28b2edf516320c1cb55,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"size, demographics, areas of research, impact, and correlation of citations with demographic attributes (age and gender)","[[    0 10799     6 23402     6   911     9   557     6   913     6     8
  22792     9 31173    19 15343 16763    36  1580     8  3959    43     2]]"
99c50d51a428db09edaca0d07f4dab0503af1b94,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"youtube video transcripts on news covering different topics like technology, human rights, terrorism and politics","[[    0 36666   569 20382    15   340  4631   430  7614   101   806     6
   1050   659     6  4952     8  2302     2]]"
4debd7926941f1a02266b1a7be2df8ba6e79311a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
0a92352839b549d07ac3f4cb997b8dc83f64ba6f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],2 accuracy points,[[   0  176 8611  332    2]]
51de39c8bad62d3cbfbec1deb74bd8a3ac5e69a8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Replacing attention mechanism to query-key attention, and adding a loss to make the attention mask as diagonal as possible","[[    0 47493  7575  1503  9562     7 25860    12  5282  1503     6     8
   1271    10   872     7   146     5  1503 11445    25 42539    25   678
      2]]"
53712f0ce764633dbb034e550bb6604f15c0cacd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
5f6fbd57cce47f20a0fda27d954543c00c4344c2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The workers were also asked to annotate both user states and system states, we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories","[[    0   133  1138    58    67   553     7 45068   877   258  3018   982
      8   467   982     6    52   341   103  1492     7  6885 45068   877
   6054  4504   309     7  3018   982     6   467   982     6     8  6054
  26271     2]]"
617c77a600be5529b3391ab0c21504cd288cc7c7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],These concept-sets are sampled from several large corpora of image/video captions,"[[    0  4528  4286    12 34462    32 36551    31   484   739 22997   102
      9  2274    73 14406 13363  2485     2]]"
5e997d4499b18f1ee1ef6fa145cadbc018b8dd87,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Google Images, Reddit Memes Dataset",[[    0 20441  2209     6  6844 20207   293 16673   281   594     2]]
79ed71a3505cf6f5e8bf121fd7ec1518cab55cae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Damage to neural modules is done by randomly initializing their weights, causing the loss of all learned information.","[[    0 48008     7 26739 22744    16   626    30 22422  2557  2787    49
  23341     6  3735     5   872     9    70  2435   335     4     2]]"
15a1df59ed20aa415a4daf0acb256747f6766f77,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"shining through, explicitation",[[    0  1193  6074   149     6 16045  1258     2]]
9651fbd887439bf12590244c75e714f15f50f73d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The advantage of pre-training gradually diminishes with the increase of labeled data, Fixed representations yield better results than fine-tuning in some cases, pre-training the Seq2Seq encoder outperforms pre-training the decoder","[[    0   133  2093     9  1198    12 32530  9097 29063 10776    19     5
    712     9 16274   414     6 28092 30464  3363   357   775    87  2051
     12 24641   154    11   103  1200     6  1198    12 32530     5  1608
   1343   176 14696  1343  9689 15362  9980 33334  1198    12 32530     5
   5044 15362     2]]"
05196588320dfb0b9d9be7d64864c43968d329bc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
db62d5d83ec187063b57425affe73fef8733dd28,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Markov random field with an optional neural parameterization,[[    0 10006  1417  9624   882    19    41 17679 26739 43797  1938     2]]
37c7c62c9216d6cf3d0858cf1deab6db4b815384,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations,"[[    0  4688 44851    21   626    19     5   244     9 45068  3629    31
   1645 35644 19683    15 39976     9  5475     2]]"
49aecc50823a60c852165e121dbc0ca54304e40f,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," The data generation scripts use a basic template to create each paradigm, pulling from a vocabulary of over 3000 words annotated for morphological, syntactic, and semantic features needed to create grammatical and semantically felicitous sentences.","[[    0    20   414  2706 26878   304    10  3280 27663     7  1045   349
  28323     6  6539    31    10 32644     9    81 23513  1617 45068  1070
     13 38675  9779     6 45774 28201     6     8 46195  1575   956     7
   1045 25187 45816     8  9031 38600 14383 17022  1827 11305     4     2]]"
7b44bee49b7cb39cb7d5eec79af5773178c27d4d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner","[[    0 36949    10   278     9 47760  3270   215    25  4266  7068   687
    329     6 29975  7831   250     6  8330   267   417     6   234  2076
    597     8  9252   254     2]]"
82596190560dc2e2ced2131779730f40a3f3eb8c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"EHRs of 183 psychosis patients from McLean Psychiatric Hospital in Belmont, MA","[[    0   717 16271    29     9 29568 41116  1484    31  1509 19242 43254
   2392    11  4231  7861     6  8981     2]]"
c87b2dd5c439d5e68841a705dd81323ec0d64c97,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Bosch 2006 (mv), LDA + LogReg (mv), LDA + Raykar, LDA + Rodrigues, Blei 2003 (mv), sLDA (mv)","[[    0   387   366   611  3503    36   119   705   238   226  3134  2055
   9359 23007    36   119   705   238   226  3134  2055  4622  8722     6
    226  3134  2055 34238  3663     6 16463   118  4999    36   119   705
    238   579   574  3134    36   119   705    43     2]]"
8e2b125426d1220691cceaeaf1875f76a6049cbd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.
On Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively.","[[    0  2191 11373   176 34875     6     5  8611     9  1850  5448    16
   2782    30  1437  7833 12413  9162  1437   132     4   466     6  1437
    158     4  5677     6   112     4  5220    13  3023 22886  1342     6
   3023  9064  7257     8  1021  9064  7257  4067     4 50118  4148 30169
  41616     6     5  8611     9  1850  5448    16  2782    30  1437  7833
  12413  9162   155     4  4015     4  1437  1437   204     4  1225     6
    204     4  3414    13  3023 22886  1342     6  3023  9064  7257     8
   1021  9064  7257     4 27387  6608     4     2]]"
551cc0401674f7c363e0018b8186a125f7b17e99,0.0,Entailment,[[    2     0 30495  3760  1757     2]],10 hashtags that can be used in an insulting or offensive way,"[[    0   698 32795  8299    14    64    28   341    11    41 22602    50
   2555   169     2]]"
519db0922376ce1e87fcdedaa626d665d9f3e8ce,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
58a00ca123d67b9be55021493384c0acef4c568d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"learn to ask unanswerable questions by editing answerable ones with word exchanges, negations, etc","[[    0 38229     7  1394   542 27740   868  1142    30  5390  1948   868
   1980    19  2136  6927     6 15183  1635     6  4753     2]]"
b8f711179a468fec9a0d8a961fb0f51894af4b31,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CNN,[[    0 16256     2]]
594a6bf37eab64a16c6a05c365acc100e38fcff1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"sentiment analysis, the disambiguation of demonstrative pronouns,","[[    0 19530  8913  1966     6     5  2982  3146  1023  9762     9 30673
   3693 43083     6     2]]"
1a43df221a567869964ad3b275de30af2ac35598,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the Yelp Challenge dataset,[[    0   627 29730 10045 41616     2]]
7aab78e90ba1336950a2b0534cc0cb214b96b4fd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"an additional morphology table including target-side affixes., We inject the decoder with morphological properties of the target language.","[[    0   260   943 46930  2103   217  1002    12  3730 11129  3181   293
    482   166 17951     5  5044 15362    19 38675  9779  3611     9     5
   1002  2777     4     2]]"
052d19b456f1795acbb8463312251869cc5b38da,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e24fbcc8be922c43f6b6037cdf2bfd4c0a926c08,0.0,Entailment,[[    2     0 30495  3760  1757     2]], the Meta-LSTM BIBREF0,[[    0     5 37622    12   574  4014   448   163  8863 45935   288     2]]
9623884915b125d26e13e8eeebe9a0f79d56954b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],documents are segmented into paragraphs and processed at the paragraph level,"[[    0 37447 30179    32  2835   196    88 36153     8 12069    23     5
  17818   672     2]]"
66125cfdf11d3bf8e59728428e02021177142c3a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],explicit projection had a negligible effect on the performance,[[    0 23242 17022 18144    56    10 36334  1683    15     5   819     2]]
0cf5132ac7904b7b81e17938d5815f70926a5180,0.0,Entailment,[[    2     0 30495  3760  1757     2]],fastText and SVM BIBREF16,[[    0 16963 39645     8   208 20954   163  8863 45935  1549     2]]
4c50f75b1302f749c1351de0782f2d658d4bea70,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Annotators went through various phases to make sure their annotations did not deviate from the mean.,"[[    0  4688  3654  3629   439   149  1337 17369     7   146   686    49
  47234   222    45  8709 10599    31     5  1266     4     2]]"
f704d182c9e01a2002381b76bf21e4bb3c0d3efc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
9646fa1abbe3102a0364f84e0a55d107d45c97f0,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," jokes of different categories (sci-fi, sports, etc) and types (puns, limerick, etc)","[[    0 11248     9   430  6363    36 43428    12  9169     6  1612     6
   4753    43     8  3505    36   642 12140     6 11369 23933     6  4753
     43     2]]"
487dc65bf8a8ecbf052cf05641caf1b90a502853,0.0,Entailment,[[    2     0 30495  3760  1757     2]],three years of online news articles from June 2016 to June 2019,[[   0 9983  107    9  804  340 7201   31  502  336    7  502  954    2]]
3fddd9f6707b9e40e35518dae7f6da7c4cb77d16,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
28e7711f94e093137eb8828f0b1eff1b05e4fa38,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d653d994ef914d76c7d4011c0eb7873610ad795f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"By using  keywords `breast' AND `cancer' in tweet collecting process. 
","[[    0  2765   634  1437 32712 22209  7805  1988   108  4248 22209 33596
    108    11  3545  8664   609     4  1437 50118     2]]"
3de27c81af3030eb2d9de1df5ec9bfacdef281a9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"$421\,829\,960$ words divided into $17\,305\,401$ sentences","[[    0  1629 38785 37457     6   398  2890 37457     6 35154  1629  1617
   6408    88    68  1360 37457     6 27062 37457     6 33871  1629 11305
      2]]"
78577fd1c09c0766f6e7d625196adcc72ddc8438,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Training datasets: TTS System dataset and embedding selection dataset. Evaluation datasets: Common Prosody Errors dataset and LFR dataset.,"[[    0 44466 42532    35   255  2685  5149 41616     8 33183 11303  4230
  41616     4 37766 42532    35  9732 26590  9956 46357 41616     8   226
   5499 41616     4     2]]"
98daaa9eaa1e1e574be336b8933b861bfd242e5e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"weakly labeled into hate or non-hate memes, depending on their source","[[    0 25785   352 16274    88  4157    50   786    12 33990 26995     6
   6122    15    49  1300     2]]"
dcdcd977f18206da3ff8ad0ffb14f7bc5e126c7d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],we use the context of the word to predict its label and by doing so our model learns label-aware context for each word in the sentence,"[[    0  1694   304     5  5377     9     5  2136     7  7006    63  6929
      8    30   608    98    84  1421 25269  6929    12 24590  5377    13
    349  2136    11     5  3645     2]]"
0a8bc204a76041a25cee7e9f8e2af332a17da67a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU, Self-BLEU, n-gram based score, probability score","[[    0 30876   791     6 12156    12 30876   791     6   295    12 28526
    716  1471     6 18102  1471     2]]"
edb068df4ffbd73b379590762125990fcd317862,0.0,Entailment,[[    2     0 30495  3760  1757     2]], They used Stanford Sentiment Treebank benchmark for sentiment classification task and   AG English news corpus for the text classification task.,"[[    0   252   341  8607 12169  8913 11077  5760  5437    13  5702 20257
   3685     8  1437  1437  5680  2370   340 42168    13     5  2788 20257
   3685     4     2]]"
6b8a3100895f2192e08973006474428319dc298e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],MNCut spectral clustering algorithm BIBREF58,"[[    0   448  6905  1182 45113 46644  2961 17194   163  8863 45935  4432
      2]]"
0c78d2fe8bc5491b5fd8a2166190c59eba069ced,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a458c649a793588911cef4c421f95117d0b9c472,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"At the moment defender can draw on methods from image area to text for improving the robustness of DNNs, e.g. adversarial training BIBREF107 , adding extra layer BIBREF113 , optimizing cross-entropy function BIBREF114 , BIBREF115 or weakening the transferability of adversarial examples.","[[    0  3750     5  1151  5142    64  2451    15  6448    31  2274   443
      7  2788    13  3927     5  6295  1825     9   211 20057    29     6
    364     4   571     4 37930 27774  1058   163  8863 45935 19000  2156
   1271  1823 10490   163  8863 45935 19547  2156 36226  2116    12  1342
  47145  5043   163  8863 45935 20695  2156   163  8863 45935 15314    50
  16188     5  2937  4484     9 37930 27774  7721     4     2]]"
fdd9dea06550a2fd0df7a1e6a5109facf3601d76,0.0,Entailment,[[    2     0 30495  3760  1757     2]], 75 meetings and about 70 hours of real-time audio duration,"[[    0  3337  2891     8    59  1510   722     9   588    12   958  6086
  13428     2]]"
03ce42ff53aa3f1775bc57e50012f6eb1998c480,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"EN<->ES
EN<->DE
EN<->IT
EN<->EO
EN<->MS
EN<->FI","[[    0  2796 41552 46613  1723 50118  2796 41552 46613 10089 50118  2796
  41552 46613  2068 50118  2796 41552 46613 15127 50118  2796 41552 46613
   6222 50118  2796 41552 46613 14071     2]]"
b0fbd4b0f02b877a0d3df1d8ccc47d90dd49147c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"token representation, self-attention encoder,, Constituent Parsing Decoder,  Dependency Parsing Decoder","[[    0 46657  8985     6  1403    12  2611 19774  9689 15362 38581 11242
  15998  1342 31443   154 49227     6  1437 43290  6761 31443   154 49227
      2]]"
a997fc1a62442fd80d1873cd29a9092043f025ad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Transformer models in their base configuration BIBREF11, using 6 encoder and decoder layers, with model and hidden dimensions of 512 and 2048 respectively, and 8 heads for all attention layers","[[    0 19163 22098  3092    11    49  1542 20393   163  8863 45935  1225
      6   634   231  9689 15362     8  5044 15362 13171     6    19  1421
      8  7397 22735     9 29600     8 46920  4067     6     8   290  3885
     13    70  1503 13171     2]]"
f01aa192d97fa3cc36b6e316355dc5da0e9b97dc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"(i) Uniform, (ii) SVR+W, (iii) SVR+O, (iv) C4.5SSL, (v) GLM","[[    0  1640   118    43 39555     6    36  4132    43   208 13055  2744
    771     6    36 31917    43   208 13055  2744   673     6    36  1879
     43   230   306     4   245 46535     6    36   705    43 12209   448
      2]]"
6da2cb3187d3f28b75ac0a61f6562a8adf716109,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Pointer-Generator, Transformer",[[    0 26170  8007    12 40025  2630     6  5428 22098     2]]
11e376f98df42f487298ec747c32d485c845b5cd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
118ff1d7000ea0d12289d46430154cc15601fd8e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],logistic regression,[[    0 12376  5580 39974     2]]
dba3d05c495e2c8ca476139e78f65059db2eb72d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
62a3dc90ba427c5985789001a02825c9434ce67d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1,160 physician logs of Medical ICU admission requests, 42,506 Wikipedia articles, 6 research papers and 2 critical care medicine textbooks","[[    0   134     6 13726 11593 24113     9  3067  8242   791  7988  5034
      6  3330     6 36647 28274  7201     6   231   557  6665     8   132
   2008   575  6150 31266     2]]"
9bfa46ad55136f2a365e090ce585fc012495393c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the single domain dataset, WoZ2.0 , the multi-domain dataset, MultiWoZ","[[    0   627   881 11170 41616     6 20963  1301   176     4   288  2156
      5  3228    12 46400 41616     6 19268 44055  1301     2]]"
2965c86467d12b79abc16e1457d848cb6ca88973,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Dialogue Act Markup in Several Layers (DAMSL) tag set,"[[    0 48201  1783  1190   658    11  3646   226 24950    36   495  2620
  11160    43  6694   278     2]]"
f3766c6937a4c8c8d5e954b4753701a023e3da74,0.0,Entailment,[[    2     0 30495  3760  1757     2]],fine-tuned the GPT-2 medium model BIBREF51 on our collected headlines and then used it to measure the perplexity (PPL) on the generated outputs,"[[    0 35093    12 24641   196     5   272 10311    12   176  4761  1421
    163  8863 45935  4708    15    84  4786  6337     8   172   341    24
      7  2450     5 33708  1571    36   510  7205    43    15     5  5129
  39512     2]]"
a22b900fcd76c3d36b5679691982dc6e9a3d34bf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
be08ef81c3cfaaaf35c7414397a1871611f1a7fd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WMD, VSM, PV-DTW, PV-TED","[[    0   771 12550     6   468 15153     6 23774    12 23858   771     6
  23774    12 43036     2]]"
08cbc9b8a8df56ec7be626f89285a621e1350f63,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the annotated dataset reported by degen2015investigating, a dataset of the utterances from the Switchboard corpus of telephone dialogues BIBREF21 that contain the word some","[[    0   627 45068  1070 41616   431    30 31295   225 14420 12406 31721
      6    10 41616     9     5 18672  5332    31     5 11171  4929 42168
      9  7377 25730  3663   163  8863 45935  2146    14  5585     5  2136
    103     2]]"
c77d6061d260f627f2a29a63718243bab5a6ed5a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the training dataset is large while the target dataset is usually much smaller,"[[    0   627  1058 41616    16   739   150     5  1002 41616    16  2333
    203  2735     2]]"
ce807a42370bfca10fa322d6fa772e4a58a8dca1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Darkode,  Hack Forums, Blackhat and Nulled.","[[    0 35268  4636     6  1437 14520 43514     6  1378  8849     8 44840
    196     4     2]]"
d9cbcaf8f0457b4be59178446f1a280d17a923fa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Direct comparison of model parameters,[[    0 33038  6676     9  1421 17294     2]]
76c8aac84152fc4bbc0d5faa7b46e40438353e77,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
327e6c6609fbd4c6ae76284ca639951f03eb4a4c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb","[[    0  2709   939    12 48219   467     6  4476    32   365     4  2545
    207 28510     7   748  4325 31828   428     4   286  3023    12 48219
    467     6  4476    32   158     4  5243   207 28510     7   748  4325
  31828   428     2]]"
9368471073c66fefebc04f1820209f563a840240,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
b9025c39838ccc2a79c545bec4a676f7cc4600eb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1. there may be situations where more than one action is reasonable, and also because writers tell a story playing with elements such as surprise or uncertainty.
2. Macro F1 = 14.6 (MLR, length 96 snippet)
Weighted F1 = 31.1 (LSTM, length 128 snippet)","[[    0   134     4    89   189    28  5458   147    55    87    65   814
     16  5701     6     8    67   142  6737  1137    10   527   816    19
   4785   215    25  2755    50  4983     4 50118   176     4 32449   274
    134  5457   501     4   401    36 10537   500     6  5933  8971 42772
     43 50118 42158   196   274   134  5457  1105     4   134    36   574
   4014   448     6  5933 13950 42772    43     2]]"
e44a5514d7464993997212341606c2c0f3a72eb4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
97dac7092cf8082a6238aaa35f4b185343b914af,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age, more women than men were given a diagnosis of depression","[[    0 40053   533 16658    12 12105  1956    16  3240     6    50 16658
  13692    32    55   533     7  9263    49  1046     6    55   390    87
    604    58   576    10  9726     9  6943     2]]"
8568c82078495ab421ecbae38ddd692c867eac09,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1, 4, 8, 16, 32, 64",[[   0  134    6  204    6  290    6  545    6 2107    6 4430    2]]
6ea63327ffbab2fc734dd5c2414e59d3acc56ea5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower.","[[    0  3908  1122   346     9 17294     6     5  7425 11801    16    59
    321     4   134   795    13   226  4014 13123   420 42532     4   520
      5   346     9 17294    11   226  4014 13123    16  1130     6    49
   7425 11801    16    62     7   321     4   406   795     4     2]]"
3ee976add83e37339715d4ae9d8aa328dd54d052,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Queensland flood which provided 96% accuracy, Alberta flood with the same configuration of train-test split which provided 95% accuracy","[[    0 31178  1290  1245  5005    61  1286  8971   207  8611     6  6055
   5005    19     5   276 20393     9  2341    12 21959  3462    61  1286
   6164   207  8611     2]]"
19c9cfbc4f29104200393e848b7b9be41913a7ac,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"2,714 ",[[    0   176     6 37901  1437     2]]
f6f8054f327a2c084a73faca16cf24a180c094ae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a71ebd8dc907d470f6bd3829fa949b15b29a0631,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"if it includes  negative utterances, negative generalizations and insults concerning ethnicity, nationality, religion and culture.","[[    0  1594    24  1171  1437  2430 18672  5332     6  2430   937 18391
      8 22536  8082 23848     6 26241     6  6825     8  2040     4     2]]"
3fb4334e5a4702acd44bd24eb1831bb7e9b98d31,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Evaluation datasets used:
CMRC 2018 - 18939 questions, 10 answers
DRCD - 33953 questions, 5 answers
NIST MT02/03/04/05/06/08 Chinese-English - Not specified

Source language train data:
SQuAD - Not specified","[[    0   717  6486  9762 42532   341    35 50118 18814  5199   199   111
  29304  3416  1142     6   158  5274 50118   495  5199   495   111 36885
   4540  1142     6   195  5274 50118   487 11595 14077  4197    73  3933
     73  3387    73  2546    73  4124    73  3669  1111    12 35007   111
   1491 17966 50118 50118  7061  2777  2341   414    35 50118   104 12444
   2606   111  1491 17966     2]]"
17f5f4a5d943c91d46552fb75940b67a72144697,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the rank-correlation for MFH model increases by 36.4% when is evaluated in VQA-HAT dataset and 7.7% when is evaluated in VQA-X,"[[    0   627  7938    12  7215 47114    13 32671   725  1421  3488    30
   2491     4   306   207    77    16 15423    11   468  1864   250    12
    725  2571 41616     8   262     4   406   207    77    16 15423    11
    468  1864   250    12  1000     2]]"
b5c3787ab3784214fc35f230ac4926fe184d86ba,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
1a7d28c25bb7e7202230e1b70a885a46dac8a384,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
7f9bc06cfa81a4e3f7df4c69a1afef146ed5a1cf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"10 Epochs: pearson-Spearman correlation  drops  60 points when error increase by 20%
50 Epochs: pearson-Spearman correlation  drops  55 points when error increase by 20%","[[    0   698 14230  4306    29    35 30023  1478    12 29235   271   397
  22792  1437  9305  1437  1191   332    77  5849   712    30   291   207
  50118  1096 14230  4306    29    35 30023  1478    12 29235   271   397
  22792  1437  9305  1437  3490   332    77  5849   712    30   291   207
      2]]"
02417455c05f09d89c2658f39705ac1df1daa0cd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"$1,728",[[    0  1629   134     6 39265     2]]
06202ab8b28dcf3991523cf163b8844b42b9fc99,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"10k training and 1k test, 1,101 sentences (26k tokens)","[[    0   698   330  1058     8   112   330  1296     6   112     6 12636
  11305    36  2481   330 22121    43     2]]"
6adec34d86095643e6b89cda5c7cd94f64381acc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.,"[[    0  4528  1575    32 16934  2024    31     5  2136     8  5604     5
    937 16699     9    10  2136   145 13314    11 24962     4     2]]"
8816333fbed2bfb1838407df9d6c084ead89751c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
521a7042b6308e721a7c8046be5084bc5e8ca246,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"graph-like structures where arcs connect nodes representing multiple hypothesized words, thus allowing multiple incoming arcs unlike 1-best sequences","[[    0 44143    12  3341  6609   147 44994  4686 32833  4561  1533 45936
   1617     6  4634  2455  1533 11433 44994  7328   112    12  7885 26929
      2]]"
312417675b3dc431eb7e7b16a917b7fed98d4376,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Axelrod's causal mapping method,[[    0 39884   523 10774    18 41214 20410  5448     2]]
f399d5a8dbeec777a858f81dc4dd33a83ba341a2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"QnAMaker Portal, QnaMaker Management APIs, Azure Search Index, QnaMaker WebApp, Bot","[[    0  1864   282  2620  4218 31802     6  1209  2133 45111  1753 35329
      6 25959 12180  4648     6  1209  2133 45111  6494 19186     6 13954
      2]]"
be7f52c4f2bad20e728785a357c383853d885d94,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"includes 1,941 citation instances from 186 papers",[[    0 25142   112     6   466  4006 31825 10960    31 30509  6665     2]]
f399d5a8dbeec777a858f81dc4dd33a83ba341a2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"QnAMaker Portal, QnaMaker Management APIs, Azure Search Index, QnaMaker WebApp, Bot","[[    0  1864   282  2620  4218 31802     6  1209  2133 45111  1753 35329
      6 25959 12180  4648     6  1209  2133 45111  6494 19186     6 13954
      2]]"
863d5c6305e5bb4b14882b85b6216fa11bcbf053,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MOCC, OCCAV, COAV, AVeer, GLAD, DistAV, Unmasking, Caravel, GenIM, ImpGI, SPATIUM and NNCD","[[    0  8756  3376     6   384  3376 10612     6  6247 10612     6 17307
  22719     6 12209  2606     6 11281 10612     6  1890 43776   154     6
   1653   102  5536     6  4380  3755     6 17550 21525     6  6178  2571
  43145     8   234  6905   495     2]]"
163c15da1aa0ba370a00c5a09294cd2ccdb4b96d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
284ea817fd79bc10b7a82c88d353e8f8a9d7e93c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
1b9119813ea637974d21862a8ace83bc1acbab8e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).,"[[    0  1213   341 45569 16859  2777     8 16859  9911     7 11857  9946
  33183   417  1033     8 41616  1286    11   289  6243  3685     7  2341
   1421    36 45839    45  2801    11  2225   322     2]]"
dc28ac845602904c2522f5349374153f378c42d3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"44,000 tweets",[[   0 3305    6  151 6245    2]]
5871d258f66b00fb716065086f757ef745645bfe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
7c45c6e5db6cfca2d6de8751e28403b35420ae38,0.0,Entailment,[[    2     0 30495  3760  1757     2]],input byte embedding matrix has dimensionality 256,[[    0 46797 47893 33183 11303 36173    34 21026  6948 22078     2]]
8a0a51382d186e8d92bf7e78277a1d48958758da,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For entity  F1 in the movie, taxi and restaurant domain it results in scores of 50.86, 64, and 60.35. For success, it results it outperforms in the movie and restaurant domain with scores of 77.95 and 71.52","[[    0  2709 10014  1437   274   134    11     5  1569     6  9955     8
   2391 11170    24   775    11  4391     9   654     4  5334     6  4430
      6     8  1191     4  2022     4   286  1282     6    24   775    24
   9980 33334    11     5  1569     8  2391 11170    19  4391     9  6791
      4  4015     8  6121     4  4429     2]]"
cdf7e60150a166d41baed9dad539e3b93b544624,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WeedsPrec BIBREF8, invCL BIBREF11, SLQS model, cosine similarity","[[    0   170 12080 22763   438   163  8863 45935   398     6 12259  7454
    163  8863 45935  1225     6 10962  1864   104  1421     6 12793   833
  37015     2]]"
cd9776d03fe48903e43e916385df12e1e798070a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
6548db45fc28e8a8b51f114635bad14a13eaec5b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"We construct a GAN model which combines different sets of word embeddings INLINEFORM4 , INLINEFORM5 , into a single set of word embeddings INLINEFORM6 . ","[[    0   170 12558    10   272  1889  1421    61 15678   430  3880     9
   2136 33183   417  1033  2808 28302 38036   306  2156  2808 28302 38036
    245  2156    88    10   881   278     9  2136 33183   417  1033  2808
  28302 38036   401   479  1437     2]]"
6dcbe941a3b0d5193f950acbdc574f1cfb007845,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Alarm
Bank
Bus
Calendar
Event
Flight
Home
Hotel
Media
Movie
Music
RentalCar
Restaurant
RideShare
Service
Travel
Weather","[[    0  7083  4526 50118 13532 50118 30840 50118 15117 29702 50118 44879
  50118 41693 50118 19457 50118 27735   523 50118 18801 50118 42963 50118
  30591 50118   500 13589  9518 50118 31921  8616   927 50118   500  1949
  11957 50118 32537 50118 39258 50118 29840     2]]"
70797f66d96aa163a3bee2be30a328ba61c40a18,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SRCC,[[    0 17973  3376     2]]
2255c36c8c7ed6084da577b480eb01d349f52943,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Such a system would benefit educators by saving time to generate quizzes and tests.,"[[    0 27331    10   467    74  1796 15969    30  6549    86     7  5368
  29316   293     8  3457     4     2]]"
0cf6d52d7eafd43ff961377572bccefc29caf612,0.0,Entailment,[[    2     0 30495  3760  1757     2]],AMT,[[   0 2620  565    2]]
c45a160d31ca8eddbfea79907ec8e59f543aab86,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Swissmetro dataset,[[    0 15417  3006  5646  1001 41616     2]]
016b59daa84269a93ce821070f4f5c1a71752a8a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d7aed39c359fd381495b12996c4dfc1d3da38ed5,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," applying the rule INLINEFORM4 to a set of natural language questions INLINEFORM5, both models are improved following the back-translation protocol that target sequences should follow the real data distribution","[[    0  9889     5  2178  2808 28302 38036   306     7    10   278     9
   1632  2777  1142  2808 28302 38036   245     6   258  3092    32  2782
    511     5   124    12 48235 11883    14  1002 26929   197  1407     5
    588   414  3854     2]]"
171ebfdc9b3a98e4cdee8f8715003285caeb2f39,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Average accuracy of proposed model vs best prevous result:
Single-task Training: 57.57 vs 55.06
Multi-task Training: 50.17 vs 50.59","[[    0 46315  8611     9  1850  1421  1954   275 21720  1827   898    35
  50118 44119    12 45025 10657    35  4981     4  4390  1954  3490     4
   4124 50118 46064    12 45025 10657    35   654     4  1360  1954   654
      4  4156     2]]"
8602160e98e4b2c9c702440da395df5261f55b1f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Data released for APDA shared task contains 3 datasets.,"[[    0 30383   703    13  1480  3134  1373  3685  6308   155 42532     4
      2]]"
2caa8726222237af482e170c51c88099cefef6fc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
6270d5247f788c4627be57de6cf30112560c863f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They experimented with sentiment analysis and natural language inference task,[[    0  1213 37672    19  5702  1966     8  1632  2777 42752  3685     2]]
8ec2ca6c7f60c46eedac1fe0530b5c4448800fec,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
a95188a0f35d3cb3ca70ae1527d57ac61710afa3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"60,000 ",[[   0 2466    6  151 1437    2]]
babe72f0491e65beff0e5889380e8e32d7a81f78,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," Moreover, our TL-TranSum method also outperforms other approaches such as MaxCover ( $5\%$ ) and MRMR ( $7\%$ )","[[    0  7905     6    84 27017    12   565  3917 38182  5448    67  9980
  33334    97  8369   215    25  4471 45158    36    68   245 37457   207
   1629  4839     8 18838 12642    36    68   406 37457   207  1629  4839
      2]]"
0b31eb5bb111770a3aaf8a3931d8613e578e07a8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Presence of only the exact unigrams 'caused', 'causing', or 'causes'","[[    0 28917  4086     9   129     5  6089   542  1023 27809   128  3245
   6199  3934   128  3245 10928  3934    50   128  3245  9764   108     2]]"
1df24849e50fcf22f0855e0c0937c1288450ed5c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
34af2c512ec38483754e94e1ea814aa76552d60a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Answer with content missing: (formula) The accuracy is defined as the ratio # of correct chains predicted to # of evaluation samples,"[[    0 33683    19  1383  1716    35    36  3899  5571    43    20  8611
     16  6533    25     5  1750   849     9  4577  9781  6126     7   849
      9 10437  7931     2]]"
ae17066634bd2731a07cd60e9ca79fc171692585,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
4c7ac51a66c15593082e248451e8f6896e476ffb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Full Testing Set Accuracy: 84.02
Cleaned Testing Set Accuracy: 93.48","[[    0 31440 25980  8504 42688    35  7994     4  4197 50118 40827   196
  25980  8504 42688    35  8060     4  3818     2]]"
5f0bb32d70ee8e4c4c59dc5c193bc0735fd751cc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],dialogue simulator,[[    0 43735 10149 35191     2]]
32ba4d2d15194e889cbc9aa1d21ff1aa6fa27679,0.0,Entailment,[[    2     0 30495  3760  1757     2]],extensive personal feedback,[[    0 16435 17355  1081  6456     2]]
da544015511e535503dee2eaf4912a5e36c806cd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BIBREF5 to train neural sequence-to-sequence, NMF topic model with scikit-learn BIBREF14","[[    0  5383   387 45935   245     7  2341 26739 13931    12   560    12
  46665     6 31736   597  5674  1421    19  2850   967   405    12 38229
    163  8863 45935  1570     2]]"
093039f974805952636c19c12af3549aa422ec43,0.0,Entailment,[[    2     0 30495  3760  1757     2]],It uses deep learning framework (pytorch),[[    0   243  2939  1844  2239  7208    36 17163 10321   611    43     2]]
08a5f8d36298b57f6a4fcb4b6ae5796dc5d944a4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],integrate clinical named entity information into pre-trained language model,"[[    0 24894  7954  5154  1440 10014   335    88  1198    12 23830  2777
   1421     2]]"
160e6d2fc6e04bb0b4ee8d59c06715355dec4a17,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the best performing model obtained an accuracy of 0.86,[[   0  627  275 4655 1421 4756   41 8611    9  321    4 5334    2]]
0fc17e51a17efce17577e2db89a24abd6607bb2b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
41173179efa6186eef17c96f7cbd8acb29105b0e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Inference task
The aim of the inference task is to predict whether the premise sentence entails or contradicts the hypothesis sentence, Document multilabel classification
The multilabel classification task predicts multiple labels from the texts., Relation extraction
The aim of the relation extraction task is to predict relations and their types between the two entities mentioned in the sentences., Named entity recognition
The aim of the named entity recognition task is to predict mention spans given in the text , Sentence similarity
The sentence similarity task is to predict similarity scores based on sentence pairs","[[    0  1121 23861  3685 50118   133  4374     9     5 42752  3685    16
      7  7006   549     5 18805  3645 29865    50 37820     5 31098  3645
      6 27246  7268   718 14286 20257 50118   133  7268   718 14286 20257
   3685 17876  1533 14105    31     5 14301   482  8136  1258 23226 50118
    133  4374     9     5  9355 23226  3685    16     7  7006  3115     8
     49  3505   227     5    80  8866  2801    11     5 11305   482 30436
  10014  4972 50118   133  4374     9     5  1440 10014  4972  3685    16
      7  7006  4521 23645   576    11     5  2788  2156 12169  4086 37015
  50118   133  3645 37015  3685    16     7  7006 37015  4391   716    15
   3645 15029     2]]"
2ec97cf890b537e393c2ce4c2b3bd05dfe46f683,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They look at the performance accuracy of explanation and the prediction performance,"[[    0  1213   356    23     5   819  8611     9  8257     8     5 16782
    819     2]]"
18fbf9c08075e3b696237d22473c463237d153f5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For event types and participant types, there was a moderate to substantial level of agreement using the Fleiss' Kappa. For coreference chain annotation, there was average agreement of 90.5%.","[[    0  2709   515  3505     8 16076  3505     6    89    21    10  7212
      7  6143   672     9  1288   634     5 12602  3006   108 37772     4
    286  2731 23861  3206 47760     6    89    21   674  1288     9  1814
      4   245  2153     2]]"
47726be8641e1b864f17f85db9644ce676861576,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
957bda6b421ef7d2839c3cec083404ac77721f14,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors, Spelling errors, Repeated characters, Capitalization, Length, Emoticon (Presence/Count), Sentiment Ratio.","[[    0   574  3134   542  1023 27809    36 28917  4086    73 23329   238
  29206 20475     6   849   487  7486 46718 22150  2485     6   849 41615
  21117  8791   994     6  8330  7633  9126     6 32021  1070  3768     6
   1867  1938     6 41852     6  3676  1242 17505    36 28917  4086    73
  23329   238 12169  8913 20475     4     2]]"
710c1f8d4c137c8dad9972f5ceacdbf8004db208,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
3c362bfa11c60bad6c7ea83f8753d427cda77de0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They think it will help human TCM practitioners make prescriptions.,"[[    0  1213   206    24    40   244  1050 21137   448 21492   146 20400
      4     2]]"
2df910c9806f0c379d7bb1bc2be2610438e487dc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BIBREF32, BIBREF23, BIBREF33, discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. ","[[    0  5383   387 45935  2881     6   163  8863 45935  1922     6   163
   8863 45935  3103     6  4404    11   237   430 11991    35  2370     6
  13053     6  3453     8  1515     6 14196    11   292  3806    81     5
    232    35   391     8   369   730     6  2027  1005     6  1505     8
   2944  1817     4  1437     2]]"
6472f9d0a385be81e0970be91795b1b97aa5a9cf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (list missing) 
Scheduled sampling: In our experiments, we found that models trained with scheduled sampling performed better (about 0.004 BLEU-4 on validation set) than the ones trained using teacher-forcing for the AVSD dataset. Hence, we use scheduled sampling for all the results we report in this paper.

Yes.","[[    0 33683    19  1383  1716    35    36  8458  1716    43  1437 50118
    104  3804 12841 20424    35    96    84 15491     6    52   303    14
   3092  5389    19  1768 20424  3744   357    36  9006   321     4 37114
    163  3850   791    12   306    15 26567   278    43    87     5  1980
   5389   634  3254    12 43837    13     5 17307  6243 41616     4 18709
      6    52   304  1768 20424    13    70     5   775    52   266    11
     42  2225     4 50118 50118  9904     4     2]]"
de03e8cc1ceaf2108383114460219bf46e00423c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a subset of the population where we can quantitatively measure reactions: the popular Reddit r/Jokes thread, These Reddit posts consist of the body of the joke, the punchline, and the number of reactions or upvotes. ","[[    0   102 37105     9     5  1956   147    52    64 24934 43127  2450
  11012    35     5  1406  6844   910    73   863  6568 15019     6  1216
   6844  4570 19438     9     5   809     9     5  8018     6     5 10064
   1902     6     8     5   346     9 11012    50    62 45547     4  1437
      2]]"
f0317e48dafe117829e88e54ed2edab24b86edb1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"if the attention loose track of the objects in the picture and ""gets lost"", the model still takes it into account and somehow overrides the information brought by the text-based annotations","[[    0  1594     5  1503  7082  1349     9     5  8720    11     5  2170
      8    22 30900   685  1297     5  1421   202  1239    24    88  1316
      8  7421 34685  4376     5   335  1146    30     5  2788    12   805
  47234     2]]"
be1c0816793a4549c811480170f30fab52a7a157,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
4688534a07a3cbd8afa738eea02cc6981a4fd285,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They use Monalog for data-augmentation to fine-tune BERT on this task,"[[    0  1213   304  3385 31263    13   414    12  1180 10757  1258     7
   2051    12    90  4438   163 18854    15    42  3685     2]]"
14323046220b2aea8f15fba86819cbccc389ed8b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
acc512c57aef4d5a15c15e3593f0a9b3e7e7e8b8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1) Connectionist Temporal Classification (CTC), 2) Attention-based methods, 3) RNN-tranducer","[[    0   134    43 29759   661  9188 46249 40509    36  7164   347   238
    132    43 35798    12   805  6448     6   155    43   248 20057    12
   4328   463 43886     2]]"
b69897deb5fb80bf2adb44f9cbf6280d747271b3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BERT,[[    0 11126   565     2]]
208e667982160cfbce49ef49ad96f6ab094292ac,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"coding scheme is defined, coders are trained with the coding scheme, Training sometimes results in changes to the coding scheme, calculation of inter-annotator agreement or inter-rater reliability., there is a process of reconciliation for disagreements","[[    0    17    48   438 19519  3552    17    46    16  6533     6 20993
    268    32  5389    19     5 25776  3552     6 10657  2128   775    11
   1022     7     5 25776  3552     6 21586     9    44    48  8007    12
  37250  2630  1288    17    46    50    44    48  8007    12   338  5109
  13677     4    17    46     6    89    16    10   609     9    44    48
    241  3865   438 26045    17    46    13 24461     2]]"
efb52bda7366d2b96545cf927f38de27de3b5b77,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
f7c34b128f8919e658ba4d5f1f3fc604fb7ff793,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Textual inputs, knowledge bases, and images.",[[    0 39645  5564 16584     6  2655  7531     6     8  3156     4     2]]
74db8301d42c7e7936eb09b2171cd857744c52eb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Comparison of test accuracies of neural network models on an inflection task and qualitative analysis of the errors,"[[    0 48080  4060     9  1296 49060 15668     9 26739  1546  3092    15
     41  4047 20576  3685     8 29981  1966     9     5  9126     2]]"
78c010db6413202b4063dc3fb6e3cc59ec16e7e3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],a trained worker consolidates existing annotations ,[[    0   102  5389  5015 24707  1626  2210 47234  1437     2]]
d1747b1b56fddb05bb1225e98fd3c4c043d74592,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Convolutional Neural Network , bidirectional Recurrent Neural Network model with attention mechanism","[[    0  9157   705 23794   337 44304  3658  2156  2311 43606   337  7382
  41937 44304  3658  1421    19  1503  9562     2]]"
dcb18516369c3cf9838e83168357aed6643ae1b8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The dataset comes with a ranked set of relevant documents. Hence the baselines do not use a retrieval system.,"[[    0   133 41616   606    19    10  4173   278     9  4249  2339     4
  18709     5 11909 38630   109    45   304    10 43372   467     4     2]]"
182c7919329bc5678cf0c79687a66c0f7782577e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"gating function, Dynamic Memory",[[    0   571  1295  5043     6 29614 25940     2]]
af82043e7d046c2fb1ed86ef9b48c35492e6a48c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
d8de12f5eff64d0e9c9e88f6ebdabc4cdf042c22,0.0,Entailment,[[    2     0 30495  3760  1757     2]],0.8 points on Binary; 0.7 points on Fine-Grained; 0.6 points on Senti140; 0.7 points on Subj,"[[    0   288     4   398   332    15 47466   131   321     4   406   332
     15 14321    12 15536  7153   131   321     4   401   332    15 12169
    118 14753   131   321     4   406   332    15  4052   267     2]]"
6b9310b577c6232e3614a1612cbbbb17067b3886,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," if a vernacular paragraph contains more poetic images used in classical literature, its generated poem usually achieves higher score, poems generated from descriptive paragraphs achieve higher scores than from logical or philosophical paragraphs","[[    0   114    10  1437 12170 28049 17818  6308    55 33709  3156   341
     11 15855 13144     6    63  5129 19340  2333 35499   723  1471     6
  25833  5129    31 42690 36153  3042   723  4391    87    31 16437    50
  29690 36153     2]]"
e051d68a7932f700e6c3f48da57d3e2519936c6d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Bidirectional LSTM based NER model of Flair,"[[    0   387   808 43606   337   226  4014   448   716   234  2076  1421
      9  4150  2456     2]]"
e2f269997f5a01949733c2ec8169f126dabd7571,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"- En-Fr (WMT14)
- En-De (WMT15)
- Skipthought (BookCorpus)
- AllNLI (SNLI + MultiNLI)
- Parsing (PTB + 1-billion word)","[[    0    12  2271    12 29220    36   771 11674  1570    43 50118    12
   2271    12 13365    36   771 11674   996    43 50118    12  6783 26086
     36 24751 25313   687    43 50118    12   404   487 27049    36 12436
  27049  2055 19268   487 27049    43 50118    12 31443   154    36 10311
    387  2055   112    12  9026  2136    43     2]]"
d42031893fd4ba5721c7d37e1acb1c8d229ffc21,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
1a69696034f70fb76cd7bb30494b2f5ab97e134d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Answer with content missing: (Table II) Proposed model has F1 score of  0.7220 compared to 0.7148 best state-state-of-the-art result.,"[[    0 33683    19  1383  1716    35    36 41836  3082    43 13695  7878
   1421    34   274   134  1471     9  1437   321     4  4956   844  1118
      7   321     4   406 26300   275   194    12  4897    12  1116    12
    627    12  2013   898     4     2]]"
05671d068679be259493df638d27c106e7dd36d0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Operation accuracy: 71.89
Execution accuracy: 55.95","[[    0 35360  8611    35  6121     4  5046 50118 46891 15175  8611    35
   3490     4  4015     2]]"
a8e4a67dd67ae4a9ebf983a90b0d256f4b9ff6c6,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," SST-1, SST-2, Subj , TREC , Irony ","[[    0   208  4014    12   134     6   208  4014    12   176     6  4052
    267  2156   255 40698  2156  9940   219  1437     2]]"
65ebed1971dca992c3751ed985fbe294cbe140d7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],a reliability study for the proposed scheme ,[[    0   102 13677   892    13     5  1850  3552  1437     2]]
d3092f78bdbe7e741932e3ddf997e8db42fa044c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],root mean square error between the actual and the predicted price of Bitcoin for every minute,"[[    0 29059  1266  3925  5849   227     5  3031     8     5  6126   425
      9  8518    13   358  2289     2]]"
f80d89fb905b3e7e17af1fe179b6f441405ad79b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
cac0119681f311b2efd14b3251a2a5b69ad5d0cd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
0aa46c132515d8830a72f263812cdf7cbd5627c6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"RS-Average , RS-Linear, RS-Item, RS-MF, Sum-Opinosis, Sum-LSTM-Att","[[    0  8105    12 46315  2156 15388    12 40253  4352     6 15388    12
  47599     6 15388    12 41583     6  9430    12   673 10188 13310     6
   9430    12   574  4014   448    12 28062     2]]"
4fa851d91388f0803e33f6cfae519548598cd37c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
9c68d6d5451395199ca08757157fbfea27f00f69,0.0,Entailment,[[    2     0 30495  3760  1757     2]],OpenIE4 and MiniIE,[[    0 25266  7720   306     8 14912  7720     2]]
1d6c42e3f545d55daa86bea6fabf0b1c52a93bbb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
228425783a4830e576fb98696f76f4c7c0a1b906,0.0,Entailment,[[    2     0 30495  3760  1757     2]],two translation directions (En-It and En-De),"[[    0  7109 19850  9969    36 16040    12   243     8  2271    12 13365
     43     2]]"
25fd61bb20f71051fe2bd866d221f87367e81027,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"NDM, LIDM, KVRN, and TSCP/RL","[[    0 13457   448     6   226  2688   448     6   229 13055   487     6
      8   255 48091    73 17868     2]]"
3be8859103016ce2afe4c0a8552b9d980f7958bf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
368317b4fd049511e00b441c2e9550ded6607c37,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
fcd0bd2db39898ee4f444ae970b80ea4d1d9b054,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
6fd07f4dc037a82c8fa0ed80469eb4171dcebf12,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
de015276dcde4e7d1d648c6e31100ec80f61960f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a48c6d968707bd79469527493a72bfb4ef217007,0.0,Entailment,[[    2     0 30495  3760  1757     2]],MultiNLI,[[    0 46064   487 27049     2]]
3c0d66f9e55a89d13187da7b7128666df9a742ce,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"In the speaker-closed condition, two episodes were set aside from each speaker as development and test sets., In the speaker-open condition, all the data except for the test speaker's were used for training","[[    0  1121     5  5385    12 25315  1881     6    80  7585    58   278
   4364    31   349  5385    25   709     8  1296  3880   482    96     5
   5385    12 12592  1881     6    70     5   414  4682    13     5  1296
   5385    18    58   341    13  1058     2]]"
4ef3bfebabda83a6d5ca55d30de0e05893f241e3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
94dc437463f7a7d68b8f6b57f1e3606eacc4490a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
691cba5713c76a6870e35bc248ce1d29c0550bc7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
b236b9827253037b2fd7884d7bfec74619d96293,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"significant MAP performance improvement compared to the previous best model, CompClip-LM (0.696 to 0.734 absolute)","[[    0 18880 36890   819  3855  1118     7     5   986   275  1421     6
  10081 11428  1588    12 21672    36   288     4 36999     7   321     4
    406  3079  7833    43     2]]"
95abda842c4df95b4c5e84ac7d04942f1250b571,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"German-English, French-English, and Japanese-English","[[    0 27709    12 35007     6  1515    12 35007     6     8  2898    12
  35007     2]]"
afe34e553c3c784dbf02add675b15c27638cdd45,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
46563a1fb2c3e1b39a185e4cbb3ee1c80c8012b7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e097c2ec6021b1c1195b953bf3e930374b74d8eb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The resolution of the low-frequency feature maps is reduced by an octave  height and width dimensions are divided by 2. In this work, we explore spatial reduction by up to 3 octaves  dividing by $2^t$, where $t=1,2,3$  and for up to 4 groups. We refer to such a layer as a multi-octave convolutional (MultiOctConv) layer,","[[    0   133  3547     9     5   614    12 36274  1905 12879    16  2906
     30    41 16874  4097   126  6958     8 22523 22735    32  6408    30
    132     4    96    42   173     6    52  5393 34999  4878    30    62
      7   155 16874  9419   126 26667    30    68   176 35227    90 47110
    147    68    90  5214   134     6   176     6   246  1629   126     8
     13    62     7   204  1134     4   166  9115     7   215    10 10490
     25    10  3228    12 32643  4097 15380 23794   337    36 46064 19144
   9157   705    43 10490     6     2]]"
dee116df92f9f92d9a67ac4d30e32822c22158a6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
bd1a3c651ca2b27f283d3f36df507ed4eb24c2b0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"No, it is a probabilistic model trained by finding feature weights through gradient ascent","[[    0  3084     6    24    16    10 16245 14148  5580  1421  5389    30
   2609  1905 23341   149 43141 33088     2]]"
67ee7a53aa57ce0d0bc1a20d41b64cb20303f4b7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"163,110,000 utterances",[[    0 29425     6 11670     6   151 18672  5332     2]]
a88a454ac1a1230263166fd824e5daebb91cb05a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],back translation between English and Chinese,[[    0  1644 19850   227  2370     8  1111     2]]
1c4cd22d6eaefffd47b93c2124f6779a06d2d9e1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"3 million webpages processed with a CCG parser for training, 220 queries for development, and 307 queries for testing","[[    0   246   153  3748 40347 12069    19    10  9841   534 48946    13
   1058     6 14576 22680    13   709     6     8 34221 22680    13  3044
      2]]"
574f17134e4dd041c357ebb75a7ef590da294d22,0.0,Entailment,[[    2     0 30495  3760  1757     2]],null model ,[[    0 15755  1421  1437     2]]
185841e979373808d99dccdade5272af02b98774,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. ","[[    0  1594    89    16    41  5849    11     5 20992     6     5  6436
     16   533     7    28 16611    19    63  3757     6     8  4634     5
   1421   197   342   513  2416    15    42  6436     4    96    97  1617
      6     5  5849  6436   197    33     5   513  2712    15     5  1421
     18 16782     9     5  1058   414     4  1437     2]]"
8e9de181fa7d96df9686d0eb2a5c43841e6400fa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Yes, CRWIZ has been used for data collection and its initial use resulted in 145 dialogues. The average time taken for the task was close to the estimate of 10 minutes, 14 dialogues (9.66%) resolved the emergency in the scenario, and these dialogues rated consistently higher in subjective and objective ratings than those which did not resolve the emergency. Qualitative results showed that participants believed that they were interacting with an automated assistant.","[[    0  9904     6  4307   771 17045    34    57   341    13   414  2783
      8    63  2557   304  4596    11 17445 25730  3663     4    20   674
     86   551    13     5  3685    21   593     7     5  3278     9   158
    728     6   501 25730  3663    36   466     4  4280  8871  8179     5
   1923    11     5  5665     6     8   209 25730  3663  5211  6566   723
     11 22262     8  4554  2945    87   167    61   222    45  5728     5
   1923     4 13559 26709   775   969    14  3597  2047    14    51    58
  23140    19    41 11554  3167     4     2]]"
de4e180f49ff187abc519d01eff14ebcd8149cad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Inconsistency in Noun Phrase Structures,  Inconsistency Between Clauses, Inconsistency Between Named Entities and Noun Phrases, Word Level Feature Using TF-IDF","[[    0  1121 10998   661  6761    11   234  7928  4129 34338 31307  4123
      6  1437   603  1790   661  6761 10414 28100  9764     6   603  1790
    661  6761 10414 30436  9860  2192     8   234  7928  4129   338  9354
      6 15690 12183 31967  8630 35690    12  2688   597     2]]"
99760276cfd699e55b827ceeb653b31b043b9ceb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Morphological analysis is the task of creating a morphosyntactic description for a given word,  inflectional realization is framed as a mapping from the pairing of a lemma with a set of morphological tags to the corresponding word form","[[    0   448 31724  9779  1966    16     5  3685     9  2351    10 38675
    366 45122 28201  8194    13    10   576  2136     6  1437  4047 20576
    337 24179    16 18699    25    10 20410    31     5 17496     9    10
   2084 35221    19    10   278     9 38675  9779 19445     7     5 12337
   2136  1026     2]]"
3de9bf4b0b667b3f1181da9f006da1354565bcbd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU, embedding-based metrics (Average, Extrema, Greedy and Coherence), , entropy-based metrics (Ent-{1,2}), distinct metrics (Dist-{1,2,3} and Intra-{1,2,3})","[[    0 30876   791     6 33183 11303    12   805 12758    36 46315     6
  19188   241  1916     6  6879 14425     8   944 40584   238  2156 47382
     12   805 12758    36 30495    12 45152   134     6   176 24303   238
  11693 12758    36 42390    12 45152   134     6   176     6   246 24303
      8  7299   763    12 45152   134     6   176     6   246 49424     2]]"
2df2f6e4efd19023434c84f5b4f29a2f00bfc9fb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"bag of words, tf-idf, bag-of-means","[[    0 14118     9  1617     6 47724    12   808   506     6  3298    12
   1116    12  1794  1253     2]]"
a234bcbf2e41429422adda37d9e926b49ef66150,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"classification accuracy, BLEU scores, model perplexities of the reconstruction","[[    0  4684  5000  8611     6   163  3850   791  4391     6  1421 33708
   2192     9     5 18228     2]]"
a88f8cae1f59cdc4f1f645e496d6d2ac4d9fba1b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d13efa7dee280c7c2f6dc451c4fbbf0240fc2efa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
858c51842fc3c1f3e6d2d7d853c94f6de27afade,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Logistic regression,[[    0 23345  5580 39974     2]]
52f1a91f546b8a25a5d72325c503ec8f9c72de23,0.0,Entailment,[[    2     0 30495  3760  1757     2]],RNNLM BIBREF11,[[    0   500 20057 21672   163  8863 45935  1225     2]]
b2254f9dd0e416ee37b577cef75ffa36cbcb8293,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"5 domains: software, stuff, african wildlife, healthcare, datatypes","[[    0   245 30700    35  2257     6  2682     6  9724 45215  7892     6
   3717     6 13516   415 48951     2]]"
7befb7a8354fca9d2a94e3fd4364625c98067ebb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
4c7b29f6e3cc1e902959a1985146ccc0b15fe521,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Wikipedia,[[    0 47681     2]]
44c7c1fbac80eaea736622913d65fe6453d72828,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"34,432 user conversations",[[    0  3079     6 37296  3018  5475     2]]
f91835f17c0086baec65ebd99d12326ae1ae87d2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Stanford CoreNLP BIBREF11 ,[[    0 36118  1891  9025   487 21992   163  8863 45935  1225  1437     2]]
7994b4001925798dfb381f9aa5c0545cdbd77220,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They randomly sample sentences from Wikipedia that contains an object RC and add them to training data,"[[    0  1213 22422  7728 11305    31 28274    14  6308    41  7626 19682
      8  1606   106     7  1058   414     2]]"
095888f6e10080a958d9cd3f779a339498f3a109,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"AI2 BIBREF2, CC BIBREF19, IL BIBREF4, MAWPS BIBREF20","[[    0 15238   176   163  8863 45935   176     6  9841   163  8863 45935
   1646     6 11935   163  8863 45935   306     6  8981   771  3888   163
   8863 45935   844     2]]"
03502826f4919e251edba1525f84dd42f21b0253,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
32e78ca99ba8b8423d4b21c54cd5309cb92191fc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],14 volunteers,[[   0 1570 4618    2]]
7ee29d657ccb8eb9d5ec64d4afc3ca8b5f3bcc9f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Best performance achieved is 0.72 F1 score,[[    0 19183   819  4824    16   321     4  4956   274   134  1471     2]]
4196d329061f5a9d147e1e77aeed6a6bd9b35d18,0.0,Entailment,[[    2     0 30495  3760  1757     2]],seq2seq translation,[[    0 47762   176 47762 19850     2]]
4c88441f8a1b5fce0ca55a6fced34f97260206ae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],biaffine attention BIBREF14,[[    0 35472  3145   833  1503   163  8863 45935  1570     2]]
b2c8c90041064183159cc825847c142b1309a849,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
948327d7aa9f85943aac59e3f8613765861f97ff,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
b0b1ff2d6515fb40d74a4538614a0db537e020ea,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
311a7fa62721e82265f4e0689b4adc05f6b74215,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific.","[[    0 10926  7767 20511    16  6533    25   164    31    65  2167  4286
      7    10    55   937    65     4  5818  7767 20511    16  6533    25
      5  5483     6   164    31    10   937  4286     7    65    14    16
     55  2167     4     2]]"
c2cbc2637761a2c2cf50f5f8caa248814277430e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF) and gradient boosting (XGB)","[[    0 38873 40419 14969    36   104 20954   238  9359  5580 39974    36
  23345     4 23007   238 34638  5761    36 30455    43     8 43141 11606
     36  1000  4377    43     2]]"
9bd938859a8b063903314a79f09409af8801c973,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WMT14 En-Fr and En-De datasets, IWSLT De-En and En-Vi datasets","[[    0   771 11674  1570  2271    12 29220     8  2271    12 13365 42532
      6    38   771 11160   565   926    12 16040     8  2271    12 43640
  42532     2]]"
7f452eb145d486c15ac4d1107fc914e48ebba60f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the Common Voice website,  iPhone app",[[    0   627  9732 10880   998     6  1437  2733  1553     2]]
e8e6986365f899dead0768ecf7b1eca8a2699f2f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
04b43deab0fd753e3419ed8741c10f652b893f02,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a linear projection and a bijective function with continuous transformation though  affine coupling layer of (Dinh et al.,2016). ","[[    0   102 26956 18144     8    10  4003 21517  2088  5043    19 11152
   7791   600  1437    44   711  3707   833 43437 10490    17    27     9
     36   495   179   298  4400  1076   482  9029   322  1437     2]]"
a93196fb0fb5f8202912971e14552fd7828976db,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Penn Treebank (PTB), end-to-end (E2E) text generation corpus","[[    0 35104 11077  5760    36 10311   387   238   253    12   560    12
   1397    36   717   176   717    43  2788  2706 42168     2]]"
83f14af3ccca4ab9deb4c6d208f624d1e79dc7eb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Answer with content missing: (Table 2) CONCAT ensemble,"[[    0 33683    19  1383  1716    35    36 41836   132    43 27105  2571
  12547     2]]"
94e17980435aaa9fc3b5328f16f3368dc8a736bd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The second method it to learn a common space for the two modalities before concatenation (project), The first method is concatenation of the text and image representation (concat)","[[    0   133   200  5448    24     7  1532    10  1537   980    13     5
     80 11134 29356   137 10146 26511  1258    36 28258   238    20    78
   5448    16 10146 26511  1258     9     5  2788     8  2274  8985    36
   3865  8729    43     2]]"
12391aab31c899bac0ecd7238c111cb73723a6b7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Our transference model extends the original transformer model to multi-encoder based transformer architecture. The transformer architecture BIBREF12 is built solely upon such attention mechanisms completely replacing recurrence and convolutions. ,"[[    0  2522  6214 23861  1421 14269     5  1461 40878  1421     7  3228
     12 14210 15362   716 40878  9437     4    20 40878  9437   163  8863
  45935  1092    16  1490  9382  2115   215  1503 14519  2198  8119  3872
  30904     8 15380 30445     4  1437     2]]"
dbf606cb6fc1d070418cc25e38ae57bbbb7087a0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1) How to introduce unsupervised pre-training into NLG tasks with cross-modal context?, 2) How to design a generic pre-training algorithm to fit a wide range of NLG tasks?, 3) How to reduce the computing resources required for large-scale pre-training?, 4) What aspect of knowledge do the pre-trained models provide for better language generation?","[[    0   134    43  1336     7  6581   542 16101 25376  1198    12 32530
     88 12817   534  8558    19  2116    12 14377   337  5377 33647   132
     43  1336     7  1521    10 14569  1198    12 32530 17194     7  2564
     10  1810  1186     9 12817   534  8558 33647   155    43  1336     7
   1888     5 11730  1915  1552    13   739    12  8056  1198    12 32530
  33647   204    43   653  6659     9  2655   109     5  1198    12 23830
   3092   694    13   357  2777  2706   116     2]]"
dd2046f5481f11b7639a230e8ca92904da75feed,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"maximum of two scores assigned by the two separate models, average score","[[    0 41976     9    80  4391  5530    30     5    80  2559  3092     6
    674  1471     2]]"
17de58c17580350c9da9c2f3612784b432154d11,0.0,Entailment,[[    2     0 30495  3760  1757     2]],multi-class Naive Bayes,[[    0 42274    12  4684  7300  2088  1501   293     2]]
be9cadaebfa0ff1a3c5a5ed56ff3aae76cf5e0a4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All)","[[    0 20365  8611    81   349   881    12 19527  1421    36 48929   238
      8  8611  4756    77  1058    15     5 10146 26511  1258     9    70
  11991    53     5  1002    65    36  3684    43     2]]"
12d77ac09c659d2e04b5e3955a283101c3ad1058,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, Health Care Reform (HCR)","[[    0 36118  1891   111   599 12169  8913 28556    36  4014   104 28556
    238  5316   111   599 12169  8913 28556     6  1309  3800 12287    36
  41022    43     2]]"
9ab43f941c11a4b09a0e4aea61b4a5b4612e7933,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Only MTMSM specifically tried to tackle the multi-span questions. Their approach consisted of two parts: first train a dedicated categorical variable to predict the number of spans to extract and the second was to generalize the single-span head method of extracting a span,"[[    0 19933 14077  6222   448  4010  1381     7  3692     5  3228    12
  36407  1142     4  2667  1548 22061     9    80  1667    35    78  2341
     10  3688 42881 30340 15594     7  7006     5   346     9 23645     7
  14660     8     5   200    21     7   937  2072     5   881    12 36407
    471  5448     9 37213    10  8968     2]]"
f319f2c3f9339b0ce47478f5aa0c32da387a156e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Penn Treebank, Text8",[[    0 35104 11077  5760     6 14159   398     2]]
26c290584c97e22b25035f5458625944db181552,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"10,001 utterances",[[    0   698     6 19089 18672  5332     2]]
2a46db1b91de4b583d4a5302b2784c091f9478cc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Around 388k examples, 194k from tst2013 (in-domain) and 194k from newstest2014 (out-of-domain)","[[    0 34623 41109   330  7721     6 31300   330    31   326   620 10684
     36   179    12 46400    43     8 31300   330    31    92   620   990
  16310    36   995    12  1116    12 46400    43     2]]"
03a911049b6d7df2b6391ed5bc129a3b65133bcd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
1898f999626f9a6da637bd8b4857e5eddf2fc729,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WordDecoding (WDec) model achieves F1 scores that are $3.9\%$ and $4.1\%$ higher than HRL on the NYT29 and NYT24 datasets respectively, PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\%$ and $1.3\%$ higher than HRL on the NYT29 and NYT24 datasets respectively","[[    0 44051 15953 19519    36   771 15953    43  1421 35499   274   134
   4391    14    32    68   246     4   466 37457   207  1629     8    68
    306     4   134 37457   207  1629   723    87   289 17868    15     5
  36319  2890     8 36319  1978 42532  4067     6   221  4328 15721 15953
  19519    36   510 13457  3204    43  1421 35499   274   134  4391    14
     32    68   246     4   288 37457   207  1629     8    68   134     4
    246 37457   207  1629   723    87   289 17868    15     5 36319  2890
      8 36319  1978 42532  4067     2]]"
1baf87437b70cc0375b8b7dc2cfc2830279bc8b5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Randomly selected from a Twitter dump, temporally matched to causal documents","[[    0 45134   352  3919    31    10   599 12371     6 39216  2368  9184
      7 41214  2339     2]]"
e8fcfb1412c3b30da6cbc0766152b6e11e17196c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Perpexity is improved from 34.7 to 28.0.,"[[    0 20823 25981  1571    16  2782    31  2631     4   406     7   971
      4   288     4     2]]"
496e81769a8d9992dae187ed60639ff2eec531f3,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," WSD is predominantly evaluated on English, we are also interested in evaluating our approach on Chinese","[[    0   305  6243    16 15351 15423    15  2370     6    52    32    67
   2509    11 15190    84  1548    15  1111     2]]"
5a33ec23b4341584a8079db459d89a4e23420494,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Public dashboard where competitors can see their results during competition, on part of the test set (public test set).","[[    0 22649 24246   147  6117    64   192    49   775   148  1465     6
     15   233     9     5  1296   278    36 15110  1296   278   322     2]]"
4f9a8b50903deb1850aee09c95d1b6204a7410b4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
3e839783d8a4f2fe50ece4a9b476546f0842b193,0.0,Entailment,[[    2     0 30495  3760  1757     2]],F1 score of 66.66%,[[   0  597  134 1471    9 5138    4 4280  207    2]]
da82b6dad2edd4911db1dc59e4ccd7f66c5fd79c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"VGG-BLSTM, character-level RNNLM","[[    0   846 24592    12  7976  4014   448     6  2048    12  4483   248
  20057 21672     2]]"
6baf5d7739758bdd79326ce8f50731c785029802,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"German, English, Italian, Chinese",[[    0 27709     6  2370     6  3108     6  1111     2]]
78661bdd4d11148e07bdf17141cf088db4ad60c6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],an official F1-score of 0.2905 on the test set,"[[    0   260   781   274   134    12 31673     9   321     4  2890  2546
     15     5  1296   278     2]]"
cd1792929b9fa5dd5b1df0ae06fc6aece4c97424,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
fee5aef7ae521ccd1562764a91edefecec34624d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (Formula 2) Formula 2 is an answer: 
\big \langle\! \log p_\theta({x}|{z}) \big \rangle_{q_\phi({z}|{x})}  -  \beta |D_{KL}\big(q_\phi({z}|{x}) || p({z})\big)-C|","[[    0 33683    19  1383  1716    35    36 30039  5571   132    43 10454
    132    16    41  1948    35  1437 50118 37457  8527 44128   462 14982
  37457   328 44128 12376   181  1215 37457   627  4349 48377  1178 24303
  15483 45152   329 49424 44128  8527 44128   338 14982 49747  1343  1215
  37457 32079 48377   329 24303 15483 45152  1178 49424 24303  1437   111
   1437 44128 44263  1721   495 49747   530   574 49712  8527  1640  1343
   1215 37457 32079 48377   329 24303 15483 45152  1178 49424 45056   181
  48377   329 24303 49394  8527 19281   347 15483     2]]"
802687121a98ba4d7df1f8040ea0dc1cc9565b69,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
fa218b297d9cdcae238cef71096752ce27ca8f4a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Our model achieves a 68.73% EM score and 77.39% F1 score,"[[    0  2522  1421 35499    10  5595     4  5352   207 14850  1471     8
   6791     4  3416   207   274   134  1471     2]]"
65e32f73357bb26a29a58596e1ac314f7e9c6c91,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The lack of background, Non-cursing aggressions and insults, the presence of controversial topic words ,  shallow meaning representation, directly ask the suspected troll if he/she is trolling or not, a blurry line between Frustrate and Neutralize, distinction between the classes Troll and Engage","[[    0   133  1762     9  3618     6  6965    12   438  4668   154 29593
   2485     8 22536     6     5  2621     9  4456  5674  1617  2156  1437
  16762  3099  8985     6  2024  1394     5  3986 29989   114    37    73
   8877    16 33220    50    45     6    10 40427   516   227    44    48
  29220  4193  7954    17    46     8    44    48 14563 37153  2072    17
     46     6 16192   227     5  4050    44    48   565  9671    17    46
      8    44    48 35180  1580    17    46     2]]"
a45edc04277a458911086752af4f17405501230f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
96c09ece36a992762860cde4c110f1653c110d96,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For task 1 best F1 score was 0.9391 on closed and 0.9414 on open test.
For task2 best result had: Ratio 0.3175 , Satisfaction 64.53, Fluency 0, Turns -1 and Guide 2","[[    0  2709  3685   112   275   274   134  1471    21   321     4   466
  35848    15  1367     8   321     4  6405  1570    15   490  1296     4
  50118  2709  3685   176   275   898    56    35 20475   321     4   246
  16925  2156 41802 10845  4430     4  4540     6 25404  6761   321     6
  27271   111   134     8 14670   132     2]]"
f95097cf4a0dc036fd8b80c007cd8d7a157b7816,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"$\textsc {Lead-X}$, $\textsc {PTGen}$, $\textsc {DRM}$, $\textsc {TConvS2S}$,  $\textsc {BottomUp}$, ABS, DRGD, SEQ$^3$, BottleSum, GPT-2","[[    0  1629 37457 29015  3866 25522 32258    12  1000 24303 47110 49959
  29015  3866 25522 10311 15887 24303 47110 49959 29015  3866 25522 10644
    448 24303 47110 49959 29015  3866 25522   565  9157   705   104   176
    104 24303 47110  1437 49959 29015  3866 25522 45959 10926 24303 47110
  27388     6 10994 38863     6  6324  1864  1629 35227   246 47110 37880
  38182     6   272 10311    12   176     2]]"
5e057e115f8976bf9fe70ab5321af72eb4b4c0fc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
56c6ff65c64ca85951fdea54d6b096f28393c128,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
bc1bc92920a757d5ec38007a27d0f49cb2dde0d1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
98785bf06e60fcf0a6fe8921edab6190d0c2cec1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Informative are those that will not be suppressed by regularization performed.,"[[    0  1121  3899  3693    32   167    14    40    45    28 31683    30
   1675  1938  3744     4     2]]"
bd26a6d5d8b68d62e1b6eaf974796f3c34a839c4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"String length, Words co-occurrences, Stems co-occurrences, MeSH similarity","[[    0 34222  5933     6 27341  1029    12 23462   710 46957     6   312
  17752  1029    12 23462   710 46957     6  1464 10237 37015     2]]"
609fbe627309775de415682f48588937d5dd8748,0.0,Entailment,[[    2     0 30495  3760  1757     2]],large-scale document classification datasets introduced by BIBREF14,"[[    0 11802    12  8056  3780 20257 42532  2942    30   163  8863 45935
   1570     2]]"
a81941f933907e4eb848f8aa896c78c1157bff20,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The model does not add new relations to the knowledge graph.,"[[    0   133  1421   473    45  1606    92  3115     7     5  2655 20992
      4     2]]"
90741b227b25c42e0b81a08c279b94598a25119d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation","[[    0 19527    61 16369    50 41030 13546    50  4410   242  1253    10
    621    50    10   333   716  2115    10 34407     9    49  3599   215
     25  3959     6 23848     6    50  1363 14497     2]]"
7aba5e4483293f5847caad144ee0791c77164917,0.0,Entailment,[[    2     0 30495  3760  1757     2]],WikiHop,[[    0 46929 30158     2]]
fed230cef7c130f6040fb04304a33bbc17ca3a36,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"for each multiple-choice question $(q,A) \in Q_\mathit {tr}$ and each choice $a \in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S, take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \in A$ and over all questions in $Q_\mathit {tr}$","[[    0  1990   349  1533    12 24335   864 44612  1343     6   250    43
  44128   179  1209  1215 37457 40051   405 25522  4328 24303  1629     8
    349  2031    68   102 44128   179    83  1629  2156    52   304    70
    786    12  8287 14742 22121    11    68  1343  1629     8    68   102
   1629    25    41 45609 39954 25860   136   208     6   185     5   299
   1878  2323     6   422  2117 18090   748   306     6     8 13884     5
   5203 13145 12349    81    70    68   102 44128   179    83  1629     8
     81    70  1142    11    68  1864  1215 37457 40051   405 25522  4328
  24303  1629     2]]"
24d06808fa3b903140659ee5a471fdfa86279980,0.0,Entailment,[[    2     0 30495  3760  1757     2]],standard Transformer Base model,[[    0 21993  5428 22098 11056  1421     2]]
c0035fb1c2b3de15146a7ce186ccd2e366fb4da2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"In terms of Subj the Average MGNC-CNN is better than the average score of baselines by 0.5.  Similarly, Scores of SST-1, SST-2, and TREC where MGNC-CNN has similar improvements. 
In case of Irony the difference is about 2.0. 
","[[    0  1121  1110     9  4052   267     5  8317 23054  6905    12 16256
     16   357    87     5   674  1471     9 11909 38630    30   321     4
    245     4  1437 12512     6 26341     9   208  4014    12   134     6
    208  4014    12   176     6     8   255 40698   147 23054  6905    12
  16256    34  1122  5139     4  1437 50118  1121   403     9  9940   219
      5  2249    16    59   132     4   288     4  1437 50118     2]]"
90f80a94fabaab72833256572db1d449c2779beb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
0c1663a7f7750b399f40ef7b4bf19d5c598890ff,0.0,Entailment,[[    2     0 30495  3760  1757     2]],we replace user embeddings with a low-dimensional image representation,"[[    0  1694  3190  3018 33183   417  1033    19    10   614    12 23944
   2274  8985     2]]"
8d989490c5392492ad66e6a5047b7d74cc719f30,0.0,Entailment,[[    2     0 30495  3760  1757     2]],choosing the answer from the network that had the highest probability and choosing no answer if any of the networks predicted no answer,"[[    0 11156 10174     5  1948    31     5  1546    14    56     5  1609
  18102     8  8348   117  1948   114   143     9     5  4836  6126   117
   1948     2]]"
4bae74eb707ed71d5f438ddb3d9c2192ac490f66,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a9d530d68fb45b52d9bad9da2cd139db5a4b2f7c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],KneserNey smoothing,[[    0   530  4977   254  2383   487  2981 35444  8223     2]]
84737d871bde8058d8033e496179f7daec31c2d3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
67a28fe78f07c1383176b89e78630ee191cf15db,0.0,Entailment,[[    2     0 30495  3760  1757     2]],on the unlabeled data of each task,[[    0   261     5 35237 14286   196   414     9   349  3685     2]]
0a521541b9e2b5c6d64fb08eb318778eba8ac9f7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SNLI, MultiNLI and SICK",[[    0 12436 27049     6 19268   487 27049     8   208 11839     2]]
973f6284664675654cc9881745880a0e88f3280e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"6 indicators:
- lexical richness
- pronunciation and fluency
- syntactical correctness
- fulfillment of delivery
- coherence and cohesion
- communicative, descriptive, narrative skills","[[    0   401 13038    35 50118    12 36912  3569 38857 50118    12 44919
      8  6626  6761 50118    12 45774  7257  3569 34726 50118    12 27125
      9  2996 50118    12  1029 40584     8 31657 50118    12 16759 41983
      6 42690     6  7122  2417     2]]"
a3b1520e3da29d64af2b6e22ff15d330026d0b36,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"facial presence, Facial Expression, General Image Features,  textual content, analytical thinking, clout, authenticity, emotional tone, Sixltr,  informal language markers, 1st person singular pronouns","[[    0   506 27015  2621     6 19127  2617 43819     6  1292  2960 21309
      6  1437 46478  1383     6 23554  2053     6 28353     6 21341     6
   3722  6328     6  5310   462  4328     6  1437 14110  2777 22462     6
    112   620   621 23429 43083     2]]"
334972ba967444f98865dea4c2bc0eb9416f2ff7,0.0,Entailment,[[    2     0 30495  3760  1757     2]], from 469 posts to 17 million,[[   0   31  204 4563 4570    7  601  153    2]]
b4f5bf3b7b37e2f22d13b724ca8fe7d0888e04a2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],speaker systems in the real world,[[    0 18462  4218  1743    11     5   588   232     2]]
9f5507a8c835c4671020d7d310fff2930d44e75a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Danish/Swedish (da/sv), Russian/Bulgarian (ru/bg), Finnish/Hungarian (fi/hu), Spanish/Portuguese (es/pt)","[[    0   495 28627    73 15417   196  1173    36  6106    73 36245   238
   1083    73 40028   571  9063    36  2070    73 35725   238 21533    73
  41416  9063    36  9169    73  6455   238  3453    73 22117  3252 30485
     36   293    73  3320    43     2]]"
e9ccc74b1f1b172224cf9f01e66b1fa9e34d2593,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"besides claim, label and claim url, it also includes a claim ID, reason, category, speaker, checker, tags, claim entities, article title, publish data and claim date","[[    0   428 46463  2026     6  6929     8  2026 46471     6    24    67
   1171    10  2026  4576     6  1219     6  4120     6  5385     6  1649
    254     6 19445     6  2026  8866     6  1566  1270     6 10732   414
      8  2026  1248     2]]"
9c423e3b44e3acc2d4b0606688d4ac9d6285ed0f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
957bda6b421ef7d2839c3cec083404ac77721f14,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors,  Spelling errors, Repeated characters, Capitalisation, Length,  Emoticon (Presence/Count ) 
 and Sentiment Ratio","[[    0   574  3134   542  1023 27809    36 28917  4086    73 23329   238
  29206 20475     6   849   487  7486 46718 22150  2485     6   849 41615
  21117  8791   994     6  1437  8330  7633  9126     6 32021  1070  3768
      6  1867  3258     6 41852     6  1437  3676  1242 17505    36 28917
   4086    73 23329  4839  1437 50118     8 12169  8913 20475     2]]"
9a596bd3a1b504601d49c2bec92d1592d7635042,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Answer with content missing: (Table II) Proposed model has F1 score of  0.7220.,"[[    0 33683    19  1383  1716    35    36 41836  3082    43 13695  7878
   1421    34   274   134  1471     9  1437   321     4  4956   844     4
      2]]"
2ba0c7576eb5b84463a59ff190d4793b67f40ccc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"attention probes, using visualizations of the activations created by different pieces of text","[[    0  2611 19774 25067     6   634  7133 18391     9     5 30264  1635
   1412    30   430  3745     9  2788     2]]"
2f4acd34eb2d09db9b5ad9b1eb82cb4a88c13f5b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the public financial news dataset released by BIBREF4,"[[    0   627   285   613   340 41616   703    30   163  8863 45935   306
      2]]"
58d50567df71fa6c3792a0964160af390556757d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
83f22814aaed9b5f882168e22a3eac8f5fda3882,0.0,Entailment,[[    2     0 30495  3760  1757     2]],rank-correlation BIBREF25,[[    0 40081    12  7215 47114   163  8863 45935  1244     2]]
bbaf7cbae88c085faa6bbe3319e4943362fe1ad4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
a7cb4f8e29fd2f3d1787df64cd981a6318b65896,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
e9b1e8e575809f7b80b1125305cfa76ae4f5bdfb,0.0,Entailment,[[    2     0 30495  3760  1757     2]], convolutional neural network (CNN) BIBREF29,"[[    0 15380 23794   337 26739  1546    36 16256    43   163  8863 45935
   2890     2]]"
da845a2a930fd6a3267950bec5928205b6c6e8e8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],how long it takes the system to lemmatize a set number of words,"[[   0 9178  251   24 1239    5  467    7 2084 5471  415 2072   10  278
   346    9 1617    2]]"
89e1e0dc5d15a05f8740f471e1cb3ddd296b8942,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the punchline of the joke ,[[    0   627 10064  1902     9     5  8018  1437     2]]
6d0f2cce46bc962c6527f7b4a77721799f2455c6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
5b029ad0d20b516ec11967baaf7d2006e8d7199f,0.0,Entailment,[[    2     0 30495  3760  1757     2]], two labels ,[[    0    80 14105  1437     2]]
be074c880263f56e0d4a8f42d9a95d2d77ac2280,0.0,Entailment,[[    2     0 30495  3760  1757     2]],landing pages of URLs,[[    0  1245   154  6052     9 44163     2]]
16f33de90b76975a99572e0684632d5aedbd957c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a reference corpus of 21,093 tokens and their correct lemmas","[[    0   102  5135 42168     9   733     6  3546   246 22121     8    49
   4577  2084  5471   281     2]]"
1d739bb8e5d887fdfd1f4b6e39c57695c042fa25,0.0,Entailment,[[    2     0 30495  3760  1757     2]],three parallel LSTM BIBREF21 layers,[[    0  9983 12980   226  4014   448   163  8863 45935  2146 13171     2]]
ca7e71131219252d1fab69865804b8f89a2c0a8f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods.,"[[    0  3084 13677 41882    32  1286     8   117 16045  6676    16   156
    227  2123  4391    50  6448     4     2]]"
0fd678d24c86122b9ab27b73ef20216bbd9847d1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Accuracy on each dataset and the average accuracy on all datasets.,"[[    0 36984 45386    15   349 41616     8     5   674  8611    15    70
  42532     4     2]]"
6c4d121d40ce6318ecdc141395cdd2982ba46cff,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BIBREF7, BIBREF26 ",[[    0  5383   387 45935   406     6   163  8863 45935  2481  1437     2]]
21a96b328b43a568f9ba74cbc6d4689dbc4a3d7b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
4debd7926941f1a02266b1a7be2df8ba6e79311a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
e42fbf6c183abf1c6c2321957359c7683122b48e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BiLSTM-XR-Dev Estimation accuracy is 83.31 for SemEval-15 and 87.68 for SemEval-16.
BiLSTM-XR accuracy is 83.31 for SemEval-15 and 88.12 for SemEval-16.
","[[    0 37426   574  4014   448    12  1000   500    12 30504  5441 39431
   8611    16  8101     4  2983    13 11202   717  6486    12   996     8
   8176     4  4671    13 11202   717  6486    12  1549     4 50118 37426
    574  4014   448    12  1000   500  8611    16  8101     4  2983    13
  11202   717  6486    12   996     8  7953     4  1092    13 11202   717
   6486    12  1549     4 50118     2]]"
340501f23ddc0abe344a239193abbaaab938cc3a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations","[[    0  3248 45068  1070  2339    19   195 31173   349  4173   112     7
    195     6   147   112    16   513  4249     8   195    16   144  4249
     13    10   746     9 13411 45068  1070 31173     2]]"
330f2cdeab689670b68583fc4125f5c0b26615a8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"he proposed model outperforms all the baselines, being the svi version the one that performs best., the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm.","[[    0   700  1850  1421  9980 33334    70     5 11909 38630     6   145
      5   579  6873  1732     5    65    14 14023   275   482     5   579
   6873  1732 25111  5641   203  3845     7   723  3266     9     5  7425
  14612 11801    77  1118     7     5 14398  1732     6    61  6771     5
   5838     9     5   579  6873 17194     4     2]]"
d2e409031f4512375dd5cecec639c7373025f277,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"(Layer Normalized Skip-Thoughts, ST-LN) BIBREF31, Cap2All, Cap2Cap, Cap2Img","[[    0  1640 48159 26411  1538  6783    12 26223  1872     6  4062    12
    574   487    43   163  8863 45935  2983     6  5133   176  3684     6
   5133   176 15791     6  5133   176 20470   571     2]]"
670c464a5dba78e0be7ec168fe36db604e172ea7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"GloVe, concatenation of average embeddings calculated separately for the interview and for the medical examination","[[    0   534  4082 30660     6 10146 26511  1258     9   674 33183   417
   1033  9658 12712    13     5  1194     8    13     5  1131  9027     2]]"
b13902af1bcf0e199a3ea42bbc8fcd8e696a381a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],parallel data available for the WMT 2016,[[    0  5489 44682   414   577    13     5   305 11674   336     2]]
e97545f4a5e7bc96515e60f2f9b23d8023d1eed9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For each source article, Retrieve aims to return a few candidate templates from the training corpus. Then, the Fast Rerank module quickly identifies a best template from the candidates.","[[    0  2709   349  1300  1566     6  9944 20080  5026     7   671    10
    367  1984 38577    31     5  1058 42168     4  1892     6     5  9612
    248   254  3153 20686  1335 18785    10   275 27663    31     5  2261
      4     2]]"
98eb245c727c0bd050d7686d133fa7cd9d25a0fb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BLEU scores,[[    0 30876   791  4391     2]]
e06e1b103483e1e58201075c03e610202968c877,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
146fe3e97d8080f04222ed20903dd0d5fd2f551c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the food dataset has 3,806 images for training ","[[    0   627   689 41616    34   155     6 38717  3156    13  1058  1437
      2]]"
4c822bbb06141433d04bbc472f08c48bc8378865,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They identify documents that contain the unigrams 'caused', 'causing', or 'causes'","[[    0  1213  3058  2339    14  5585     5   542  1023 27809   128  3245
   6199  3934   128  3245 10928  3934    50   128  3245  9764   108     2]]"
37753fbffc06ce7de6ada80c89f1bf5f190bbd88,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Preceding and following sentence of each metaphor and paraphrase are added as document context,"[[    0 22763  7618   154     8   511  3645     9   349 26600     8 40127
  34338    32   355    25  3780  5377     2]]"
90dd5c0f5084a045fd6346469bc853c33622908f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU-2, average accuracies over 3 test trials on different randomly sampled test sets","[[    0 30876   791    12   176     6   674 49060 15668    81   155  1296
   7341    15   430 22422 36551  1296  3880     2]]"
2d307b43746be9cedf897adac06d524419b0720b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Travel dataset contains 4100 raw samples, 11291 clauses, Hotel dataset contains 3825 raw samples, 11264 clauses, and the Mobile dataset contains 3483 raw samples and 8118 clauses","[[    0 39258 41616  6308   204  1866  6087  7931     6 12730  6468 30756
      6  5085 41616  6308  2843  1244  6087  7931     6 12730  4027 30756
      6     8     5  6698 41616  6308   155 37932  6087  7931     8   290
  21369 30756     2]]"
e79a5e435fcf5587535f06c9215d19a66caadaff,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
63337fd803f6fdd060ebd0f53f9de79d451810cd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e6c163f80a11bd057bbd0b6e1451ac82edddc78d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
6e51af9088c390829703c6fa966e98c3a53114c1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Modern Standard Arabic (MSA), MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine","[[    0 39631  5787 19645    36   448  3603   238   256  3603    25   157
     25 37508    29    23  1337  4176     9 17227 42664   215    25 10377
      6  4602     6     8 31500   833     2]]"
f6496b8d09911cdf3a9b72aec0b0be6232a6dba1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
c165ea43256d7ee1b1fb6f5c0c8af5f7b585e60d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"most of the models have similar performance on BPRA: DSTC2 (+0.0015), Maluuba (+0.0729)
GDP achieves the best performance in APRA: DSTC2 (+0.2893), Maluuba (+0.2896)
GDP significantly outperforms the baselines on BLEU: DSTC2 (+0.0791), Maluuba (+0.0492)","[[    0  7877     9     5  3092    33  1122   819    15 12184  4396    35
    211  4014   347   176 16998   288     4   612   996   238  2529   257
  10452 16998   288     4  3570  2890    43 50118   534  5174 35499     5
    275   819    11  1480  4396    35   211  4014   347   176 16998   288
      4  2517  6478   238  2529   257 10452 16998   288     4  2517  5607
     43 50118   534  5174  3625  9980 33334     5 11909 38630    15   163
   3850   791    35   211  4014   347   176 16998   288     4  3570  6468
    238  2529   257 10452 16998   288     4   288 38852    43     2]]"
be6971827707afcd13af3085d0a775a0bd61c5dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d469c7de5c9e6dd8a901190e95688c446f12118f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
53c8416f2983e07a7fa33bcb4c4281bbf49c8164,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Outputs from models that are better in the sense of cross-entropy or perplexity are harder to distinguish from authentic text.,"[[    0 48293    29    31  3092    14    32   357    11     5  1472     9
   2116    12  1342 47145    50 33708  1571    32  4851     7 22929    31
  12757  2788     4     2]]"
b984612ceac5b4cf5efd841af2afddd244ee497a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],approximately equal parameterization,[[    0 39073  3871 43797  1938     2]]
39cfb8473c8be4e5d8ecc3227b800a10477c5f80,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the extent to which the text obtained from the two platforms of Yahoo! Answers and Twitter reflect the true attributes of neighbourhoods,"[[    0   627  5239     7    61     5  2788  4756    31     5    80  4818
      9 10354   328 41034     8   599  4227     5  1528 16763     9 26075
      2]]"
4670e1be9d6a260140d055c7685bce365781d82b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
dfbab3cd991f86d998223726617d61113caa6193,0.0,Entailment,[[    2     0 30495  3760  1757     2]],reviews under distinct product categories are considered specific domain knowledge,"[[    0 22459    29   223 11693  1152  6363    32  1687  2167 11170  2655
      2]]"
45b28a6ce2b0f1a8b703a3529fd1501f465f3fdf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"special dedicated discriminator is added to the model to control that the latent representation does not contain stylistic information, shifted autoencoder or SAE, combination of both approaches","[[    0 19423  3688 40846 19936    16   355     7     5  1421     7   797
     14     5 42715  8985   473    45  5585 15240  5580   335     6  9679
   7241 18057   438 15362    50   208 16329     6  4069     9   258  8369
      2]]"
ff28d34d1aaa57e7ad553dba09fc924dc21dd728,0.0,Entailment,[[    2     0 30495  3760  1757     2]],High correlation results range from 0.472 to 0.936,"[[    0 18522 22792   775  1186    31   321     4 37401     7   321     4
    466  3367     2]]"
5e032de729ce9fc727b547e3064be04d30009324,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"one needs to develop mechanisms to recognize valid argumentative structures, we ignore trustworthiness and credibility issues","[[    0  1264   782     7  2179 14519     7  5281  8218  4795  3693  6609
      6    52  8861  2416 26041     8 10796   743     2]]"
58c1b162a4491d4a5ae0ff86cc8bd64e98739620,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
bf7cb53f4105f2e6a413d1adef5349ff1e673500,0.0,Entailment,[[    2     0 30495  3760  1757     2]],WikiTableQuestions,[[    0 46929 41836 46865     2]]
5efa19058f815494b72c44d746c157e9403f726e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],micro-averaged F1 score,[[    0 35228    12  9903  4628   274   134  1471     2]]
cfd67b9eeb10e5ad028097d192475d21d0b6845b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
fd6c194632230e392088fc1f574c8626c6a2fa96,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"many news articles begin with reporter names, media agencies, dates or other contents irrelevant to the content, to ensure that the summary is concise and the article contains enough salient information, we only keep articles with 10-150 words in the top three sentences and 150-1200 words in the rest, and that contain at least 6 sentences in total, we try to remove articles whose top three sentences may not form a relevant summary","[[    0 19827   340  7201  1642    19  4439  2523     6   433  2244     6
   5461    50    97 13654 21821     7     5  1383     6     7  1306    14
      5  4819    16  6561     8     5  1566  6308   615 41159   335     6
     52   129   489  7201    19   158    12  6115  1617    11     5   299
    130 11305     8  3982    12 36643  1617    11     5  1079     6     8
     14  5585    23   513   231 11305    11   746     6    52   860     7
   3438  7201  1060   299   130 11305   189    45  1026    10  4249  4819
      2]]"
70f84c73172211186de1a27b98f5f5ae25a94e55,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Stanford Sentiment Treebank (SST) BIBREF15 and AG News BIBREF16,"[[    0 36118  1891 12169  8913 11077  5760    36   104  4014    43   163
   8863 45935   996     8  5680   491   163  8863 45935  1549     2]]"
bdc93ac1b8643617c966e91d09c01766f7503872,0.0,Entailment,[[    2     0 30495  3760  1757     2]],1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation,"[[    0   134 12096 16274 25730  3663    13  1058     8 15452 35237 14286
    196 25730  3663    13 10437     2]]"
24c0f3d6170623385283dfda7f2b6ca2c7169238,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Twitter API,[[    0 22838 21013     2]]
88d9d32fb7a22943e1f4868263246731a1726e6e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Exemplars aim to provide appropriate context., joint image-caption embedding for the supporting exemplar are closer to that of the target image-caption","[[    0  9089 33037  2726  4374     7   694  3901  5377   482  2660  2274
     12  3245 24802 33183 11303    13     5  3117 22249   271    32  2789
      7    14     9     5  1002  2274    12  3245 24802     2]]"
f9751e0ca03f49663a5fc82b33527bc8be1ed0aa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"simplified set of input data, in a variety of different formats that occur frequently in a healthcare setting","[[    0 13092  2911  3786   278     9  8135   414     6    11    10  3143
      9   430 19052    14  5948  5705    11    10  3717  2749     2]]"
90ad8d7ee27192b89ffcfa4a68302f370e6333a8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
c034f38a570d40360c3551a6469486044585c63c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Perplexity of proposed MEED model is 19.795 vs 19.913 of next best result on test set.,"[[    0 20823 26028  1571     9  1850 12341  1691  1421    16   753     4
  36346  1954   753     4   466  1558     9   220   275   898    15  1296
    278     4     2]]"
de12e059088e4800d7d89e4214a3997994dbc0d9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The system is compared to baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM","[[    0   133   467    16  1118     7 18043  3092    35   226  4014   448
      6 28483    12  4186  2444   487     8   272  4179   523 11077    12
    574  4014   448     2]]"
4d8b3928f89d73895a7655850a227fbac08cdae9,0.0,Entailment,[[    2     0 30495  3760  1757     2]], largest improvement ($22-26\%$ E.R) when text-based unsupervised models are combined with image representations,"[[    0  1154  3855  1358  2036    12  2481 37457   207  1629   381     4
    500    43    77  2788    12   805   542 16101 25376  3092    32  2771
     19  2274 30464     2]]"
85aa125b3a15bbb6f99f91656ca2763e8fbdb0ff,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Precision@1, Mean Average Precision, Mean Reciprocal Rank","[[    0 22763 37938  1039   134     6 30750  8317 29484     6 30750  7382
   1588 46966 17816     2]]"
ed6462da17c553bda112ef35917fefe6942fce3c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Feature selection, Random forest, XGBoost, Hierarchical Model","[[    0 47702  4230     6 34638  6693     6  1577  4377   139  2603     6
  37859 13161  3569  7192     2]]"
5dc1aca619323ea0d4717d1f825606b2b7c21f01,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Northeast U.S, South U.S., West U.S. and Midwest U.S.","[[    0   487  2723 44085   121     4   104     6   391   121     4   104
    482   580   121     4   104     4     8 11741   121     4   104     4
      2]]"
090fd9ce9a21438cdec1ea51ed216941d52eb3b6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Hierarchical Disentangled Self-Attention,"[[    0   725   906 13161  3569  6310  1342 19605 12156    12 28062 19774
      2]]"
8c8a32592184c88f61fac1eef12c7d233dbec9dc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Both supervised and unsupervised, depending on the task that needs to be solved.","[[    0 16991 20589     8   542 16101 25376     6  6122    15     5  3685
     14   782     7    28 15960     4     2]]"
ffc5ad48b69a71e92295a66a9a0ff39548ab3cf1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"GloVe embeddings trained by BIBREF10 on Wikipedia and Gigaword 5 (vocab: 400K, dim: 300), w2v-gn, Word2vec BIBREF5 trained on the Google News dataset (vocab: 3M, dim: 300), DeepWalk , node2vec","[[    0   534  4082 30660 33183   417  1033  5389    30   163  8863 45935
    698    15 28274     8 14448  1584  3109   195    36 31375   873    35
   3675   530     6 14548    35  2993   238   885   176   705    12 16993
      6 15690   176 25369   163  8863 45935   245  5389    15     5  1204
    491 41616    36 31375   873    35   155   448     6 14548    35  2993
    238  8248 33150  2156 37908   176 25369     2]]"
ff8557d93704120b65d9b597a4fab40b49d24b6d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
2ebd7a59baad1f935fe83f90526557bfa9df4047,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
323e100a6c92d3fe503f7a93b96d821408f92109,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BIBREF13 , BIBREF18",[[    0  5383   387 45935  1558  2156   163  8863 45935  1366     2]]
93b299acfb6fad104b9ebf4d0585d42de4047051,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ABSA SemEval 2014-2016 datasets
Yelp Academic Dataset
Wikipedia dumps","[[    0  4546  3603 11202   717  6486   777    12  9029 42532 50118   975
    523   642 24242 16673   281   594 50118 47681 37017     2]]"
ad7b13579823cbc7825421c84d16f23ed863f6ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"VATEX, WMT 2014 English-to-German, and VQA-v2 datasets","[[    0   846  8625  1000     6   305 11674   777  2370    12   560    12
  27709     6     8   468  1864   250    12   705   176 42532     2]]"
1383ddd4619cf81227c72f3d9f30c10c47a0cdad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Our baseline system (Baseline_1850K) is taken from BIBREF13 . ,"[[    0  2522 18043   467    36 40258  7012  1215  1366  1096   530    43
     16   551    31   163  8863 45935  1558   479  1437     2]]"
80de3baf97a55ea33e0fe0cafa6f6221ba347d0a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
4ecb6674bcb4162bf71aea8d8b82759255875df3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BIBREF5,[[    0  5383   387 45935   245     2]]
86be8241737dd8f7b656a3af2cd17c8d54bf1553,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
18ad60f97f53af64cb9db2123c0d8846c57bfa4a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"word embeddings to generate a new feature, i.e., summarizing a local context","[[    0 14742 33183   417  1033     7  5368    10    92  1905     6   939
      4   242   482 39186  2787    10   400  5377     2]]"
6852217163ea678f2009d4726cb6bd03cf6a8f78,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WN18RR BIBREF26, FB15k-237 BIBREF18, YAGO3-10 BIBREF27","[[    0 29722  1366 25733   163  8863 45935  2481     6 13042   996   330
     12 30069   163  8863 45935  1366     6   854  3450   673   246    12
    698   163  8863 45935  2518     2]]"
fb2593de1f5cc632724e39d92e4dd82477f06ea1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],performances of a purely content-based model naturally stays stable,"[[    0  1741  3899  5332     9    10 15430  1383    12   805  1421  8366
  10117  4375     2]]"
c309e87c9e08cf847f31e554577d6366faec1ea0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
8a7bd9579d2783bfa81e055a7a6ebc3935da9d20,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WAS, LipCH-Net-seq, CSSMCM-w/o video","[[    0   771  2336     6 15914  3764    12 15721    12 47762     6 39887
   6018   448    12   605    73   139   569     2]]"
923b12c0a50b0ee22237929559fad0903a098b7b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Plackett-Luce Model for SMT Reranking,"[[    0 16213  2990  2645    12   574 15431  7192    13  7346   565   248
    254 20327     2]]"
fc6cfac99636adda28654e1e19931c7394d76c7c,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," We devise a simple clustering algorithm to approximate this process. First, we initialize with random cluster assignments and define cluster strength to be the relative difference between intra-group Euclidean distance and inter-group Euclidean distance. Then, we iteratively propose random exchanges of memberships, only accepting these proposals when the cluster strength increases, until convergence. ","[[    0   166 34901    10  2007 46644  2961 17194     7 32161    42   609
      4  1234     6    52 49161    19  9624 18016 21350     8  9914    44
     48  3998 10504  2707    17    46     7    28     5  5407  2249   227
     44    48  2544   763    12 13839    17    46 36873  1949   260  4472
      8    44    48  8007    12 13839    17    46 36873  1949   260  4472
      4  1892     6    52 41393 10481 15393  9624  6927     9   453  7903
      6   129  8394   209  5327    77     5 18016  2707  3488     6   454
  33345     4  1437     2]]"
894c086a2cbfe64aa094c1edabbb1932a3d7c38a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For sentiment analysis UWB, INF-UFRGS-OPINION-MINING, LitisMind, pkudblab and SVM + n-grams + sentiment and for emotion analysis MaxEnt, SVM, LSTM, BiLSTM and CNN","[[    0  2709  5702  1966 19945   387     6 25657    12   791  5499 10729
     12  5733  2444  7744    12 24765  1862     6   226 10100 34875     6
    181   330  1906  3662   873     8   208 20954  2055   295    12 28526
     29  2055  5702     8    13 11926  1966  4471 30495     6   208 20954
      6   226  4014   448     6  6479   574  4014   448     8  3480     2]]"
1835f65694698a9153857e33cd9b86a96772fff5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
954c4756e293fd5c26dc50dc74f505cc94b3f8cc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Similar to standard convolutional networks but instead they skip some input values effectively operating on a broader scale.,"[[    0 46444     7  2526 15380 23794   337  4836    53  1386    51 14514
    103  8135  3266  4296  1633    15    10  5153  3189     4     2]]"
6ff240d985bbe96b9d5042c9b372b4e8f498f264,0.0,Entailment,[[    2     0 30495  3760  1757     2]],$0.3$ million records,[[   0 1629  288    4  246 1629  153 2189    2]]
33d864153822bd378a98a732ace720e2c06a6bc6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],In closed setting 84.22 F1 and in open 87.35 F1.,"[[   0 1121 1367 2749 7994    4 2036  274  134    8   11  490 8176    4
  2022  274  134    4    2]]"
9adcc8c4a10fa0d58f235b740d8d495ee622d596,0.0,Entailment,[[    2     0 30495  3760  1757     2]],2 for the ADE dataset and 3 for the CoNLL04 dataset,"[[    0   176    13     5  4516   717 41616     8   155    13     5   944
    487  6006  3387 41616     2]]"
8c073b7ea8cb5cc54d7fecb8f4bf88c1fb621b19,0.0,Entailment,[[    2     0 30495  3760  1757     2]],cosine similarity,[[    0 16254   833 37015     2]]
1d9b953a324fe0cfbe8e59dcff7a44a2f93c568d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d4a6f5034345036dbc2d4e634a8504f79d42ca69,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the WMT'14 English-French (En-Fr) and English-German (En-De) datasets.,"[[    0   627   305 11674   108  1570  2370    12 28586    36 16040    12
  29220    43     8  2370    12 27709    36 16040    12 13365    43 42532
      4     2]]"
01866fe392d9196dda1d0b472290edbd48a99f66,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones","[[    0   241    12 21714     5 14514    12 28526  1421     8  2935     5
  21554  5456 27405    15    92 24780  5814     6    77 24480  5405     7
     92  2841  4203   354     6     5  5456    64  3104     5    92  2841
   4203   354     7     5   793  1980     2]]"
a2be2bd84e5ae85de2ab9968147b3d49c84dfb7f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"genre, entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement","[[    0 44205     6  4000     6  1194     6  6970     6   310     6  1569
      6   748 12376     6   697  2308     6  1901     6  4149     6  3872
  12257     8  6859     2]]"
e8647f9dc0986048694c34ab9ce763b3167c3deb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
37103369e5792ece49a71666489016c4cea94cda,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
bc9c31b3ce8126d1d148b1025c66f270581fde10,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs ","[[    0   229  1344  4128     8  3076  2655 36386     6   854  3450   673
    246    12   698     8   305   487  1366   530 28393  2655 36386  1437
      2]]"
f748cb05becc60e7d47d34f4c5f94189bc184d33,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Bulgarian, Czech, French, German, Korean, Polish, Portuguese, Russian, Thai, Vietnamese, South African English, These features are typically obtained by training a deep neural network jointly on several languages for which labelled data is available., The final shared layer often has a lower dimensionality than the input layer, and is therefore referred to as a `bottleneck'.","[[    0 40028   571  9063     6  9096     6  1515     6  1859     6  2238
      6 11145     6 13053     6  1083     6  9130     6 16859     6   391
   1704  2370     6  1216  1575    32  3700  4756    30  1058    10  1844
  26739  1546 13521    15   484 11991    13    61 22434   414    16   577
    482    20   507  1373 10490   747    34    10   795 21026  6948    87
      5  8135 10490     6     8    16  3891  4997     7    25    10 22209
  33840 42041  2652     2]]"
6583e8bfa7bcc3a792a90b30abb316e6d423f49b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Direct source$\rightarrow $target: A standard NMT model trained on given source$\rightarrow $target, Multilingual: A single, shared NMT model for multiple translation directions, Many-to-many: Trained for all possible directions among source, target, and pivot languages, Many-to-one: Trained for only the directions to target language","[[    0 33038  1300  1629 37457  4070 17214    68 23976    35    83  2526
    234 11674  1421  5389    15   576  1300  1629 37457  4070 17214    68
  23976     6 14910 41586    35    83   881     6  1373   234 11674  1421
     13  1533 19850  9969     6  1876    12   560    12 19827    35  2393
   7153    13    70   678  9969   566  1300     6  1002     6     8 27475
  11991     6  1876    12   560    12  1264    35  2393  7153    13   129
      5  9969     7  1002  2777     2]]"
e2427f182d7cda24eb7197f7998a02bc80550f15,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By using Apache Spark which stores all executions in a lineage graph and recovers to the previous steady state from any fault,"[[    0  2765   634 27563 20255    61  2326    70 23729    11    10 37512
  20992     8 29283     7     5   986  5204   194    31   143  7684     2]]"
5be94c7c54593144ba2ac79729d7545f27c79d37,0.0,Entailment,[[    2     0 30495  3760  1757     2]],not researched as much as English,[[    0  3654 27285    25   203    25  2370     2]]
657edbf39c500b2446edb9cca18de2912c628b7d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Perplexity score 142.84 on dev and 138.91 on test,"[[    0 20823 26028  1571  1471 23703     4  6232    15  8709     8 21436
      4  6468    15  1296     2]]"
282aa4e160abfa7569de7d99b8d45cabee486ba4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the weighted sum of the new opinion representations, according to their associations with the current aspect representation","[[    0   627 19099  6797     9     5    92  2979 30464     6   309     7
     49 14697    19     5   595  6659  8985     2]]"
891cab2e41d6ba962778bda297592c916b432226,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Python,[[    0 48659     2]]
423bb905e404e88a168e7e807950e24ca166306c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"GraphParser without paraphrases, monolingual machine translation based model for paraphrase generation","[[    0 45288 49707   396 40127   338  9354     6  6154 31992  5564  3563
  19850   716  1421    13 40127 34338  2706     2]]"
5d9b088bb066750b60debfb0b9439049b5a5c0ce,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Remove numbers and interjections,[[    0 47583  1530     8  3222 21517  2485     2]]
7f5059b4b5e84b7705835887f02a51d4d016316a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
096f5c59f43f49cab1ef37126341c78f272c0e26,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"51,104",[[    0  4708     6 17573     2]]
f268b70b08bd0436de5310e390ca5f38f7636612,0.0,Entailment,[[    2     0 30495  3760  1757     2]],GIZA++ BIBREF3 or fast_align BIBREF4 ,"[[    0   534 17045   250 42964   163  8863 45935   246    50  1769  1215
  44022   163  8863 45935   306  1437     2]]"
d3839c7acee4f9c8db0a4a475214a8dcbd0bc26f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"76.58% of the shared links are related to hip-hop music, gangster rap, and the culture that surrounds this music genre","[[    0  5067     4  4432   207     9     5  1373  5678    32  1330     7
   6605    12  9547   930     6  5188  3121 11619     6     8     5  2040
     14 29010    42   930 11581     2]]"
a4d8fdcaa8adf99bdd1d7224f1a85c610659a9d3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Performance was comparable, with the proposed method quite close and sometimes exceeding performance of baseline method.","[[    0 44002    21 10451     6    19     5  1850  5448  1341   593     8
   2128 17976   819     9 18043  5448     4     2]]"
c33d0bc5484c38de0119c8738ffa985d1bd64424,0.0,Entailment,[[    2     0 30495  3760  1757     2]],monolingual,[[    0  5806 31992  5564     2]]
aaec98481defc4c230f84a64cdcf793d89081a76,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Lead-3,[[    0 32258    12   246     2]]
2e632eb5ad611bbd16174824de0ae5efe4892daf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Noisy data, Complexity and diversity of multimodal relations, Small set of multimodal examples","[[    0  3084 26351   414     6 14219  1571     8  5845     9 27250  1630
    337  3115     6  7090   278     9 27250  1630   337  7721     2]]"
41fd359b8c1402b31b6f5efd660143d1414783a0,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," close to random,",[[   0  593    7 9624    6    2]]
cb370692fe0beef90cdaa9c8e43a0aab6f0e117a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
53bf6238baa29a10f4ff91656c470609c16320e1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Users' tweets,[[    0 46110   108  6245     2]]
8a276dfe748f07e810b3944f4f324eaf27e4a52c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The distribution of joke scores varies wildly, ranging from 0 to 136,354 upvotes. We found that there is a major jump between the 0-200 upvote range and the 200 range and onwards, with only 6% of jokes scoring between 200-20,000. We used this natural divide as the cutoff to decide what qualified as a funny joke, giving us 13884 not-funny jokes and 2025 funny jokes.","[[    0   133  3854     9  8018  4391 23653 17824     6  6272    31   321
      7 23272     6 34615    62 45547     4   166   303    14    89    16
     10   538  3704   227     5   321    12  2619    62 28465  1186     8
      5  1878  1186     8 26485     6    19   129   231   207     9 11248
   2314   227  1878    12   844     6   151     4   166   341    42  1632
  11079    25     5 40002     7  2845    99  6048    25    10  6269  8018
      6  1311   201   508 40903    45    12 18317  2855 11248     8 10380
   6269 11248     4     2]]"
85e45b37408bb353c6068ba62c18e516d4f67fe9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The baseline is a multi-task architecture inspired by another paper.,"[[    0   133 18043    16    10  3228    12 45025  9437  4083    30   277
   2225     4     2]]"
076928bebde4dffcb404be216846d9d680310622,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window, connects only adjacent words in the so called word adjacency networks","[[    0   179    10  1029    12 23462 30904  1546   349   430  2136  3374
     10 37908     8 15716    32  2885  1241  1029    12 23462 30904    11
     10 12762  2931     6 15230   129 12142  1617    11     5    98   373
   2136 34703  1043  6761  4836     2]]"
ab78f066144936444ecd164dc695bec1cb356762,0.0,Entailment,[[    2     0 30495  3760  1757     2]],jointly trained with slots,[[    0   267 15494   352  5389    19 20063     2]]
02945c85d6cc4cdd1757b2f2bfa5e92ee4ed14a0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],dialectal tweet data,[[    0 43735  9041   337  3545   414     2]]
a4fe5d182ddee24e5bbf222d6d6996b3925060c8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0","[[    0  8739   487  6006  4999     6 24063   717  6486   777     6   944
    487  6006  5241     6 22375  6435  8954     6   256 12945   406     6
  40823  1023   279     6 12341 11088 28417     6   208  2191  2747    12
    134     6   660   438  4330   132     4   288     2]]"
2a1069ae3629ae8ecc19d2305f23445c0231dc39,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
184e1f28f96babf468f2bb4e1734f69646590cda,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the knowledge graph is used to prune this space by ranking actions based on their presence in the current knowledge graph and the relations between the objects in the graph as in BIBREF7,"[[    0   627  2655 20992    16   341     7  3349  4438    42   980    30
   7141  2163   716    15    49  2621    11     5   595  2655 20992     8
      5  3115   227     5  8720    11     5 20992    25    11   163  8863
  45935   406     2]]"
ace60950ccd6076bf13e12ee2717e50bc038a175,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They pre-train the models using 600000 articles as an unsupervised dataset and then fine-tune the models on small training set.,"[[    0  1213  1198    12 21714     5  3092   634  5594   151  7201    25
     41   542 16101 25376 41616     8   172  2051    12    90  4438     5
   3092    15   650  1058   278     4     2]]"
321429282557e79061fe2fe02a9467f3d0118cdd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"phrase-based word embedding, Abstract Syntax Tree(AST)","[[    0 40726    12   805  2136 33183 11303     6 43649 33221  3631 11077
   1640 10388    43     2]]"
71a7153e12879defa186bfb6dbafe79c74265e10,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
5ba6f7f235d0f5d1d01fd97dd5e4d5b0544fd212,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Class Membership Tests, Class Distinction Test, Word Equivalence Test","[[    0 21527 35522 18136     6  4210 11281 40890  4500     6 15690  8510
  11221  4086  4500     2]]"
c691b47c0380c9529e34e8ca6c1805f98288affa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
1124804c3702499b78cf0678bab5867e81284b6c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Non-contextual properties of a word, Word usage in an OP or PC (two groups), How a word connects an OP and PC, General OP/PC properties","[[    0 33239    12 46796  5564  3611     9    10  2136     6 15690  9453
     11    41 24839    50  4985    36  7109  1134   238  1336    10  2136
  15230    41 24839     8  4985     6  1292 24839    73  4794  3611     2]]"
65461516098ed63c45a567648e8e47c38ea7e58a,0.0,Entailment,[[    2     0 30495  3760  1757     2]], concatenating to the embedding vector,[[    0 10146 26511  1295     7     5 33183 11303 37681     2]]
2d47cdf2c1e0c64c73518aead1b94e0ee594b7a5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Dataset has 1737 train, 497 dev and 559 test sentences.","[[    0 43703   281   594    34   601  3272  2341     6  2766   406  8709
      8   195  4156  1296 11305     4     2]]"
5bb96b255dab3e47a8a68b1ffd7142d0e21ebe2a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
a9cc4b17063711c8606b8fc1c5eaf057b317a0c9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For task 1, we use F1-score, Task completion ratio, User satisfaction degree, Response fluency, Number of dialogue turns, Guidance ability for out of scope input","[[    0  2709  3685   112     6    52   304   274   134    12 31673     6
  12927  5687  1750     6 27913 11658  3093     6 19121  6626  6761     6
  12270     9  6054  4072     6 27419  2389  1460    13    66     9  7401
   8135     2]]"
feb4e92ff1609f3a5e22588da66532ff689f3bcc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],character bigram CNN classifier,[[    0 23375   380  4040  3480  1380 24072     2]]
6ead576ee5813164684a8cdda36e6a8c180455d9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Rouge-L, Bleu-1",[[    0   500  1438  1899    12   574     6 16463   257    12   134     2]]
b7fe91e71da8f4dc11e799b3bd408d253230e8c6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],target-side affixes,[[    0 23976    12  3730 11129  3181   293     2]]
1771a55236823ed44d3ee537de2e85465bf03eaf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Between the model and Stanford, Spacy and Flair the differences are 42.91, 25.03, 69.8 with Traditional NERs as reference and  49.88, 43.36, 62.43 with Wikipedia titles as reference.","[[    0 37257     5  1421     8  8607     6  2064  5073     8  4150  2456
      5  5550    32  3330     4  6468     6   564     4  3933     6  5913
      4   398    19 22530   234  2076    29    25  5135     8  1437  2766
      4  4652     6  3557     4  3367     6  5356     4  3897    19 28274
   4867    25  5135     4     2]]"
6333845facb22f862ffc684293eccc03002a4830,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
2555ca85ff6b56bd09c3919aa6b277eb7a4d4631,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Stanford Sentiment Treebank,[[    0 36118  1891 12169  8913 11077  5760     2]]
3758669426e8fb55a4102564cf05f2864275041b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"allows the annotator to define each markable as a certain mention type (pronoun, NP, VP or clause), The mentions referring to the same discourse item are linked between each other., chain members are annotated for their correctness","[[    0 37984     5 45068  2630     7  9914   349  2458   868    25    10
   1402  4521  1907    36 26404  7928     6 26266     6 13913    50 13166
    238    20 19197  5056     7     5   276 19771  6880    32  3307   227
    349    97   482  3206   453    32 45068  1070    13    49 34726     2]]"
5f9bd99a598a4bbeb9d2ac46082bd3302e961a0f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They evaluate F1 score and agent's test performance on their own built interactive datasets (iSQuAD and iNewsQA),"[[    0  1213 10516   274   134  1471     8  2936    18  1296   819    15
     49   308  1490 10813 42532    36   118   104 12444  2606     8   939
   5532  1864   250    43     2]]"
00e9f088291fcf27956f32a791f87e4a1e311e41,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"multi-lingual NMT, natural language inference, constituency parsing, skip-thought vectors","[[    0 42274    12  1527  5564   234 11674     6  1632  2777 42752     6
  12329 46563     6 14514    12 26086 44493     2]]"
ec8043290356fcb871c2f5d752a9fe93a94c2f71,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"general classification tasks, use of the methodology in other networked systems, a network could be enriched with embeddings obtained from graph embeddings techniques","[[    0 15841 20257  8558     6   304     9     5 18670    11    97  1546
    196  1743     6    10  1546   115    28 33389    19 33183   417  1033
   4756    31 20992 33183   417  1033  7373     2]]"
3604c4fba0a82d7139efd5ced47612c90bd10601,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
2af66730a85b29ff28dbfa58342e0ae6265d2963,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"78,976",[[   0 5479    6  466 5067    2]]
f52b2ca49d98a37a6949288ec5f281a3217e5ae8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group.","[[    0  1213   304   130  1134   765    73 21113    73  3479 19850  4050
      7  1532  5933 19233     6    61    16    11 42752   341     7  9415
   1546     7  5368 12762  5933   333     4     2]]"
b03e8e9a0cd2a44a215082773c7338f2f3be412a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a two layer recurrent neural language model with GRU cells of hidden size 512, a two layer neural sequence to sequence model equipped with bi-linear attention function with GRU cells of hidden size 512, a linear dynamical system, semi-supervised SLDS models with varying amount of labelled sentiment tags","[[    0   102    80 10490 35583 26739  2777  1421    19  8837   791  4590
      9  7397  1836 29600     6    10    80 10490 26739 13931     7 13931
   1421  8895    19  4003    12 43871  1503  5043    19  8837   791  4590
      9  7397  1836 29600     6    10 26956 29713  3569   467     6  4126
     12 16101 25376 10962  5433  3092    19 15958  1280     9 22434  5702
  19445     2]]"
c7f087c78768d5c6f3ff26921858186d627fd4fd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],features per admission were extracted as inputs to the readmission risk classifier,"[[    0 46076   228  7988    58 27380    25 16584     7     5  1166 12478
    810  1380 24072     2]]"
aa60b0a6c1601e09209626fd8c8bdc463624b0b3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"With both test sets performances decrease, varying between 94-97%","[[    0  3908   258  1296  3880  4476  7280     6 15958   227  8940    12
   6750   207     2]]"
1d72770d075b22411ec86d8bdee532f8c643740b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"3.1 F1 gain on the original dev set, 11 F1 gain on the multi-hop dev set, 10 F1 gain on the out-of-domain dev set.","[[    0   246     4   134   274   134  2364    15     5  1461  8709   278
      6   365   274   134  2364    15     5  3228    12  9547  8709   278
      6   158   274   134  2364    15     5    66    12  1116    12 46400
   8709   278     4     2]]"
c359ab8ebef6f60c5a38f5244e8c18d85e92761d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"10*n paraphrases, where n depends on the number of paraphrases that contain the entity mention spans","[[    0   698  3226   282 40127   338  9354     6   147   295  7971    15
      5   346     9 40127   338  9354    14  5585     5 10014  4521 23645
      2]]"
58ad7e8f7190e2a4f1588cae9a7842c56b37694d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"27,534 messages ",[[   0 2518    6  245 3079 3731 1437    2]]
8de64483ae96c0a03a8e527950582f127b43dceb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
b84bce289c6c81d0a7507ae183b94982533576b3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],systems were optimized on the tst2014 using Minimum error rate training BIBREF20,"[[    0 19675    29    58 29854    15     5   326   620 16310   634 32600
   5849   731  1058   163  8863 45935   844     2]]"
a6b99b7f32fb79a7db996fef76e9d83def05c64b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Active Intent Accuracy, Requested Slot F1, Average Goal Accuracy, Joint Goal Accuracy","[[    0 42586 39612 42688     6 18593   196 43811   274   134     6  8317
  17432 42688     6  9490 17432 42688     2]]"
3996438cef34eb7bedaa6745b190c69553cf246b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
b14f13f2a3a316e5a5de9e707e1e6ed55e235f6f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
52f7e42fe8f27d800d1189251dfec7446f0e1d3b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Accuracy of best proposed method KANE (LSTM+Concatenation) are 0.8011, 0.8592, 0.8605 compared to best state-of-the art method R-GCN + LR 0.7721, 0.8193, 0.8229 on three datasets respectively.","[[    0 36984 45386     9   275  1850  5448   229 19621    36   574  4014
    448  2744  9157  8729   225  1258    43    32   321     4   398 35620
      6   321     4   398 39642     6   321     4   398 34764  1118     7
    275   194    12  1116    12   627  1808  5448   248    12 11961   487
   2055 40815   321     4  4718  2146     6   321     4   398 30995     6
    321     4   398 26957    15   130 42532  4067     4     2]]"
3f326c003be29c8eac76b24d6bba9608c75aa7ea,0.0,Entailment,[[    2     0 30495  3760  1757     2]],F1 and Weighted-F1,[[    0   597   134     8 17515   196    12   597   134     2]]
d0bc782961567dc1dd7e074b621a6d6be44bb5b4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],30 words,[[   0  541 1617    2]]
2e3265d83d2a595293ed458152d3ee76ad19e244,0.0,Entailment,[[    2     0 30495  3760  1757     2]],collection of headlines published by HuffPost BIBREF12 between 2012 and 2018,"[[    0 44443     9  6337  1027    30 24884   163  8863 45935  1092   227
   1125     8   199     2]]"
8fcbae7c3bd85034ae074fa58a35e773936edb5b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Support Vector Machine (SVM), Logistic Regression (LR), Random Forest (RF)","[[    0 38873 40419 14969    36   104 20954   238  9359  5580  6304 21791
     36 33919   238 34638  5761    36 30455    43     2]]"
81686454f215e28987c7ad00ddce5ffe84b37195,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
534f69c8c90467d5aa4e38d7c25c53dbc94f4b24,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Amazon Mechanical Turk (AMT),[[    0 25146 35644 19683    36  2620   565    43     2]]
a09633584df1e4b9577876f35e38b37fdd83fa63,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Through Amazon MTurk annotators to determine plausibility and content richness of the response,"[[    0 23803  1645   256 38110   330 45068  3629     7  3094 44819 12203
      8  1383 38857     9     5  1263     2]]"
f5db12cd0a8cd706a232c69d94b2258596aa068c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (Table 1) The performance of all the target models raises significantly, while that on the original
examples remain comparable (e.g. the overall accuracy of BERT on modified examples raises from 24.1% to 66.0% on Quora)","[[    0 33683    19  1383  1716    35    36 41836   112    43    20   819
      9    70     5  1002  3092  7700  3625     6   150    14    15     5
   1461 50118  3463 45598  1091 10451    36   242     4   571     4     5
   1374  8611     9   163 18854    15 10639  7721  7700    31   706     4
    134   207     7  5138     4   288   207    15  3232  4330    43     2]]"
79620a2b4b121b6d3edd0f7b1d4a8cc7ada0b516,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"To the best of our knowledge, our method achieves state-of-the-art results in weighted-accuracy and standard accuracy on the dataset","[[    0  3972     5   275     9    84  2655     6    84  5448 35499   194
     12  1116    12   627    12  2013   775    11 19099    12  7904 45386
      8  2526  8611    15     5 41616     2]]"
44a2a8e187f8adbd7d63a51cd2f9d2d324d0c98d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"HEOT , A labelled dataset for a corresponding english tweets","[[    0 17779  3293  2156    83 22434 41616    13    10 12337 47510  6245
      2]]"
d0f831c97d345a5b8149a9d51bf321f844518434,0.0,Entailment,[[    2     0 30495  3760  1757     2]],binary label of stress or not stress,[[    0 40155  6929     9  3992    50    45  3992     2]]
30803eefd7cdeb721f47c9ca72a5b1d750b8e03b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"EER 16.04, Cmindet 0.6012, Cdet 0.6107","[[    0   717  2076   545     4  3387     6   230 28583   594   321     4
    401 37524     6   230 17701   321     4   401 19000     2]]"
ad4658c64056b6eddda00d3cbc55944ae01eb437,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," task-specific embedding of the claim together with all the above evidence about it, which comes from the last hidden layer of the NN","[[    0  3685    12 14175 33183 11303     9     5  2026   561    19    70
      5  1065  1283    59    24     6    61   606    31     5    94  7397
  10490     9     5   234   487     2]]"
1a252ffeaebdb189317aefd6c606652ba9677112,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"disabling the first layer in the RTE task gives a significant boost, resulting in an absolute performance gain of 3.2%,  this operation vary across tasks","[[    0  7779 37368     5    78 10490    11     5   248  6433  3685  2029
     10  1233  2501     6  5203    11    41  7833   819  2364     9   155
      4   176  4234  1437    42  2513 10104   420  8558     2]]"
32c149574edf07b1a96d7f6bc49b95081de1abd2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
1adbdb5f08d67d8b05328ccc86d297ac01bf076c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Train languages are: Cantonese, Bengali, Pashto, Turkish, Vietnamese, Haitian, Tamil, Kurdish, Tokpisin and Georgian, while Assamese, Tagalog, Swahili, Lao are used as target languages.","[[    0 40249 11991    32    35 10909  6909   242     6 30244  3644     6
    221  1671   560     6  4423     6 16859     6 32935     6 12634     6
   9225     6 25282   642 29761     8 28356     6   150  6331 12336   242
      6 12650 31263     6  3323   895  4715     6  1587   139    32   341
     25  1002 11991     4     2]]"
9299fe72f19c1974564ea60278e03a423eb335dc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MT developers to which crowd workers were compared are usually not professional translators, evaluation of sentences in isolation prevents raters from detecting translation errors, used not originally written Chinese test set
","[[    0 11674  5485     7    61  2180  1138    58  1118    32  2333    45
   2038 37297  3629     6 10437     9 11305    11 13084 17410 12378   268
     31 30985 19850  9126     6   341    45  3249  1982  1111  1296   278
  50118     2]]"
930c51b9f3936d936ee745716536a4b40f531c7f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Quora, MRPC",[[    0 12444  4330     6 18838  4794     2]]
639c145f0bcb1dd12d08108bc7a02f9ec181552e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Phase I: $\langle cc \rangle $ increases smoothly for $\wp < 0.4$, indicating that for this domain there is a small correlation between word neighborhoods. Full vocabularies are attained also for $\wp < 0.4$, Phase II: a drastic transition appears at the critical domain $\wp ^* \in (0.4,0.6)$, in which $\langle cc \rangle $ shifts abruptly towards 1. An abrupt change in $V(t_f)$ versus $\wp $ is also found (Fig. FIGREF16) for $\wp ^*$, Phase III: single-word languages dominate for $\wp > 0.6$. The maximum value of $\langle cc \rangle $ indicate that word neighborhoods are completely correlated","[[    0 45657    38    35 49959   462 14982 41754 44128   338 14982    68
   3488 17359    13 49959 12251 28696   321     4   306 47110  9172    14
     13    42 11170    89    16    10   650 22792   227  2136  9100     4
   6583 28312   873  8244   918    32 32069    67    13 49959 12251 28696
    321     4   306 47110 11028  3082    35    10 19167  3868  2092    23
      5  2008 11170 49959 12251 37249  3226 44128   179    36   288     4
    306     6   288     4   401    43 47110    11    61 49959   462 14982
  41754 44128   338 14982    68 10701 18489  1567   112     4   660 24573
    464    11    68   846  1640    90  1215   506    43  1629  4411 49959
  12251    68    16    67   303    36 44105     4 37365 45935  1549    43
     13 49959 12251 37249  3226 47110 11028  6395    35   881    12 14742
  11991 11781    13 49959 12251  8061   321     4   401 48292    20  4532
    923     9 49959   462 14982 41754 44128   338 14982    68  6364    14
   2136  9100    32  2198 34852     2]]"
48fa2ccc236e217fcf0e5aab0e7a146faf439b02,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
35b10e0dc2cb4a1a31d5692032dc3fbda933bf7d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],ensemble of hand-crafted syntactic and frame-semantic features BIBREF16,"[[    0  9401 46831     9   865    12 29496 45774 28201     8  5120    12
  26976 26970  1575   163  8863 45935  1549     2]]"
0c09a0e8f9c5bdb678563be49f912ab6e3f97619,0.0,Entailment,[[    2     0 30495  3760  1757     2]],12,[[   0 1092    2]]
14eb2b89ba39e56c52954058b6b799a49d1b74bf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The changes are evaluated based on accuracy of intent and entity recognition on SNIPS dataset,"[[    0   133  1022    32 15423   716    15  8611     9  5927     8 10014
   4972    15 13687 33792 41616     2]]"
37016cc987d33be5ab877013ef26ec7239b48bd9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"To achieve this purpose, we introduce a trainable class weight $\mathbf {w}$ to reweigh source domain examples by class when performing DIRL, with $\mathbf {w}_i > 0$","[[    0  3972  3042    42  3508     6    52  6581    10  2341   868  1380
   2408 49959 40051 36920 25522   605 24303  1629     7   769  1694  8774
   1300 11170  7721    30  1380    77  4655   211  5216   574     6    19
  49959 40051 36920 25522   605 24303  1215   118  8061   321  1629     2]]"
14634943d96ea036725898ab2e652c2948bd33eb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Authors report their best models have following accuracy: English CELEX (98.5%), Dutch CELEX (99.47%), Festival (99.990%), OpenLexique (100%), IIT-Guwahat (95.4%), E-Hitz (99.83%)","[[    0 44298   994   266    49   275  3092    33   511  8611    35  2370
  16327  3850  1000    36  5208     4   245 20186  5979 16327  3850  1000
     36  2831     4  3706 20186  3502    36  2831     4 27204 20186  2117
  43551  5150    36  1866 20186    38  2068    12 14484   605   895   415
     36  4015     4   306 20186   381    12   725  4494    36  2831     4
   6361  8871     2]]"
587885bc86543b8f8b134c20e2c62f6251195571,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English, Spanish and Zulu",[[    0 35007     6  3453     8   525 12709     2]]
b69ffec1c607bfe5aa4d39254e0770a3433a191b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Chinese dataset BIBREF0,[[    0 24727 41616   163  8863 45935   288     2]]
7a53668cf2da4557735aec0ecf5f29868584ebcf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],tutorial videos for a photo-editing software,[[    0    90 48960  3424    13    10  1345    12   196  2838  2257     2]]
2fffff59e57b8dbcaefb437a6b3434fc137f813b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining,"[[    0 46400    12 23976   196    68 34437  1629  1812   530 11305     8
  20688  7216     9 10798  2788 27380    31  3748  6052   341    30   163
   8863 45935   401 38939   139  9029    35 15302  6074     2]]"
5c70fdd3d6b67031768d3e28336942e49bf9a500,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"displays three different versions of a story written by three distinct models for a human to compare, human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages","[[    0  7779 33593   130   430  7952     9    10   527  1982    30   130
  11693  3092    13    10  1050     7  8933     6  1050    64  5163     5
   1421     7 10754    19    36  8024 16722    71   519  4986    24  1241
   2116    12 21818   238     8    64 16075    23    70  5612     2]]"
4226a1830266ed5bde1b349205effafe7a0e2337,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"high-order representation of a relation, loss gradient of relation meta","[[    0  3530    12 10337  8985     9    10  9355     6   872 43141     9
   9355 32820     2]]"
2ed02be0c183fca7031ccb8be3fd7bc109f3694b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1.08 points in ROUGE-L over our base pointer-generator model , 0.6 points in ROUGE-1","[[    0   134     4  3669   332    11   248  5061  8800    12   574    81
     84  1542 41515    12 20557  2630  1421  2156   321     4   401   332
     11   248  5061  8800    12   134     2]]"
12cfbaace49f9363fcc10989cf92a50dfe0a55ea,0.0,Entailment,[[    2     0 30495  3760  1757     2]],91.93% F1 score on CoNLL 2003 NER task and 96.37% F1 score on CoNLL 2000 Chunking task,"[[   0 6468    4 6478  207  274  134 1471   15  944  487 6006 4999  234
  2076 3685    8 8971    4 3272  207  274  134 1471   15  944  487 6006
  3788  732 6435  154 3685    2]]"
702e2d02c25a2f3f6b1be8ad3d448b502b8ced9c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"derive rewards from human-human dialogues by assigning positive values to contextualised responses seen in the data, and negative values to randomly chosen responses due to lacking coherence","[[    0  3624  2088 12840    31  1050    12 19003 25730  3663    30 36457
   1313  3266     7 37617  1720  8823   450    11     5   414     6     8
   2430  3266     7 22422  4986  8823   528     7 12622  1029 40584     2]]"
ba6422e22297c7eb0baa381225a2f146b9621791,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Difference is around 1 BLEU score lower on average than state of the art methods.,"[[    0 45985 24935    16   198   112   163  3850   791  1471   795    15
    674    87   194     9     5  1808  6448     4     2]]"
6992f8e5a33f0af0f2206769484c72fecc14700b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
47e6c3e6fcc9be8ca2437f41a4fef58ef4c02579,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Logistic regression model with character-level n-gram features,"[[    0 23345  5580 39974  1421    19  2048    12  4483   295    12 28526
   1575     2]]"
d95180d72d329a27ddf2fd5cc6919f469632a895,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d83304c70fe66ae72e78aa1d183e9f18b7484cd6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"True, Likely (i.e. Answerable), or Unsure (i.e. Unanswerable), why they are unsure from two choices (Not stated in the article or Other), The summary text boxes","[[    0 36948     6 37259    36   118     4   242     4 31652   868   238
     50 47307    36   118     4   242     4  1890 27740   868   238   596
     51    32 17118    31    80  5717    36    17    48  7199  2305    11
      5  1566    17    46    50    44    48 24989    17    46   238    20
     44    48 48600    17    46  2788  7644     2]]"
2d62a75af409835e4c123a615b06235a352a67fe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"feedforward neural networks, convolutional neural networks",[[    0 18790 16135 26739  4836     6 15380 23794   337 26739  4836     2]]
4704cbb35762d0172f5ac6c26b67550921567a65,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"In task 1 best transfer learning strategy improves F1 score by 4.4% and accuracy score by 3.3%, in task 2 best transfer learning strategy improves F1 score by 2.9% and accuracy score by 1.7%","[[    0  1121  3685   112   275  2937  2239  1860 15296   274   134  1471
     30   204     4   306   207     8  8611  1471    30   155     4   246
   4234    11  3685   132   275  2937  2239  1860 15296   274   134  1471
     30   132     4   466   207     8  8611  1471    30   112     4   406
    207     2]]"
b6ffa18d49e188c454188669987b0a4807ca3018,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SPARQL,[[    0  4186  2747 42111     2]]
5872279c5165cc8a0c58cf1f89838b7c43217b0e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e9cfe3f15735e2b0d5c59a54c9940ed1d00401a2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
e70236c876c94dbecd9a665d9ba8cefe7301dcfd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
13f7d50b3b8b0b97d90401eeb0a4e97c9eab3a76,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
8ad815b29cc32c1861b77de938c7269c9259a064,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO","[[    0  2796     6 38860     6 18366     6  5495     6  7008     6 24721
      6  8640     6  7136     6  5758     6 40114     6  3779     6  5885
      6 12901     6 12817     6 17678     6 22753     6  6236     6 11663
      6 20027     6  7038     6   987     6 35393     6  9036     6   289
    791     6  8228     6 10033     6 13579     6 38669     6 29594     6
  38343     6 16667     6 18838     6   163   487     6  2808     6 30105
      6  4799     6 10962     6 22294     6 32854     6   525   725     6
    230 36640     6  3703     6 23614     6 10725     6 17982     6    38
    771     6 12462     6 32046     6 20060     6 27017     6   229   250
      6  9963     2]]"
71d59c36225b5ee80af11d3568bdad7425f17b0c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Best BLSTM-CNN-CRF had F1 score 86.87 vs 86.69 of best BLSTM-CRF ,"[[    0 19183 12413  4014   448    12 16256    12  9822   597    56   274
    134  1471  8162     4  5677  1954  8162     4  4563     9   275 12413
   4014   448    12  9822   597  1437     2]]"
81e8d42dad08a58fe27eea838f060ec8f314465e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],neural attention model with a convolutional encoder with an RNN decoder and RNN encoder-decoder,"[[    0   858  9799  1503  1421    19    10 15380 23794   337  9689 15362
     19    41   248 20057  5044 15362     8   248 20057  9689 15362    12
  11127 15362     2]]"
a74886d789a5d7ebcf7f151bdfb862c79b6b8a12,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a BiLSTM over all words in the respective sequences with randomly initialised word embeddings, following BIBREF30","[[    0   102  6479   574  4014   448    81    70  1617    11     5  7091
  26929    19 22422  2557  1720  2136 33183   417  1033     6   511   163
   8863 45935   541     2]]"
2e1ededb7c8460169cf3c38e6cde6de402c1e720,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"mean prediction accuracy 0.99582651
S&P 500 Accuracy 0.99582651","[[    0 43348 16782  8611   321     4  2831  4432  2481  4708 50118   104
    947   510  1764 42688   321     4  2831  4432  2481  4708     2]]"
71538776757a32eee930d297f6667cd0ec2e9231,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions","[[    0 14377 13587     5  1291   227  2136  3212     8     5    80 12758
      9  3018  4921    36  2137  1250   691     6  1266   346     9  4072
     43    11  2559 26956 44702  2485     2]]"
0ee20a3a343e1e251b74a804e9aa1393d17b46d6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"quality of the classifier predictions is too low to be integrated into the network analysis right away, the classifier drastically facilitates the annotation process for human annotators compared to annotating unfiltered tweets","[[    0  8634     9     5  1380 24072 12535    16   350   614     7    28
   6818    88     5  1546  1966   235   409     6     5  1380 24072 17811
  34444     5 47760   609    13  1050 45068  3629  1118     7 45068  1295
   9515   718 10001  6245     2]]"
89ce18ee52c52a78b38c49b14574407b7ea2fb02,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Attention-based LSTM with emojis,"[[    0 28062 19774    12   805   226  4014   448    19  2841  4203   354
      2]]"
09621c9cd762e1409f22d501513858d67dcd3c7c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],a tutorial website about an image editing program ,[[    0   102 35950   998    59    41  2274  5390   586  1437     2]]
aa54e12ff71c25b7cff1e44783d07806e89f8e54,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The health benefits of alcohol consumption are more limited than previously thought, researchers say","[[   0  133  474 1795    9 3766 4850   32   55 1804   87 1433  802    6
  2634  224    2]]"
5f4e6ce4a811c4b3ab07335d89db2fd2a8d8d8b2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],accuracy,[[    0  7904 45386     2]]
96dcabaa8b6bd89b032da609e709900a1569a0f9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"These ask which choices are morally required, forbidden, or permitted, norms are understood as universal rules of what to do and what not to do","[[    0  4528  1394    61  5717    32 28404  1552     6 27686     6    50
  11047     6 14513    32  6238    25 10547  1492     9    99     7   109
      8    99    45     7   109     2]]"
5fa464a158dc8abf7cef8ca7d42a7080670c1edd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
fa2a384a23f5d0fe114ef6a39dced139bddac20e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],903019 references,[[    0   466  3933 36632 13115     2]]
76d62e414a345fe955dc2d99562ef5772130bc7e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"neural question-answering technique to extract relations from a story text, OpenIE5, a commonly used rule-based information extraction technique","[[    0   858  9799   864    12  1253   605  2961  9205     7 14660  3115
     31    10   527  2788     6  2117  7720   245     6    10 10266   341
   2178    12   805   335 23226  9205     2]]"
082bc58e1a2a65fc1afec4064a51e4c785674fd7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Long-short Term Hybrid Memory (LSTHM) is an extension of the Long-short Term Memory (LSTM) ,"[[    0 21001    12 20263 25569 25367 25940    36   574  4014 35441    43
     16    41  5064     9     5  2597    12 20263 25569 25940    36   574
   4014   448    43  1437     2]]"
9a5d02062fa7eec7097f1dc1c38b5e6d5c82acdf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the CIDEr-D BIBREF22 , SPICE BIBREF23 , BLEU BIBREF24 , METEOR BIBREF25 , and ROUGE-L BIBREF26 metrics","[[    0   627   230  2688 28012    12   495   163  8863 45935  2036  2156
   6178  9292   163  8863 45935  1922  2156   163  3850   791   163  8863
  45935  1978  2156 30782   717  3411   163  8863 45935  1244  2156     8
    248  5061  8800    12   574   163  8863 45935  2481 12758     2]]"
d2473c039ab85f8e9e99066894658381ae852e16,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"image feature, question feature, label vector for the user's answer","[[    0 20094  1905     6   864  1905     6  6929 37681    13     5  3018
     18  1948     2]]"
7cf726db952c12b1534cd6c29d8e7dfa78215f9e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],It is a network used to encode speech lattices to maintain a rich hypothesis space.,"[[    0   243    16    10  1546   341     7 46855  1901 39832  6355     7
   3014    10  4066 31098   980     4     2]]"
8d1f9d3aa2cc2e2e58d3da0f5edfc3047978f3ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"To have an estimation about human performance in each metric, we iteratively treat every reference sentence in dev/test data as the prediction to be compared with all references (including itself).","[[    0  3972    33    41 32809    59  1050   819    11   349 14823     6
     52 41393 10481  3951   358  5135  3645    11  8709    73 21959   414
     25     5 16782     7    28  1118    19    70 13115    36  8529  1495
    322     2]]"
4d223225dbf84a80e2235448a4d7ba67bfb12490,0.0,Entailment,[[    2     0 30495  3760  1757     2]],removing AltLexC and adding Progression into our sense hierarchy,"[[    0  5593 13871  7330 43551   347     8  1271  1698 42223    88    84
   1472 24393     2]]"
320d72a9cd19b52c29dda9ddecd520c9938a717f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
4640793d82aa7db30ad7b88c0bf0a1030e636558,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Chiu and Nichols (2016), Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), Hashimoto et al. (2016), Sgaard and Goldberg (2016) ","[[    0  4771  9060     8 18641    36  9029   238   226 29069  4400  1076
      4    36  9029   238  3066     8   289 35664    36  9029   238 13262
   4400  1076     4    36  3789   238 21653 36066  4400  1076     4    36
   9029   238   208 14562 25039     8 18835    36  9029    43  1437     2]]"
d4b9cdb4b2dfda1e0d96ab6c3b5e2157fd52685e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Two models will make the same predictions if and only if they use the same reasoning process., On similar inputs, the model makes similar decisions if and only if its reasoning is similar., Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other.","[[    0  9058  3092    40   146     5   276 12535   114     8   129   114
     51   304     5   276 20511   609   482   374  1122 16584     6     5
   1421   817  1122  2390   114     8   129   114    63 20511    16  1122
    482 25122  1667     9     5  8135    32    55   505     7     5  1421
  20511    87   643     4  7905     6     5  5694     9   430  1667     9
      5  8135    32  2222    31   349    97     4     2]]"
564dcaf8d0bcc274ab64c784e4c0f50d7a2c17ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur","[[    0  8138     6   163   571     6  8316     6   230    29     6  8318
      6   926     6  2271     6  7065     6   381   257     6 16125     6
  19643     6  4967     6 10160     6    91     6 12289     6  8003     6
     85     6  1587     6  8713     6 45739     6   234   428     6   234
    462     6   234   282     6 12901     6 42191     6  3830     6 10318
      6  4424     6 24384     6  2393     6 27090     6  9163     2]]"
07c79edd4c29635dbc1c2c32b8df68193b7701c6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"HEOT , A labelled dataset for a corresponding english tweets ","[[    0 17779  3293  2156    83 22434 41616    13    10 12337 47510  6245
   1437     2]]"
01dc6893fc2f49b732449dfe1907505e747440b0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Ethics, Gender, Human rights, Sports, Freedom of Speech, Society, Religion, Philosophy, Health, Culture, World, Politics, Environment, Education, Digital Freedom, Economy, Science and Law","[[    0 42301  2857     6 25262     6  3861   659     6  1847     6  7978
      9 27242     6  3930     6 34530     6 42232     6  1309     6 11886
      6   623     6 16226     6  9356     6  3061     6  6282  7978     6
  15735     6  4662     8  2589     2]]"
417dabd43d6266044d38ed88dbcb5fdd7a426b22,0.0,Entailment,[[    2     0 30495  3760  1757     2]],domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining,"[[    0 46400    12 23976   196    68 34437  1629  1812   530 11305     8
  20688  7216     9 10798  2788 27380    31  3748  6052   341    30   163
   8863 45935   401 38939   139  9029    35 15302  6074     2]]"
dccc3b182861fd19ccce5bd00ce9c3f40451ed6e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
27b01883ed947b457d3bab0c66de26c0736e4f90,0.0,Entailment,[[    2     0 30495  3760  1757     2]],syllables,[[   0 8628  890 6058    2]]
f5707610dc8ae2a3dc23aec63d4afa4b40b7ec1e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Variables in the set {str, prec, attr} indicating in which mode the mention should be resolved.","[[    0 48269  6058    11     5   278 25522  6031     6 16057     6 15095
    338 24303  9172    11    61  5745     5  4521   197    28  8179     4
      2]]"
4e4946c023211712c782637fcca523deb126e519,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
4b128f9e94d242a8e926bdcb240ece279d725729,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"DBQA, KBRE",[[    0 10842  1864   250     6 29006  4629     2]]
e7329c403af26b7e6eef8b60ba6fefbe40ccf8ce,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The model outperforms at every point in the
implicit-tuples PR curve reaching almost 0.8 in recall","[[    0   133  1421  9980 33334    23   358   477    11     5 50118 38731
  17022    12 10109 12349  4729  9158  3970   818   321     4   398    11
   6001     2]]"
d32b6ac003cfe6277f8c2eebc7540605a60a3904,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"(1) Rank by the number of times a citation is mentioned in the document., (2) Rank by the number of times the citation is cited in the literature (citation impact)., (3) Rank using Google Scholar Related Articles., (4) Rank by the TF*IDF weighted cosine similarity., (5) Rank using a learning-to-rank model trained on text similarity rankings.","[[    0  1640   134    43 17816    30     5   346     9   498    10 31825
     16  2801    11     5  3780   482    36   176    43 17816    30     5
    346     9   498     5 31825    16  4418    11     5 13144    36   438
  12257   913   322     6    36   246    43 17816   634  1204 35553  3283
  15219   482    36   306    43 17816    30     5 35690  3226  2688   597
  19099 12793   833 37015   482    36   245    43 17816   634    10  2239
     12   560    12 40081  1421  5389    15  2788 37015  8359     4     2]]"
4fa2faa08eeabc09d78d89aaf0ea86bb36328172,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
4d4b9ff2da51b9e0255e5fab0b41dfe49a0d9012,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
a2103e7fe613549a9db5e65008f33cf2ee0403bd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"wealth , democracy , population, levels of ODA, conflict ","[[    0 26257  2156  4593  2156  1956     6  1389     9   384  3134     6
   3050  1437     2]]"
369b0a481a4b75439ade0ec4f12b44414c4e5164,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Turkish news-web corpus,  TS TweetS by Sezer-2013 and 20M Turkish Tweets by Bolat and Amasyal","[[    0 37300   340    12 10534 42168     6  1437 12932 12244   104    30
   1608  6403    12 10684     8   291   448  4423 18868  2580    30  9166
    415     8  1918 17400   337 10031     2]]"
fc06502fa62803b62f6fd84265bfcfb207c1113b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"annotators who were not security experts, researchers in either NLP or computer security","[[    0 37250  3629    54    58    45   573  2320     6  2634    11  1169
    234 21992    50  3034   573     2]]"
352a1bf734b2d7f0618e9e2b0dbed4a3f1787160,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
fa2ffc6b4b046e17bc41e199855c4941673e2caf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Parallel monolingual corpus in English and Mandarin,[[    0 22011 44682  6154 31992  5564 42168    11  2370     8 33830     2]]
3d34a02ceebcc93ee79dc073c408651d25e538bc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Support Vector Machines (SVM) classifier,[[    0 38873 40419 28413    36   104 20954    43  1380 24072     2]]
3d7ab856a5cade7ab374fc2f2713a4d0a30bbd56,0.0,Entailment,[[    2     0 30495  3760  1757     2]], a multilingual word representation which aims to learn a linear mapping from a source to a target embedding space,"[[    0    10  7268 41586  2136  8985    61  5026     7  1532    10 26956
  20410    31    10  1300     7    10  1002 33183 11303   980     2]]"
36a9230fadf997d3b0c5fc8af8d89bd48bf04f12,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (Skip-thought vectors-Natural Language Inference paragraphs) The encoder for the current sentence and the decoders for the previous (STP) and next sentence (STN) are typically parameterized as separate RNNs
- RNN","[[    0 33683    19  1383  1716    35    36 28056    12 26086 44493    12
  42968 22205    96 23861 36153    43    20  9689 15362    13     5   595
   3645     8     5  5044  1630   268    13     5   986    36  4014   510
     43     8   220  3645    36  4014   487    43    32  3700 43797  1538
     25  2559   248 20057    29 50118    12   248 20057     2]]"
a3783e42c2bf616c8a07bd3b3d503886660e4344,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
9d5153a7553b7113716420a6ddceb59f877eb617,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
62048ea0aab61abe21fb30d70c4a1bc5fb946137,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
67d8e50ddcc870db71c94ad0ad7f8a59a6c67ca6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],3 ,[[   0  246 1437    2]]
439af1232a012fc4d94ef2ffe305dd405bee3888,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Base , Base+Noise, Cleaning , Dynamic-CM ,  Global-CM,  Global-ID-CM, Brown-CM ,  K-Means-CM","[[    0 34164  2156 11056  2744  3084  1496     6 10326   154  2156 29614
     12 18814  2156  1437  1849    12 18814     6  1437  1849    12  2688
     12 18814     6  1547    12 18814  2156  1437   229    12  5096  1253
     12 18814     2]]"
591231d75ff492160958f8aa1e6bfcbbcd85a776,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CNN-mean, CNN-avgmax",[[    0 16256    12 43348     6  3480    12  1469   571 29459     2]]
4040f5c9f365f9bc80b56dce944ada85bb8b4ab4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
ad5898fa0063c8a943452f79df2f55a5531035c7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Word embeddings trained on GoogleNews and Word embeddings trained on Reddit dataset,"[[    0 44051 33183   417  1033  5389    15  1204  5532     8 15690 33183
    417  1033  5389    15  6844 41616     2]]"
0e510d918456f3d2b390b501a145d92c4f125835,0.0,Entailment,[[    2     0 30495  3760  1757     2]],constructively by selecting the parameters of the multi-head self-attention layer so that the latter acts like a convolutional layer,"[[    0 42843  6608    30 18099     5 17294     9     5  3228    12  3628
   1403    12  2611 19774 10490    98    14     5  5442  4504   101    10
  15380 23794   337 10490     2]]"
73cd785d474bae7af974802715ef7ba5468d9139,0.0,Entailment,[[    2     0 30495  3760  1757     2]],manually inspect,[[    0   397 13851 18973     2]]
2e1660405bde64fb6c211e8753e52299e269998f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"645, 600000",[[    0 33611     6  5594   151     2]]
14421b7ae4459b647033b3ccba635d4ba7bb114b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],experts in Washington Post,[[    0 26786  1872    11   663  1869     2]]
6b6360fab2edc836901195c0aba973eae4891975,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Switchboard conversational English corpus,[[    0 45112  4929 28726  5033  2370 42168     2]]
ef04182b6ae73a83d52cb694cdf4d414c81bf1dc,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," disaster data from BIBREF5, Queensland flood in Queensland, Australia and Alberta flood in Alberta, Canada","[[    0  4463   414    31   163  8863 45935   245     6  7088  5005    11
   7088     6  1221     8  6055  5005    11  6055     6   896     2]]"
c418deef9e44bc8448d9296c6517824cb95bd554,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"F1-score, BLEU score",[[    0   597   134    12 31673     6   163  3850   791  1471     2]]
b6a6bdca6dee70f8fe6dd1cfe3bb2c5ff03b1605,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d557752c4706b65dcdb7718272180c59d77fb7a7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],unsupervised word segmentation method latticelm,"[[    0   879 16101 25376  2136  2835  1258  5448 39832   636   523   119
      2]]"
1a06b7a2097ebbad0afc787ea0756db6af3dadf4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Bulgarian, Czech, French, German, Korean, Polish, Portuguese, Russian, Thai, Vietnamese","[[    0 40028   571  9063     6  9096     6  1515     6  1859     6  2238
      6 11145     6 13053     6  1083     6  9130     6 16859     2]]"
a65e5c97ade6e697ec10bcf3c3190dc6604a0cd5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"E2E NLG challenge Dataset, The Wikipedia Company Dataset","[[    0   717   176   717 12817   534  1539 16673   281   594     6    20
  28274  1260 16673   281   594     2]]"
0b411f942c6e2e34e3d81cc855332f815b6bc123,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Two neural networks: an extractor based on an encoder (BERT) and a decoder (LSTM Pointer Network BIBREF22) and an abstractor identical to the one proposed in BIBREF8.,"[[    0  9058 26739  4836    35    41 14660   368   716    15    41  9689
  15362    36 11126   565    43     8    10  5044 15362    36   574  4014
    448  6002  8007  3658   163  8863 45935  2036    43     8    41 20372
    368 14085     7     5    65  1850    11   163  8863 45935   398     4
      2]]"
bb4de896c0fa4bf3c8c43137255a4895f52abeef,0.0,Entailment,[[    2     0 30495  3760  1757     2]],a RNN-based seq2seq VC model called ATTS2S based on the Tacotron model,"[[    0   102   248 20057    12   805 48652   176 47762 19390  1421   373
   3263  2685   176   104   716    15     5 34355  1242  2839  1421     2]]"
4d844c9453203069363173243e409698782bac3f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
cdf1bf4b202576c39e063921f6b63dc9e4d6b1ff,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Accuracy and F1 score for supervised tasks, Pearson's and Spearman's correlation for unsupervised tasks","[[    0 36984 45386     8   274   134  1471    13 20589  8558     6 16116
     18     8 37828   397    18 22792    13   542 16101 25376  8558     2]]"
36feaac9d9dee5ae09aaebc2019b014e57f61fbf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By the number of parameters.,[[    0  2765     5   346     9 17294     4     2]]
d05d667822cb49cefd03c24a97721f1fe9dc0f4c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain.","[[    0 26039  4932    10   923     7     5  9355   716    15   549 19197
   5948    11     5   276  3780     6   114 19197    32 14085     6    50
    114 19197    32    11     5   276  2731 23861  3206     4     2]]"
3f85cc5be84479ba668db6d9f614fedbff6d77f1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],eight GB,[[    0 19491  7216     2]]
a979749e59e6e300a453d8a8b1627f97101799de,0.0,Entailment,[[    2     0 30495  3760  1757     2]],because word pair similarity increases if the two words translate to similar parts of the cross-lingual embedding space,"[[    0 13437  2136  1763 37015  3488   114     5    80  1617 14620     7
   1122  1667     9     5  2116    12  1527  5564 33183 11303   980     2]]"
e75f5bd7cc7107f10412d61e3202a74b082b0934,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the Transformer with 10 encoder layers and 2 decoder layers is $2.32$ times as fast as the 6-layer Transformer,"[[    0   627  5428 22098    19   158  9689 15362 13171     8   132  5044
  15362 13171    16    68   176     4  2881  1629   498    25  1769    25
      5   231    12 39165  5428 22098     2]]"
b3dc6d95d1570ad9a58274539ff1def12df8f474,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Through the experiments, we empirically studied our analysis on DIRL and the effectiveness of our proposed solution in dealing with the problem it suffered from.","[[    0 23803     5 15491     6    52 46533  3435  8069    84  1966    15
    211  5216   574     8     5 12833     9    84  1850  2472    11  4098
     19     5   936    24  2152    31     4     2]]"
7975c3e1f61344e3da3b38bb12e1ac6dcb153a18,0.0,Entailment,[[    2     0 30495  3760  1757     2]],each embedding version is crucial for good performance,[[    0 37782 33183 11303  1732    16  4096    13   205   819     2]]
482b4cc7676cf13912e27899c718f4dc5d92846d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],identify all abbreviations using regular expressions,[[    0  8009  4591    70 40993  1635   634  1675 17528     2]]
0d7f514f04150468b2d1de9174c12c28e52c5511,0.0,Entailment,[[    2     0 30495  3760  1757     2]],agreement of 0.85 and Kappa value of 0.83,"[[    0  1073 43563     9   321     4  4531     8 37772   923     9   321
      4  6361     2]]"
d6d29040e7fafceb188e62afba566016b119b23c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"PDP-60, WSC-273",[[    0  6153   510    12  2466     6   305  3632    12 26273     2]]
edc43e1b75c0970b7003deeabfe3ad247cb1ed83,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Akkadian.,[[    0   250 27762 33852     4     2]]
37eba8c3cfe23778498d95a7dfddf8dfb725f8e2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Sequential (Denoising) Autoencoder, TF-IDF BOW, SkipThought, FastSent, Siamese C-BOW, C-BOW, C-PHRASE, ParagraphVector","[[    0 48245 12986    36 27267   139  3009    43  9496 18057   438 15362
      6 35690    12  2688   597   163  4581     6  6783 11329 12807     6
   9612 35212     6 11065 12336   242   230    12   387  4581     6   230
     12   387  4581     6   230    12  7561   500 24199     6  2884 44947
  48417     2]]"
47ecaca8adc7306e3014e8c4358e306a5f0e1716,0.0,Entailment,[[    2     0 30495  3760  1757     2]],This article presented a brief overview of embedding models of entity and relationships for KB completion. ,"[[    0   713  1566  2633    10  4315 13433     9 33183 11303  3092     9
  10014     8  4158    13 29006  5687     4  1437     2]]"
45a2ce68b4a9fd4f04738085865fbefa36dd0727,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The dataset from a joint ADAPT-Microsoft project,"[[    0   133 41616    31    10  2660  4516   591   565    12 35460   695
      2]]"
e8029ec69b0b273954b4249873a5070c2a0edb8a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],performance is significantly degraded without pixel data,[[    0 15526    16  3625 38918   396 29597   414     2]]
0b8d64d6cdcfc2ba66efa41a52e09241729a697c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
6d54bad91b6ccd1108d1ddbff1d217c6806e0842,0.0,Entailment,[[    2     0 30495  3760  1757     2]],only the first word sense (usually the most common) is taken into account,"[[    0  8338     5    78  2136  1472    36 33834     5   144  1537    43
     16   551    88  1316     2]]"
e111925a82bad50f8e83da274988b9bea8b90005,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Randomly from Twitter,[[    0 45134   352    31   599     2]]
1959e0ebc21fafdf1dd20c6ea054161ba7446f61,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained., Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. ","[[    0 11428 44839  2788 29916  5206    36  7164   104    43    16    10
   2008  3685    13 23366   154  1131   557   414    31  5175   474  2189
     36   717 16271    29   238   147  9825  3186  1131   414     6   215
     25   549     5  3186    34  2167  5298     6  6357     6    50    99
      5 16570  1836    16     6   141   444    31     5 16570    16   847
     23   148     5  3012     6    50    99     5  2167 13154  1296   898
     16     6    32  4756   482  8280     5  2065   230  2685  3685     6
     84  1209   250    12  7164   104  3685  5026     7  8286     5   144
   1330  2788    31  1461 17818  2788     4  1437     2]]"
a08b5018943d4428f067c08077bfff1af3de9703,0.0,Entailment,[[    2     0 30495  3760  1757     2]],neutral class,[[    0 12516  1380     2]]
34fab25d9ceb9c5942daf4ebdab6c5dd4ff9d3db,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"weibo-100k, Ontonotes, LCQMC and XNLI","[[    0  1694 18232    12  1866   330     6 13302   261 17467     6 24756
   1864  6018     8  1577   487 27049     2]]"
3d39e57e90903b776389f1b01ca238a6feb877f3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
aa979aed5a454b6705d0085ba2777859feb6fc62,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
80f19be1cbe1f0ec89bbafb9c5f7a8ded37881fb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CBOW and Skip-gram methods in the FastText tool BIBREF9,"[[    0 25392  4581     8  6783    12 28526  6448    11     5  9612 39645
   3944   163  8863 45935   466     2]]"
ebb7313eee2ea447abc83cb08b658b57c7eaa600,0.0,Entailment,[[    2     0 30495  3760  1757     2]],automatic translator with Moses,[[    0 27106 28894    19 16590     2]]
5a230fe4f0204bf2eebc0e944cf8defaf33d165c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"One of the several formats into which FHIR can be serialized is RDF, there is the potential for a slight mismatch between the models","[[    0  3762     9     5   484 19052    88    61   274   725  5216    64
     28 13603  1538    16   248  9380     6    89    16     5   801    13
     10  7019 37109   227     5  3092     2]]"
944d5dbe0cfc64bf41ea36c11b1d378c408d40b8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],x-vector,[[    0  1178    12 48219     2]]
150af1f5f4ce0ec94a7114397cffc59c4798441e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
bc473c5bd0e1a8be9b2037aa7006fd68217c3f47,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," Choose professional translators as raters,  Evaluate documents, not sentences, Evaluate fluency in addition to adequacy, Do not heavily edit reference translations for fluency, Use original source texts","[[    0 25964  2038 37297  3629    25 12378   268     6  1437 40378 21269
   2339     6    45 11305     6 40378 21269  6626  6761    11  1285     7
  27438  5073     6  1832    45  4008 17668  5135 41762    13  6626  6761
      6  7627  1461  1300 14301     2]]"
0bfed6f9cfe93617c5195c848583e3945f2002ff,0.0,Entailment,[[    2     0 30495  3760  1757     2]],gated neural network ,[[    0   571  1070 26739  1546  1437     2]]
c21d26130b521c9596a1edd7b9ef3fe80a499f1e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations","[[    0 43976  9366     6  2351  4485  5989 11076     6  2749    62  3068
    544     6 12926  1569  3308     6 12926  3895  6696     8   442  2391
  13747     2]]"
b67420da975689e47d3ea1c12b601851018c4071,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer.","[[    0  2137  1250  9437     9     5  1850 35798   337 14813 15362  3658
     36   250  2796   238    61  4412 10726     9    41 33183 11303 10490
      6    41  1503   337  9689 15362 10490     6    10  1002    12 14175
   1503 10490     6     8    41  4195 10490     4     2]]"
8c0e8a312b85c4ffdffabeef0d29df1ef8ff7fb2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"3,200 sentences",[[    0   246     6  2619 11305     2]]
094ce2f912aa3ced9eb97b171745d38f58f946dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The Online Retail Data Set consists of a clean list of 25873 invoices, totaling 541909 rows and 8 columns.","[[    0   133  5855  9362  5423  8504 10726     9    10  2382   889     9
  36586  5352 12259   139  6355     6 17405  4431  1646  3546 22162     8
    290 18315     4     2]]"
bb570d4a1b814f508a07e74baac735bf6ca0f040,0.0,Entailment,[[    2     0 30495  3760  1757     2]],better sentence pair representations,[[    0 24836  3645  1763 30464     2]]
a035472a5c6cf238bed62b63d28100c546d40bd5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"target some heuristically extracted contents, treat INLINEFORM1 as a latent variable and co-train selector and generator by maximizing the marginal data likelihood, reinforcement learning to approximate the marginal likelihood,  Variational Reinforce-Select (VRS) which applies variational inference BIBREF10 for variance reduction","[[    0 23976   103    37   710 18281 27380 13654     6  3951  2808 28302
  38036   134    25    10 42715 15594     8  1029    12 21714 41851     8
  22538    30 35195     5 14612   414 11801     6 37700  2239     7 32161
      5 14612 11801     6  1437 41058  5033 17789  9091    12 45356    36
    846  8105    43    61 11459 44243  5033 42752   163  8863 45935   698
     13 37832  4878     2]]"
dcd22abfc9e7211925c0393adc30dbd4711a9f88,0.0,Entailment,[[    2     0 30495  3760  1757     2]],10 million sentences gathered from Wikipedia,[[    0   698   153 11305  4366    31 28274     2]]
a96a1a354cb3a2a434b085e4d9c8844d0b672ec4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
a91abc7983fffa6b2e1e46133f559cec3d7d9438,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
2a7c40a72b6380e76511e722b4b02b3a1e5078fd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
86cd1228374721db67c0653f2052b1ada6009641,0.0,Entailment,[[    2     0 30495  3760  1757     2]],YouTube videos,[[    0 36169  3424     2]]
1e2ffa065b640e912d6ed299ff713a12195e12c4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],a simulated binning task in which the robot is tasked to place a cube into a bowl as outlined by the verbal command,"[[    0   102 27361  6870  3509  3685    11    61     5  9916    16 12639
      7   317    10 37179    88    10  5749    25 10634    30     5 14580
   5936     2]]"
1165fb0b400ec1c521c1aef7a4e590f76fee1279,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The data from collected travel surveys is used to model travel behavior.,"[[    0   133   414    31  4786  1504 12092    16   341     7  1421  1504
   3650     4     2]]"
722e9b6f55971b4c48a60f7a9fe37372f5bf3742,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The proposed system consists of a Bi-directional Long Short-Term Memory (BiLSTM) BIBREF16, a two-level attention mechanism BIBREF29, BIBREF30 and a shared representation for emotion and sentiment analysis tasks., Each of the shared representations is then fed to the primary attention mechanism","[[    0   133  1850   467 10726     9    10  6479    12 42184   337  2597
   7787    12 23036 25940    36 37426   574  4014   448    43   163  8863
  45935  1549     6    10    80    12  4483  1503  9562   163  8863 45935
   2890     6   163  8863 45935   541     8    10  1373  8985    13 11926
      8  5702  1966  8558   482  4028     9     5  1373 30464    16   172
   9789     7     5  2270  1503  9562     2]]"
0fa81adf00662694e1dc74475ae2b9283c50748c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],parameter sharing,[[    0 46669  5906  3565     2]]
13b36644357870008d70e5601f394ec3c6c07048,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
ab0bb4d0a9796416d3d7ceba0ba9ab50c964e9d6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
2173809eb117570d289cefada6971e946b902bd6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
bdd8368debcb1bdad14c454aaf96695ac5186b09,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"defined into four categories from high risk, moderate risk, to low risk","[[    0 30764    88   237  6363    31   239   810     6  7212   810     6
      7   614   810     2]]"
94edac71eea1e78add678fb5ed2d08526b51016b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Parallel Scan Inference, Vectorized Parsing, Semiring Matrix Operations","[[    0 22011 44682 24753    96 23861     6 40419  1538 31443   154     6
  11202  9798 29830 10919     2]]"
6dc9960f046ec6bd280a721724458f66d5a9a585,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Text Overlap Metrics, including BLEU, Perplexity, Parameterized Metrics","[[    0 39645  2306 22489  4369 18715     6   217   163  3850   791     6
   2595 26028  1571     6 34559  5906  1538  4369 18715     2]]"
b68d2549431c524a86a46c63960b3b283f61f445,0.0,Entailment,[[    2     0 30495  3760  1757     2]],fragments are interchangeable if they occur in at least one lexical environment that is exactly the same,"[[    0 12997  1073  2963    32 40627   114    51  5948    11    23   513
     65 36912  3569  1737    14    16  2230     5   276     2]]"
58e65741184c81c9e7fe0ca15832df2d496beb6f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
3a9d391d25cde8af3334ac62d478b36b30079d74,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
1afd550cbee15b753db45d7db2c969fc3d12a7d9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
e21a8581cc858483a31c6133e53dd0cfda76ae4c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
44c7c1fbac80eaea736622913d65fe6453d72828,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"34,432 ",[[    0  3079     6 37296  1437     2]]
cb78e280e3340b786e81636431834b75824568c3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],9,[[  0 466   2]]
5471766ca7c995dd7f0f449407902b32ac9db269,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"2.36 point increase in the F1 score with respect to the best SEM architecture, on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM), lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa, For POS tagging, we observe error reductions of respectively 0.71% for GSD, 0.81% for Sequoia, 0.7% for Spoken and 0.28% for ParTUT, For parsing, we observe error reductions in LAS of 2.96% for GSD, 3.33% for Sequoia, 1.70% for Spoken and 1.65% for ParTUT","[[    0   176     4  3367   477   712    11     5   274   134  1471    19
   2098     7     5   275 39703  9437     6    15     5 26933 11160  8625
     12   565  4396  2444  2749    36  6668     4   176  1954     4  1812
      4   176    13 14402   448   238   784  8299   639  3092  5389    15
      5  1461  2370  1058   278    11     5 26933 11160  8625    12   565
   4923  2749     6  7694     4   176  1954     4  7383     4  6468    13
   3830 11126 38495     6   286 29206 34694     6    52 14095  5849 14138
      9  4067   321     4  5339   207    13   272  6243     6   321     4
   6668   207    13 26400   139   493     6   321     4   406   207    13
   2064 22036     8   321     4  2517   207    13  2884   565  6972     6
    286 46563     6    52 14095  5849 14138    11   226  2336     9   132
      4  5607   207    13   272  6243     6   155     4  3103   207    13
  26400   139   493     6   112     4  3083   207    13  2064 22036     8
    112     4  3506   207    13  2884   565  6972     2]]"
dcd8138f0cba0dcd109ccb21c228da5c110a68eb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
76377e5bb7d0a374b0aefc54697ac9cd89d2eba8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By considering words as vertices and generating directed edges between neighboring words within a sentence,"[[    0  2765  2811  1617    25 33566  6355     8 10846  3660 15716   227
  10935  1617   624    10  3645     2]]"
9447ec36e397853c04dcb8f67492ca9f944dbd4b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Italian Wikipedia and Google News extraction producing final vocabulary of 618224 words,"[[    0 39251 28274     8  1204   491 23226  5591   507 32644     9   231
   1366 28835  1617     2]]"
f1bd66bb354e3dabf5dc4a71e6f08b17d472ecc9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],by adding extra supervision to generate the slots that will be present in the response,"[[    0  1409  1271  1823 13702     7  5368     5 20063    14    40    28
   1455    11     5  1263     2]]"
d3bcfcea00dec99fa26283cdd74ba565bc907632,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"133,287 images",[[    0 25037     6 30928  3156     2]]
85d1831c28d3c19c84472589a252e28e9884500f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"QANet BIBREF39, BERT-Base BIBREF26","[[    0  1864  1889   594   163  8863 45935  3416     6   163 18854    12
  34164   163  8863 45935  2481     2]]"
47822fec590e840438a3054b7f512fec09dbd1e1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each","[[    0 23295 20846  5585   504     6  2965  1617     6     8    51    32
  21853  7664   566   209   262 23409    19   158  7201   228   349     2]]"
93b1b94b301a46251695db8194a2536639a22a88,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
5ba6f7f235d0f5d1d01fd97dd5e4d5b0544fd212,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"coverage metric, being distinct (cosine INLINEFORM0 0.7 or 0.8), belonging to the same class (cosine INLINEFORM1 0.7 or 0.8), being equivalent (cosine INLINEFORM2 0.85 or 0.95)","[[    0   876 33943 14823     6   145 11693    36 16254   833  2808 28302
  38036   288   321     4   406    50   321     4   398   238 11441     7
      5   276  1380    36 16254   833  2808 28302 38036   134   321     4
    406    50   321     4   398   238   145  6305    36 16254   833  2808
  28302 38036   176   321     4  4531    50   321     4  4015    43     2]]"
ce18c50dadab7b9f28141fe615fd7de69355d9dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"RDF was designed as an abstract information model and FHIR was designed for operational use in a healthcare setting, RDF makes statements of fact, whereas FHIR makes records of events, RDF is intended to have the property of monotonicity, meaning that previous facts cannot be invalidated by new facts","[[    0   500  9380    21  1887    25    41 20372   335  1421     8   274
    725  5216    21  1887    13  5903   304    11    10  3717  2749     6
    248  9380   817  1997     9   754     6  9641   274   725  5216   817
   2189     9  1061     6   248  9380    16  3833     7    33     5  1038
      9  6154 27334 24414     6  3099    14   986  4905  1395    28 21567
   1070    30    92  4905     2]]"
fac052c4ad6b19a64d7db32fd08df38ad2e22118,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Calinski-Harabasz score, t-SNE, UMAP","[[    0 15117 12166    12 17488   873   281   329  1471     6   326    12
    104  9009     6   121 43861     2]]"
90b2154ec3723f770c74d255ddfcf7972fe136a2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],human evaluation task about the style strength,[[    0 19003 10437  3685    59     5  2496  2707     2]]
42c02c554ab4ceaf30a8ca770be4f271887554c2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Non-English code is a large-scale phenomena., Transliteration is common in identifiers for all languages., Languages clusters into three distinct groups based on how speakers use identifiers/comments/transliteration., Non-latin script users write comments in their L1 script but write identifiers in English., Right-to-left (RTL) language scripts, such as Arabic, have no observed prevalence on GitHub identifiers","[[    0 33239    12 35007  3260    16    10   739    12  8056 32242   482
   5428 19402  1258    16  1537    11 46033    13    70 11991   482 44166
  28255    88   130 11693  1134   716    15   141  6864   304 46033    73
  46788    73  9981 19402  1258   482  6965    12   462 16779  8543  1434
   3116  1450    11    49   226   134  8543    53  3116 46033    11  2370
    482  5143    12   560    12  6960    36 13963   574    43  2777 26878
      6   215    25 19645     6    33   117  6373 21087    15 39097 46033
      2]]"
3334f50fe1796ce0df9dd58540e9c08be5856c23,0.0,Entailment,[[    2     0 30495  3760  1757     2]],to calculate the possible scores of each survey question using PTSD Linguistic Dictionary ,"[[    0   560 15756     5   678  4391     9   349  2658   864   634 24679
    226 35308  5580 41243  1437     2]]"
d32b6ac003cfe6277f8c2eebc7540605a60a3904,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Rank by the number of times a citation is mentioned in the document,  Rank by the number of times the citation is cited in the literature (citation impact). , Rank using Google Scholar Related Articles., Rank by the TF*IDF weighted cosine similarity. , ank using a learning-to-rank model trained on text similarity rankings","[[    0 46052    30     5   346     9   498    10 31825    16  2801    11
      5  3780     6  1437 17816    30     5   346     9   498     5 31825
     16  4418    11     5 13144    36   438 12257   913   322  2156 17816
    634  1204 35553  3283 15219   482 17816    30     5 35690  3226  2688
    597 19099 12793   833 37015     4  2156 44935   634    10  2239    12
    560    12 40081  1421  5389    15  2788 37015  8359     2]]"
75b69eef4a38ec16df63d60be9708a3c44a79c56,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Pearson correlation to human judgement - proposed vs next best metric
Sample level comparison:
- Story generation: 0.387 vs 0.148
- Dialogue: 0.472 vs 0.341
Model level comparison:
- Story generation:  0.631 vs 0.302
- Dialogue: 0.783 vs 0.553","[[    0 42933  1478 22792     7  1050 17219   111  1850  1954   220   275
  14823 50118 47241   672  6676    35 50118    12  3718  2706    35   321
      4 32498  1954   321     4 26300 50118    12 33854    35   321     4
  37401  1954   321     4 34801 50118 45149   672  6676    35 50118    12
   3718  2706    35  1437   321     4   401  2983  1954   321     4 29187
  50118    12 33854    35   321     4 40184  1954   321     4 39252     2]]"
c635dc8013e63505084b9daaa9ddb021a2d24543,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
ba05a53f5563b9dd51cc2db241c6e9418bc00031,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the best permutation is decided by $\mathcal {J}_{\text{SEQ}}(\mathbf {L}_{un}^{(s^{\prime })},\mathbf {L}_{un}^{(r)})$ , which is the sequence discriminative criterion of taking the $s^{\prime }$ -th permutation in $n$ -th output inference stream at utterance $u$","[[    0   627   275 25639 31320    16  1276    30 49959 40051 11762 25522
    863 24303 49747 37457 29015 45152  3388  1864 46961 49921 40051 36920
  25522   574 24303 49747   879 24303 49688  1640    29 35227 49918 28752
  49000 48268 37457 40051 36920 25522   574 24303 49747   879 24303 49688
   1640   338    43 49424  1629  2156    61    16     5 13931 40846   179
   3693 42689     9   602     5    68    29 35227 49918 28752 35524  1629
    111   212 25639 31320    11    68   282  1629   111   212  4195 42752
   4615    23 18672  2389    68   257  1629     2]]"
a53683d1a0647c80a4398ff8f4a03e11c0929be2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"We propose a listening comprehension model for the task defined above, the Attention-based Multi-hop Recurrent Neural Network (AMRNN) framework, and show that this model is able to perform reasonably well for the task. In the proposed approach, the audio of the stories is first transcribed into text by ASR, and the proposed model is developed to process the transcriptions for selecting the correct answer out of 4 choices given the question. ","[[    0   170 15393    10  6288 40494  1421    13     5  3685  6533  1065
      6     5 35798    12   805 19268    12  9547  7382 41937 44304  3658
     36  2620   500 20057    43  7208     6     8   311    14    42  1421
     16   441     7  3008 15646   157    13     5  3685     4    96     5
   1850  1548     6     5  6086     9     5  1652    16    78 27472 33273
     88  2788    30  6015   500     6     8     5  1850  1421    16  2226
      7   609     5 12348  2485    13 18099     5  4577  1948    66     9
    204  5717   576     5   864     4  1437     2]]"
f197e0f61f7980c64a76a3a9657762f1f0edb65b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
aecd09a817c38cf7606e2888d0df7f14e5a74b95,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Ordinal position, Length of sentence, The Ratio of Nouns, The Ratio of Numerical entities, Cue Words, Cosine position, Relative Length, TF-ISF, POS features, Document sentences, Document words, Topical category, Ratio of Verbs, Ratio of Adjectives, and Ratio of Adverbs","[[    0 43670  6204   737     6 41852     9  3645     6    20 20475     9
    234  7928    29     6    20 20475     9   234 26229  3569  8866     6
  28231 27341     6 11013   833   737     6 27490 41852     6 35690    12
   1729   597     6 29206  1575     6 27246 11305     6 27246  1617     6
   3107  3569  4120     6 20475     9  3060  4311     6 20475     9  1614
  21517  3699     6     8 20475     9  1614 44713     2]]"
c54de73b36ab86534d18a295f3711591ce9e1784,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
aa4b38f601cc87bf93849245d5f65124da3dc112,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Title-to-Story system,[[    0 46525    12   560    12 39630   467     2]]
4e50e9965059899d15d3c3a0c0a2d73e0c5802a0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"9710 passages, with an average of 6.24 sentences per passage, 16.16 words per sentence, and an average length of 86 words","[[    0  6750   698 33083     6    19    41   674     9   231     4  1978
  11305   228  9078     6   545     4  1549  1617   228  3645     6     8
     41   674  5933     9  8162  1617     2]]"
87cb19e453cf7e248f24b5f7d1ff9f02d87fc261,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule;, A Nave Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class;","[[    0   250  7838 36866  1421 43668   349   278     9  5018  1617    30
     63   421 37681    11 33183 11303   980     6     8  1380 10687 14198
     88     5  1380     9  8099   421 33183 11303    11 36873  1949   260
   4472   511    10  3793 29459  2178   131     6    83  7300 48350  1501
    293  1421  9857   258  1266     8 37832     6   223     5 15480     9
   5201   566 33183 11303 22735     6    30 12365    10  2340  3854    19
   1266 37681     8 42539 49033  2389 36173     7     5   278     9  5018
   1617     9   349  1380   131     2]]"
4cc5ba404d6a47363f119d9db7266157d3bb246b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],$\textsc {BERT}_{\textsc {BASE}}$ ensemble from BIBREF3,"[[    0  1629 37457 29015  3866 25522 11126   565 24303 49747 37457 29015
   3866 25522   387 24199 46961  1629 12547    31   163  8863 45935   246
      2]]"
b6e4b98fad3681691bcce13f57fb173aee30c592,0.0,Entailment,[[    2     0 30495  3760  1757     2]],similarity is computed as the cosine of the produced $h_{L}$ and $h_{R}$ sentence/image representations,"[[    0 42116  1571    16 43547    25     5 12793   833     9     5  2622
     68   298 49747   574 24303  1629     8    68   298 49747   500 24303
   1629  3645    73 20094 30464     2]]"
63496705fff20c55d4b3d8cdf4786f93e742dd3d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
2b78052314cb730824836ea69bc968df7964b4e4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SQUAD,[[    0   104 15513  2606     2]]
25e4dbc7e211a1ebe02ee8dff675b846fb18fdc5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Raw data from Gigaword, Automatically segmented text from Gigaword, Heterogenous training data from People's Daily, POS data from People's Daily","[[    0 40039   414    31 14448  1584  3109     6  6628 23050  2835   196
   2788    31 14448  1584  3109     6   289  5906 42825  1058   414    31
   1806    18  1681     6 29206   414    31  1806    18  1681     2]]"
bee74e96f2445900e7220bc27795bfe23accd0a7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
2d1c0618b6106a57777b8d6bbf897712d9db7abc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
5b1cd21936aeec85233c978ba8d7282931522a3a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The number of fake news that have a duplicate in the training dataset are 1018 whereas, the number of articles with genuine content that have a duplicate article in the training set is 322.","[[    0   133   346     9  4486   340    14    33    10 33196    11     5
   1058 41616    32  6560   398  9641     6     5   346     9  7201    19
   8916  1383    14    33    10 33196  1566    11     5  1058   278    16
  37244     4     2]]"
6959e87cf2668a03854da3f042c87e6fdb2ade8a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e90ac9ee085dc2a9b6fe132245302bbce5f3f5ab,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
eeaceee98ef1f6c971dac7b0b8930ee8060d71c2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks., Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves.","[[    0 42945  3092     8  8558    35    20  3093    36   281  4435  4113
  34012    43     9  3975 21843    23     5   672     9  2167  3092     8
   8558   482 15581  8135   980    35    20  3093     9  3975 21843    23
      5   672     9  2849  4182 11667     9     5  8135   980     6   215
     25  9100     9  1122 16584     6    50 23429 16584  1235     4     2]]"
f4e1d2276d3fc781b686d2bb44eead73e06fbf3f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Language Modeling,[[    0 46969  7192   154     2]]
2c576072e494ab5598667cd6b40bc97fdd7d92d7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we manually label an in-house dataset of 1,100 users with gender tags, we randomly sample 20,000 tweets for each class from an in-house dataset gold labeled with the same 15 classes as the shared task","[[    0  1694 24704  6929    41    11    12  3138 41616     9   112     6
   1866  1434    19  3959 19445     6    52 22422  7728   291     6   151
   6245    13   349  1380    31    41    11    12  3138 41616  1637 16274
     19     5   276   379  4050    25     5  1373  3685     2]]"
828ce5faed7783297cf9ce202364f999b8d4a1f6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"F-score, micro-F, macro-F, weighted-F ","[[    0   597    12 31673     6  5177    12   597     6 12303    12   597
      6 19099    12   597  1437     2]]"
955ca31999309685c1daa5cb03867971ca99ec52,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WN18, FB15k",[[    0 29722  1366     6 13042   996   330     2]]
97159b8b1ab360c34a1114cd81e8037474bd37db,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
18482658e0756d69e39a77f8fcb5912545a72b9b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
0427ca83d6bf4ec113bc6fec484b2578714ae8ec,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LSTM+attention, Transformer , Universal Transformer","[[    0   574  4014   448  2744  2611 19774     6  5428 22098  2156  9973
   5428 22098     2]]"
b556fd3a9e0cff0b33c63fa1aef3aed825f13e28,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"16 different datasets from several popular review corpora used in BIBREF20, CoNLL 2000 BIBREF22","[[    0  1549   430 42532    31   484  1406  1551 22997   102   341    11
    163  8863 45935   844     6   944   487  6006  3788   163  8863 45935
   2036     2]]"
0a7ac8eccbc286e0ab55bc5949f3f8d2ea2d1a60,0.0,Entailment,[[    2     0 30495  3760  1757     2]],one,[[   0 1264    2]]
08b87a90139968095433f27fc88f571d939cd433,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"As the baseline, we simply judge the input token as IOCs on the basis of the spelling features described in BIBREF12","[[    0  1620     5 18043     6    52  1622  1679     5  8135 19233    25
  16147    29    15     5  1453     9     5 24684  1575  1602    11   163
   8863 45935  1092     2]]"
9219eef636ddb020b9d394868959325562410f83,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BIBREF7, BIBREF39, BIBREF37, LitisMind, Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN","[[    0  5383   387 45935   406     6   163  8863 45935  3416     6   163
   8863 45935  3272     6   226 10100 34875     6 35540 47382     6   208
  20954     6   226  4014   448     6  6479    12   574  4014   448     6
      8  3480     2]]"
54c9147ffd57f1f7238917b013444a9743f0deb8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The sequence model architectures which this method is transferred to are: LSTM and Transformer-based models,"[[    0   133 13931  1421 41885    61    42  5448    16  7225     7    32
     35   226  4014   448     8  5428 22098    12   805  3092     2]]"
0be0c8106df5fde4b544af766ec3d4a3d7a6c8a2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d27f23bcd80b12f6df8e03e65f9b150444925ecf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e477e494fe15a978ff9c0a5f1c88712cdaec0c5c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
50716cc7f589b9b9f3aca806214228b063e9695b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"- Font & Keyboard
- Speech-to-Text
- Text-to-Speech
- Text Prediction
- Spell Checker
- Grammar Checker
- Text Search
- Machine Translation
- Voice to Text Search
- Voice to Speech Search","[[    0    12 15440   359 41132 50118    12 27242    12   560    12 39645
  50118    12 14159    12   560    12 29235  7529 50118    12 14159 27851
  50118    12 38808  4254   254 50118    12 14171  3916  4254   254 50118
     12 14159 12180 50118    12 14969 41737 50118    12 10880     7 14159
  12180 50118    12 10880     7 27242 12180     2]]"
a8f1029f6766bffee38a627477f61457b2d6ed5c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
95abda842c4df95b4c5e84ac7d04942f1250b571,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"multiple language pairs including German-English, French-English, and Japanese-English.","[[    0 34654  2777 15029   217  1859    12 35007     6  1515    12 35007
      6     8  2898    12 35007     4     2]]"
928828544e38fe26c53d81d1b9c70a9fb1cc3feb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"29,500 documents in the CORD-19 corpus (2020-03-13)","[[    0  2890     6  1497  2339    11     5   230 11200    12  1646 42168
     36 24837    12  3933    12  1558    43     2]]"
5845d1db7f819dbadb72e7df69d49c3f424b5730,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Hindi,[[   0  725 2028  118    2]]
44c4bd6decc86f1091b5fc0728873d9324cdde4e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus","[[    0   406 33413 15029     9  1061    58 27380    31     5  2898  6494
  42168     6 37822 19270 15029     9  1061    58 27380    31     5  7224
    510 42168     2]]"
b3857a590fd667ecc282f66d771e5b2773ce9632,0.0,Entailment,[[    2     0 30495  3760  1757     2]],String kernel is a technique that uses character n-grams to measure the similarity of strings,"[[    0 34222 34751    16    10  9205    14  2939  2048   295    12 28526
     29     7  2450     5 37015     9 22052     2]]"
f46a907360d75ad566620e7f6bf7746497b6e4a9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Kyubyong Park, Edouard Grave et al BIBREF11","[[    0 25301  1792   219  1657   861     6  2344  1438  1120 38467  4400
   1076   163  8863 45935  1225     2]]"
baeb6785077931e842079e9d0c9c9040947ffa4e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The E2E NLG challenge dataset BIBREF21,"[[    0   133   381   176   717 12817   534  1539 41616   163  8863 45935
   2146     2]]"
5dfa59c116e0ceb428efd99bab19731aa3df4bbd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Totally 6980 validation and test image-sentence pairs have been corrected.,"[[    0   565 27082  5913  2940 26567     8  1296  2274    12 19530  4086
  15029    33    57 17261     4     2]]"
1e68a1232ab09b6bff506e442acc8ad742972102,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"text-transformations to the messages, vector space model, Support Vector Machine","[[    0 29015    12 44648  1635     7     5  3731     6 37681   980  1421
      6  7737 40419 14969     2]]"
922f1b740f8b13fdc8371e2a275269a44c86195e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
281cb27cfa0eea12180fd82ae33035945476609e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],relations,[[    0 36275     2]]
e5a965e7a109ae17a42dd22eddbf167be47fca75,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Some sentences are associated to ambiguous dimensions in the hidden state output,"[[    0  6323 11305    32  3059     7 33406 22735    11     5  7397   194
   4195     2]]"
3518d8eb84f6228407cfabaf509fd63d60351203,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
76ce9e02d97e2d77fe28c0fa78526809e7c195c6,0.0,Entailment,[[    2     0 30495  3760  1757     2]], MADAMIRA BIBREF6 system,[[    0 27557 17581  4396   163  8863 45935   401   467     2]]
e07df8f613dbd567a35318cd6f6f4cb959f5c82d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],perplexity,[[    0  1741 26028  1571     2]]
0fcac64544842dd06d14151df8c72fc6de5d695c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLSTM+Attention+BLSTM
Hierarchical BLSTM-CRF
CRF-ASN
Hierarchical CNN (window 4)
mLSTM-RNN
DRLM-Conditional
LSTM-Softmax
RCNN
CNN
CRF
LSTM
BERT","[[    0  7976  4014   448  2744 28062 19774  2744  7976  4014   448 50118
    725   906 13161  3569 12413  4014   448    12  9822   597 50118  9822
    597    12  2336   487 50118   725   906 13161  3569  3480    36 42996
    204    43 50118 43619  4014   448    12   500 20057 50118 10644 21672
     12 43597 24176 50118   574  4014   448    12 38805 29459 50118  5199
  20057 50118 16256 50118  9822   597 50118   574  4014   448 50118 11126
    565     2]]"
9d016eb3913b41f7a18c6fa865897c12b5fe0212,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
f7070b2e258beac9b09514be2bfcc5a528cc3a0e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
b622f57c4e429b458978cb8863978d7facab7cfe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Once this classifier has been trained, we can then use it to predict conceptual neighborhood for categories for which only few instances are known.","[[    0 11475    42  1380 24072    34    57  5389     6    52    64   172
    304    24     7  7006 28647  3757    13  6363    13    61   129   367
  10960    32   684     4     2]]"
c5980fe1a0c53bce1502cc674c8a2ed8c311f936,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"3,206",[[    0   246     6 27639     2]]
82a28c1ed7988513d5984f6dcacecb7e90f64792,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The negative effects were insignificant.,[[    0   133  2430  3038    58 33710     4     2]]
a999761aa976458bbc7b4f330764796446d030ff,0.0,Entailment,[[    2     0 30495  3760  1757     2]],cross-lingual NE recognition,[[    0 15329    12  1527  5564 12462  4972     2]]
a33ab5ce8497ff63ca575a80b03e0ed9c6acd273,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
33d1f53cf25a7701db605b6b7ac36946af588bb7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],local businesses (i.e. restaurants),[[    0 18076  1252    36   118     4   242     4  4329    43     2]]
94e0cf44345800ef46a8c7d52902f074a1139e1a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MUC, CoNLL, ACE, OntoNotes, MSM, Ritter, UMBC","[[    0   448 12945     6   944   487  6006     6 36211     6 13302   139
  44691     6 43596     6   248  7915     6 22759  3573     2]]"
e76139c63da0f861c097466983fbe0c94d1d9810,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"No, supervised models perform better for this task.",[[    0  3084     6 20589  3092  3008   357    13    42  3685     4     2]]
e87f47a293e0b49ab8b15fc6633d9ca6dc9de071,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Egyptian (EGY), Levantine (LEV), Gulf (GLF), North African (NOR)","[[    0 37552   811    36  7170   975   238 31500   833    36  3850   846
    238  4602    36 10020   597   238   369  1704    36 25565    43     2]]"
70e596dd4334a94844454fa7b565889556e2358d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],180221 of 231162 author names could be matched successfully,"[[    0 14515 28730     9   883  1225  5379  2730  2523   115    28  9184
   5116     2]]"
2948015c2a5cd6a7f2ad99b4622f7e4278ceb0d4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
f60629c01f99de3f68365833ee115b95a3388699,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"NNC SU4 F1, NNC top 5, Support Vector Classification (SVC)","[[    0 20057   347  9557   306   274   134     6   234  6905   299   195
      6  7737 40419 40509    36   104 14858    43     2]]"
519db0922376ce1e87fcdedaa626d665d9f3e8ce,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
f608fbc7a4a10a79698f340e2948c4c7034642d5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Bi-directional LSTM, self-attention ","[[    0 37426    12 42184   337   226  4014   448     6  1403    12  2611
  19774  1437     2]]"
45306b26447ea4b120655d6bb2e3636079d3d6e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
dee7383a92c78ea49859a2d5ff2a9d0a794c1f0f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the dropout technique of Gal & Ghahramani gal,"[[    0   627  1874   995  9205     9  4537   359  5977   895  4040  1543
  22101     2]]"
234ccc1afcae4890e618ff2a7b06fc1e513ea640,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Data augmentation (es)  improved Adv es by 20% comparing to baseline 
Data augmentation (cs) improved Adv cs by 16.5% comparing to baseline
Data augmentation (cs+es) improved both Adv cs and Adv es by at least 10% comparing to baseline 
All models show improvements over adversarial sets  
","[[    0 30383 26713 36197    36   293    43  1437  2782 17638  2714    30
    291   207 12818     7 18043  1437 50118 30383 26713 36197    36 11365
     43  2782 17638 42123    30   545     4   245   207 12818     7 18043
  50118 30383 26713 36197    36 11365  2744   293    43  2782   258 17638
  42123     8 17638  2714    30    23   513   158   207 12818     7 18043
   1437 50118  3684  3092   311  5139    81 37930 27774  3880  1437  1437
  50118     2]]"
6073fa9050da76eeecd8aa3ccc7ecb16a238d83f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],F1 score,[[   0  597  134 1471    2]]
caf9819be516d2c5a7bfafc80882b07517752dfa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They evaluate quantitatively.,[[    0  1213 10516 24934 43127     4     2]]
3bf429633ecbbfec3d7ffbcfa61fa90440cc918b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],apply an ensemble of deep learning and linguistics t,[[    0 44847    41 12547     9  1844  2239     8 38954 16307   326     2]]
90bc60320584ebba11af980ed92a309f0c1b5507,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative).,"[[    0  1213  6581    92 46681   261 22356 45278    61 12035   335    59
    737  2939   943  5933   335    36 10155    50  5407   322     2]]"
62c4c8b46982c3fcf5d7c78cd24113635e2d7010,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
6f8881e60fdaca7c1b35a5acc7125994bb1206a3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
4a4616e1a9807f32cca9b92ab05e65b05c2a1bf5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Test set 1 contained 57 drug labels and 8208 sentences and test set 2 contained 66 drug labels and 4224 sentences,"[[    0 34603   278   112  5558  4981  1262 14105     8   290 26919 11305
      8  1296   278   132  5558  5138  1262 14105     8  3330  1978 11305
      2]]"
fbe149bd76863575b98fafb3679f411d3d21b4a3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],translations that were reasonable but not consistent with the labels,"[[    0  9981 48111    14    58  5701    53    45  4292    19     5 14105
      2]]"
32e8eda2183bcafbd79b22f757f8f55895a0b7b2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],3,[[  0 246   2]]
5b6aec1b88c9832075cd343f59158078a91f3597,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Proposed SG model vs SINDHI FASTTEXT:
Average cosine similarity score: 0.650 vs 0.388
Average semantic relatedness similarity score between countries and their capitals: 0.663 vs 0.391","[[    0 41895  7878 16324  1421  1954   208 13796 15473   274 10388 49347
     35 50118 46315 12793   833 37015  1471    35   321     4 16316  1954
    321     4 34067 50118 46315 46195  1330  1825 37015  1471   227   749
      8    49 31505    35   321     4 39505  1954   321     4 35848     2]]"
8d4f0815f8a23fe45c298c161fc7a27f3bb0d338,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For different numbers of shared layers, the results are in Table TABREF14. We respectively disable the constituent and the dependency parser to obtain a separate learning setting for both parsers in our model. ","[[    0  2709   430  1530     9  1373 13171     6     5   775    32    11
   9513   255  4546 45935  1570     4   166  4067 33022     5 31350     8
      5 31492 48946     7  6925    10  2559  2239  2749    13   258 28564
    268    11    84  1421     4  1437     2]]"
8f8f2b0046e1a78bd34c0c3d6b6cb24463a8ed7f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English, Chinese",[[    0 35007     6  1111     2]]
0d6d5b6c00551dd0d2519f117ea81d1e9e8785ec,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Google's machine translation system (GMT),[[    0 20441    18  3563 19850   467    36 39621    43     2]]
766e2e35968ef7434b56330aa41957c5d5f8d0ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],970 hours of audio data with corresponding text transcripts (around 10M word tokens) and an additional 800M word token text only dataset,"[[    0 35115   722     9  6086   414    19 12337  2788 20382    36 13837
    158   448  2136 22121    43     8    41   943  5735   448  2136 19233
   2788   129 41616     2]]"
c6aa8a02597fea802890945f0b4be8d631e4d5cd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Semantic similarity structure, Semantic direction structure",[[    0 37504 26970 37015  3184     6 11202 26970  2698  3184     2]]
fa9df782d743ce0ce1a7a5de6a3de226a7e423df,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The languages considered were English, Chinese, German, Russian, Arabic, Spanish, French","[[    0   133 11991  1687    58  2370     6  1111     6  1859     6  1083
      6 19645     6  3453     6  1515     2]]"
593e307d9a9d7361eba49484099c7a8147d3dade,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"networks where nodes represent causes and effects, and directed edges represent cause-effect relationships proposed by humans","[[    0  4135 11655   147 32833  3594  4685     8  3038     6     8  3660
  15716  3594  1303    12 26715  4158  1850    30  5868     2]]"
f129c97a81d81d32633c94111018880a7ffe16d1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Soft attention, Hard Stochastic attention, Local Attention","[[    0 38805  1503     6  6206   312  4306 11599  1503     6  4004 35798
      2]]"
4ce4db7f277a06595014db181342f8cb5cb94626,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"No attacks to any community,  racist, sexist, homophobic, religion based attacks, attacks to other communities","[[    0  3084  1912     7   143   435     6  1437  7159     6 24758     6
  29062     6  6825   716  1912     6  1912     7    97  1822     2]]"
defc17986d3c4aed9eccdbaebda5eb202fbcb6cf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
ce14b87dacfd5206d2a5af7c0ed1cfeb7b181922,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"does not just consider the question tokens, but also the relationship between those tokens and the properties","[[    0 26692    45    95  1701     5   864 22121     6    53    67     5
   1291   227   167 22121     8     5  3611     2]]"
3f9ef59ac06db3f99b8b6f082308610eb2d3626a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"langid.py library, encoder-decoder EquiLID system, GRU neural network LanideNN system, CLD2, CLD3","[[    0 32373   808     4 17163  5560     6  9689 15362    12 11127 15362
   8510   118   574  2688   467     6  8837   791 26739  1546 12046  1949
  20057   467     6  5289   495   176     6  5289   495   246     2]]"
8d258899e36326183899ebc67aeb4188a86f682c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"$ f_r(h, t) & = & \Vert \textbf {W}_{r,1}\textbf {h} + \textbf {r} - \textbf {W}_{r,2}\textbf {t}\Vert _{\ell _{1/2}} $","[[    0  1629   856  1215   338  1640   298     6   326    43   359  5457
    359 44128 42195 44128 29015 36920 25522   771 24303 49747   338     6
    134 49712 29015 36920 25522   298 24303  2055 44128 29015 36920 25522
    338 24303   111 44128 29015 36920 25522   771 24303 49747   338     6
    176 49712 29015 36920 25522    90 49712 42195 18134 49918  1641 18134
  45152   134    73   176 46961    68     2]]"
dc69256bdfe76fa30ce4404b697f1bedfd6125fe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Hindi, English and German (German task won)","[[    0   725  2028   118     6  2370     8  1859    36 27709  3685   351
     43     2]]"
dad8cc543a87534751f9f9e308787e1af06f0627,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"AIDA-CoNLL, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI, OURSELF-WIKI","[[    0   250 28887    12  8739   487  6006     6 36211 34972     6 20054
      6    83 15513   250 17831     6   305   487  1691    12   347  9112
    387     6   305   487  1691    12   771 20458   100     6 24975   104
  29734    12   771 20458   100     2]]"
bb8a0035b767688a98602c33f4714f8ac8ae60db,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ROUGE-1, ROUGE-2 and ROUGE-L, F-measure ROUGE on XSUM and CNN/DailyMail, and use limited-length recall-measure ROUGE on NYT and DUC","[[    0   500  5061  8800    12   134     6   248  5061  8800    12   176
      8   248  5061  8800    12   574     6   274    12  1794 24669   248
   5061  8800    15  1577   104  5725     8  3480    73 26339 23969     6
      8   304  1804    12 16096  6001    12  1794 24669   248  5061  8800
     15 36319     8   211 12945     2]]"
4c1847f0f3e6f9cc6ac3dfbac9e135d34641a854,0.0,Entailment,[[    2     0 30495  3760  1757     2]],JavaScript,[[    0 32379 30276     2]]
c47e87efab11f661993a14cf2d7506be641375e4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Answer with content missing: (formula for CIC) it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities,"[[    0 33683    19  1383  1716    35    36  3899  5571    13   230  2371
     43    24  2349    13     5   144   505   335   624   349 25730 11170
      4   230  2371    64    28  5049     7   143 39186  1938  3685    19
  12574 38716  4499  8866     2]]"
fe90eec1e3cdaa41d2da55864c86f6b6f042a56c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"User reviews written in Chinese collected online for hotel, mobile phone, and travel domains","[[    0 44518  6173  1982    11  1111  4786   804    13  2303     6  1830
   1028     6     8  1504 30700     2]]"
fa3312ae4bbed11a5bebd77caf15d651962e0b26,0.0,Entailment,[[    2     0 30495  3760  1757     2]],F1 scores of 86.16 on slot filling and 94.56 on intent detection,"[[    0   597   134  4391     9  8162     4  1549    15  8534  8794     8
   8940     4  4419    15  5927 12673     2]]"
cc28919313f897358ef864948c65318dc61cb03c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"string kernels, SST, KE-Meta, SFA, CORAL, TR-TrAdaBoost, Transductive string kernels, transductive kernel classifier","[[    0 20951 45256     6   208  4014     6 37112    12 48454     6   208
   5944     6 19328  2118     6  5758    12 12667  9167   102 43389     6
   5428 43518  2088  6755 45256     6  6214 43518  2088 34751  1380 24072
      2]]"
6389d5a152151fb05aae00b53b521c117d7b5e54,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Semantic Enhancement GANs: DC-GANs, MC-GAN
Resolution Enhancement GANs: StackGANs, AttnGAN, HDGAN
Diversity Enhancement GANs: AC-GAN, TAC-GAN etc.
Motion Enhancement GAGs: T2S, T2V, StoryGAN","[[    0 37504 26970 42764   272  1889    29    35  5815    12 38416    29
      6 11826    12 38416 50118 20028 23794 42764   272  1889    29    35
  31197 38416    29     6  7279   282 38416     6  7951 38416 50118   495
  31104 42764   272  1889    29    35  7224    12 38416     6   255  2562
     12 38416  4753     4 50118 44088 42764   272  3450    29    35   255
    176   104     6   255   176   846     6  3718 38416     2]]"
bd78483a746fda4805a7678286f82d9621bc45cf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26,"[[    0  4897    12  1116    12   627    12  2013   864 15635  3092    36
    118     4   242     4  1209  1889   594   163  8863 45935  3416    43
      8   163 18854    12 34164   163  8863 45935  2481     2]]"
8cc56fc44136498471754186cfa04056017b4e54,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Proposed model is better than both lexical based models by significan margin in all metrics: BLEU 0.261 vs 0.250, ROUGLE 0.162 vs 0.155 etc.","[[    0 41895  7878  1421    16   357    87   258 36912  3569   716  3092
     30 40042   260  2759    11    70 12758    35   163  3850   791   321
      4 32972  1954   321     4  5714     6   248  5061   534  3850   321
      4 29899  1954   321     4 21403  4753     4     2]]"
1bc8904118eb87fa5949ad7ce5b28ad3b3082bd0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Twitter,[[    0 22838     2]]
b6f5860fc4a9a763ddc5edaf6d8df0eb52125c9e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English, Chinese, French, Japanese and Arabic",[[    0 35007     6  1111     6  1515     6  2898     8 19645     2]]
50e3fd6778dadf8ec0ff589aa8b18c61bdcacd41,0.0,Entailment,[[    2     0 30495  3760  1757     2]],WikiText-TL-39,[[    0 46929 39645    12 22290    12  3416     2]]
d94ac550dfdb9e4bbe04392156065c072b9d75e1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
04495845251b387335bf2e77e2c423130f43c7d9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
1951cde612751410355610074c3c69cec94824c2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CNN,[[    0 16256     2]]
9213159f874b3bdd9b4de956a88c703aac988411,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
bf00808353eec22b4801c922cce7b1ec0ff3b777,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
238ec3c1e1093ce2f5122ee60209b969f7669fae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:
1) Setting N, the size of the neighbor.
2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.
3) Computing the surrounding uniformity for ai(0 < i  N) and w.
4) Computing the mean m and the sample variance  for the uniformities of ai .
5) Checking whether the uniformity of w is less than m  3. If the value is less than m  3, we may regard w as a polysemic word.","[[    0  2522  5448 14023    10 17325  1296     7  3094   549    10   576
   2136    16   341 11424 26976  9412    11     5  2788     6   309     7
      5   511  2402    35 50118   134    43 32048   234     6     5  1836
      9     5  8523     4 50118   176    43 11501 10174   234 10935  1617
     10   118    11     5   645  1060 11792    19     5 37681     9     5
    576  2136   885    16     5 15654     4 50118   246    43 34572     5
   3817  8284  1571    13    10   118  1640   288 28696   939 48623   234
     43     8   885     4 50118   306    43 34572     5  1266   475     8
      5  7728 37832 46927   862    13     5  8284  2192     9    10   118
    479 50118   245    43 44237   549     5  8284  1571     9   885    16
    540    87   475 42736   155 47721     4   318     5   923    16   540
     87   475 42736   155 47721     6    52   189  6203   885    25    10
  11424  1090 15796  2136     4     2]]"
7348e781b2c3755b33df33f4f0cab4b94fcbeb9b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Only automatic methods,[[    0 19933  8408  6448     2]]
63850ac98a47ae49f0f49c1c1a6e45c6c447272c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (whole introduction) However, recent
studies observe the limits of ROUGE and find in
some cases, it fails to reach consensus with human.
judgment (Paulus et al., 2017; Schluter, 2017).","[[    0 33683    19  1383  1716    35    36 11613  4104  7740    43   635
      6   485 50118 26302   918 14095     5  4971     9   248  5061  8800
      8   465    11 50118 12465  1200     6    24 10578     7  1338  2899
     19  1050     4 50118 12864 10757    36 12083   687  4400  1076   482
    193   131 21351 13080     6   193   322     2]]"
b7708cbb50085eb41e306bd2248f1515a5ebada8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],These are well-known formal languages some of which was used in the literature to evaluate the learning capabilities of RNNs.,"[[    0  4528    32   157    12  6421  4828 11991   103     9    61    21
    341    11     5 13144     7 10516     5  2239  5587     9   248 20057
     29     4     2]]"
4e379d6d5f87554fabf6f7f7b6ed92d2025e7280,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CSKS task,[[    0  6842 18307  3685     2]]
515e10a71d78ccd9c7dc93cd942924a4c85d3a30,0.0,Entailment,[[    2     0 30495  3760  1757     2]],perplexity of the models,[[    0  1741 26028  1571     9     5  3092     2]]
62ba1fefc1eb826fe0cbac092d37a3e2098967e9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"random method , LSTM ",[[    0 45041  5448  2156   226  4014   448  1437     2]]
ab9453fa2b927c97b60b06aeda944ac5c1bfef1e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Sequence Copy Task and WMT'17,[[    0 48245  4086 22279 12927     8   305 11674   108  1360     2]]
7fdeef2b1c8f6bd5d7c3a44e533d8aae2bbc155f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],tweets about `ObamaCare' in USA collected during march 2010,"[[    0    90  1694  2580    59 22209 33382 16431   108    11  2805  4786
    148  6674  1824     2]]"
cda4612b4bda3538d19f4b43dde7bc30c1eda4e5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"TextRank, Word2vec BIBREF19, GloVe BIBREF20","[[    0 39645 46052     6 15690   176 25369   163  8863 45935  1646     6
   4573   139 30660   163  8863 45935   844     2]]"
c38a48d65bb21c314194090d0cc3f1a45c549dd6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Conll, Weblogs, Newsgroups, Reviews, Answers","[[    0  9157   890     6   166 45848     6   491 36378     6 35769     6
  41034     2]]"
fb427239c8d44f524a6c1bf1ce5c3383d5c33e52,0.0,Entailment,[[    2     0 30495  3760  1757     2]],model has around 836M parameters,[[    0 21818    34   198   290  3367   448 17294     2]]
c2432884287dca4af355698a543bc0db67a8c091,0.0,Entailment,[[    2     0 30495  3760  1757     2]],number of relevant output words as a function of the headlines category label,"[[    0 30695     9  4249  4195  1617    25    10  5043     9     5  8707
     17    27    29  4120  6929     2]]"
da495e2f99ee2d5db9cc17eca5517ddaa5ea8e42,0.0,Entailment,[[    2     0 30495  3760  1757     2]],LDC corpus,[[    0   574  5949 42168     2]]
2419b38624201d678c530eba877c0c016cccd49f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a6717e334c53ebbb87e5ef878a77ef46866e3aed,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
946676f1a836ea2d6fe98cb4cfc26b9f4f81984d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
aaf50a6a9f449389ef212d25d0fae59c10b0df92,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"news publications, wine reviews, and Reddit",[[    0  2926 16043     6  3984  6173     6     8  6844     2]]
ca26cfcc755f9d0641db0e4d88b4109b903dbb26,0.0,Entailment,[[    2     0 30495  3760  1757     2]],F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61.,"[[    0   597   134  1471     9   275  7601   108  1421    16  3490     4
   5208  1118     7  6479   574  4014   448     8  9612 39645    14    33
    274   134  1471  3369  8774   462  2553   723    87  4059     4  5606
      4     2]]"
40a45d59a2ef7a67c8ab0f2b2d5b43fc85b85498,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
cf74ff49dfcdda2cd67a896b4b982a1c3ee51531,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Nigeria, Benin, Ghana, Cameroon, Togo, Cte d'Ivoire, Chad, Burkina Faso, and Sudan, Republic of Togo, Ghana, Cte d'Ivoire, Sierra Leone, Cuba and Brazil","[[    0   487  1023  6971     6  1664   179     6  5498     6 14781     6
    255 14744     6   230 10456   859   385   108   100  5766  1885     6
   7999     6 18294  1243 21433   139     6     8  6312     6  3497     9
    255 14744     6  5498     6   230 10456   859   385   108   100  5766
   1885     6  8682 16910     6  8455     8  2910     2]]"
adae0c32a69928929101d0ba37d36c0a45298ad6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Support Vector Machines (SVMs), logistic regression, Nave Bayes, Perceptron, and decision trees, a two-layer bidirectional Gated Recurrent Neural Network (GRNN) BIBREF20 and Convolutional Neural Network (CNN) (as designed in BIBREF21)","[[    0 38873 40419 28413    36   104   846 13123   238  7425  5580 39974
      6  7300 48350  1501   293     6  2595 16771  2839     6     8   568
   3980     6    10    80    12 39165  2311 43606   337   272  1070  7382
  41937 44304  3658    36 11621 20057    43   163  8863 45935   844     8
  30505 23794   337 44304  3658    36 16256    43    36   281  1887    11
    163  8863 45935  2146    43     2]]"
9eb5b336b3dcb7ab63f673ba9ab1818573cce6c3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1.1 million sentences, 119 different relation types (unique predicates)","[[    0   134     4   134   153 11305     6 16491   430  9355  3505    36
  34755 12574 23020    43     2]]"
6608f171b3e0dcdcd51b3e0c697d6e5003ab5f02,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"adjective and adverb patterns, verb, subject, and object arguments, verbal patterns","[[    0   625 21517  2088     8  2329 37644  8117     6 33760     6  2087
      6     8  7626  7576     6 14580  8117     2]]"
936878cff0e6e327b2554ee5d46686797ee92cf2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
68ff2a14e6f0e115ef12c213cf852a35a4d73863,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The dataset contains about 590 tweets about DDos attacks.,"[[    0   133 41616  6308    59   195  3248  6245    59 27932   366  1912
      4     2]]"
34dd0ee1374a3afd16cf8b0c803f4ef4c6fec8ac,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"standard CNN, C-CNN, MVCNN ","[[    0 21993  3480     6   230    12 16256     6   256 14858 20057  1437
      2]]"
5499527beadb7f5dd908bd659cad83d6a81119bd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Wiktionary, Oxford Dictionary of English Idioms, UsingEnglish.com (UE), Sporleder corpus, VNC dataset, SemEval-2013 Task 5 dataset","[[    0 45445 24659  1766     6  9238 41243     9  2370 10367   118  6806
      6  8630 35007     4   175    36  9162   238  2064   368  1329   254
  42168     6   468  6905 41616     6 11202   717  6486    12 10684 12927
    195 41616     2]]"
63a77d2640df8315bf0bc3925fdd7e27132b1244,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
085147cd32153d46dd9901ab0f9195bfdbff6a85,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MC-CNN
MVCNN
CNN",[[    0  6018    12 16256 50118   448 14858 20057 50118 16256     2]]
54a2c08aa55c3db9b30ae2922c96528d3f4fc733,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ontology-based knowledge tree, heuristics-based, n-grams model","[[    0  2533  4383    12   805  2655  3907     6    37   710 16307    12
    805     6   295    12 28526    29  1421     2]]"
a91878129583fcb6d16067ba8ba3600e39d70021,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"k-means, hierarchical clustering with Ward's method for merging clusters BIBREF23","[[    0   330    12  1794  1253     6 44816 46644  2961    19  5986    18
   5448    13 29002 28255   163  8863 45935  1922     2]]"
99cf494714c67723692ad1279132212db29295f3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"AQA diverges from well structured language in favour of less fluent, but more effective, classic information retrieval (IR) query operations","[[    0   250  1864   250 13105  5641    31   157 16697  2777    11  5976
      9   540 34574     6    53    55  2375     6  4187   335 43372    36
   5216    43 25860  1414     2]]"
82fa2b99daa981fc42a882bb6db8481bdbbb9675,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
0bd864f83626a0c60f5e96b73fb269607afc7c09,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BERT generates sentence embeddings that represent words in context. These sentence embeddings are merged into a single conversational-context vector that is used to calculate a gated embedding and is later combined with the output of the decoder h to provide the gated activations for the next hidden layer.,"[[    0 11126   565 17382  3645 33183   417  1033    14  3594  1617    11
   5377     4  1216  3645 33183   417  1033    32 21379    88    10   881
  28726  5033    12 46796 37681    14    16   341     7 15756    10   821
   1070 33183 11303     8    16   423  2771    19     5  4195     9     5
   5044 15362  1368     7   694     5   821  1070 30264  1635    13     5
    220  7397 10490     4     2]]"
db9021ddd4593f6fadf172710468e2fdcea99674,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d27e3a099954e917b6491e81b2e907478d7f1233,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
dd8f72cb3c0961b5ca1413697a00529ba60571fe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"there are several related passages for each question in the MS-MARCO dataset., MS-MARCO also annotates which passage is correct","[[    0  8585    32   484  1330 33083    13   349   864    11     5  6253
     12 36659  6335 41616   482  6253    12 36659  6335    67 45068  1626
     61  9078    16  4577     2]]"
046ff04d1018447b22e00acb125125cae5a23fb7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"small_parallel_enja, Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF5","[[    0 23115  1215  5489 44682  1215   225  1910     6  3102 17141 14479
   3015 39406 28556    36  2336   510  3586    43   163  8863 45935   245
      2]]"
afdad4c9bdebf88630262f1a9a86ac494f06c4c1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the new DMN+ model does not require that supporting facts (i.e. the facts that are relevant for answering a particular question) are labeled during training., In addition, we introduce a new input module to represent images.","[[    0   627    92 18695   487  2744  1421   473    45  2703    14  3117
   4905    36   118     4   242     4     5  4905    14    32  4249    13
  15635    10  1989   864    43    32 16274   148  1058   482    96  1285
      6    52  6581    10    92  8135 20686     7  3594  3156     4     2]]"
f1831b2e96ff8ef65b8fde8b4c2ee3e04b7ac4bf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"NMI between cluster assignments and ground truth tones for all sylables is:
Mandarin: 0.641
Cantonese: 0.464","[[    0   487  7539   227 18016 21350     8  1255  3157 23225    13    70
  13550   462  6058    16    35 50118 37136 15394    35   321     4 39138
  50118   347   927  6909   242    35   321     4 38888     2]]"
f64531e460e0ac09b58584047b7616fdb7dd5b3f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
cafa6103e609acaf08274a2f6d8686475c6b8723,0.0,Entailment,[[    2     0 30495  3760  1757     2]],improves the DAR accuracy over Bi-LSTM-CRF by 2.1% and 0.8% on SwDA and MRDA respectively,"[[    0 33491  3677     5 37268  8611    81  6479    12   574  4014   448
     12  9822   597    30   132     4   134   207     8   321     4   398
    207    15  3323  3134     8 18838  3134  4067     2]]"
551a17fc1d5b5c3d18bdc4923363cbbda7eb2516,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
2869d19e54fb554fcf1d6888e526135803bb7d75,0.0,Entailment,[[    2     0 30495  3760  1757     2]],F1 score of 82.10%,[[   0  597  134 1471    9 7383    4  698  207    2]]
869aaf397c9b4da7ab52d6dd0961887ae08da9ae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Bengali, English, German, Spanish, Dutch, Amharic, Arabic, Hindi, Somali ","[[    0   387  3314  3644     6  2370     6  1859     6  3453     6  5979
      6  1918  4759   636     6 19645     6 19840     6 21271  1437     2]]"
eacd7e540cc34cb45770fcba463f4bf968681d59,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
07c59824f5e7c5399d15491da3543905cfa5f751,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"4,261  days for France and 4,748 for the UK","[[    0   306     6 32972  1437   360    13  1470     8   204     6 39373
     13     5   987     2]]"
b99948ac4810a7fe3477f6591b8cf211d6398e67,0.0,Entailment,[[    2     0 30495  3760  1757     2]],five,[[   0 9579    2]]
d0444cbf01efdcc247b313c7487120a2f047f421,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
7cdce4222cea6955b656c1a3df1129bb8119e2d0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"decision trees to predict individual hidden state dimensions, apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters","[[    0 11127 10699  3980     7  7006  1736  7397   194 22735     6  3253
    449    12  1794  1253 46644  2961     7     5   226  4014   448   194
  44493     6     8  3195    12 20414     5  1058   414    19     5 28255
      2]]"
39c78924df095c92e058ffa5a779de597e8c43f4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Using Latent Dirichlet Allocation on TF-IDF transformed from the corpus,"[[    0 36949  9882  1342 32024  1725  2716   404 15644    15 35690    12
   2688   597 11229    31     5 42168     2]]"
9f2634c142dc4ad2c68135dbb393ecdfd23af13f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we obtain 52,053 dialogues and 460,358 utterances","[[    0  1694  6925  3135     6  2546   246 25730  3663     8 33888     6
  34392 18672  5332     2]]"
46bca122a87269b20e252838407a2f88f644ded8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
fbb85cbd41de6d2818e77e8f8d4b91e431931faa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],asked the authors to rank by closeness five citations we selected from their paper,"[[    0  4970   196     5  7601     7  7938    30 20956 14186   292 31173
     52  3919    31    49  2225     2]]"
7b89515d731d04dd5cbfe9c2ace2eb905c119cbc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The three baseline models are the i-vector model, a standard RNN LID system and a multi-task RNN LID system. ","[[    0   133   130 18043  3092    32     5   939    12 48219  1421     6
     10  2526   248 20057   226  2688   467     8    10  3228    12 45025
    248 20057   226  2688   467     4  1437     2]]"
9fd137bf7eabaf8bc234a18b6ea34471cf4a3b95,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"trained using Nematus, default configuration",[[    0 23830   634 22492 29913     6  6814 20393     2]]
93f4ad6568207c9bd10d712a52f8de25b3ebadd4,0.0,Entailment,[[    2     0 30495  3760  1757     2]], the average specificity of all utterances,[[    0     5   674 42561     9    70 18672  5332     2]]
bfad30f51ce3deea8a178944fa4c6e8acdd83a48,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"three main components, namely data processing, task processing, and query processing","[[    0  9983  1049  6411     6 13953   414  5774     6  3685  5774     6
      8 25860  5774     2]]"
384d571e4017628ebb72f3debb2846efaf0cb0cb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Aristo Corpus
Regents 4th
Regents 8th
Regents `12th
ARC-Easy
ARC-challenge ","[[    0   250 11516   139 28556 50118 23007  4189   204   212 50118 23007
   4189   290   212 50118 23007  4189 22209  1092   212 50118 11969    12
  43361 50118 11969    12 25324 20526  1437     2]]"
7eb3852677e9d1fb25327ba014d2ed292184210c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"from YouTube videos, with associated transcripts obtained from semi-supervised caption filtering, from a Voice Search service","[[    0  7761  4037  3424     6    19  3059 20382  4756    31  4126    12
  16101 25376  3747 35060     6    31    10 10880 12180   544     2]]"
6adfa9eee76b96953a76c03356bf41d8a9378851,0.0,Entailment,[[    2     0 30495  3760  1757     2]],1.6% lower phone error rate on average,[[   0  134    4  401  207  795 1028 5849  731   15  674    2]]
30c6d34b878630736f819fd898319ac4e71ee50b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a891039441e008f1fd0a227dbed003f76c140737,0.0,Entailment,[[    2     0 30495  3760  1757     2]],machine comprehension,[[    0 37556 40494     2]]
e96adf8466e67bd19f345578d5a6dc68fd0279a1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],unsupervised ,[[    0   879 16101 25376  1437     2]]
b217d9730ba469f48426280945dbb77542b39183,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Caravel, COAV and NNCD",[[    0   347  1742  5536     6  6247 10612     8   234  6905   495     2]]
b44ce9aae8b1479820555b99ce234443168dc1fe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MultiUN BIBREF20, OpenSubtitles BIBREF21","[[    0 46064  4154   163  8863 45935   844     6  2117 23055    90 46913
    163  8863 45935  2146     2]]"
96992460cfc5f0b8d065ee427067147293746b7a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"F1, accuracy",[[   0  597  134    6 8611    2]]
e574f0f733fb98ecef3c64044004aa7a320439be,0.0,Entailment,[[    2     0 30495  3760  1757     2]],DISPLAYFORM0,[[    0 37056 43784 38036   288     2]]
6c8bd7fa1cfb1b2bbeb011cc9c712dceac0c8f06,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"word embedding, input encoder, alignment, aggregation, and prediction.","[[    0 14742 33183 11303     6  8135  9689 15362     6 22432     6 40796
      6     8 16782     4     2]]"
5fa431b14732b3c47ab6eec373f51f2bca04f614,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"TF-IDF, NVDM",[[    0 20249    12  2688   597     6   234 28732   448     2]]
c45feda62f23245f53e855706e2d8ea733b7fd03,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Attention-based translation model with convolution sequence to sequence model,"[[    0 28062 19774    12   805 19850  1421    19 15380 23794 13931     7
  13931  1421     2]]"
cb8a6f5c29715619a137e21b54b29e9dd48dad7d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],well-formed sentences vs concise answers,[[    0  3056    12 10312 11305  1954  6561  5274     2]]
271019168ed3a2b0ef5e3780b48a1ebefc562b57,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Bi-LSTM: For low resource <17k clean data: Using distant supervision resulted in huge boost of F1 score (1k eg. ~9 to ~36 wit distant supervision)
BERT: <5k clean data boost of F1 (1k eg. ~32 to ~47 with distant supervision)","[[    0 37426    12   574  4014   448    35   286   614  5799 28696  1360
    330  2382   414    35  8630 13258 13702  4596    11  1307  2501     9
    274   134  1471    36   134   330 29230     4 14434   466     7 14434
   3367 22094 13258 13702    43 50118 11126   565    35 28696   245   330
   2382   414  2501     9   274   134    36   134   330 29230     4 14434
   2881     7 14434  3706    19 13258 13702    43     2]]"
4d4739682d540878a94d8227412e9e1ec1bb3d39,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"2014 i2b2 de-identification challenge data set BIBREF2, nursing notes corpus BIBREF3","[[    0 16310   939   176   428   176   263    12  8009  5000  1539   414
    278   163  8863 45935   176     6  8701  2775 42168   163  8863 45935
    246     2]]"
056fc821d1ec1e8ca5dc958d14ea389857b1a299,0.0,Entailment,[[    2     0 30495  3760  1757     2]],3 feature maps for a given tuple,[[    0   246  1905 12879    13    10   576 49683     2]]
e39d90b8d959697d9780eddce3a343e60543be65,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"widely used method for classifying misleading content is to use distant annotations, for example to classify a tweet based on the domain of a URL that is shared by the tweet, or a hashtag that is contained in the tweet, Natural Language Processing (NLP) models can be used to automatically label text content","[[    0  6445   352   341  5448    13  1380  4945 12030  1383    16     7
    304 13258 47234     6    13  1246     7 36029    10  3545   716    15
      5 11170     9    10 33000    14    16  1373    30     5  3545     6
     50    10 15493    14    16  5558    11     5  3545     6  7278 22205
  28395    36   487 21992    43  3092    64    28   341     7  6885  6929
   2788  1383     2]]"
003f884d3893532f8c302431c9f70be6f64d9be8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
04a4b0c6c8bd4c170c93ea7ea1bf693965ef38f4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
92d09654011d424cfef5691eec28ee934f88d954,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"clear mapping between plans and text helps to reduce these issues greatly, the system in BIBREF0 still has 2% errors, work in neural text generation and summarization attempt to address these issues","[[    0 18763 20410   227   708     8  2788  2607     7  1888   209   743
   8908     6     5   467    11   163  8863 45935   288   202    34   132
    207  9126     6   173    11 26739  2788  2706     8 39186  1938  2120
      7  1100   209   743     2]]"
2cf8825639164a842c3172af039ff079a8448592,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The data are self-reported by Twitter users and then verified by two human experts.,"[[    0   133   414    32  1403    12 28422    30   599  1434     8   172
  13031    30    80  1050  2320     4     2]]"
4146e1d8f79902c0bc034695998b724515b6ac81,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CoNLL-2012 shared task BIBREF21 corpus,"[[    0  8739   487  6006    12 14517  1373  3685   163  8863 45935  2146
  42168     2]]"
9dd65dca9dffd2bf78ecc22b17824edc885d1fa2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
12eba1598dca14db64dbc8b73484639363a4618e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"word unigrams, bigrams, and trigrams","[[    0 14742   542  1023 27809     6   380 27809     6     8 46681 27809
      2]]"
65b579b2c62982e2ff154c8160288c2950d509f2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MRPC, STS-B, SST-2, QQP, RTE, QNLI, MNLI","[[    0 12642  4794     6  4062   104    12   387     6   208  4014    12
    176     6  1209  1864   510     6   248  6433     6  1209   487 27049
      6 18138 27049     2]]"
d2b3f2178a177183b1aeb88784e48ff7e3e5070c,0.0,Entailment,[[    2     0 30495  3760  1757     2]], between 0.81 and 0.88,[[   0  227  321    4 6668    8  321    4 4652    2]]
ef4d6c9416e45301ea1a4d550b7c381f377cacd9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"standard linguistic features, such as Part-Of-Speech (POS) and chunk tag, series of features representing tokens' left and right context","[[    0 21993 39608  1575     6   215    25  4657    12 10643    12 29235
   7529    36 42740    43     8 15836  6694     6   651     9  1575  4561
  22121   108   314     8   235  5377     2]]"
57e783f00f594e08e43a31939aedb235c9d5a102,0.0,Entailment,[[    2     0 30495  3760  1757     2]],AUC-ROC,[[    0   250 12945    12   500  4571     2]]
b807dd3d42251615b881632caa5e331e2203d269,0.0,Entailment,[[    2     0 30495  3760  1757     2]],GANN obtained the state-of-the-art APC performance on the Chinese review datasets,"[[    0   534 15118  4756     5   194    12  1116    12   627    12  2013
   1480   347   819    15     5  1111  1551 42532     2]]"
b48cd91219429f910b1ea6fcd6f4bd143ddf096f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU, Distinct-1 & distinct-2","[[    0 30876   791     6 11281 24115    12   134   359 11693    12   176
      2]]"
7cc22fd8c9d0e1ce5e86d0cbe90bf3a177f22a68,0.0,Entailment,[[    2     0 30495  3760  1757     2]],1000 conversations composed of 6833 sentences and 88047 tokens,"[[    0 20078  5475 14092     9  5595  3103 11305     8   290  2940  3706
  22121     2]]"
382bef47d316d7c12ea190ae160bf0912a0f4ffe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Manual verification,[[    0  6407  5564 14925     2]]
d6e8b32048ff83c052e978ff3b8f1cb097377786,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By annotators on Amazon Mechanical Turk.,[[    0  2765 45068  3629    15  1645 35644 19683     4     2]]
abad9beb7295d809d7e5e1407cbf673c9ffffd19,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
48cf360a7753a23342f53f116eeccc2014bcc8eb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
818c89b11471a6ca4f13c838713864fdf282c2ca,0.0,Entailment,[[    2     0 30495  3760  1757     2]],LSTM-LM ,[[    0   574  4014   448    12 21672  1437     2]]
16646ee77975fed372b76ce639e2664ae2105dcf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
3c3b4797e2b21e2c31cf117ad9e52f327721790f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English, German, Spanish, Italian, Japanese and Portuguese,  English, Arabic and Chinese","[[    0 35007     6  1859     6  3453     6  3108     6  2898     8 13053
      6  1437  2370     6 19645     8  1111     2]]"
d60a3887a0d434abc0861637bbcd9ad0c596caf4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],rules that compute polarity of words after POS tagging or parsing steps,"[[    0 43915    14 37357  8385 21528     9  1617    71 29206 34694    50
  46563  2402     2]]"
d028dcef22cdf0e86f62455d083581d025db1955,0.0,Entailment,[[    2     0 30495  3760  1757     2]],optimize single task with no synthetic data,[[    0 35692  2072   881  3685    19   117 16807   414     2]]
0747cecb3c72594c5d15ba18490566be1ffdbfad,0.0,Entailment,[[    2     0 30495  3760  1757     2]], strong competitive methods on the SQuAD leaderboard and TriviaQA leaderboard,"[[    0   670  2695  6448    15     5   208 12444  2606   884  4929     8
   6892 11409  1864   250   884  4929     2]]"
9cbea686732b5b85f77868ca47d2f93cf34516ed,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we extract the emotion information from the utterances in $\mathbf {X}$ by leveraging an external text analysis program, and use an RNN to encode it into an emotion context vector $\mathbf {e}$, which is combined with $\mathbf {c}_t$ to produce the distribution","[[    0  1694 14660     5 11926   335    31     5 18672  5332    11 49959
  40051 36920 25522  1000 24303  1629    30 21221    41  6731  2788  1966
    586     6     8   304    41   248 20057     7 46855    24    88    41
  11926  5377 37681 49959 40051 36920 25522   242 24303 47110    61    16
   2771    19 49959 40051 36920 25522   438 24303  1215    90  1629     7
   2592     5  3854     2]]"
710fa8b3e74ee63d2acc20af19f95f7702b7ce5e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],WordDecoding (WDec) model,[[    0 44051 15953 19519    36   771 15953    43  1421     2]]
bd7039f81a5417474efa36f703ebddcf51835254,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Reasoner model, also implemented with the MatchLSTM architecture, Ranker model","[[    0 48147   254  1421     6    67  6264    19     5  9018   574  4014
    448  9437     6 17816   254  1421     2]]"
b5484a0f03d63d091398d3ce4f841a45062438a7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"proposed method comprises of two steps: a neighbourhood reconstruction step (Section ""Nearest Neighbour Reconstruction"" ), and a projection step (Section ""Projection to Meta-Embedding Space"" ). In the reconstruction step, we represent the embedding of a word by the linearly weighted combination of the embeddings of its nearest neighbours in each source embedding space. ","[[    0 27128  7878  5448 16755     9    80  2402    35    10 12258 18228
   1149    36 43480    22 14563 18759 16853 22482 35411   113 31311     8
     10 18144  1149    36 43480    22 33347  1499     7 37622    12 42578
  13093   154  5374   113 32801    96     5 18228  1149     6    52  3594
      5 33183 11303     9    10  2136    30     5 24248 23099 19099  4069
      9     5 33183   417  1033     9    63 14712 10689    11   349  1300
  33183 11303   980     4  1437     2]]"
879bec20c0fdfda952444018e9435f91e34d8788,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
bfd4fc82ffdc5b2b32c37f4222e878106421ce2a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the input fusion layer to allow interactions between input facts and a novel attention based GRU that allows for logical reasoning over ordered inputs. ,"[[    0   627  8135 24904 10490     7  1157 11324   227  8135  4905     8
     10  5808  1503   716  8837   791    14  2386    13 16437 20511    81
   2740 16584     4  1437     2]]"
5cb3d69607f60e1c5be2120462726a477ead9570,0.0,Entailment,[[    2     0 30495  3760  1757     2]],classification,[[   0 4684 5000    2]]
26c64edbc5fa4cdded69ace66fdba64a9648b78e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
aaed6e30cf16727df0075b364873df2a4ec7605b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],efficiency task aimed  at reducing the number of parameters while minimizing drop in performance,"[[    0 33939  3685  3448  1437    23  4881     5   346     9 17294   150
  34655  1874    11   819     2]]"
d43e868cae91b3dc393c05c55da0754b0fb3a46a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e93b4a15b54d139b768d5913fb5fd1aed8ab25da,0.0,Entailment,[[    2     0 30495  3760  1757     2]],manually cleaned human-produced utterances,[[    0   397 13851 17317  1050    12 25617 18672  5332     2]]
c65b6470b7ed0a035548cc08e0bc541c2c4a95a7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the latest CLWE developments almost exclusively focus on fully unsupervised approaches BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 , BIBREF30 : they fully abandon any source of (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces","[[    0   627   665  5289  9112  5126   818  8992  1056    15  1950   542
  16101 25376  8369   163  8863 45935  1922  2156   163  8863 45935  1978
   2156   163  8863 45935  1244  2156   163  8863 45935  2481  2156   163
   8863 45935  2518  2156   163  8863 45935  2517  2156   163  8863 45935
   2890  2156   163  8863 45935   541  4832    51  1950 12506   143  1300
      9    36 12963  3953    43 13702     8 14660     5  2557  5018 36451
     30 26005   299  9779 20097   227  1198    12 23830  6154 31992  5564
  33183 11303  5938     2]]"
8cb9006bcbd2f390aadc6b70d54ee98c674e45cc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary website, novels, history and religious books from Sindhi Adabi Board,  tweets regarding news and sports are collected from twitter","[[    0 16624 13694  1173     8 11614  5602 11614  1222 13538  3592  9911
      6 28274 37017     6   765  1652     8  1612   340    31   305  1725
    102   271   592  5059     6   340    31 12523 15690  1228  5059     6
   4566 31757     6 19405     6  1652     6  2799    31 13538   298 35020
    415 17205   998     6 19405     6   750     8  3458  2799    31 13538
   3592  1614 10810  1785     6  1437  6245  2624   340     8  1612    32
   4786    31  7409     2]]"
5d164651a4aed7cf24d53ba9685b4bee8c965933,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
565d668947ffa6d52dad019af79289420505889b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
eddabb24bc6de6451bcdaa7940f708e925010912,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Answer with content missing: (Data and pre-processing section) The data is suited for our experiments because the annotators were explicitly asked to provide annotations on a clausal level.,"[[    0 33683    19  1383  1716    35    36 30383     8  1198    12 39221
   2810    43    20   414    16 14756    13    84 15491   142     5 45068
   3629    58 16369   553     7   694 47234    15    10 42108 25016   672
      4     2]]"
0bd683c51a87a110b68b377e9a06f0a3e12c8da0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"bilingual dictionary induction, monolingual and cross-lingual word similarity, and cross-lingual hypernym discovery","[[    0   428 41586 36451 26076     6  6154 31992  5564     8  2116    12
   1527  5564  2136 37015     6     8  2116    12  1527  5564  8944  2855
    119  6953     2]]"
86a93a2d1c19cd0cd21ad1608f2a336240725700,0.0,Entailment,[[    2     0 30495  3760  1757     2]],interpretation of Frege's work are examples of holistic approaches to meaning,"[[    0 41111  1258     9  5547  1899    18   173    32  7721     9 23015
   8369     7  3099     2]]"
dc57ae854d78aa5d5e8c979826d3e2524d4e9165,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
b540cd4fe9dc4394f64d5b76b0eaa4d9e30fb728,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLUE utilizes different metrics for each of the tasks: Pearson correlation coefficient, F-1 scores, micro-averaging, and accuracy","[[    0  7976  9162 33778   430 12758    13   349     9     5  8558    35
  16116 22792 45979     6   274    12   134  4391     6  5177    12  9903
   6257     6     8  8611     2]]"
87159024d4b6dac8c456bb74a91044df292f6b99,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
3f6610d1d68c62eddc2150c460bf1b48a064e5e6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
3070d6d6a52aa070f0c0a7b4de8abddd3da4f056,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BPC, Perplexity",[[    0   387  4794     6  2595 26028  1571     2]]
385dc96604e077611fbd877c7f39d3c17cd63bf2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
61fb982b2c67541725d6db76b9c710dd169b533d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],There are is a strong conjecture that it might be the reason but it is not proven.,"[[    0   970    32    16    10   670 43132    14    24   429    28     5
   1219    53    24    16    45  5401     4     2]]"
6ee27ab55b1f64783a9e72e3f83b7c9ec5cc8073,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the CMU ARCTIC database BIBREF33,  the M-AILABS speech dataset BIBREF34 ","[[    0   627 10712   791  5495  7164  2371  8503   163  8863 45935  3103
      6  1437     5   256    12   250 24570  3297  1901 41616   163  8863
  45935  3079  1437     2]]"
7e54c7751dbd50d9d14b9f8b13dc94947a46e42f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],ensemble model,[[    0  9401 46831  1421     2]]
39cf0b3974e8a19f3745ad0bcd1e916bf20eeab8,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital, an additional data set for training our vector space model, comprised of EHR texts queried from the Research Patient Data Registry (RPDR)","[[    0    10 42168     9 15462 32933  5119     6  7988  2775     6  1736
   6376  2775     6     8    97  5154  2775    31 14576  1484    11     5
    374 39901 14386   586    23  1509 19242  2392     6    41   943   414
    278    13  1058    84 37681   980  1421     6 13822     9   381 16271
  14301 31212  2550    31     5  1624 27690  5423 33967    36   500  6153
    500    43     2]]"
2210178facc0e7b3b6341eec665f3c098abef5ac,0.0,Entailment,[[    2     0 30495  3760  1757     2]],GRU,[[    0 11621   791     2]]
4ade72bfa28bd1f6b75cc7fa687fa634717782f2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],We see that A-gen performance improves significantly with the joint model: both F1 and EM increase by about 10 percentage points. ,"[[    0   170   192    14    83    12  4138   819 15296  3625    19     5
   2660  1421    35   258   274   134     8 14850   712    30    59   158
   3164   332     4  1437     2]]"
df934aa1db09c14b3bf4bc617491264e2192390b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],2-layer LSTM model with 500 hidden units in both encoder and decoder,"[[    0   176    12 39165   226  4014   448  1421    19  1764  7397  2833
     11   258  9689 15362     8  5044 15362     2]]"
518d0847b02b4f23a8f441faa38b935c9b892e1e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Honk, DeepSpeech-finetune",[[    0 35846   330     6  8248 29235  7529    12 13597   594  4438     2]]
ccf7415b515fe5c59fa92d4a8af5d2437c591615,0.0,Entailment,[[    2     0 30495  3760  1757     2]],by setting a non-zero positive constraint ($C\ge 0$) on the KL term ($|D_{KL}\big (q_\phi ({z}|{x}) || p({z})\big )-C|$),"[[    0  1409  2749    10   786    12 18556  1313 39816  1358   347 37457
   1899   321  1629    43    15     5 26544  1385  1358 15483   495 49747
    530   574 49712  8527    36  1343  1215 37457 32079 49698   329 24303
  15483 45152  1178 49424 45056   181 48377   329 24303 49394  8527  4839
     12   347 15483  1629    43     2]]"
e09e89b3945b756609278dcffb5f89d8a52a02cd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],5575 speeches,[[    0  3118  2545 13467     2]]
a85c2510f25c7152940b5ac4333a80e0f91ade6e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Greens-EFA, S&D, and EPP exhibit the highest cohesion, non-aligned members NI have the lowest cohesion, followed by EFDD and ENL, two methods disagree is the level of cohesion of GUE-NGL","[[    0   534 18656    12   717  5944     6   208   947   495     6     8
    381  5756  8483     5  1609 31657     6   786    12 36967   453 17378
     33     5  3912 31657     6  1432    30 17112 27140     8 13245   574
      6    80  6448 11967    16     5   672     9 31657     9   272  9162
     12   487 10020     2]]"
52e8f79814736fea96fd9b642881b476243e1698,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BULATS i-vector/PLDA
BULATS x-vector/PLDA
VoxCeleb x-vector/PLDA
PLDA adaptation (X1)
 Extractor fine-tuning (X2) ","[[    0   387  6597 26063   939    12 48219    73  7205  3134 50118   387
   6597 26063  3023    12 48219    73  7205  3134 50118   846  4325 31431
    428  3023    12 48219    73  7205  3134 50118  7205  3134 14082    36
   1000   134    43 50118 41668   368  2051    12 24641   154    36  1000
    176    43  1437     2]]"
37bc8763eb604c14871af71cba904b7b77b6e089,0.0,Entailment,[[    2     0 30495  3760  1757     2]],pre-trained to identify the presence of behavior from a sequence of word using the Couples Therapy Corpus,"[[    0  5234    12 23830     7  3058     5  2621     9  3650    31    10
  13931     9  2136   634     5  7812 12349 25889 28556     2]]"
846a1992d66d955fa1747bca9a139141c19908e8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, Health Care Reform (HCR)","[[    0 36118  1891   111   599 12169  8913 28556    36  4014   104 28556
    238  5316   111   599 12169  8913 28556     6  1309  3800 12287    36
  41022    43     2]]"
89d1687270654979c53d0d0e6a845cdc89414c67,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Using crowdsourcing ,[[    0 36949  8817 27824  1437     2]]
e75685ef5f58027be44f42f30cb3988b509b2768,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"set of related tasks are learned (e.g., emotional activation), primary task (e.g., emotional valence)","[[    0  8738     9  1330  8558    32  2435    36   242     4   571   482
   3722 29997   238  2270  3685    36   242     4   571   482  3722  7398
   4086    43     2]]"
a9c5252173d3df1c06c770c180a77520de68531b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CoNLL 2003, CoNLL 2000",[[   0 8739  487 6006 4999    6  944  487 6006 3788    2]]
45bd22f2cfb62a5f79ec3c771c8324b963567cc0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
bd3ccb63fd8ce5575338d7332e96def7a3fabad6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ROMULUS dataset, NLU-Benchmark dataset","[[    0 41944  6597  3048 41616     6 12817   791    12 46131  6920 41616
      2]]"
7bf3a7d19f17cf01f2c9fa16401ef04a3bef65d8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we sort the speech segments by length, we take segments in pairs, zero-padding the shorter segment so both have the same length, These pairs are then mixed together","[[    0  1694  2345     5  1901  5561    30  5933     6    52   185  5561
     11 15029     6  4276    12 48921     5 10941  2835    98   258    33
      5   276  5933     6  1216 15029    32   172  4281   561     2]]"
5d5a571ff04a5fdd656ca87f6525a60e917d6558,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
0481a8edf795768d062c156875d20b8fb656432c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"output of global LSTM network at time $V_{m_i}^t$5 , which encodes the mention context and target entity information from $V_{m_i}^t$6 to $V_{m_i}^t$7","[[    0 46234     9   720   226  4014   448  1546    23    86    68   846
  49747   119  1215   118 24303 35227    90  1629   245  2156    61  9689
  19160     5  4521  5377     8  1002 10014   335    31    68   846 49747
    119  1215   118 24303 35227    90  1629   401     7    68   846 49747
    119  1215   118 24303 35227    90  1629   406     2]]"
e37c32fce68759b2272adc1e44ea91c1a7c47059,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"movies , restaurants, English , Korean",[[    0   119 40527  2156  4329     6  2370  2156  2238     2]]
5dc2f79cd8078d5976f2df9ab128d4517e894257,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BBC and CNN ,[[    0 28713     8  3480  1437     2]]
ac706631f2b3fa39bf173cd62480072601e44f66,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
1f6180bba0bc657c773bd3e4269f87540a520ead,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
81669c550d32d756f516dab5d2b76ff5f21c0f36,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Syn Dep, OpenIE, SRL, BiDAF, QANet, BERT, NAQANet, NAQANet+","[[    0 46437  6748     6  2117  7720     6   208 17868     6  6479  3134
    597     6  1209  1889   594     6   163 18854     6  8438  1864  1889
    594     6  8438  1864  1889   594  2744     2]]"
749a307c3736c5b06d7b605dc228d80de36cbabe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WMT 2019 parallel dataset, a restricted dataset containing the full TED corpus from MUST-C BIBREF10, sampled sentences from WMT 2019 dataset","[[    0   771 11674   954 12980 41616     6    10  9393 41616  8200     5
    455 32690 42168    31 39282    12   347   163  8863 45935   698     6
  36551 11305    31   305 11674   954 41616     2]]"
03f4e5ac5a9010191098d6d66ed9bbdfafcbd013,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"by also learning source embeddings for not only unigrams but also n-grams present in each sentence, and averaging the n-gram embeddings along with the words","[[    0  1409    67  2239  1300 33183   417  1033    13    45   129   542
   1023 27809    53    67   295    12 28526    29  1455    11   349  3645
      6     8  8343     5   295    12 28526 33183   417  1033   552    19
      5  1617     2]]"
3c378074111a6cc7319c0db0aced5752c30bfffb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The multi-task model outperforms the single-task model at all data sizes, but none have an overall benefit from the open vocabulary system","[[    0   133  3228    12 45025  1421  9980 33334     5   881    12 45025
   1421    23    70   414 10070     6    53  4146    33    41  1374  1796
     31     5   490 32644   467     2]]"
02e4bf719b1a504e385c35c6186742e720bcb281,0.0,Entailment,[[    2     0 30495  3760  1757     2]],cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity,"[[    0 27037  9355    35   258  1061    11     5  9355   197    33     5
    276  8385 21528   131 17287  9355    35  1061   197    33  5483  8385
  21528     2]]"
9d9b11f86a96c6d3dd862453bf240d6e018e75af,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The training dataset is augmented by swapping all gendered words by their other gender counterparts,"[[    0   133  1058 41616    16 21621    30 32093    70   821 36318  1617
     30    49    97  3959 10428     2]]"
5908d7fb6c48f975c5dfc5b19bb0765581df2b25,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Resulting dataset was 7934 messages for train and 700 messages for test.,"[[    0 48136   154 41616    21  7589  3079  3731    13  2341     8  7417
   3731    13  1296     4     2]]"
a779d452d11f368c66f7b51f7190d0fe9402f505,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"(infixes 700K, 318K, and 40K) each representing the number of approximate parameters","[[    0  1640  9433  3181   293  7417   530     6 37207   530     6     8
    843   530    43   349  4561     5   346     9 32161 17294     2]]"
7835d8f578386834c02e2c9aba78a345059d56ca,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
44a2a8e187f8adbd7d63a51cd2f9d2d324d0c98d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],HEOT,[[    0 17779  3293     2]]
a9610cbcca813f4376fbfbf21cc14689c7fbd677,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"In the overall available data there are 40,071 training, 4,988 validation, and 5,050 usable testing stories.","[[    0  1121     5  1374   577   414    89    32   843     6  3570   134
   1058     6   204     6   466  4652 26567     6     8   195     6 25423
  33927  3044  1652     4     2]]"
daf624f7d1623ccd3facb1d93d4d9d616b3192f4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
53a0763eff99a8148585ac642705637874be69d4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Active learning methods has a learning engine (mainly used for training of classification problems) and the selection engine (which chooses samples that need to be relabeled by annotators from unlabeled data). Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively.","[[    0 42586  2239  6448    34    10  2239  3819    36 17894   352   341
     13  1058     9 20257  1272    43     8     5  4230  3819    36  5488
  19662  7931    14   240     7    28  6258 14286   196    30 45068  3629
     31 35237 14286   196   414   322  1892     6  6258 14286   196  7931
     32   355     7  1058   278    13  1380 24072     7   769    12 21714
      6  4634 13307  3927     5  8611     9     5  1380 24072     4    96
     42  2225     6  4307   597    12   805  2835   254     8    10  2314
   1421    32  7460    25  2239  3819     8  4230  3819     6  4067     4
      2]]"
f74eaee72cbd727a6dffa1600cdf1208672d713e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],QA pairs per predicate,[[    0  1864   250 15029   228 48206     2]]
c000a43aff3cb0ad1cee5379f9388531b5521e9a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They pre-train forward and backward LMs separately, remove top layer softmax, and concatenate to obtain the bidirectional LMs.","[[    0  1213  1198    12 21714   556     8 18173   226 13123 12712     6
   3438   299 10490  3793 29459     6     8 10146 26511   877     7  6925
      5  2311 43606   337   226 13123     4     2]]"
b29b5c39575454da9566b3dd27707fced8c6f4a1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"As the question has integrated previous utterances, the model needs to directly relate previously mentioned concept with the current question. This is helpful for concept carry-over and coreference resolution.","[[    0  1620     5   864    34  6818   986 18672  5332     6     5  1421
    782     7  2024 12155  1433  2801  4286    19     5   595   864     4
    152    16  7163    13  4286  2324    12  2137     8  2731 23861  3547
      4     2]]"
cd1ad7e18d8eef8f67224ce47f3feec02718ea1a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"TransE, DistMult, ComplEx, ConvE, RotatE","[[    0 19163   717     6 11281 45287     6 20722  9089     6 30505   717
      6  9104   415   717     2]]"
9e04730907ad728d62049f49ac828acb4e0a1a2a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%","[[    0  4148 12180 37790  5600  2580 41616 10018  6791     4  2663  4234
    234  7539  5356     4  6405  4234    15 31197 10777 19322 41616 10018
   4074     4  1570  4234   234  7539  2766     4  3669  4234    15  6479
  39820 41616 10018  3557     4   612  4234   234  7539  2843     4  1366
    207     2]]"
347e86893e8002024c2d10f618ca98e14689675f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],high-quality,[[   0 3530   12 8634    2]]
77af93200138f46bb178c02f710944a01ed86481,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
5c95808cd3ee9585f05ef573b0d4a52e86d04c60,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CL Journal and EMNLP conference,[[    0  7454  3642     8 14850   487 21992  1019     2]]
a1c4f9e8661d4d488b8684f055e0ee0e2275f767,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Recurrent Neural Network (RNN), ActionLSTM, Generative Recurrent Neural Network Grammars (RNNG)","[[    0 21109 41937 44304  3658    36   500 20057   238  5828   574  4014
    448     6 15745  3693  7382 41937 44304  3658 14171   119  2726    36
    500 20057   534    43     2]]"
d2804ac0f068e9c498e33582af9c66906b26cac3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we extract sentences from Wikipedia in languages for which public multilingual is pretrained. For each sentence, we use the open-source BERT wordpiece tokenizer BIBREF4 , BIBREF1 and compute cross-entropy loss for each wordpiece: INLINEFORM0","[[    0  1694 14660 11305    31 28274    11 11991    13    61   285  7268
  41586    16 11857 26492     4   286   349  3645     6    52   304     5
    490    12 17747   163 18854  2136 10449 19233  6315   163  8863 45935
    306  2156   163  8863 45935   134     8 37357  2116    12  1342 47145
    872    13   349  2136 10449    35  2808 28302 38036   288     2]]"
7772cb23b7609f1d4cfd6511ac3fcdc20f8481ba,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Table TABREF44, Table TABREF44, Table TABREF47, Table TABREF47","[[    0 41836   255  4546 45935  3305     6  9513   255  4546 45935  3305
      6  9513   255  4546 45935  3706     6  9513   255  4546 45935  3706
      2]]"
5679fabeadf680e35a4f7b092d39e8638dca6b4d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
3685bf2409b23c47bfd681989fb4a763bcab6be2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],300 Dimensional Glove,[[    0  2965   211 30846  4573  7067     2]]
ecb680d79e847beb7c1aa590d288a7313908d64a,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," To test our proposed category induction model, we consider all BabelNet categories with fewer than 50 known instances. This is motivated by the view that conceptual neighborhood is mostly useful in cases where the number of known instances is small. For each of these categories, we split the set of known instances into 90% for training and 10% for testing.","[[    0   598  1296    84  1850  4120 26076  1421     6    52  1701    70
  46060 15721  6363    19  4163    87   654   684 10960     4   152    16
   7958    30     5  1217    14 28647  3757    16  2260  5616    11  1200
    147     5   346     9   684 10960    16   650     4   286   349     9
    209  6363     6    52  3462     5   278     9   684 10960    88  1814
    207    13  1058     8   158   207    13  3044     4     2]]"
3554ac92d4f2d00dbf58f7b4ff2b36a852854e95,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The sentence we want to generate from the target-aspect pair is a question, and the format needs to be the same., For the NLI task, the conditions we set when generating sentences are less strict, and the form is much simpler., For QA-B, we add the label information and temporarily convert TABSA into a binary classification problem ( INLINEFORM0 ) to obtain the probability distribution, auxiliary sentence changes from a question to a pseudo-sentence","[[    0   133  3645    52   236     7  5368    31     5  1002    12   281
  13771  1763    16    10   864     6     8     5  7390   782     7    28
      5   276   482   286     5 12817   100  3685     6     5  1274    52
    278    77 10846 11305    32   540  8414     6     8     5  1026    16
    203 20584   482   286  1209   250    12   387     6    52  1606     5
   6929   335     8  8059 10304   255  4546  3603    88    10 32771 20257
    936    36  2808 28302 38036   288  4839     7  6925     5 18102  3854
      6 34590  3645  1022    31    10   864     7    10 38283    12 19530
   4086     2]]"
2e9c6e01909503020070ec4faa6c8bf2d6c0af42,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the author and the supervisor,[[    0   627  2730     8     5 13759     2]]
8e857e44e4233193c7b2d538e520d37be3ae1552,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a basic scenario, a health gathering scenario, a scenario in which the agent must take cover from fireballs, a scenario in which the agent must defend itself from charging enemies, and a super scenario, where a mixture of the above scenarios","[[    0   102  3280  5665     6    10   474  5660  5665     6    10  5665
     11    61     5  2936   531   185  1719    31   668 21849     6    10
   5665    11    61     5  2936   531  4538  1495    31  6489 11058     6
      8    10  2422  5665     6   147    10 12652     9     5  1065 12593
      2]]"
8670989ca39214eda6c1d1d272457a3f3a92818b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
117aa7811ed60e84d40cd8f9cb3ca78781935a98,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
d9c6493e1c3d8d429d4ca608f5acf29e4e7c4c9b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CFQ contains the most query patterns by an order of magnitude and also contains significantly more queries and questions than the other datasets,"[[    0 25388  1864  6308     5   144 25860  8117    30    41   645     9
  11259     8    67  6308  3625    55 22680     8  1142    87     5    97
  42532     2]]"
8266642303fbc6a1138b4e23ee1d859a6f584fbb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BIBREF3, BIBREF4, BIBREF9","[[    0  5383   387 45935   246     6   163  8863 45935   306     6   163
   8863 45935   466     2]]"
951098f0b7169447695b47c142384f278f451a1e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact","[[    0   245   678   913 14105    13    10  1989  2026    35   117   913
      6   614   913     6  4761   913     6   239   913     8   182   239
    913     2]]"
f1c70baee0fd02b8ecb0af4b2daa5a56f3e9ccc3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"239,357 English question-answer pairs",[[    0 29561     6 30628  2370   864    12 27740 15029     2]]
3d99bc8ab2f36d4742e408f211bec154bc6696f7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
50690b72dc61748e0159739a9a0243814d37f360,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
25b24ab1248f14a621686a57555189acc1afd49c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],DyNet,[[    0   495   219 15721     2]]
8434974090491a3c00eed4f22a878f0b70970713,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Proposed model has 1.16 million parameters and 11.04 MB.,"[[    0 41895  7878  1421    34   112     4  1549   153 17294     8   365
      4  3387 17025     4     2]]"
b1f2db88a6f89d0f048803e38a0a568f5ba38fc5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"cases of singular/plural, subject pronoun/object pronoun, etc.","[[    0 28162     9 23429    73  2911  9799     6  2087 44811    73 40412
  44811     6  4753     4     2]]"
be3e020ba84bc53dfb90b8acaf549004b66e31e2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"precision, recall, and F-measure on boundaries (BP, BR, BF), and tokens (WP, WR, WF),  exact-match (X) metric","[[    0  5234 37938     6  6001     6     8   274    12  1794 24669    15
  10156    36 21792     6  6823     6 43603   238     8 22121    36 28435
      6  8692     6   305   597   238  1437  6089    12 10565    36  1000
     43 14823     2]]"
2efdcebebeb970021233553104553205ce5d6567,0.0,Entailment,[[    2     0 30495  3760  1757     2]],two LSTM layers,[[    0  7109   226  4014   448 13171     2]]
d803b782023553bbf9b36551fbc888ad189b1f29,0.0,Entailment,[[    2     0 30495  3760  1757     2]],to judge each utterance from 1 (bad) to 3 (good) in terms of informativeness and naturalness,"[[    0   560  1679   349 18672  2389    31   112    36 10999    43     7
    155    36  8396    43    11  1110     9  6296   415 12367     8  1632
   1825     2]]"
212495af630c16745d0fcb614119d75327952271,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
7c794fa0b2818d354ca666969107818a2ffdda0c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"We also report the metrics in BIBREF7 for consistency, we report the span F1,  Exact Match (EM) accuracy of the entire sequence of labels, metric that combines intent and entities","[[    0   170    67   266     5 12758    11   163  8863 45935   406    13
  12787     6    52   266     5  8968   274   134     6  1437  3015  7257
   9018    36  5330    43  8611     9     5  1445 13931     9 14105     6
  14823    14 15678  5927     8  8866     2]]"
5d22746b3004c5e90ea714b24bf8bc7b4d15bd88,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Karpathy and Fei-Fei's split for MS-COCO dataset BIBREF10,"[[    0   530 11230 17543     8 46262    12 29037   118    18  3462    13
   6253    12   347  4571   673 41616   163  8863 45935   698     2]]"
637aa32a34b20b4b0f1b5dfa08ef4e0e5ed33d52,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
68e3f3908687505cb63b538e521756390c321a1c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],2.7 accuracy points,[[   0  176    4  406 8611  332    2]]
7ff48fe5b7bd6b56553caacc891ce3d7e0070440,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
0ec4143a4f1a8f597b435f83c0451145be2ab95b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Frequency masking, Time masking, Additive noise, Speed and volume perturbation","[[    0   597 48249 11445   154     6  3421 11445   154     6  4287 15589
   6496     6 15707     8  3149 32819 13157  1258     2]]"
00bcdffff7e055f99aaf1b05cf41c98e2748e948,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For the emotion recognition from text they use described neural network as baseline.
For audio and face there is no baseline.","[[    0  2709     5 11926  4972    31  2788    51   304  1602 26739  1546
     25 18043     4 50118  2709  6086     8   652    89    16   117 18043
      4     2]]"
864b5c1fe8c744f80a55e87421b29d6485b7efd0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Precision, Recall and INLINEFORM0 score",[[    0 22763 37938     6 35109     8  2808 28302 38036   288  1471     2]]
8fa7011e7beaa9fb4083bf7dd75d1216f9c7b2eb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
d47c074012eae27426cd700f841fd8bf490dcc7b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
3c3cb51093b5fd163e87a773a857496a4ae71f03,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word","[[    0 10993     6 20410     5  2835   196  3645     7    10 13931     9
   1984  2136 33183   417  1033     4  1892     6     5  2314  1421  1239
      5  2136 33183 11303 13931    25  8135     6  2314    81   349  1736
   1984  2136     2]]"
c180f44667505ec03214d44f4970c0db487a8bae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the neural approach is generally preferred by a greater percentage of participants than the rules or random, human-made game outperforms them all","[[    0   627 26739  1548    16  3489  6813    30    10  2388  3164     9
   3597    87     5  1492    50  9624     6  1050    12  7078   177  9980
  33334   106    70     2]]"
959490ba72bd02f742db1e7b19525d4b6c419772,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
12eaaf3b6ebc51846448c6e1ad210dbef7d25a96,0.0,Entailment,[[    2     0 30495  3760  1757     2]],wav2vec has 12 convolutional layers,[[    0 48479   176 25369    34   316 15380 23794   337 13171     2]]
891c4af5bb77d6b8635ec4109572de3401b60631,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
43d8057ff0d3f0c745a7164aed7ed146674630e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],national dialects of English,[[    0 11535 37508    29     9  2370     2]]
1a6e2bd41ee43df83fef2a1c1941e6f95a619ae8,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," entity recognition, semantic role labeling and co-reference resolution","[[    0 10014  4972     6 46195   774 27963     8  1029    12 45927  3547
      2]]"
47b00652ac66039aafe886780e86961bfc5b466e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
81303f605da57ddd836b7c121490b0ebb47c60e7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Sexist/Racist (SR) data set, HATE dataset, HAR","[[    0 35581   661    73   500 25058    36 17973    43   414   278     6
    289  8625 41616     6 32721     2]]"
acda028a21a465c984036dcbb124b7f03c490b41,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MADL BIBREF0 extends the dual learning BIBREF1, BIBREF2 framework by introducing multiple primal and dual models.","[[    0   448  2606   574   163  8863 45935   288 14269     5  6594  2239
    163  8863 45935   134     6   163  8863 45935   176  7208    30 10345
   1533 43243     8  6594  3092     4     2]]"
b69f0438c1af4b9ed89e531c056d9812d4994016,0.0,Entailment,[[    2     0 30495  3760  1757     2]],3600 user-generated comments,[[    0  3367   612  3018    12 21842  1450     2]]
25b2ae2d86b74ea69b09c140a41593c00c47a82b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],using Amazon Mechanical Turk using simulated environments with topological maps,"[[    0 10928  1645 35644 19683   634 27361 11534    19   299  9779 12879
      2]]"
b3a09d2e3156c51bd5fdc110a2a00a67bb8c0e42,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (Names of many identifiers missing) TextCat, ChromeCLD, LangDetect, langid.py, whatlang, whatthelang, YALI, LDIG, Polyglot 3000, Lextek Language Identifier and Open Xerox Language Identifier.","[[    0 33683    19  1383  1716    35    36 47995     9   171 46033  1716
     43 14159 34440     6 17571  7454   495     6  7883 43780     6 22682
    808     4 17163     6    99 32373     6    99   627 32373     6   854
   2118   100     6 34744  5969     6 10415  7210  1242 23513     6 14786
  28205 22205 28763 24072     8  2117 34436  4325 22205 28763 24072     4
      2]]"
a1097ce59270d6f521d92df8d2e3a279abee3e67,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"model points out plausible signals which were passed over by an annotator, it also picks up on a recurring tendency in how-to guides in which the second person pronoun referring to the reader is often the benefactee of some action","[[    0 21818   332    66 27099  8724    61    58  1595    81    30    41
  45068  2630     6    24    67  5916    62    15    10 16331 16699    11
    141    12   560 17928    11    61     5   200   621 44811  5056     7
      5 10746    16   747     5 19018  7257  1942     9   103   814     2]]"
b8d5e9fa08247cb4eea835b19377262d86107a9d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"IBM-UB-1 dataset BIBREF25, IAM-OnDB dataset BIBREF42, The ICDAR-2013 Competition for Online Handwriting Chinese Character Recognition BIBREF45, ICFHR2018 Competition on Vietnamese Online Handwritten Text Recognition using VNOnDB BIBREF50","[[    0  8863   448    12 12027    12   134 41616   163  8863 45935  1244
      6    38  2620    12  4148 10842 41616   163  8863 45935  3714     6
     20    38 11579  2747    12 10684 12791    13  5855  7406 13293  1111
  35177 23288  7469   163  8863 45935  1898     6  8242   597 16271  2464
  12791    15 16859  5855  7406 23460 14159 23288  7469   634   468   487
   4148 10842   163  8863 45935  1096     2]]"
13b36644357870008d70e5601f394ec3c6c07048,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
b512ab8de26874ee240cffdb3c65d9ac8d6023d9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
edcde2b675cf8a362a63940b2bbdf02c150fe01f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],supply an NMT system with knowledge regarding the speaker and interlocutor of first-person sentences,"[[    0 16714   352    41   234 11674   467    19  2655  2624     5  5385
      8  3222 26516 35853     9    78    12  5970 11305     2]]"
134598831939a3ae20d177cec7033d133625a88e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"In the MULT method, two datasets are simultaneously trained, and the weights are tuned based on the inputs which come from both datasets. The hyper-parameter $\lambda \in (0,1)$ is calculated based on a brute-force search or using general global search. This hyper parameter is used to calculate the final cost function which is computed from the combination of the cost function of the source dataset and the target datasets. , this paper proposes to use the most relevant samples from the source dataset to train on the target dataset. One way to find the most similar samples is finding the pair-wise distance between all samples of the development set of the target dataset and source dataset., we propose using a clustering algorithm on the development set. The clustering algorithm used ihere is a hierarchical clustering algorithm. The cosine similarity is used as a criteria to cluster each question and answer. Therefore, these clusters are representative of the development set of the target dataset and the corresponding center for each cluster is representative of all the samples on that cluster. In the next step, the distance of each center is used to calculate the cosine similarity. Finally, the samples in the source dataset which are far from these centers are ignored. In other words, the outliers do not take part in transfer learning.","[[    0  1121     5   256 25938  5448     6    80 42532    32 11586  5389
      6     8     5 23341    32 14536   716    15     5 16584    61   283
     31   258 42532     4    20  8944    12 46669  5906 49959 49744 44128
    179    36   288     6   134    43  1629    16  9658   716    15    10
  40828    12  9091  1707    50   634   937   720  1707     4   152  8944
  43797    16   341     7 15756     5   507   701  5043    61    16 43547
     31     5  4069     9     5   701  5043     9     5  1300 41616     8
      5  1002 42532     4  2156    42  2225 21037     7   304     5   144
   4249  7931    31     5  1300 41616     7  2341    15     5  1002 41616
      4   509   169     7   465     5   144  1122  7931    16  2609     5
   1763    12 10715  4472   227    70  7931     9     5   709   278     9
      5  1002 41616     8  1300 41616   482    52 15393   634    10 46644
   2961 17194    15     5   709   278     4    20 46644  2961 17194   341
    939 10859    16    10 44816 46644  2961 17194     4    20 12793   833
  37015    16   341    25    10  8608     7 18016   349   864     8  1948
      4  9068     6   209 28255    32  4915     9     5   709   278     9
      5  1002 41616     8     5 12337  1312    13   349 18016    16  4915
      9    70     5  7931    15    14 18016     4    96     5   220  1149
      6     5  4472     9   349  1312    16   341     7 15756     5 12793
    833 37015     4  3347     6     5  7931    11     5  1300 41616    61
     32   444    31   209  5228    32  8266     4    96    97  1617     6
      5 31187  4733   109    45   185   233    11  2937  2239     4     2]]"
ed7a3e7fc1672f85a768613e7d1b419475950ab4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],single-domain setting,[[    0 25382    12 46400  2749     2]]
b54fc86dc2cc6994e10c1819b6405de08c496c7b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred,"[[    0   958 43291   227     5  1151     5  3104    50  1383    21  1278
     73    90 21210   196     8     5  1151    14     5  4289  1129    50
   3545  2756     2]]"
3aa43a0d543b88d40e4f3500c7471e263515be40,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"translated the responses in multiple languages into English using machine translation, words without functional meaning (e.g. `I'), rare words that occurred in only one narrative, numbers, and punctuation were all removed, remaining words were stemmed to remove plural forms of nouns or conjugations of verbs","[[    0  9981 32914     5  8823    11  1533 11991    88  2370   634  3563
  19850     6  1617   396 12628  3099    36   242     4   571     4 22209
    100 41734  3159  1617    14  2756    11   129    65  7122     6  1530
      6     8 15760  9762    58    70  2928     6  2405  1617    58 23494
      7  3438 35691  4620     9 44875    29    50 21044  3252  1635     9
  47041     2]]"
5112bbf13c7cf644bf401daecb5e3265889a4bfc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
4ec538e114356f72ef82f001549accefaf85e99c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"all caps, quotation marks, emoticons, emojis, hashtags","[[    0  1250  9686     6 42854  4863     6 36355 46043     6  2841  4203
    354     6 32795  8299     2]]"
205163715f345af1b5523da6f808e6dbf5f5dd47,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"44,896 articles",[[    0  3305     6 41256  7201     2]]
4b8a0e99bf3f2f6c80c57c0e474c47a5ee842b2c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LSTMs with and without attention, HRED, VHRED with and without attention, MMI and Reranking-RL","[[    0   574  4014 13123    19     8   396  1503     6 11683  1691     6
    468 16271  1691    19     8   396  1503     6   256  7539     8   248
    254 20327    12 17868     2]]"
ac706631f2b3fa39bf173cd62480072601e44f66,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d24acc567ebaec1efee52826b7eaadddc0a89e8b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],10700,[[   0  698 5987    2]]
1949d84653562fa9e83413796ae55980ab7318f2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],mean reciprocal rank,[[    0 43348 36285  7938     2]]
18dab362ae4587408a291a55299f347f8870e9f1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
3b745f086fb5849e7ce7ce2c02ccbde7cfdedda5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average,"[[    0  1121     5  5702 20257  3685    30   231   207     7   290   207
      8    11     5  5927 20257  3685    30   321     4  6405   207    15
    674     2]]"
fd8b6723ad5f52770bec9009e45f860f4a8c4321,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"A pointer network decodes the answer from a bidirectional LSTM with attention flow layer and self-matching layer, whose inputs come from word and character embeddings of the query and input text fed through a highway layer.","[[    0   250 41515  1546  5044 19160     5  1948    31    10  2311 43606
    337   226  4014   448    19  1503  3041 10490     8  1403    12 10565
    154 10490     6  1060 16584   283    31  2136     8  2048 33183   417
   1033     9     5 25860     8  8135  2788  9789   149    10  6418 10490
      4     2]]"
b99d100d17e2a121c3c8ff789971ce66d1d40a4d,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models)","[[    0    52   109    45 16369  8933     7   986   557   187   144  2210
   1364  1169 14958  2735   414    36   463    98    24    40    45    28
     10  2105  6676   238   304  6448  1198    12 40651   163 18854    36
    463    98    40   533    28  9980 10312    30    84  3092    43     2]]"
61404466cf86a21f0c1783ce535eb39a01528ce8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
ee9b95d773e060dced08705db8d79a0a6ef353da,0.0,Entailment,[[    2     0 30495  3760  1757     2]],they are used as additional features in a supervised classification task,"[[    0 10010    32   341    25   943  1575    11    10 20589 20257  3685
      2]]"
8a5254ca726a2914214a4c0b6b42811a007ecfc6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Transcribed data is available for duration of 38h 54m 38s for 8 speakers.,"[[    0 19163 38131   414    16   577    13 13428     9  2843   298  4431
    119  2843    29    13   290  6864     4     2]]"
9e2e5918608a2911b341d4887f58a4595d7d1429,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
da1994421934082439e8fe5071a01d3d17b56601,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
b366706e2fff6dd8edc89cc0c6b9d5b0790f43aa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BPRA, APRA, BLEU",[[    0 21792  4396     6  1480  4396     6   163  3850   791     2]]
792d7b579cbf7bfad8fe125b0d66c2059a174cf9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Ternary Trans-CNN,[[    0   565  3281  1766  5428    12 16256     2]]
a83a351539fb0b6acb5bdee32323dd924f4fd1e7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],100 ,[[   0 1866 1437    2]]
ff27d6e6eb77e55b4d39d343870118d1a6debd5e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SVM,[[    0   104 20954     2]]
9a65cfff4d99e4f9546c72dece2520cae6231810,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev","[[    0   133  1850  1421 35499  1437 14850  6791     6  5449     8   274
    134  1812     6  5352  1437    15     5  1296     8 14850  1437  5553
      6  4015     8  1437   274   134  1812     6  1244    15     5  8709
      2]]"
cc680cb8f45aeece10823a3f8778cf215ccc8af0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"number of epochs is an important parameter and its increase leads to results that rank our two worst models almost equal, or even better than others","[[    0 30695     9 43660    29    16    41   505 43797     8    63   712
   3315     7   775    14  7938    84    80  2373  3092   818  3871     6
     50   190   357    87   643     2]]"
47796c7f0a7de76ccb97ccbd43dc851bb8a613d5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5, Zemberek, BIBREF12","[[    0 15188 42464  1794  2835  1258   163  8863 45935   306     8 46594
  34587 14813 19519    36   387 16035    43   163  8863 45935   245     6
    525 20506 37623     6   163  8863 45935  1092     2]]"
cf3af2b68648fa8695e7234b6928d014e3b141f1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
1898f999626f9a6da637bd8b4857e5eddf2fc729,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Our WordDecoding (WDec) model achieves F1 scores that are $3.9\%$ and $4.1\%$ higher than HRL on the NYT29 and NYT24 datasets respectively, In the ensemble scenario, compared to HRL, WDec achieves $4.2\%$ and $3.5\%$ higher F1 scores","[[    0  2522 15690 15953 19519    36   771 15953    43  1421 35499   274
    134  4391    14    32    68   246     4   466 37457   207  1629     8
     68   306     4   134 37457   207  1629   723    87   289 17868    15
      5 36319  2890     8 36319  1978 42532  4067     6    96     5 12547
   5665     6  1118     7   289 17868     6   305 15953 35499    68   306
      4   176 37457   207  1629     8    68   246     4   245 37457   207
   1629   723   274   134  4391     2]]"
dac2591f19f5bbac3d4a7fa038ff7aa09f6f0d96,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Optimized TF-IDF, iterated TF-IDF, BERT re-ranking.","[[    0 36542   757  1538 35690    12  2688   597     6 41393  1070 35690
     12  2688   597     6   163 18854   769    12 19217     4     2]]"
8ca31caa34cc5b65dc1d01d0d1f36bf8c4928805,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
3aee5c856e0ee608a7664289ffdd11455d153234,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81","[[    0  2709  1296    12 36456  1070   278     6 14850  1471     9  5659
      4  1360     6   274   134     9  8060     4  4283     6 12714     9
    321     4  2545     8  5323     9  5659     4  3367     4   286  1296
     12  4651   278     6 14850  1471     9  3492     4  5339     6   274
    134     9  8301     4  4197     6 12714     9   112     4  2036     8
   5323     9  3492     4  6668     2]]"
9b1d789398f1f1a603e4741a5eee63ccaf0d4a4f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"confusion matrices, $\text{F}_1$ score","[[    0 17075 15727  7821 45940     6 49959 29015 45152   597 24303  1215
    134  1629  1471     2]]"
7426a6e800d6c11795941616fc4a243e75716a10,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Which events authors choose to include in their history, which they leave out, and the way the events chosen relate to the march","[[    0 32251  1061  7601  2807     7   680    11    49   750     6    61
     51   989    66     6     8     5   169     5  1061  4986 12155     7
      5  6674     2]]"
4bd894c365d85e20753d9d2cb6edebb8d6f422e9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we devise a test set consisting of adversarial examples, i.e, perturbed examples that can potentially change the base model's prediction. , We use two approaches described in literature: back-translation and noisy sequence autoencoder.","[[    0  1694 34901    10  1296   278 17402     9    44   711   625  3697
  27774    17    27  7721     6   939     4   242     6 32819 35250  7721
     14    64  2905   464     5  1542  1421    18 16782     4  2156   166
    304    80  8369  1602    11 13144    35   124    12 48235     8 28269
  13931  7241 18057   438 15362     4     2]]"
c0122190119027dc3eb51f0d4b4483d2dbedc696,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Stacking method, LSTMCNN, SARNN, simple LSTM bidirectional model, TextCNN","[[    0  5320  7361  5448     6   226  4014  6018 20057     6 34565 20057
      6  2007   226  4014   448  2311 43606   337  1421     6 14159 16256
      2]]"
e07df8f613dbd567a35318cd6f6f4cb959f5c82d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],perplexity,[[    0  1741 26028  1571     2]]
2cfcc5864a30259fd35f1cc035fab956802c1c5b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Language Modeling (LM), PTB BIBREF25 , WikiText-103 BIBREF26 and One-Billion Word benchmark BIBREF27 datasets, neural machine translation (NMT), WMT 2016 English-German dataset","[[    0 46969  7192   154    36 21672   238  7008   387   163  8863 45935
   1244  2156 45569 39645    12 18159   163  8863 45935  2481     8   509
     12   387 10077 15690  5437   163  8863 45935  2518 42532     6 26739
   3563 19850    36   487 11674   238   305 11674   336  2370    12 27709
  41616     2]]"
dc1fe3359faa2d7daa891c1df33df85558bc461b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
9a94dcee17cdb9a39d39977191e643adece58dfc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
ed522090941f61e97ec3a39f52d7599b573492dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (Chapter 3) The concept can be easily explained with an example, visualized in Figure 1. Consider the Portuguese (Pt) word trabalho which, according to the MUSE PtEn dictionary, has the words job and work as possible En translations. In turn, these two En words can be translated to 4 and 5 Czech (Cs) words respectively. By utilizing the transitive property (which translation should exhibit) we can identify the set of 7 possible Cs translations for the Pt word trabalho.","[[    0 33683    19  1383  1716    35    36 45642   155    43    20  4286
     64    28  2773  2002    19    41  1246     6  7133  1538    11 17965
    112     4 13626     5 13053    36   510    90    43  2136  2664 21781
   5410    61     6   309     7     5   256 27291 42191  2383 16040 36451
      6    34     5  1617   633     8   173    25   678  2271 41762     4
     96  1004     6   209    80  2271  1617    64    28 16877     7   204
      8   195  9096    36 31229    43  1617  4067     4   870 21437     5
   6214 15589  1038    36  5488 19850   197  8483    43    52    64  3058
      5   278     9   262   678   230    29 41762    13     5 42191  2136
   2664 21781  5410     4     2]]"
102de97c123bb1e247efec0f1d958f8a3a86e2f6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BLEU and TER scores,[[    0 30876   791     8 28107  4391     2]]
8db45a8217f6be30c31f9b9a3146bf267de68389,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"random , Output length, Input length, Output pattern, Input pattern","[[    0 45041  2156 38252  5933     6 41327  5933     6 38252  6184     6
  41327  6184     2]]"
9bcc1df7ad103c7a21d69761c452ad3cd2951bda,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Gender prediction task,[[    0 46049 16782  3685     2]]
859e0bed084f47796417656d7a68849eb9cb324f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],low-frequency words,[[    0  5481    12 36274  1617     2]]
871c34219eb623bde9ac3937aa0f28fc3ad69445,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"character unit the RNN-transducer with additional attention module, For subword units, classic RNN-transducer, RNN-transducer with attention and joint CTC-attention show comparable performance","[[    0 23375  1933     5   248 20057    12  9981  6588  7742    19   943
   1503 20686     6   286  2849 14742  2833     6  4187   248 20057    12
   9981  6588  7742     6   248 20057    12  9981  6588  7742    19  1503
      8  2660   230  6078    12  2611 19774   311 10451   819     2]]"
f68bd65b5251f86e1ed89f0c858a8bb2a02b233a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Randomly from a Twitter dump,[[    0 45134   352    31    10   599 12371     2]]
cda4612b4bda3538d19f4b43dde7bc30c1eda4e5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"automated attribute-value extraction, score the attributes using the Bayes model, evaluate their importance with several different frequency metrics, aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model, OntoRank algorithm","[[    0  4255  1075  1070 21643    12 19434 23226     6  1471     5 16763
    634     5  1501   293  1421     6 10516    49  3585    19   484   430
  13135 12758     6 13884     5 23341    31   430  1715    88    65  4292
   6097  1571  1471   634    10 31563   208 20954  1421     6 13302   139
  46052 17194     2]]"
f6556d2a8b42b133eaa361f562745edbe56c0b51,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
8ea4bd4c1d8a466da386d16e4844ea932c44a412,0.0,Entailment,[[    2     0 30495  3760  1757     2]], text-code parallel corpus,[[    0  2788    12 20414 12980 42168     2]]
93ac147765ee2573923f68aa47741d4bcbf88fa8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Non-contextual properties of a word, Word usage in an OP or PC (two groups), How a word connects an OP and PC., General OP/PC properties","[[    0 33239    12 46796  5564  3611     9    10  2136     6 15690  9453
     11    41 24839    50  4985    36  7109  1134   238  1336    10  2136
  15230    41 24839     8  4985   482  1292 24839    73  4794  3611     2]]"
fbd47705262bfa0a2ba1440a2589152def64cbbd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively, over INLINEFORM0 increase in EM and GM between our model and the next best two models","[[    0 32313  8611    30  1718   207     8   564   207    11  6676     7
      5  7093  7012     8  2060 35019  3092     6  4067     6    81  2808
  28302 38036   288   712    11 14850     8  5323   227    84  1421     8
      5   220   275    80  3092     2]]"
d353a6bbdc66be9298494d0c853e0d8d752dec4b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method)","[[    0   991 17303  3435  8933  8408 37225    36  2606     6    84  5574
    716    15  2893   625    43     8 37920 37225    36 13457     6   716
     15 40001  2249  5448    43     2]]"
df2839dbd68ed9d5d186e6c148fa42fce60de64f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1448 sentences more than the dataset from Bhat et al., 2017","[[    0  1570  3818 11305    55    87     5 41616    31   163  8849  4400
   1076   482   193     2]]"
f92ee3c5fce819db540bded3cfcc191e21799cb1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions),"[[    0   170  3253    41   160    12   627    12  8877 13491  3944    13
  11926  4972    36   627  7508  1395    28  4638   528     7 10765  5165
     43     2]]"
93e8ce62361b9f687d5200d2e26015723721a90f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
2fd8688c8f475ab43edaf5d189567f8799b018e1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
9d1135303212356f3420ed010dcbe58203cc7db4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English$\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)","[[    0 35007  1629 37457  4070 17214    68 39251    73 27709 14566     9
      5  6186  4014    12   347 42168     6   287   943   414     6    52
    304    10  3344     9   285     8 14101   414    13    59   545   153
   3645 15029    13  2370    12 39251    36 16040    12   243    43     8
     68   306     4   306  1629   153   305 11674  1570  3645 15029    13
      5  2370    12 27709    36 16040    12 13365    43     2]]"
62afbf8b1090e56fdd2a2fa2bdb687c3995477f6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
50be9e6203c40ed3db48ed37103f967ef0ea946c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"standard benchmarks BIBREF36 , BIBREF37, to use our learned representations as features for a low complexity classifier (typically linear) on a novel supervised task/domain unseen during training without updating the parameters, transfer learning evaluation in an artificially constructed low-resource setting","[[    0 21993 22485   163  8863 45935  3367  2156   163  8863 45935  3272
      6     7   304    84  2435 30464    25  1575    13    10   614 13879
   1380 24072    36 44661 26956    43    15    10  5808 20589  3685    73
  46400 30772   148  1058   396 18796     5 17294     6  2937  2239 10437
     11    41 30768 11236   614    12 44814  2749     2]]"
47a30eb4d0d6f5f2ff4cdf6487265a25c1b18fd8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
f8d32088d17b32b0c877d59965b35c4f51f0ceea,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
fc69f5d9464cdba6db43a525cecde2bf6ddaaa57,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
ab3737fbf17b7a0e790e1315fffe46f615ebde64,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
71f2b368228a748fd348f1abf540236568a61b07,0.0,Entailment,[[    2     0 30495  3760  1757     2]],unshuffled version of the French OSCAR corpus,"[[    0   879  1193  5865  1329  1732     9     5  1515   384  3632  2747
  42168     2]]"
74b0d3ee0cc9b0a3d9b264aba9901ff97048a897,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the parser first learns to parse simple sentences, then proceeds to learn more complex ones. The induction method is iterative, semi-automatic and based on frequent patterns","[[    0   627 48946    78 25269     7 43756  2007 11305     6   172  6938
      7  1532    55  2632  1980     4    20 26076  5448    16 41393  3693
      6  4126    12 27106     8   716    15  7690  8117     2]]"
d92f1c15537b33b32bfc436e6d017ae7d9d6c29a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"four different languages: English, Portuguese, Spanish and French","[[    0 10231   430 11991    35  2370     6 13053     6  3453     8  1515
      2]]"
100cf8b72d46da39fedfe77ec939fb44f25de77f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Chinese dataset BIBREF0,[[    0 24727 41616   163  8863 45935   288     2]]
84aef81dae38e0dca0ad041141df60ab9ac29761,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
52ed2eb6f4d1f74ebdc4dcddcae201786d4c0463,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
49ea25af6f75e2e96318bad5ecf784ce84e4f76b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the Pasokh dataset BIBREF42 ,[[    0   627 11920  1638   298 41616   163  8863 45935  3714  1437     2]]
13d92cbc2c77134626e26166c64ca5c00aec0bf5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"HotspotQA: Yang, Ding, Muppet
Fever: Hanselowski, Yoneda, Nie","[[    0   725  5992  8024  1864   250    35 13262     6 27385     6   256
  43317 50118   597  6294    35  6090  5317  7897     6   854 11469   102
      6   234   324     2]]"
0153f563f5e2680c2de1a5f6d0e443454dc1ef2a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
ff0f77392abc905fe76e0b8c28a76dfb0372a0ec,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"direct similarity over ConceptNet Numberbatch embeddings, the relationships inferred over ConceptNet by SME, features that compose ConceptNet with other resources (WordNet and Wikipedia), and a purely corpus-based feature that looks up two-word phrases in the Google Books dataset","[[    0 27555 37015    81 32253 15721 12270 35001 33183   417  1033     6
      5  4158 42870    81 32253 15721    30  7346   717     6  1575    14
  38003 32253 15721    19    97  1915    36 44051 15721     8 28274   238
      8    10 15430 42168    12   805  1905    14  1326    62    80    12
  14742 22810    11     5  1204 16206 41616     2]]"
c1477a6c86bd1670dd17407590948000c9a6b7c6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"give more independence to the several learning methods (e.g. less human intervention) involved in the studies, increasing the size of the output images","[[    0 26650    55  5201     7     5   484  2239  6448    36   242     4
    571     4   540  1050  6530    43   963    11     5  3218     6  2284
      5  1836     9     5  4195  3156     2]]"
9d578ddccc27dd849244d632dd0f6bf27348ad81,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. 
Using a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.","[[    0 36949    70   414     7  2341    35  6019   480  6479 11621   791
   4824   321     4   398  3897  8611     6  6019   480   163 18854  4824
    321     4  5334   246  8611     6  6019  2744  4054  2744  6335   480
   6479 11621   791  4824   321     4 30205  8611     6  6019  2744  4054
   2744  6335   480   163 18854  4824   321     4   398  2022     6  8611
      6  7224   510   480  6479 11621   791  4824   321     4   466  1646
   8611     6  7224   510   480   163 18854    10   611 19133   321     4
    466  3103     6  8611     6  7224   510  2744  2118  2744  4054  2744
   6335   480  6479 11621   791  4824   321     4   466  1360  8611     6
   7224   510  2744  2118  2744  4054  2744  6335   480   163 18854  4824
    321     4   466  1558  8611     4  1437 50118 36949    10 37105     7
   2341    35   163 18854  4824   321     4   398  5067  8611   634  7224
    510    36   401   530   238   163 18854  4824   321     4 40294  8611
    634  7224   510    36   401   530    43  2055  6019     6  6479 11621
    791  4824   321     4 35781  8611   634  7224   510    36   401   530
    238  6479 11621   791  4824   321     4   398  5220  8611   634  7224
    510    36   401   530    43  2055  6019  2055  5267  2055  6247     4
      2]]"
2d4d0735c50749aa8087d1502ab7499faa2f0dd8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)","[[    0 41895  7878  6532 20057    34   321     4   406  4563     6   112
      4 29567     6   321     4   398  1366     6   321     4 38949  1118
      7   321     4 35534     6   112     4 30043     6   321     4   398
   1558     6   321     4 40149     9   275   194     9     5  1808   898
     15 30750 42001 37943    36  5273   717   238 12303    12  9903  4628
  30750 42001 37943    36  5273  5330 31311 32771 20257  8611    36 36984
   1592     8 19099 32771 20257  8611    36   771    90     4  5438  1592
      2]]"
ca8e023d142d89557714d67739e1df54d7e5ce4b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],inspired by the OntoNotes5 corpus BIBREF7 as well as the ACE (Automatic Content Extraction) English Annotation Guidelines for Entities Version 6.6 2008.06.13 BIBREF8,"[[    0 13028    30     5 13302   139 44691   245 42168   163  8863 45935
    406    25   157    25     5 36211    36 37434 29177 12803 19188 22870
     43  2370   660 44851 36701    13  9860  2192 35110   231     4   401
   2266     4  4124     4  1558   163  8863 45935   398     2]]"
cd8de03eac49fd79b9d4c07b1b41a165197e1adb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The image feature vectors are mapped into BERT embedding dimensions and treated like a text sequence afterwards.,"[[    0   133  2274  1905 44493    32 33493    88   163 18854 33183 11303
  22735     8  3032   101    10  2788 13931 11795     4     2]]"
94c5f5b1eb8414ad924c3568cedd81dc35f29c48,0.0,Entailment,[[    2     0 30495  3760  1757     2]],3000 hard samples are selected from the test set,[[    0 31640   543  7931    32  3919    31     5  1296   278     2]]
65ba7304838eb960e3b3de7c8a367d2c2cd64c54,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
5450f27ccc0406d3bffd08772d8b59004c2716da,0.0,Entailment,[[    2     0 30495  3760  1757     2]],a new metric to reveal a model's robustness against exposure bias,"[[    0   102    92 14823     7  4991    10  1421    18  6295  1825   136
   4895  9415     2]]"
bc1e3f67d607bfc7c4c56d6b9763d3ae7f56ad5b,0.0,Entailment,[[    2     0 30495  3760  1757     2]], improvements of up to 1.5 BLEU over the seq2seq baseline,"[[    0  5139     9    62     7   112     4   245   163  3850   791    81
      5 48652   176 47762 18043     2]]"
7ee660927e2b202376849e489faa7341518adaf9,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," skip-gram, LDA",[[    0 14514    12 28526     6   226  3134     2]]
58f3bfbd01ba9768172be45a819faaa0de2ddfa4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
a3a871ca2417b2ada9df1438d282c45e4b4ad668,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Table TABREF20 , Table TABREF22, Table TABREF23","[[    0 41836   255  4546 45935   844  2156  9513   255  4546 45935  2036
      6  9513   255  4546 45935  1922     2]]"
458dbf217218fcab9153e33045aac08a2c8a38c6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Total number of annotated data:
Semeval'15: 10712
Semeval'16: 28632
Tass'15: 69000
Sentipol'14: 6428","[[    0 37591   346     9 45068  1070   414    35 50118 37504 28017   108
    996    35 13674  1092 50118 37504 28017   108  1549    35 38708  2881
  50118   565  2401   108   996    35  5913   151 50118 35212  1588  1168
    108  1570    35  4430  2517     2]]"
68ba5bf18f351e8c83fae7b444cc50bef7437f13,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"De-En, En-Fr and En-Vi translation tasks","[[    0 13365    12 16040     6  2271    12 29220     8  2271    12 43640
  19850  8558     2]]"
539eb559744641e6a4aefe267cbc4c79e2bcceae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Reddit,[[    0 47758     2]]
79bb1a1b71a1149e33e8b51ffdb83124c18f3e9c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Combined per-pixel accuracy for character line segments is 74.79,"[[    0 42606  6158   228    12 44546  8611    13  2048   516  5561    16
   6657     4  5220     2]]"
c2553166463b7b5ae4d9786f0446eb06a90af458,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL).","[[    0   627   163  8863 45935   245   579 16637   467    31     5  2178
     12   805 28323    36   241 36267     7    25   248 39477   238   163
   8863 45935   401    31     5 17325 28323    36 40106   238     8     5
    163  8863 45935  1225  1844 37700   467    31     5 26739 28323    36
   9009  2492  2118   322     2]]"
102a0439739428aac80ac11795e73ce751b93ea1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],KFTT BIBREF12 and BTEC BIBREF13,"[[    0   530 11615   565   163  8863 45935  1092     8 12482  3586   163
   8863 45935  1558     2]]"
a6d00f44ff8f83b6c1787e39333e759b0c3daf15,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"user holds or points weapons, is seen in a group fashion which displays a gangster culture, or is showing off graffiti, hand signs, tattoos and bulk cash","[[    0 12105  3106    50   332  2398     6    16   450    11    10   333
   2734    61  8612    10  5188  3121  2040     6    50    16  2018   160
  22622     6   865  2434     6 21462     8  8533  1055     2]]"
0d0959dba3f7c15ee4f5cdee51682656c4abbd8f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed of several sememes","[[    0 37504   991   293    32  3527 46195  2833     9  2136 39314     6
      8     5  3099     9   349  2136  1472    16  3700 14092     9   484
   9031   991   293     2]]"
dfd9302615b27abf8cbef1a2f880a73dd5f0c753,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"bert-large-wwm, bert-base, bert-large","[[    0  6747    12 11802    12 33130   119     6   741  2399    12 11070
      6   741  2399    12 11802     2]]"
dc49746fc98647445599da9d17bc004bafdc4579,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
675d7c48541b6368df135f71f9fc13a398f0c8c6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the transformer models,[[    0   627 40878  3092     2]]
acf278679c584ae4f332f6134711602af26edfb4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
73738e42d488b32c9db89ac8adefc75403fa2653,0.0,Entailment,[[    2     0 30495  3760  1757     2]], 69.10%/78.38%,[[   0 5913    4  698  207   73 5479    4 3170  207    2]]
4f1a5eed730fdcf0e570f9118fc09ef2173c6a1b,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," Seq2seq, CVAE, Hierarchical Gated Fusion Unit (HGFU), Mechanism-Aware Neural Machine (MANM)","[[    0  1608  1343   176 47762     6   230  9788   717     6 37859 13161
   3569   272  1070 20914  7545    36   725 27150   791   238 32241  1809
     12   250 10680 44304 14969    36 11350   448    43     2]]"
f08502e952e711c629d40b22ee3f5ff626d62ba8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MT metrics, Readability metrics and other sentence-level features, Metrics based on the baseline QuEst features, Metrics based on other features","[[    0 11674 12758     6  1163  4484 12758     8    97  3645    12  4483
   1575     6  4369 18715   716    15     5 18043  3232 27602  1575     6
   4369 18715   716    15    97  1575     2]]"
1ed006dde28f6946ad2f8bd204f61eda0059a515,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
a3f108f60143d13fe38d911b1cc3b17bdffde3bd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively.","[[    0 41895  7878  1421 35499   321     4  5334     6   321     4   466
   1978     6   321     4  5339   274   134  1471    15 13579     6   289
   8625     6 32721 42532  4067     4     2]]"
eacc1eb65daad055df934e0e878f417b73b2ecc1,4.0,Entailment,[[    2     0 30495  3760  1757     2]],"tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem","[[    0 47173    10  4069     9 43372     8 46478 31648  1757  5587     4
    598 12881    10  2026    11     5 41616    25  2800     6 33898     6
     50 28598     6    10   467   531 22661  4249  7201     8 11305    31
  28274     4  1892    24   531  2845   549   349     9   167 11305     6
     50   103  4069     9   106     6 29865    50  4885 11282     5  2026
      6    61    16    41 31648  1757   936     2]]"
4e8233826f9e04f5763b307988298e73f841af74,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
892c346617a3391c7dafc9da1b65e5ea3890294d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],convolutional neural networks (CNN),[[    0 40474 23794   337 26739  4836    36 16256    43     2]]
f14ff780c28addab1d738f676c4ec0b4106356b6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Meta vertex candidate identification. Edit distance and word lengths distance are used to determine whether two words should be merged into a meta vertex (only if length distance threshold is met, the more expensive edit distance is computed)., The meta vertex creation. As common identifiers, we use the stemmed version of the original vertices and if there is more than one resulting stem, we select the vertex from the identified candidates that has the highest centrality value in the graph and its stemmed version is introduced as a novel vertex (meta vertex).","[[    0 48454 49102  1984 10614     4 39391  4472     8  2136 18915  4472
     32   341     7  3094   549    80  1617   197    28 21379    88    10
  32820 49102    36  8338   114  5933  4472 11543    16  1145     6     5
     55  3214 17668  4472    16 43547   322     6    20 32820 49102  5012
      4   287  1537 46033     6    52   304     5 23494  1732     9     5
   1461 33566  6355     8   114    89    16    55    87    65  5203 10114
      6    52  5163     5 49102    31     5  2006  2261    14    34     5
   1609  1353  1571   923    11     5 20992     8    63 23494  1732    16
   2942    25    10  5808 49102    36 46876 49102   322     2]]"
55c840a2f1f663ab2bff984ae71501b17429d0c0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
6cd01609c8afb425fbed941441a2528123352940,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
6b55b558ed581759425ede5d3a6fcdf44b8082ac,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Naive Bayes, SVM, Maximum Entropy classifiers","[[    0 30612  2088  1501   293     6   208 20954     6 35540  9860 47145
   1380 27368     2]]"
412aff0b2113b7d61c914edf90b90f2994390088,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
3752bbc5367973ab5b839ded08c57f51336b5c3d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Training-22, NLM-180",[[    0 44466    12  2036     6 12817   448    12 14515     2]]
dfc393ba10ec4af5a17e5957fcbafdffdb1a6443,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BiMPM, ESIM, Decomposable Attention Model, KIM, BERT","[[    0 37426   448  5683     6 18366  3755     6  1502  1075 11474   868
  35798  7192     6   229  3755     6   163 18854     2]]"
21a9f1cddd7cb65d5d48ec4f33fe2221b2a8f62e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"$150,000$ tweets",[[   0 1629 6115    6  151 1629 6245    2]]
c1429f7fed5a4dda11ac7d9643f97af87a83508b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation,"[[    0   991 17303  3435  1296     7    99  5239  1022    11     5 10437
   1521  3327     5  4258     9     5  1050 10437     2]]"
eac042734f76e787cb98ba3d0c13a916a49bdfb3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],GENIA corpus,[[    0 30965  2889 42168     2]]
b42323d60827ecf0d9e478c9a31f90940cfae975,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"contains thousands of XML files, each of which are constructed by several records","[[    0 10800  5069  1583     9 46917  6773     6   349     9    61    32
  11236    30   484  2189     2]]"
04aff4add28e6343634d342db92b3ac36aa8c255,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"visual attention is very sparse,  visual component of the attention hasn't learnt any variation over the source encodings","[[    0 42431  1503    16   182 28593     6  1437  7133  7681     9     5
   1503  2282    75 13973   143 21875    81     5  1300  9689  1630  1033
      2]]"
1405824a6845082eae0458c94c4affd7456ad0f7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
0c557b408183630d1c6c325b5fb9ff1573661290,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"73.02% on the uncorrected SNLI-VE test set, achieves 73.18% balanced accuracy when tested on the corrected test set","[[    0  5352     4  4197   207    15     5 16511 42892   196 13687 27049
     12  8856  1296   278     6 35499  6521     4  1366   207  9320  8611
     77  4776    15     5 17261  1296   278     2]]"
586b7470be91efe246c3507b05e30651ea6b9832,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"To capture both high-order structural information of KGs, we used an attention-based embedding propagation method.","[[    0  3972  5604   258   239    12 10337  9825   335     9   229 28393
      6    52   341    41  1503    12   805 33183 11303 44441  5448     4
      2]]"
ee31c8a94e07b3207ca28caef3fbaf9a38d94964,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU, Micro Entity F1, quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5","[[    0 30876   791     6 10719 46718   274   134     6  1318     9     5
   8823   309     7 34726     6  6626  6761     6     8  1050 16741 14186
     15    10  3189    31   112     7   195     2]]"
828615a874512844ede9d7f7d92bdc48bb48b18d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
e9d9bb87a5c4faa965ceddd98d8b80d4b99e339e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"On subtask 3 best proposed model has F1 score of 92.18 compared to best previous F1 score of 88.58.
On subtask 4 best proposed model has 85.9, 89.9 and 95.6 compared to best previous results of 82.9, 84.0 and 89.9 on 4-way, 3-way and binary aspect polarity.","[[    0  4148 30757  4970   155   275  1850  1421    34   274   134  1471
      9  8403     4  1366  1118     7   275   986   274   134  1471     9
   7953     4  4432     4 50118  4148 30757  4970   204   275  1850  1421
     34  5663     4   466     6  8572     4   466     8  6164     4   401
   1118     7   275   986   775     9  7383     4   466     6  7994     4
    288     8  8572     4   466    15   204    12  1970     6   155    12
   1970     8 32771  6659  8385 21528     4     2]]"
20be7a776dfda0d3c9dc10270699061cb9bc8297,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
6a31bd676054222faf46229fc1d283322478a020,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"[error, correction] pairs",[[    0 10975 44223     6 14921   742 15029     2]]
bc9c31b3ce8126d1d148b1025c66f270581fde10,0.0,Entailment,[[    2     0 30495  3760  1757     2]],WN18 and YAGO3-10,[[    0 29722  1366     8   854  3450   673   246    12   698     2]]
bb169a0624aefe66d3b4b1116bbd152d54f9e31b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
fbd094918b493122b3bba99cefe5da80cf88959c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
8e5e03a5f35f0820a3a1651e148dd6faf646fb67,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a BiLSTM without attention (BiLSTM) as well as a single forward-LSTM layer with attention (LSTM+att) and without attention (LSTM), baselines are defined by BIBREF32 who already proposed an LSTM-based architecture that only uses non-temporal features, and the SVM-based estimation model as originally used for reward estimation by BIBREF24","[[    0   102  6479   574  4014   448   396  1503    36 37426   574  4014
    448    43    25   157    25    10   881   556    12   574  4014   448
  10490    19  1503    36   574  4014   448  2744  2611    43     8   396
   1503    36   574  4014   448   238 11909 38630    32  6533    30   163
   8863 45935  2881    54   416  1850    41   226  4014   448    12   805
   9437    14   129  2939   786    12 17950 46249  1575     6     8     5
    208 20954    12   805 32809  1421    25  3249   341    13  7970 32809
     30   163  8863 45935  1978     2]]"
c3d50f1e6942c9894f9a344e7cbc411af01e419c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
ef081d78be17ef2af792e7e919d15a235b8d7275,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MNLI, AG's News, DBPedia",[[    0 35306 27049     6  5680    18   491     6 18496   510 24220     2]]
a9acd1af4a869c17b95ec489cdb1ba7d76715ea4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
5d6cc65b73f428ea2a499bcf91995ef5441f63d4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Through human evaluation where they are asked to evaluate the generated output on a likert scale.,"[[    0 23803  1050 10437   147    51    32   553     7 10516     5  5129
   4195    15    10 26669  2399  3189     4     2]]"
99a10823623f78dbff9ccecb210f187105a196e9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],large Portuguese corpus,[[    0 11802 13053 42168     2]]
536e4a39b654b78228bf55fd09d1b433e0dae447,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
4f12b41bd3bb2610abf7d7835291496aa69fb78c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Appending the domain tag <2domain>"" to the source sentences of the respective corpora","[[    0 19186  4345     5 11170  6694    44    48 41552   176 46400 49721
      7     5  1300 11305     9     5  7091 22997   102     2]]"
29e5e055e01fdbf7b90d5907158676dd3169732d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"merging, concatenating, or averaging the entity and its features to compute its embeddings, graph embedding approaches, matrix factorization to jointly embed KB and textual relations","[[    0  2089  3923     6 10146 26511  1295     6    50  8343     5 10014
      8    63  1575     7 37357    63 33183   417  1033     6 20992 33183
  11303  8369     6 36173  3724  1938     7 13521 33183 29006     8 46478
   3115     2]]"
c69f4df4943a2ca4c10933683a02b179a5e76f64,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"PPL: SVT
Diversity: GVT
Embeddings Similarity: SVT
Human Evaluation: SVT","[[    0   510  7205    35 22753   565 50118   495 31104    35   272 35707
  50118 42578 13093  1033 17110  1571    35 22753   565 50118 33837 37766
     35 22753   565     2]]"
7aa8375cdf4690fc3b9b1799b0f5a9ec1c1736ed,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"No, other baseline metrics they use besides ROUGE-L are n-gram overlap, negative cross-entropy, perplexity, and BLEU.","[[    0  3084     6    97 18043 12758    51   304 12035   248  5061  8800
     12   574    32   295    12 28526 27573     6  2430  2116    12  1342
  47145     6 33708  1571     6     8   163  3850   791     4     2]]"
6b91fe29175be8cd8f22abf27fb3460e43b9889a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Gospel, Sertanejo, MPB, Forr, Pagode, Rock, Samba, Pop, Ax, Funk-carioca, Infantil, Velha-guarda, Bossa-nova and Jovem-guarda","[[    0   534 46895     6   208  2399  1728  3548     6  3957   387     6
    286   338  1479     6 20306  4636     6  2751     6  1960  3178     6
   7975     6 14249  1140     6 28864    12   438  1512 14075     6  7412
    927   718     6 12570  1999    12 12984   102     6 12398   102    12
  38823     8   344  7067   119    12 12984   102     2]]"
82c4863293a179fe5c0d9a1ff17d224bde952f54,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The proposed Multimodal Differential Network (MDN) consists of a representation module and a joint mixture module.,"[[    0   133  1850 14910   757  1630   337 22248  2617  3658    36 12550
    487    43 10726     9    10  8985 20686     8    10  2660 12652 20686
      4     2]]"
fffbd6cafef96eeeee2f9fa5d8ab2b325ec528e6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],58,[[   0 4432    2]]
2376c170c343e2305dac08ba5f5bda47c370357f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. , Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context., Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states., Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. ","[[    0 49187  8911    35    52 38475  1504   335    11  3332    31     5
   6494     6   217  5085     6  7279 22870     6     8 11561 30700    36
  10859 10669    52   766     5   130 30700    25 32721 30700   322  1892
      6    52   341     5 12716   335     9  8866    11 32721 30700     7
   1119     5 12716  8503     4  2156 17432 17362    35    10  3228    12
  46400   724 22538    21  1887   716    15     5  8503     4    20  9355
    420 30700    16  4705    11    80  1319     4   509    16     7 10759
   9946    80  3247    14 12982   583   349    97     4    20    97    16
      7   304    10  9955    50 12716     7 17620   227    80  3247    11
  32721 30700  2801    11     5  5377   482 33854 15937    35   137     5
   4828   414  2783  2012     6    52  1552     5  1138     7   146    10
    650   346     9 25730  3663     8   851   106  6456    59     5  6054
   1318     4  1892     6   157    12 23830  1138    58 11153     7  2764
  15189   309     7     5   576  1175     4    20  1138    58    67   553
      7 45068   877   258  3018   982     8   467   982   482 33854   660
  44851    35    52   341   103  1492     7  6885 45068   877  6054  4504
    309     7  3018   982     6   467   982     6     8  6054 26271     4
   1437     2]]"
75773ee868c0429ccb913eceb367ff0782eeda8a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
827c58f6cab6c6fe7a6c43bdc71150b61ba0eed4,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," Relevance and Subject categorizations are annotated at a percent agreement of $0.71$ and $0.70$, their agreement scores are only fair, at $\alpha =0.27$ and $\alpha =0.29$, Stance and Sentiment, which carry more categories than the former two, is $0.54$ for both. Their agreement scores are also fair, at $\alpha =0.35$ and $\alpha =0.34$, This holds for the Relevant category ($0.81$), the Vaccine category ($0.79$) and the Positive category ($0.64$),  The Negative category yields a mutual F-score of $0.42$, which is higher than the more frequently annotated categories Neutral ($0.23$) and Not clear ($0.31$).","[[    0  1223  9525  2389     8 36994 18072 18391    32 45068  1070    23
     10   135  1288     9    68   288     4  5339  1629     8    68   288
      4  3083 47110    49  1288  4391    32   129  2105     6    23 49959
  29135  5457   288     4  2518  1629     8 49959 29135  5457   288     4
   2890 47110   312  2389     8 12169  8913     6    61  2324    55  6363
     87     5   320    80     6    16    68   288     4  4283  1629    13
    258     4  2667  1288  4391    32    67  2105     6    23 49959 29135
   5457   288     4  2022  1629     8 49959 29135  5457   288     4  3079
  47110   152  3106    13     5  1223 43986  4120  1358   288     4  6668
   1629   238     5 27610   833  4120  1358   288     4  5220  1629    43
      8     5 25968  4120  1358   288     4  4027  1629   238  1437    20
  28150  4120  5167    10  7628   274    12 31673     9    68   288     4
   3714 47110    61    16   723    87     5    55  5705 45068  1070  6363
  35133  1358   288     4  1922  1629    43     8  1491   699  1358   288
      4  2983  1629   322     2]]"
07c9863e1e86c31b740b5b5a77fe8000be00c273,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
3ebdc15480250f130cf8f5ab82b0595e4d870e2f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],77 genres,[[    0  4718 23409     2]]
f12a282571f842b818d4bee86442751422b52337,0.0,Entailment,[[    2     0 30495  3760  1757     2]],TF.IDF-based features,[[    0 20249     4  2688   597    12   805  1575     2]]
f7789313a804e41fcbca906a4e5cf69039eeef9f,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," 20-Newsgroups benchmark corpus , Reuters-21578, LabelMe","[[    0   291    12  5532 36378  5437 42168  2156  1201    12 24355  5479
      6 36870  5096     2]]"
cdc5a998cb73262594cdae1dda49576044da3d3d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],We evaluate the false-reject (FR) and false-accept (FA) tradeoff across several end-to-end models of distinct sizes and computational complexities.,"[[    0   170 10516     5  3950    12   241 21517    36  5499    43     8
   3950    12 35468    36  5944    43   721  1529   420   484   253    12
    560    12  1397  3092     9 11693 10070     8 38163 31163     4     2]]"
6f2f304ef292d8bcd521936f93afeec917cbe28a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],It eliminates non-termination in some models fixing for some models up to 6% of non-termination ratio.,"[[    0   243 28458   786    12 47632    11   103  3092 15435    13   103
   3092    62     7   231   207     9   786    12 47632  1750     4     2]]"
94ec0e205790ec663a5353f3c68c8d77701573c7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
8e630c5a4a8ba0a4f5d8c483a2bf09c4ac8020ce,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
37db7ba2c155c2f89fc7fb51fffd7f193c103a34,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF), gradient boosting (XGB)","[[    0 38873 40419 14969    36   104 20954   238  9359  5580 39974    36
  23345     4 23007   238 34638  5761    36 30455   238 43141 11606    36
   1000  4377    43     2]]"
c1ce652085ef9a7f02cb5c363ce2b8757adbe213,0.0,Entailment,[[    2     0 30495  3760  1757     2]],crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk),"[[    0   438 34298    12    29 22241     5  2783     9     5 41616    15
   1645 35644 19683    36 11674   710   330    43     2]]"
3e0c9469821cb01a75e1818f2acb668d071fcf40,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"overall rating, mean number of turns",[[   0 2137 1250  691    6 1266  346    9 4072    2]]
b3238158392684a5a6b62a7eabaa2a10fbecf3e6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"most relevant content of the website, including all subsites","[[    0  7877  4249  1383     9     5   998     6   217    70 18621  5110
      2]]"
568ce2f5355d009ec9bc1471fb5ea74655f7e554,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
41830ebb8369a24d490e504b7cdeeeaa9b09fd9c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
ed4fb6bce855ca932548689e45fde21f26a71035,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
931a2a13a1f6a8d9107d26811089bdccc39b0800,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"simply split the image into two parts. One for the text input, and the other for the tabular data","[[    0 13092 26308  3462     5  2274    88    80  1667     4   509    13
      5  2788  8135     6     8     5    97    13     5 12207  8244   414
      2]]"
69ef007fc131b04b5b71b0b446db2f77f434f1b3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Tesla and Ford are investigated on how Twitter sentiment could impact the stock price,"[[    0 39971     8  2493    32  6807    15   141   599  5702   115   913
      5   388   425     2]]"
b5a2b03cfc5a64ad4542773d38372fffc6d3eac7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Qualitatively through efficiency, effectiveness and satisfaction aspects and quantitatively through metrics such as precision, recall, accuracy, BLEU score and even human judgement.","[[    0 42170 43127   149  5838     6 12833     8 11658  5894     8 24934
  43127   149 12758   215    25 15339     6  6001     6  8611     6   163
   3850   791  1471     8   190  1050 17219     4     2]]"
10ddac87daf153cf674589cc1c64a795907d5d9a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],significantly improves the accuracy and F1 score of aspect polarity classification,"[[    0 13033 32333 15296     5  8611     8   274   134  1471     9  6659
   8385 21528 20257     2]]"
9aca4b89e18ce659c905eccc78eda76af9f0072a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
a939a53cabb4893b2fd82996f3dbe8688fdb7bbb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
cd37ad149d500e1c7d2de9de1f4bae8dcc443a72,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"task-specific architecture during pre-training (task-specific methods), aim at building a general pre-training architecture to fit all downstream tasks (task-agnostic methods)","[[    0 45025    12 14175  9437   148  1198    12 32530    36 45025    12
  14175  6448   238  4374    23   745    10   937  1198    12 32530  9437
      7  2564    70 18561  8558    36 45025    12 11244 31831  6448    43
      2]]"
c7d99e66c4ab555fe3d616b15a5048f3fe1f3f0e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Visualization of State of the union addresses,[[    0 47008  1938     9   331     9     5  2918  8480     2]]
1ebd6f703458eb6690421398c79abf3fc114148f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"first two systems are transformer models trained on different amounts of data, The third system includes a modification to consider the information of full coreference chains","[[    0  9502    80  1743    32 40878  3092  5389    15   430  5353     9
    414     6    20   371   467  1171    10 29685     7  1701     5   335
      9   455  2731 23861  9781     2]]"
7b47aa6ba247874eaa8ab74d7cb6205251c01eb5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
892e42137b14d9fabd34084b3016cf3f12cac68a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
252677c93feb2cb0379009b680f0b4562b064270,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"6,127 scientific entities, including 2,112 Process, 258 Method, 2,099 Material, and 1,658 Data entities","[[    0   401     6 24174  6441  8866     6   217   132     6 17729 19149
      6 36586 16410     6   132     6   288  2831 26188     6     8   112
      6 36688  5423  8866     2]]"
e4a19b91b57c006a9086ae07f2d6d6471a8cf0ce,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," They focus on exclusivity and semantic coherence measures: Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. They select select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence.","[[    0   252  1056    15 25948  9866     8 46195  1029 40584  1797    35
  37183  7690  1617    11    10   576  5674    14   109    45  2082   182
    747    11    97  7614    32  5915    25   442    14  5674  5451     4
    252  5163  5163     5   545    12 45260  1421     6    61    34     5
   1154  1313 30848    11     5 39974  2564     6     8  1639   723 25948
   9866    23     5   276   672     9 46195  1029 40584     4     2]]"
b458ebca72e3013da3b4064293a0a2b4b5ef1fa6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BIBREF2 , BIBREF1 ",[[    0  5383   387 45935   176  2156   163  8863 45935   134  1437     2]]
8cc56fc44136498471754186cfa04056017b4e54,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Under the retrieval evaluation setting, their proposed model + IR2 had better MRR than NVDM by 0.3769, better MR by 4.6, and better Recall@10 by  20 . 
Under the generative evaluation setting the proposed model + IR2 had better BLEU by 0.044 , better CIDEr by 0.033, better ROUGE by 0.032, and better METEOR by 0.029","[[    0 17245     5 43372 10437  2749     6    49  1850  1421  2055 11594
    176    56   357 18838   500    87   234 28732   448    30   321     4
   3272  4563     6   357 18838    30   204     4   401     6     8   357
  35109  1039   698    30  1437   291   479  1437 50118 17245     5 20181
   3693 10437  2749     5  1850  1421  2055 11594   176    56   357   163
   3850   791    30   321     4 40847  2156   357   230  2688 28012    30
    321     4 40958     6   357   248  5061  8800    30   321     4 40935
      6     8   357 30782   717  3411    30   321     4 41168     2]]"
f513e27db363c28d19a29e01f758437d7477eb24,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"AS Reader, GA Reader, CAS Reader",[[    0  2336 27019     6  9575 27019     6 24775 27019     2]]
9f3444c9fb2e144465d63abf58520cddd4165a01,0.0,Entailment,[[    2     0 30495  3760  1757     2]],gu-EtAl:2018:EMNLP1,"[[    0  5521    12   717    90  7083    35  2464    35  5330   487 21992
    134     2]]"
65e2f97f2fe8eb5c2fa41cb95c02b577e8d6e5ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],number of dialogs that resulted in launching a skill divided by total number of dialogs,"[[    0 30695     9 25730    29    14  4596    11  6627    10  6707  6408
     30   746   346     9 25730    29     2]]"
b9f2a30f5ef664ff845d860cf4bfc2afb0a46e5a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By assessing similarity of 360 pairs of relations from a subset of Wikidata using an integer similarity score from 0 to 4,"[[    0  2765 16629 37015     9 10253 15029     9  3115    31    10 37105
      9 40823   808  2186   634    41 48335 37015  1471    31   321     7
    204     2]]"
12f1919a3e8ca460b931c6cacc268a926399dff4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],AdaBoost-based classifier,[[    0  9167   102 43389    12   805  1380 24072     2]]
bb5697cf352dd608edf119ca9b82a6b7e51c8d21,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained.","[[    0 44863    31    49   173     6    52  2807     7 10334     5  1461
   3780    30 22422  8201  1233  4745     9  1617     6     8  3594     5
   3780   634   129     5 33183   417  1033     9     5  1617  2442     4
      2]]"
242f96142116cf9ff763e97aecd54e22cb1c8b5a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
b85ab5f862221fac819cf2fef239bcb08b9cafc6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],localization accuracy,[[    0 18076  1938  8611     2]]
58f08d38bbcffb2dd9d660faa8026718d390d64b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For each cluster, its overall sentiment score is quantified by the mean of the sentiment scores among all tweets","[[    0  2709   349 18016     6    63  1374  5702  1471    16 24934  3786
     30     5  1266     9     5  5702  4391   566    70  6245     2]]"
5a29b1f9181f5809e2b0f97b4d0e00aea8996892,0.0,Entailment,[[    2     0 30495  3760  1757     2]],It takes into account the agreement between different systems,[[   0  243 1239   88 1316    5 1288  227  430 1743    2]]
6412e97373e8e9ae3aa20aa17abef8326dc05450,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Human evaluators,[[    0 33837 37131   257  3629     2]]
57586358dd01633aa2ebeef892e96a549b1d1930,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"DSTC2, Maluuba",[[    0   495  4014   347   176     6  2529   257 10452     2]]
ff9495982b8821240b8a65eafcc9bb8ed8b8e084,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"estimated test accuracy is highly correlated with actual test accuracy for various datasets, appropriateness of the proposed function for modeling the complex error landscape","[[    0   990 24985  1296  8611    16  2200 34852    19  3031  1296  8611
     13  1337 42532     6 40194   415 14186     9     5  1850  5043    13
  19039     5  2632  5849  5252     2]]"
d58c264068d8ca04bb98038b4894560b571bab3e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
4059c6f395640a6acf20a0ed451d0ad8681bc59b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Answer with content missing: (Formula) Formula is the answer.,"[[    0 33683    19  1383  1716    35    36 30039  5571    43 10454    16
      5  1948     4     2]]"
07580f78b04554eea9bb6d3a1fc7ca0d37d5c612,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"There is no reason to think that this approach wouldn't also be successful for other technical domains. Technical terms are replaced with tokens, therefore so as long as there is a corresponding process for identifying and replacing technical terms in the new domain this approach could be viable.","[[    0   970    16   117  1219     7   206    14    42  1548  1979    75
     67    28  1800    13    97  3165 30700     4 12920  1110    32  4209
     19 22121     6  3891    98    25   251    25    89    16    10 12337
    609    13  9397     8  8119  3165  1110    11     5    92 11170    42
   1548   115    28 10676     4     2]]"
6ad92aad46d2e52f4e7f3020723922255fd2b603,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
8bfbf78ea7fae0c0b8a510c9a8a49225bbdb5383,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the task of detecting anglicisms can be approached as a sequence labeling problem where only certain spans of texts will be labeled as anglicism (in a similar way to an NER task). The chosen model was conditional random field model (CRF), which was also the most popular model in both Shared Tasks on Language Identification for Code-Switched Data","[[    0   627  3685     9 30985  5667  5895 28599    64    28  5508    25
     10 13931 27963   936   147   129  1402 23645     9 14301    40    28
  16274    25  5667  5895  1809    36   179    10  1122   169     7    41
    234  2076  3685   322    20  4986  1421    21 23431  9624   882  1421
     36  9822   597   238    61    21    67     5   144  1406  1421    11
    258 38559   255 40981    15 22205 36309    13  8302    12 15417 17670
   5423     2]]"
6852217163ea678f2009d4726cb6bd03cf6a8f78,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WN18RR, FB15k-237, YAGO3-10","[[    0 29722  1366 25733     6 13042   996   330    12 30069     6   854
   3450   673   246    12   698     2]]"
31b631a8634f6180b20a72477040046d1e085494,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
3fa638e6167e1c7a931c8ee5c0e2e397ec1b6cda,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
a1557ec0f3deb1e4cd1e68f4880dcecda55656dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Northeast U.S., West U.S. and South U.S.","[[    0   487  2723 44085   121     4   104   482   580   121     4   104
      4     8   391   121     4   104     4     2]]"
589be705a5cc73a23f30decba23ce58ec39d313b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the Dutch section of the OSCAR corpus,[[    0   627  5979  2810     9     5   384  3632  2747 42168     2]]
0ca02893bda50007f7a76e7c8804101718fbb01c,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," non-canonical text such as spelling mistakes, typographic errors, colloquialisms, abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs, mentions), non standard syntactic constructions and spelling variations, grammatically incorrect text, mixture of two or more languages","[[    0   786    12 47696  3569  2788   215    25 24684  6160     6 33488
  25510  9126     6  9843 32689  2617 28599     6 40993  1635     6 41046
      6  2888 40163     6  2841  4203   354     6 14224 39535    36 16918
     25 32795  8299     6 44163     6 19197   238   786  2526 45774 28201
  12558  2485     8 24684 18746     6 25187 45236 17401  2788     6 12652
      9    80    50    55 11991     2]]"
77e57d19a0d48f46de8cbf857f5e5284bca0df2b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],30M utterances,[[    0   541   448 18672  5332     2]]
4547818a3bbb727c4bb4a76554b5a5a7b5c5fedb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development","[[    0 44466   414    19 28083   151     6   290 14200     6   204 14200
      6  1878   612     6 45563     8 23221 11305     6     8   262 38601
  11305    13   709     2]]"
78102422a5dc99812739b8dd2541e4fdb5fe3c7a,0.0,Entailment,[[    2     0 30495  3760  1757     2]], current model,[[   0  595 1421    2]]
281cd4e78b27a62713ec43249df5000812522a89,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Average claim length is 8.9 tokens.,[[    0 46315  2026  5933    16   290     4   466 22121     4     2]]
c515269b37cc186f6f82ab9ada5d9ca176335ded,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Using model gradients with respect to input features they presented that the most important model inputs are verbs associated with entities which shows that the model attends to shallow context clues,"[[    0 36949  1421 12003 20676    19  2098     7  8135  1575    51  2633
     14     5   144   505  1421 16584    32 47041  3059    19  8866    61
    924    14     5  1421 16298     7 16762  5377 14885     2]]"
da55878d048e4dca3ca3cec192015317b0d630b1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
599d9ca21bbe2dbe95b08cf44dfc7537bde06f98,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
9257c578ee19a7d93e2fba866be7b0bf1142c393,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
65ad17f614b7345f0077424c04c94971c831585b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BiLSTM with max pooling,[[    0 37426   574  4014   448    19 19220  3716   154     2]]
e930f153c89dfe9cff75b7b15e45cd3d700f4c72,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
14e259a312e653f8fc0d52ca5325b43c3bdfb968,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Yes, Transformer based seq2seq is evaluated with average BLEU 0.519, METEOR 0.388, ROUGE 0.631 CIDEr 2.531 and SER 2.55%.","[[    0  9904     6  5428 22098   716 48652   176 47762    16 15423    19
    674   163  3850   791   321     4 39327     6 30782   717  3411   321
      4 34067     6   248  5061  8800   321     4   401  2983   230  2688
  28012   132     4   245  2983     8 28408   132     4  3118  2153     2]]"
d71772bfbc27ff1682e552484bc7c71818be50cf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the $\mathbf {C}$hinese $\mathbf {A}$rtificial $\mathbf {I}$ntelligence $\mathbf {S}$peakers (CAIS),"[[    0   627 49959 40051 36920 25522   347 24303  1629   298 33965 49959
  40051 36920 25522   250 24303  1629  9713 37465 49959 40051 36920 25522
    100 24303  1629  3999 45852 49959 40051 36920 25522   104 24303  1629
   2379  6650    36  4054  1729    43     2]]"
ba61ed892b4f7930430389e80a0c8e3b701c8e5d,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," functional dissimilarity score, nearest neighbours experiment",[[    0 12628 14863 36692  1571  1471     6 14712 10689  9280     2]]
496304f63006205ee63da376e02ef1b3010c4aa1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
942eb1f7b243cdcfd47f176bcc71de2ef48a17c4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
288613077787159e512e46b79190c91cd4e5b04d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Bi-LSTM, BERT",[[    0 37426    12   574  4014   448     6   163 18854     2]]
0cfaca6f3f33ebdb338c5f991f6a7a33ff33844d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],DeepDive BIBREF1,[[    0 35166   495  2088   163  8863 45935   134     2]]
58b3b630a31fcb9bffb510390e1ec30efe87bfbf,0.0,Entailment,[[    2     0 30495  3760  1757     2]], the facts that are relevant for answering a particular question) are labeled during training.,"[[    0     5  4905    14    32  4249    13 15635    10  1989   864    43
     32 16274   148  1058     4     2]]"
334f90bb715d8950ead1be0742d46a3b889744e7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"No feature is given, only discussion that semantic features are use in practice and yet to be discovered how to embed that knowledge into statistical decision theory framework.","[[    0  3084  1905    16   576     6   129  3221    14 46195  1575    32
    304    11  1524     8   648     7    28  2967   141     7 33183    14
   2655    88 17325   568  6680  7208     4     2]]"
42812113ec720b560eb9463ff5e74df8764d1bff,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
a4422019d19f9c3d95ce8dc1d529bf3da5edcfb1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
675f28958c76623b09baa8ee3c040ff0cf277a5a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"300,000 sentences with 1.5 million single-quiz questions","[[    0  2965     6   151 11305    19   112     4   245   153   881    12
   2253  1210  1142     2]]"
b43a8a0f4b8496b23c89730f0070172cd5dca06a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent.","[[    0  1694  9637    10  2788 13931  2849    12 34728    19    10 37681
   8985  2849    12 34728    25  2343    11 17965 37365 45935   245   479
     20  2788 13931  2849    12 34728 10726     9    41 33183 11303 10490
  49271    19  1878    12 23944  4573   139 30660 33183   417  1033   163
   8863 45935   996  1432    30    80   112    12 23944 15380 23794 13171
      6   172    10 19220    12 10416   154 10490  1432    30    10 19790
  10490     4    20 37681  8985  2849    12 34728 10726     9    80 19790
  13171     4   166 14518   335    31   258  2849    12  4135 11655   149
  10146 26511  1070 37342  2788 26929     8 37681 30464     9 35247   226
  35308  5580 29760     8 15690 12440    36 27049 17314    43  1575   163
   8863 45935  1549    13     5  2788     9   349   618     8    63  4095
      4     2]]"
70f9358dc01fd2db01a6b165e0b4e83e4a9141a7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MMB, DeepWalk, LINE,  Node2vec, TADW, CENE, CANE, WANE, DMTE","[[    0   448  8651     6  8248 33150     6 33654     6  1437 44058   176
  25369     6   255  2606   771     6   230 33978     6 12471   717     6
    305 19621     6 18695  6433     2]]"
54c7fc08598b8b91a8c0399f6ab018c45e259f79,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Proposed method vs best baseline result on Vecmap (Accuracy P@1):
EN-IT: 50 vs 50
IT-EN: 42.67 vs 42.67
EN-DE: 51.6 vs 51.47
DE-EN: 47.22 vs 46.96
EN-FI: 35.88 vs 36.24
FI-EN: 39.62 vs 39.57
EN-ES: 39.47 vs 39.30
ES-EN: 36.43 vs 36.06","[[    0 41895  7878  5448  1954   275 18043   898    15 39800 32557    36
  36984 45386   221  1039   134  3256 50118  2796    12  2068    35   654
   1954   654 50118  2068    12  2796    35  3330     4  4111  1954  3330
      4  4111 50118  2796    12 10089    35  4074     4   401  1954  4074
      4  3706 50118 10089    12  2796    35  4034     4  2036  1954  4059
      4  5607 50118  2796    12 14071    35  1718     4  4652  1954  2491
      4  1978 50118 14071    12  2796    35  3191     4  5379  1954  3191
      4  4390 50118  2796    12  1723    35  3191     4  3706  1954  3191
      4   541 50118  1723    12  2796    35  2491     4  3897  1954  2491
      4  4124     2]]"
af75ad21dda25ec72311c2be4589efed9df2f482,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The system outperforms by 27.7% the LSTM model, 38.5% the RL-SPINN model and 41.6% the Gumbel Tree-LSTM","[[    0   133   467  9980 33334    30   974     4   406   207     5   226
   4014   448  1421     6  2843     4   245   207     5 28483    12  4186
   2444   487  1421     8  3492     4   401   207     5   272  4179   523
  11077    12   574  4014   448     2]]"
11d2f0d913d6e5f5695f8febe2b03c6c125b667c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],using the BLEU score as a quantitative metric and human evaluation for quality,"[[    0 10928     5   163  3850   791  1471    25    10 17836 14823     8
   1050 10437    13  1318     2]]"
5e65bb0481f3f5826291c7cc3e30436ab4314c61,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Entity grid with grammatical relations and RST discourse relations.,"[[    0 49448  7961    19 25187 45816  3115     8   248  4014 19771  3115
      4     2]]"
ae60079da9d3d039965368acbb23c6283bc3da94,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d77c9ede2727c28e0b5a240b2521fd49a19442e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],word embeddings,[[    0 14742 33183   417  1033     2]]
b5bfa6effdeae8ee864d7d11bc5f3e1766171c2d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],all regions except those that are colored black,[[    0  1250  3806  4682   167    14    32 20585   909     2]]
0b39c20db6e60ce07bf5465bd3c08fedc0587780,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," previous emoji embedding methods fail to handle the situation when the semantics or sentiments of the learned emoji embeddings contradict the information from the corresponding contexts BIBREF5 , or when the emojis convey multiple senses of semantics and sentiments ","[[    0   986 21554 33183 11303  6448  5998     7  3679     5  1068    77
      5 46264    50 18316     9     5  2435 21554 33183   417  1033 28204
      5   335    31     5 12337 38270   163  8863 45935   245  2156    50
     77     5  2841  4203   354 15451  1533 24074     9 46264     8 18316
   1437     2]]"
38854255dbdf2f36eebefc0d9826aa76df9637c6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],FarsNet,[[    0   597  2726 15721     2]]
5633d93ef356aca02592bae3dfc1b3ec8fce27dc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
bdd8368debcb1bdad14c454aaf96695ac5186b09,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively, the estimated intensity  is established as mean  squared error.","[[    0 18377    52    33   237 10603     6   440 24679     6  6207 12366
  24679     6 39829 12366 24679     8   755 12366 24679    19    10  1471
      9   321     6   112     6   132     8   155  4067     6     5  2319
  10603  1437    16  2885    25  1266  1437 33756  5849     4     2]]"
f0317e48dafe117829e88e54ed2edab24b86edb1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"if the attention loose track of the objects in the picture and ""gets lost"", the model still takes it into account and somehow overrides the information brought by the text-based annotations","[[    0  1594     5  1503  7082  1349     9     5  8720    11     5  2170
      8    22 30900   685  1297     5  1421   202  1239    24    88  1316
      8  7421 34685  4376     5   335  1146    30     5  2788    12   805
  47234     2]]"
7e161d9facd100544fa339b06f656eb2fc64ed28,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
21656039994cab07f79e89553cbecc31ba9853d4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],document-level variants of the SQuAD dataset ,"[[    0 43017    12  4483 21740     9     5   208 12444  2606 41616  1437
      2]]"
96526a14820b7debfd6f7c5beeade0a854b93d1a,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," trained annotators BIBREF4, crowdsourcing BIBREF5 ","[[    0  5389 45068  3629   163  8863 45935   306     6  8817 27824   163
   8863 45935   245  1437     2]]"
938688871913862c9f8a28b42165237b7324e0de,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
96c20af8bbef435d0d534d10c42ae15ff2f926f8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"potentially indicating a shining through effect, explicitation effect","[[    0  8024 16722  9172    10 21003   149  1683     6 16045  1258  1683
      2]]"
6367877c05beebfdbb31e83c1f25dfddf925b6b6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Cora, Hepth, Zhihu",[[    0   347  4330     6 21658   212     6 20045 30247     2]]
b11ee27f3de7dd4a76a1f158dc13c2331af37d9f,0.0,Entailment,[[    2     0 30495  3760  1757     2]], path ranking-based KGC (PRKGC),"[[    0  2718  7141    12   805   229 11961    36  4454   530 11961    43
      2]]"
4d5e2a83b517e9c082421f11a68a604269642f29,0.0,Entailment,[[    2     0 30495  3760  1757     2]],2,[[  0 176   2]]
cca3301f20db16f82b5d65a102436bebc88a2026,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al, HEOT obtained from one of the past studies done by Mathur et al","[[    0   250 22434 41616    13    10 12337 47510  6245    58    67  4756
     31    10   892  2964    30 10553  4400  1076     6 12925  3293  4756
     31    65     9     5   375  3218   626    30 11945   710  4400  1076
      2]]"
88f8ab2a417eae497338514142ac12c3cec20876,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e196e2ce72eb8b2d50732c26e9bf346df6643f69,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d10e256f2f724ad611fd3ff82ce88f7a78bad7f7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],macro F1 score of 0.62,[[    0 15439  1001   274   134  1471     9   321     4  5379     2]]
99ef97336c0112d9f60df108f58c8b04b519a854,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e9d882775a132e172eea68ab6ab4621a924bb6b8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],attention parsing,[[    0  2611 19774 46563     2]]
6bc45d4f908672945192390642da5a2760971c40,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
46570c8faaeefecc8232cfc2faab0005faaba35f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SemEval 2018 Task 3, BIBREF20, BIBREF4, SARC 2.0, SARC 2.0 pol, Sarcasm Corpus V1 (SC-V1), Sarcasm Corpus V2 (SC-V2)","[[    0 37504   717  6486   199 12927   155     6   163  8863 45935   844
      6   163  8863 45935   306     6   208 11969   132     4   288     6
    208 11969   132     4   288  8385     6   208  9636 16836 28556   468
    134    36  3632    12   846   134   238   208  9636 16836 28556   468
    176    36  3632    12   846   176    43     2]]"
d28260b5565d9246831e8dbe594d4f6211b60237,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"boost the training BLEU very greatly, the over-fitting problem of the Plackett-Luce models PL($k$) is alleviated with moderately large $k$","[[    0 33934     5  1058   163  3850   791   182  8908     6     5    81
     12 22605   936     9     5  3037  2990  2645    12   574 15431  3092
  12901 44688   330  1629    43    16 32216 15253    19 30389   739    68
    330  1629     2]]"
f463db61de40ae86cf5ddd445783bb34f5f8ab67,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Perceptron model using the local features.,[[    0 20823 16771  2839  1421   634     5   400  1575     4     2]]
235c156d9c2adc895c9113f53c60f2dd8df45834,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Mandarin, English",[[    0 37136 15394     6  2370     2]]
2ff3898fbb5954aa82dd2f60b37dd303449c81ba,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Penn Treebank, Text8, WSJ10",[[    0 35104 11077  5760     6 14159   398     6 18483   863   698     2]]
5c4a2a3d6e02bcbeae784e439441524535916e85,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
7ac0cec79c8c2b1909b0a1cc0d4646fce09884ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d7d41a1b8bbb1baece89b28962d23ee4457b9c3a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Mandarin, English",[[    0 37136 15394     6  2370     2]]
545ff2f76913866304bfacdb4cc10d31dbbd2f37,0.0,Entailment,[[    2     0 30495  3760  1757     2]],WMT 2014 En-Fr parallel corpus,[[    0   771 11674   777  2271    12 29220 12980 42168     2]]
f9c5799091e7e35a8133eee4d95004e1b35aea00,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Exp. 5.1,[[    0 39891     4   195     4   134     2]]
5455b3cdcf426f4d5fc40bc11644a432fa7a5c8f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],well-formed sentences vs concise answers,[[    0  3056    12 10312 11305  1954  6561  5274     2]]
11c77ee117cb4de825016b6ccff59ff021f84a38,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"$2.2\%$ absolute accuracy improvement on the laptops test set, $3.6\%$ accuracy improvement on the restaurants test set","[[    0  1629   176     4   176 37457   207  1629  7833  8611  3855    15
      5 15962  1296   278     6    68   246     4   401 37457   207  1629
   8611  3855    15     5  4329  1296   278     2]]"
5f6fac08c97c85d5f4f4d56d8b0691292696f8e6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
96be67b1729c3a91ddf0ec7d6a80f2aa75e30a30,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English,[[    0 35007     2]]
9555aa8de322396a16a07a5423e6a79dcd76816a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],w.r.t Rouge-1 their model outperforms by 0.98% and w.r.t Rouge-L their model outperforms by 0.45%,"[[    0   605     4   338     4    90 14941    12   134    49  1421  9980
  33334    30   321     4  5208   207     8   885     4   338     4    90
  14941    12   574    49  1421  9980 33334    30   321     4  1898   207
      2]]"
728a55c0f628f2133306b6bd88af00eb54017b12,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Winter and summer words formed two separate clusters. Week day and week-end day words also formed separate clusters.,"[[    0 33120     8  1035  1617  4829    80  2559 28255     4  2852   183
      8   186    12  1397   183  1617    67  4829  2559 28255     4     2]]"
7d3c036ec514d9c09c612a214498fc99bf163752,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Online sites tagged as fake news site by Verafiles and NUJP and news website in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera","[[    0 19449  3091 20390    25  4486   340  1082    30  3060  2001  4755
      8   234   791 12887     8   340   998    11     5  5639     6   217
  13601  1588  1696  2141   234 22019   261     6  2060  5285     6     8
    163  6072   102     2]]"
4b0ba460ae3ba7a813f204abd16cf631b871baca,0.0,Entailment,[[    2     0 30495  3760  1757     2]],text clustering on the embeddings of texts,[[    0 29015 46644  2961    15     5 33183   417  1033     9 14301     2]]
9c4ed8ca59ba6d240f031393b01f634a9dc3615d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"VecMap, Muse, Barista",[[    0   846  3204 41151     6 19345     6  1731  6377     2]]
4e468ce13b7f6ac05371c62c08c3cec1cd760517,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
13e87f6d68f7217fd14f4f9a008a65dd2a0ba91c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Experiment 1: ACC around 0.5 with 50% noise rate in worst case - clearly higher than baselines for all noise rates
Experiment 2: ACC on real noisy datasets: 0.7 on Movie, 0.79 on Laptop, 0.86 on Restaurant (clearly higher than baselines in almost all cases)","[[    0 45395  8913   112    35 10018   198   321     4   245    19   654
    207  6496   731    11  2373   403   111  2563   723    87 11909 38630
     13    70  6496  1162 50118 45395  8913   132    35 10018    15   588
  28269 42532    35   321     4   406    15 13131     6   321     4  5220
     15   226 39782     6   321     4  5334    15 11561    36 18763   352
    723    87 11909 38630    11   818    70  1200    43     2]]"
03fb4b31742820df58504575c562bee672e016be,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
022e5c996a72aeab890401a7fdb925ecd0570529,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards","[[    0 48147   254 25269     7 14660     5 14135 10014    31  9781  3919
     30    10   157    12 23830 17816   254     6     8    24  1795     5
  17816   254  1058    30  1976  1823 12840     2]]"
f1214a05cc0e6d870c789aed24a8d4c768e1db2f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"German-English, Turkish-English, English-German","[[    0 27709    12 35007     6  4423    12 35007     6  2370    12 27709
      2]]"
def3d623578bf84139d920886aa3bd6cdaaa7c41,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Arabic, Czech and Turkish",[[    0 33054   636     6  9096     8  4423     2]]
fc5f9604c74c9bb804064f315676520937131e17,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BLEU scores and the slot error rate (ERR),"[[    0 30876   791  4391     8     5  8534  5849   731    36  2076   500
     43     2]]"
b27f7993b1fe7804c5660d1a33655e424cea8d10,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Profile pictures from the Twitter users' profiles.,[[    0 47555  3493    31     5   599  1434   108 11729     4     2]]
c1c611409b5659a1fd4a870b6cc41f042e2e9889,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence.","[[    0 30876   791  4391     6  6089  2856     9  1617    11   258 41762
      8  5674 30283     6     8 12793   833 20097     9 12142 11305    13
   1029 40584     4     2]]"
d3341eefe4188ee8a68914a2e8c9047334997e84,0.0,Entailment,[[    2     0 30495  3760  1757     2]],concatenation consistently outperforms the gated-attention mechanism for both training and testing instructions,"[[    0  3865  8729   225  1258  6566  9980 33334     5   821  1070    12
   2611 19774  9562    13   258  1058     8  3044  9223     2]]"
d86c7faf5a61d73a19397a4afa2d53206839b8ad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Language, Vision, Acoustic",[[    0 46969     6 13396     6  6208 38645     2]]
427252648173c3ba78c211b86fa89fc9f4406653,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (Experimental setup not properly rendered) In our experiments we used seven target domains: Business and Commerce (BUS), Government and Politics (GOV), Physical and Mental Health (HEA), Law and Order (LAW),
Lifestyle (LIF), Military (MIL), and General Purpose (GEN). Exceptionally, GEN does
not have a natural root category.","[[    0 33683    19  1383  1716    35    36 45395 40955 11808    45  5083
  18728    43    96    84 15491    52   341   707  1002 30700    35    44
     48 18562     8  5669    17    46    36 22295   238    44    48 28747
      8 16226    17    46    36   534 13565   238    44    48 46577     8
  16860  1309    17    46    36 17779   250   238    44    48 22532     8
   9729    17    46    36   574 10751   238 50118    17    48   574 27210
     17    46    36   574  7025   238    44    48 43441    17    46    36
    448  3063   238     8    44    48  9344 33482    17    46    36 30965
    322 47617  2368     6 32520   473 50118  3654    33    10  1632  9749
   4120     4     2]]"
be595b2017545b0359db6abf4914a155bdd10d23,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," text in their tweets and profile descriptions, their emoji use, their profile images, and music interests embodied by links to YouTube music videos, can help a classifier distinguish between gang and non-gang member profiles","[[    0  2788    11    49  6245     8  4392 24173     6    49 21554   304
      6    49  4392  3156     6     8   930  3168 34799    30  5678     7
   4037   930  3424     6    64   244    10  1380 24072 22929   227  5188
      8   786    12 21251   919 11729     2]]"
940873658ee64e131cafcf9b3d26a45a98920cc2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
8e898bec123c70315db44f6c8002adc8bf4486ad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
cc9f0ac8ead575a9b485a51ddc06b9ecb2e2a44d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Compared with the previous SOTA without BERT on SParC, our model improves Ques.Match and Int.Match by $10.6$ and $5.4$ points, respectively.","[[    0 44891    19     5   986   208 24881   396   163 18854    15  6178
    271   347     6    84  1421 15296  1209  3663     4 42633     8  7299
      4 42633    30    68   698     4   401  1629     8    68   245     4
    306  1629   332     6  4067     4     2]]"
e020677261d739c35c6f075cde6937d0098ace7f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"HDGAN produced relatively better visual results on the CUB and Oxford datasets while AttnGAN produced far more impressive results than the rest on the more complex COCO dataset, In terms of inception score (IS), which is the metric that was applied to majority models except DC-GAN, the results in Table TABREF48 show that StackGAN++ only showed slight improvement over its predecessor, text to image synthesis is continuously improving the results for better visual perception and interception","[[    0 13022 38416  2622  3487   357  7133   775    15     5   230 12027
      8  9238 42532   150  7279   282 38416  2622   444    55  3444   775
     87     5  1079    15     5    55  2632   230  4571   673 41616     6
     96  1110     9 17692  1471    36  1729   238    61    16     5 14823
     14    21  5049     7  1647  3092  4682  5815    12 38416     6     5
    775    11  9513   255  4546 45935  3818   311    14 31197 38416 42964
    129   969  7019  3855    81    63  9933     6  2788     7  2274 37423
     16 13307  3927     5   775    13   357  7133 10518     8 11949     2]]"
542a87f856cb2c934072bacaa495f3c2645f93be,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Care / Harm, Fairness / Cheating, Loyalty / Betrayal, Authority / Subversion, and Sanctity / Degradation","[[    0 16431  1589 33619     6  3896  1825  1589  3576  1295     6 33872
   2553  1589  6525  5022   337     6  4305  1589  4052 21747     6     8
  46080  1571  1589   926 47055     2]]"
cfbccb51f0f8f8f125b40168ed66384e2a09762b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They perform t-SNE clustering to analyze discourse embeddings,"[[    0  1213  3008   326    12   104  9009 46644  2961     7 11526 19771
  33183   417  1033     2]]"
b0799e26152197aeb3aa3b11687a6cc9f6c31011,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), Textual Kernels Model (TKM)","[[    0 47702  2585  8729   225  1258  7192    36  5268   448   238  2064
  37438  2585  8729   225  1258  7192    36  3632   448   238 14159  5564
    229 47909  7192    36   565   530   448    43     2]]"
480e10e5a1b9c0ae9f7763b7611eeae9e925096b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
bc26eee4ef1c8eff2ab8114a319901695d044edb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"pairing crowdworkers and having half of them acting as Wizards by limiting their dialogue options only to relevant and plausible ones, at any one point in the interaction","[[    0 44170   154  2180 16941     8   519   457     9   106  3501    25
  17173    30 11361    49  6054  1735   129     7  4249     8 27099  1980
      6    23   143    65   477    11     5 10405     2]]"
1cbca15405632a2e9d0a7061855642d661e3b3a7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Their GTRS approach got an improvement of 3.89% compared to SVM and 27.91% compared to Pawlak.,"[[    0 16837   272  6997   104  1548   300    41  3855     9   155     4
   5046   207  1118     7   208 20954     8   974     4  6468   207  1118
      7 15822   462   677     4     2]]"
1ccc4f63268aa7841cc6fd23535c9cbe85791007,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a48cc6d3d322a7b159ff40ec162a541bf74321eb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Word Sense Induction & Disambiguation,[[    0 44051 19484  4619 27345   359  6310  3146  1023  9762     2]]
225a567eeb2698a9d3f1024a8b270313a6d15f82,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"RNN model, CNN model , RNN-CNN model, attn1511 model, Deep Averaging Network model, avg mean of word embeddings in the sentence with projection matrix","[[    0   500 20057  1421     6  3480  1421  2156   248 20057    12 16256
   1421     6 15095   282   996  1225  1421     6  8248    83  2802  6257
   3658  1421     6 44678  1266     9  2136 33183   417  1033    11     5
   3645    19 18144 36173     2]]"
ef081d78be17ef2af792e7e919d15a235b8d7275,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MNLI BIBREF21, AG's News BIBREF22, DBPedia BIBREF23","[[    0 35306 27049   163  8863 45935  2146     6  5680    18   491   163
   8863 45935  2036     6 18496   510 24220   163  8863 45935  1922     2]]"
38e11663b03ac585863742044fd15a0e875ae9ab,0.0,Entailment,[[    2     0 30495  3760  1757     2]], peoples' sentiments expressed over social media,[[    0 16592   108 18316  2327    81   592   433     2]]
20df24165b881f97dc1ac32f343939554dd68011,0.0,Entailment,[[    2     0 30495  3760  1757     2]],linear logistic regression to a set of stock technical signals,"[[    0 43871  7425  5580 39974     7    10   278     9   388  3165  8724
      2]]"
5908d7fb6c48f975c5dfc5b19bb0765581df2b25,0.0,Entailment,[[    2     0 30495  3760  1757     2]],3189 rows of text messages,[[    0   246 28945 22162     9  2788  3731     2]]
86083a02cc9a80b31cac912c42c710de2ef4adfd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"model is trained to predict language IDs as well as the subwords, we add language IDs in the CS point of transcriptio","[[    0 21818    16  5389     7  7006  2777 35172    25   157    25     5
   2849 30938     6    52  1606  2777 35172    11     5  7038   477     9
  12348  1020     2]]"
04e90c93d046cd89acef5a7c58952f54de689103,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CMRC-2017, People's Daily (PD), Children Fairy Tales (CFT) , Children's Book Test (CBT)","[[    0 18814  5199    12  3789     6  1806    18  1681    36  6153   238
   4278 37857 32419    36   347 11615    43  2156  4278    18  5972  4500
     36   347 13269    43     2]]"
48b12eb53e2d507343f19b8a667696a39b719807,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"feelings of suspense experienced in narratives not only respond to the trajectory of the plot's content, but are also directly predictive of aesthetic liking (or disliking), Emotions that exhibit this dual capacity have been defined as aesthetic emotions","[[    0 35702  1033     9 31803  2984    11 24493    45   129  2519     7
      5 15217     9     5  6197    18  1383     6    53    32    67  2024
  27930     9 17338 25896    36   368 19131 26897   238  3676 26859    14
   8483    42  6594  2148    33    57  6533    25    44    48   102 38248
   8597    17    46     2]]"
56a3c9bd74c6573abce3805177cdebf941db0b71,0.0,Entailment,[[    2     0 30495  3760  1757     2]],manually reviewed,[[    0   397 13851  7123     2]]
f398587b9a0008628278a5ea858e01d3f5559f65,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SPNet vs best baseline:
ROUGE-1: 90.97 vs 90.68
CIC: 70.45 vs 70.25","[[    0  4186 15721  1954   275 18043    35 50118   500  5061  8800    12
    134    35  1814     4  6750  1954  1814     4  4671 50118   347  2371
     35  1510     4  1898  1954  1510     4  1244     2]]"
d7d611f622552142723e064f330d071f985e805c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Total number of utterances available is: 70607 (37344 ENG + 33263 GER),"[[    0 37591   346     9 18672  5332   577    16    35  1510 32774    36
   3272 31907 40565  2055  2357 29969 35683    43     2]]"
9a9338d0e74fd315af643335e733445031bd7656,0.0,Entailment,[[    2     0 30495  3760  1757     2]], AMI IHM meeting corpus,[[    0  3326   100    38 35441   529 42168     2]]
a7510ec34eaec2c7ac2869962b69cc41031221e5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],52.0%,[[   0 4429    4  288  207    2]]
2c4003f25e8d95a3768204f52a7a5f5e17cb2102,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
8b43201e7e648c670c02e16ba189230820879228,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
21c104d14ba3db7fe2cd804a191f9e6258208235,0.0,Entailment,[[    2     0 30495  3760  1757     2]],PAR score,[[    0 14280  1471     2]]
5367f8979488aaa420d8a69fec656851095ecacb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"attention probabilities learned tend to respect the conditions of Lemma UNKREF15, corroborating our hypothesis, validate that our model learns a meaningful classifier we compare it to the standard ResNet18","[[    0  2611 19774 43471  2435  3805     7  2098     5  1274     9 13956
   1916  2604   530 45935   996     6 25223  1295    84 31098     6 28754
     14    84  1421 25269    10  6667  1380 24072    52  8933    24     7
      5  2526  4787 15721  1366     2]]"
c95fd189985d996322193be71cf5be8858ac72b5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"sentiment analysis, information extraction, document summarization, spoken dialogue, cross lingual (research), dialogue, systems, language generation","[[    0 19530  8913  1966     6   335 23226     6  3780 39186  1938     6
   5826  6054     6  2116 24433  5564    36  6762   238  6054     6  1743
      6  2777  2706     2]]"
cf63a4f9fe0f71779cf5a014807ae4528279c25a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Automatic transcription of 5000 tokens through sequential neural models trained on the annotated part of the corpus,"[[    0 37434 29177 37118     9 23221 22121   149 29698 26739  3092  5389
     15     5 45068  1070   233     9     5 42168     2]]"
62736ad71c76a20aee8e003c462869bab4ab4b1e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"draw our data from news publications, wine reviews, and Reddit, develop new metrics for the agreement of binomial orderings across communities and the movement of binomial orderings over time,  develop a null model to determine how much variation in binomial orderings we might expect across communities and across time","[[    0 24686    84   414    31   340 16043     6  3984  6173     6     8
   6844     6  2179    92 12758    13     5  1288     9  6870 48866   645
   1033   420  1822     8     5  2079     9  6870 48866   645  1033    81
     86     6  1437  2179    10 23796  1421     7  3094   141   203 21875
     11  6870 48866   645  1033    52   429  1057   420  1822     8   420
     86     2]]"
e28019afcb55c01516998554503bc1b56f923995,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Personal thought of the annotator.,[[    0 43854   802     9     5 45068  2630     4     2]]
06095a4dee77e9a570837b35fc38e77228664f91,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences ,"[[    0   627 41616 10726     9 38951   690   217 11305     8  1142     8
   5274    59 16570  1836     8 25806 14970  5510    98    24   473   680
    943 11305  1437     2]]"
d82ec1003a3db7370994c7522590f7e5151b1f33,0.0,Entailment,[[    2     0 30495  3760  1757     2]],We collected the historical 52 week stock prices prior to this event and calculated the daily stock price change. The distribution of the daily price change of the previous 52 weeks is Figure FIGREF13 with a mean INLINEFORM1 and standard deviation INLINEFORM2 . ,"[[    0   170  4786     5  4566  3135   186   388   850  2052     7    42
    515     8  9658     5  1230   388   425   464     4    20  3854     9
      5  1230   425   464     9     5   986  3135   688    16 17965 37365
  45935  1558    19    10  1266  2808 28302 38036   134     8  2526 38369
   2808 28302 38036   176   479  1437     2]]"
479d334b79c1eae3f2aa3701d28aa0d8dd46036a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
3a3a65c65cebc2b8c267c334e154517d208adc7d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Multi-Encoder, Constrained-Decoder model","[[    0 46064    12 45780 15362     6  2585  6031  7153    12 15953 15362
   1421     2]]"
19225e460fff2ac3aebc7fe31fcb4648eda813fb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Common Crawl ,[[    0 40437   230 33889  1437     2]]
cb20aebfedad1a306e82966d6e9e979129fcd9f9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],weighted F1-score,[[    0  4301   196   274   134    12 31673     2]]
02348ab62957cb82067c589769c14d798b1ceec7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BiGRUs with attention, ROUGE, Language model, and next sentence prediction ","[[    0 37426 11621 16419    19  1503     6   248  5061  8800     6 22205
   1421     6     8   220  3645 16782  1437     2]]"
3241f90a03853fa85d287007d2d51e7843ee3d9b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],non-native speech from the BULATS test ,"[[    0 13424    12 34551  1901    31     5   163  6597 26063  1296  1437
      2]]"
572458399a45fd392c3a4e07ce26dcff2ad5a07d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For the Oshiete-goo dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, Trans, by 0.021, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.037.  For the nfL6 dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, CLSTM, by 0.028, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.040. Human evaluation of the NAGM's generated outputs for the Oshiete-goo dataset had 47% ratings of (1), the highest rating, while CLSTM only received 21% ratings of (1). For the nfL6 dataset, the comparison of (1)'s was NAGM's 50% to CLSTM's 30%. ","[[    0  2709     5 16519  5810   242    12 43391 41616     6     5   234
   3450   448  1421    18   248  5061  8800    12   574  1471    16   723
     87     5  1609  4655  9164  1421     6  5428     6    30   321     4
  41656     6     8    63   163  3850   791    12   306  1471    16   723
     87     5  1609  4655  1421  5289  4014   448    30   321     4   288
   3272     4  1437   286     5   295   506   574   401 41616     6     5
    234  3450   448  1421    18   248  5061  8800    12   574  1471    16
    723    87     5  1609  4655  9164  1421     6  5289  4014   448     6
     30   321     4 40709     6     8    63   163  3850   791    12   306
   1471    16   723    87     5  1609  4655  1421  5289  4014   448    30
    321     4 36016     4  3861 10437     9     5   234  3450   448    18
   5129 39512    13     5 16519  5810   242    12 43391 41616    56  4034
    207  2945     9    36   134   238     5  1609   691     6   150  5289
   4014   448   129   829   733   207  2945     9    36   134   322   286
      5   295   506   574   401 41616     6     5  6676     9    36   134
  30171    29    21   234  3450   448    18   654   207     7  5289  4014
    448    18   389  2153  1437     2]]"
676c874266ee0388fe5b9a75e1006796c68c3c13,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
508580af51483b5fb0df2630e8ea726ff08d537b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"We experiment with three different pretrained representations: ELMo BIBREF5 , BERT BIBREF6 , and GloVe BIBREF18 . To produce a single city embedding, we compute the TF-IDF weighted element-wise mean of the token-level representations. For all pretrained methods, we additionally reduce the dimensionality of the city embeddings to 40 using PCA for increased compatibility with our clustering algorithm.","[[    0   170  9280    19   130   430 11857 26492 30464    35 17678 17357
    163  8863 45935   245  2156   163 18854   163  8863 45935   401  2156
      8  4573   139 30660   163  8863 45935  1366   479   598  2592    10
    881   343 33183 11303     6    52 37357     5 35690    12  2688   597
  19099  7510    12 10715  1266     9     5 19233    12  4483 30464     4
    286    70 11857 26492  6448     6    52 31151  1888     5 21026  6948
      9     5   343 33183   417  1033     7   843   634  4985   250    13
   1130 29988    19    84 46644  2961 17194     4     2]]"
b6858c505936d981747962eae755a81489f62858,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BiLSTMs + CRF architecture BIBREF36, sententce-state LSTM BIBREF21","[[    0 37426   574  4014 13123  2055  4307   597  9437   163  8863 45935
   3367     6  1051  1342  1755    12  4897   226  4014   448   163  8863
  45935  2146     2]]"
3837ae1e91a4feb27f11ac3b14963e9a12f0c05e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"6)Contributor first names, 7)Contributor last names, 8)Contributor types (""soprano"", ""violinist"", etc.), 9)Classical work types (""symphony"", ""overture"", etc.), 10)Musical instruments, 11)Opus forms (""op"", ""opus""), 12)Work number forms (""no"", ""number""), 13)Work keys (""C"", ""D"", ""E"", ""F"" , ""G"" , ""A"", ""B"", ""flat"", ""sharp""), 14)Work Modes (""major"", ""minor"", ""m"")","[[    0   401    43 47222   368    78  2523     6   262    43 47222   368
     94  2523     6   290    43 47222   368  3505  6697    29  1517 20426
   1297    22 37834   179   661  1297  4753 12345   361    43 21527  3569
    173  3505  6697 45266 29788  1297    22   139  9942  2407  1297  4753
  12345   158    43 35136  3569  9571     6   365    43 13926   687  4620
   6697  1517  1297    22 33259 16844   316    43 21461   346  4620  6697
   2362  1297    22 30695 16844   508    43 21461 10654  6697   347  1297
     22   495  1297    22   717  1297    22   597   113  2156    22   534
    113  2156    22   250  1297    22   387  1297    22 35429  1297    22
  41081 16844   501    43 21461 47440  6697 25430  1297    22  4691   368
   1297    22   119  8070     2]]"
7efbe48e84894971d7cd307faf5f6dae9d38da31,0.0,Entailment,[[    2     0 30495  3760  1757     2]],300-hour English conversational speech,[[    0  2965    12  4509  2370 28726  5033  1901     2]]
a37ef83ab6bcc6faff3c70a481f26174ccd40489,0.0,Entailment,[[    2     0 30495  3760  1757     2]], four different annotators,[[    0   237   430 45068  3629     2]]
bea60603d78baeeb6df1afb53ed08d8296b42f1e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
61c9f97ee1ac5a4b8654aa152f05f22e153e7e6e,0.0,Entailment,[[    2     0 30495  3760  1757     2]], Wikipedia toxic comments,[[    0 28274  8422  1450     2]]
d216d715ec27ee2d4949f9e908895a18fb3238e2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"word length, number of phonemes, number of syllables, alphabetical order, and frequency","[[    0 14742  5933     6   346     9 43676   991   293     6   346     9
  37201  6058     6 34555  3569   645     6     8 13135     2]]"
66f0dee89f084fe0565539a73f5bbe65f3677814,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
a5abd4dd91e6f2855e9098bd6ae1481c0fdb0d4a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"TB-Dense,  MATRES",[[    0 16566    12   495  9401     6  1437 25899 32030     2]]
0e45aae0e97a6895543e88705e153f084ce9c136,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
ad362365656b0b218ba324ae60701eb25fe664c1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"syntactic information, semantic and topical information",[[    0  8628  3999 28201   335     6 46195     8 33469   335     2]]
981443fce6167b3f6cadf44f9f108d68c1a3f4ab,0.0,Entailment,[[    2     0 30495  3760  1757     2]],german ,[[   0 2403  397 1437    2]]
24014a040447013a8cf0c0f196274667320db79f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"model overall still gives 1.0% higher average UAS and LAS than the previous best parser, BIAF, our model reports more than 1.0% higher average UAS than STACKPTR and 0.3% higher than BIAF","[[    0 21818  1374   202  2029   112     4   288   207   723   674   121
   2336     8   226  2336    87     5   986   275 48946     6   163  2889
    597     6    84  1421   690    55    87   112     4   288   207   723
    674   121  2336    87  4062 14940   510  6997     8   321     4   246
    207   723    87   163  2889   597     2]]"
04f72eddb1fc73dd11135a80ca1cf31e9db75578,0.0,Entailment,[[    2     0 30495  3760  1757     2]],278 more annotations,[[    0 30442    55 47234     2]]
eea089baedc0ce80731c8fdcb064b82f584f483a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members, within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers ","[[    0 27217  2192    14    32 17407    30 14120     6  5861    12   658
  40651  1383    33   723  3018 16580  1162     6    53    67  8483  2514
  39608 10778    14  2559 19298    31  2885   453     6   624 16141  1822
      6  2885  1434    33    41  1130 36186     7  4949    19     5   435
     18 14120  1383     6  1118     7 19298  1437     2]]"
537b2d7799124d633892a1ef1a485b3b071b303d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],WNLaMPro dataset,[[    0 29722 10766   448 10653 41616     2]]
21f615bf19253fc27ea838012bc088f4d10cdafd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
8e8097cada29d89ca07166641c725e0f8fed6676,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument.","[[    0  5771 15190     5   913     9    10  2026     6  1434    33   899
      7     5   455  4795  5377     8  3891     6    51    64  7118   141
    913  2650    10  2026    16    11     5   576  5377     9    41  4795
      4     2]]"
63bb39fd098786a510147f8ebc02408de350cb7c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
0fec9da2bc80a12a7a6d6600b9ecf3e122732b60,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
415f35adb0ef746883fb9c33aa53b79cc4e723c3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Gendered characters in the dataset,[[    0   534 36318  3768    11     5 41616     2]]
fb56743e942883d7e74a73c70bd11016acddc348,0.0,Entailment,[[    2     0 30495  3760  1757     2]], BABEL speech corpus ,[[    0   163  4546  3721  1901 42168  1437     2]]
1d791713d1aa77358f11501f05c108045f53c8aa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],1061,[[   0  698 5606    2]]
6e2ad9ad88cceabb6977222f5e090ece36aa84ea,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254.,"[[    0   133 18043  1421    16    10  1844 13931    12   560    12 46665
   9689 15362    73 11127 15362  1421    19  1503     4    20  9689 15362
     16    10  2311 43606   337  2597    12 34256 25569 25940  1640   574
   4014   448    43  3551   163  8863 45935  1570     8     5  5044 15362
     10   881   226  4014   448  3551    19  1503  9562     4    20  1503
   9562    16 43547    25    11   163  8863 45935   466     8    52   304
     10 34405  1707    13 46133     4   166  2341   253    12   560    12
   1397   217     5  1617 33183   417  1033     4    20 33183 11303  1836
    341    16     9 13950     8     5  7397   194  1836     9     5   226
   4014   448  4590    16     9 35381     4     2]]"
43f86cd8aafe930ebb35ca919ada33b74b36c7dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"In four entity-centric ways - entity-first, entity-last, document-level and sentence-level","[[    0  1121   237 10014    12 19483  1319   111 10014    12  9502     6
  10014    12 13751     6  3780    12  4483     8  3645    12  4483     2]]"
b3bd217287b8c765b0d461dc283afec779dbf039,0.0,Entailment,[[    2     0 30495  3760  1757     2]],hybrid approach,[[    0 11108 21676  1548     2]]
f0e8f045e2e33a2129e67fb32f356242db1dc280,0.0,Entailment,[[    2     0 30495  3760  1757     2]],applying reasoning BIBREF36 or irony detection methods BIBREF37,"[[    0  3340 13010 20511   163  8863 45935  3367    50 21490 12673  6448
    163  8863 45935  3272     2]]"
586566de02abdf20b7bfd0d5a43ba93cb02795c3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],a non-parameter optimized linear-kernel SVM that uses TF-IDF bag-of-word vectors as inputs,"[[    0   102   786    12 46669  5906 29854 26956    12 48658   208 20954
     14  2939 35690    12  2688   597  3298    12  1116    12 14742 44493
     25 16584     2]]"
163a21c0701d5cda15be2d0eb4981a686e54a842,0.0,Entailment,[[    2     0 30495  3760  1757     2]], bossa-nova and jovem-guarda genres,"[[    0  3504   102    12 38823     8  1236  7067   119    12 12984   102
  23409     2]]"
3b40799f25dbd98bba5b526e0a1d0d0bb51173e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
f08a66665f01c91cb9dfe082e9d1015ecf3df71d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
4f4892f753b1d9c5e5e74c7c94d8c9b6ef523e7b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
6e76f114209f59b027ec3b3c8c9cdfc3e682589f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
44104668796a6ca10e2ea3ecf706541da1cec2cf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Accuracy of best interpretible system was 0.3945 while accuracy of LSTM-ELMo net was 0.6818.,"[[    0 36984 45386     9   275 18107  4748   467    21   321     4  3416
   1898   150  8611     9   226  4014   448    12  3721 17357  1161    21
    321     4  4671  1366     4     2]]"
2348d68e065443f701d8052018c18daa4ecc120e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"highly data-inefficient, underperform phrase-based statistical machine translation","[[    0 29003   414    12   833 43788     6   223  1741  3899 11054    12
    805 17325  3563 19850     2]]"
41d3ab045ef8e52e4bbe5418096551a22c5e9c43,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German","[[    0   100   771 11160   565  1570  1859    12 35007     6    38   771
  11160   565  1570  4423    12 35007     6   305 11674  1570  2370    12
  27709     2]]"
38e2f07ba965b676a99be06e8872dade7c04722a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
db264e363f3b3aa83526952bef02f826dff70042,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
a3d83c2a1b98060d609e7ff63e00112d36ce2607,0.0,Entailment,[[    2     0 30495  3760  1757     2]],27.41 transformation on average of single seed sentence is available in dataset.,"[[    0  2518     4  4006  7791    15   674     9   881  5018  3645    16
    577    11 41616     4     2]]"
c8f11561fc4da90bcdd72f76414421e1527c0287,0.0,Entailment,[[    2     0 30495  3760  1757     2]],LJSpeech,[[    0   574   863 29235  7529     2]]
39a450ac15688199575798e72a2cc016ef4316b5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Compared to baselines SAN (Table 1) shows  improvement of 1.096% on EM and 0.689% F1. Compared to other published SQuAD results (Table 2) SAN is ranked second. ,"[[    0 44891     7 11909 38630 14055    36 41836   112    43   924  1437
   3855     9   112     4  3546   401   207    15 14850     8   321     4
  38349   207   274   134     4 23570     7    97  1027   208 12444  2606
    775    36 41836   132    43 14055    16  4173   200     4  1437     2]]"
b653f55d1dad5cd262a99502f63bf44c58ccc8cf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Fisher Corpus English Part 1,[[    0   597 13761 28556  2370  4657   112     2]]
dd9883f4adf7be072d314d7ed13fe4518c5500e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Task processing: converting data exploration tasks to algebraic operations on the embedding space, Query processing: executing semantic query on the embedding space and return results","[[    0 47744  5774    35 18841   414  6942  8558     7 45429   636  1414
     15     5 33183 11303   980     6 44489  5774    35 19275 46195 25860
     15     5 33183 11303   980     8   671   775     2]]"
5f7850254b723adf891930c6faced1058b99bd57,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"A continuous emission HMM uses the hidden states of a 2-layer LSTM as features and a discrete emission HMM uses data as features. 
The interpretability of the model is shown in Figure 2. ","[[    0   250 11152 22679   289 16261  2939     5  7397   982     9    10
    132    12 39165   226  4014   448    25  1575     8    10 34014 22679
    289 16261  2939   414    25  1575     4  1437 50118   133 18107  4484
      9     5  1421    16  2343    11 17965   132     4  1437     2]]"
097ab15f58cb1fce5b5ffb5082b8d7bbee720659,0.0,Entailment,[[    2     0 30495  3760  1757     2]],thai,[[   0  212 1439    2]]
b37fd665dfa5fad43977069d5623f4490a979305,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"$({1})$ SC-LSTM BIBREF3, $({2})$ GPT-2 BIBREF6 , $({3})$ HDSA BIBREF7","[[    0  1629 48377   134 49424  1629  4998    12   574  4014   448   163
   8863 45935   246     6    68 48377   176 49424  1629   272 10311    12
    176   163  8863 45935   401  2156    68 48377   246 49424  1629  7951
   3603   163  8863 45935   406     2]]"
ec2b8c43f14227cf74f9b49573cceb137dd336e7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Speech recognition system is evaluated using WER metric.,"[[    0 29235  7529  4972   467    16 15423   634   305  2076 14823     4
      2]]"
73e715e485942859e1db75bfb5f35f1d5eb79d2e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Using the answer labels in the training set, we can find appropriate articles that include the information requested in the question.","[[    0 36949     5  1948 14105    11     5  1058   278     6    52    64
    465  3901  7201    14   680     5   335  5372    11     5   864     4
      2]]"
06c340c7c2ad57d7621c3e8baba6a3d0ed9f4696,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
efb3a87845460655c53bd7365bcb8393c99358ec,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR","[[    0  7904 45386     9  8162     4  5449    15  4062   104     6  5663
      4  1570    15  5316     8  1812     4   466    15   289  9822     2]]"
f981781021d4bacbaf3d076c895dc42d7fa108ba,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
2ec640e6b4f1ebc158d13ee6589778b4c08a04c8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e9cf1b91f06baec79eb6ddfd91fc5d434889f652,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"previous, next, Ctrl+F $<$query$>$, stop","[[    0  5234 24963     6   220     6 46778  2744   597    68 41552  1629
  48360  1629 15698 47110   912     2]]"
a130aa735de3b65c71f27018f20d3c068bafb826,0.0,Entailment,[[    2     0 30495  3760  1757     2]],16k images and 740k corresponding region descriptions,[[    0  1549   330  3156     8   262  1749   330 12337   976 24173     2]]
a6665074b067abb2676d5464f36b2cb07f6919d3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],". On PTB, our model achieves 93.90 F1 score of constituent parsing and 95.91 UAS and 93.86 LAS of dependency parsing., On CTB, our model achieves a new state-of-the-art result on both constituent and dependency parsing.","[[    0     4   374  7008   387     6    84  1421 35499  8060     4  3248
    274   134  1471     9 31350 46563     8  6164     4  6468   121  2336
      8  8060     4  5334   226  2336     9 31492 46563   482   374 12464
    387     6    84  1421 35499    10    92   194    12  1116    12   627
     12  2013   898    15   258 31350     8 31492 46563     4     2]]"
394cf73c0aac8ccb45ce1b133f4e765e8e175403,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
144714fe0d5a2bb7e21a7bf50df39d790ff12916,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ISOT dataset: LLVM
Liar dataset: Hybrid CNN and LSTM with attention","[[    0  1729  3293 41616    35 30536 20954 50118   574 12202 41616    35
  25367  3480     8   226  4014   448    19  1503     2]]"
3e3f5254b729beb657310a5561950085fa690e83,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"We define the Effective Word Score of score x as

EFWS(x) = N(+x) - N(-x),

where N(x) is the number of words in the tweet with polarity score x.","[[    0   170  9914     5 33355 15690 14702     9  1471  3023    25 50118
  50118 22390 13691  1640  1178    43  5457   234  1640  2744  1178    43
    111   234 48614  1178   238 50118 50118  8569   234  1640  1178    43
     16     5   346     9  1617    11     5  3545    19  8385 21528  1471
   3023     4     2]]"
69ca609e86888c7e4e2e3d33435a0a36f77601b5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],a standard beam search decoder BIBREF5 with several straightforward performance optimizations,"[[    0   102  2526 23480  1707  5044 15362   163  8863 45935   245    19
    484 15196   819 46675     2]]"
a0ef0633d8b4040bf7cdc5e254d8adf82c8eed5e,0.0,Entailment,[[    2     0 30495  3760  1757     2]], single layer LSTM with a 150-dimensional hidden state for hate / not hate classification,"[[    0   881 10490   226  4014   448    19    10  3982    12 23944  7397
    194    13  4157  1589    45  4157 20257     2]]"
09cd7ae01fe97bba230c109d0234fee80a1f013b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],French-Mboshi 5K corpus,[[    0 28586    12   448   428 23552   195   530 42168     2]]
bf25a202ac713a34e09bf599b3601058d9cace46,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Randomwalk, Walktrap, Louvain clustering","[[    0 45134 10097     6  9693 41970     6  9294   705  1851 46644  2961
      2]]"
ff1595a388769c6429423a75b6e1734ef88d3e46,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard. Predefined messages can also trigger other associated events such as pop-ups or follow-up non-verbal actions.","[[    0   133 31893    64  5163    65     9   484 12574 38716  3731     7
   2142     6    50  1907    49   308  1579   114   956     4  3130  2788
   3731   109    45   464     5  6054   194    11     5   274 15153     6
     98    24    16   505     7 15970  1496    49   304    30  1976   615
   6054  1735     7     5 31893     4 24268 38716  3731    64    67  7691
     97  3059  1061   215    25  3495    12  4489    50  1407    12   658
    786    12 40328  2163     4     2]]"
2941874356e98eb2832ba22eae9cb08ec8ce0308,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"TF-IDF + SVM, Depeche + SVM, NRC + SVM, TF-NRC + SVM, Doc2Vec + SVM,  Hierarchical RNN, BiRNN + Self-Attention, ELMo + BiRNN,  Fine-tuned BERT","[[    0 20249    12  2688   597  2055   208 20954     6   926  2379  2871
   2055   208 20954     6   234  5199  2055   208 20954     6 35690    12
    487  5199  2055   208 20954     6 19761   176   846  3204  2055   208
  20954     6  1437 37859 13161  3569   248 20057     6  6479   500 20057
   2055 12156    12 28062 19774     6 17678 17357  2055  6479   500 20057
      6  1437 14321    12 24641   196   163 18854     2]]"
1fd969f53bc714d9b5e6604a7780cbd6b12fd616,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"fine-tuning schedules that elaborately design the control of learning rates for optimization, proxy tasks that leverage labeled data to help the pre-trained model better fit the target data distribution, knowledge distillation approaches that ditch the paradigm of initialization with pre-trained parameters by adopting the pre-trained model as a teacher network","[[    0 35093    12 24641   154 15579    14 25499  7223  1521     5   797
      9  2239  1162    13 25212     6 16453  8558    14  6270 16274   414
      7   244     5  1198    12 23830  1421   357  2564     5  1002   414
   3854     6  2655  7018 34775  8369    14 16740     5 28323     9 49164
     19  1198    12 23830 17294    30 15059     5  1198    12 23830  1421
     25    10  3254  1546     2]]"
b24b56ccc5d4b04fee85579b2dee77306ec829b2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Our annotation scheme introduces opportunities for the educational community to conduct further research , Once automated classifiers are developed, such relations between talk and learning can be examined at scale,  automatic labeling via a standard coding scheme can support the generalization of findings across studies, and potentially lead to automated tools for teachers and students, collecting and annotating corpora that can be used by the NLP community to advance the field in this particular area","[[    0  2522 47760  3552 22007  1616    13     5  5984   435     7  2883
    617   557  2156  3128 11554  1380 27368    32  2226     6   215  3115
    227  1067     8  2239    64    28 10543    23  3189     6  1437  8408
  27963  1241    10  2526 25776  3552    64   323     5   937  1938     9
   4139   420  3218     6     8  2905   483     7 11554  3270    13  2948
      8   521     6  8664     8 45068  1295 22997   102    14    64    28
    341    30     5   234 21992   435     7  3316     5   882    11    42
   1989   443     2]]"
9f6e877e3bde771595e8aee10c2656a0e7b9aeb2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
346f10ddb34503dfba72b0e49afcdf6a08ecacfa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],46 documents makes up our base corpus,[[    0  3761  2339   817    62    84  1542 42168     2]]
1571e16063b53409f2d1bd6ec143fccc5b29ebb9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Majority Class baseline (MC) , Random selection baseline (RAN)","[[    0 37038  1571  4210 18043    36  6018    43  2156 34638  4230 18043
     36   500  1889    43     2]]"
e2cfaa2ec89b944bbc46e5edf7753b3018dbdc8f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e91692136033bbc3f19743d0ee5784365746a820,0.0,Entailment,[[    2     0 30495  3760  1757     2]],using multiple pivot sentences,[[    0 10928  1533 27475 11305     2]]
d70ba6053e245ee4179c26a5dabcad37561c6af0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],ConciergeQA and AmazonQA,[[   0 9157  438  906 1899 1864  250    8 1645 1864  250    2]]
b65b1c366c8bcf544f1be5710ae1efc6d2b1e2f1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The Lemming model in BIBREF17,[[    0   133 13956  7059  1421    11   163  8863 45935  1360     2]]
0db1ba66a7e75e91e93d78c31f877364c3724a65,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Sentiment Classification, Transferability of Shared Sentence Representation, Introducing Sequence Labeling as Auxiliary Task","[[    0 35212  8913 40509     6 18853  4484     9 38559 12169  4086 27893
   1258     6 32687 11162 47134  8250 10244    25 32119 34114 12927     2]]"
3ee721c3531bf1b9a1356a40205d088c9a7a44fc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers ","[[    0 13092 11264    12   805  6054  2706     6   147     5  3018     8
    467  4502    32 27361     7  5368    10  1498  1607  3041     6    61
     64   172    28  8417     7  1632  2777   634  2180  1138  1437     2]]"
b0dca7b74934f51ff3da0c074ad659c25d84174d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CAEVO,[[    0  4054 19896   673     2]]
02e4bf719b1a504e385c35c6186742e720bcb281,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event ","[[    0   805    15     5  9355   227  1061     6     5  2528  8385 21528
      9    65   515    64  3094     5   678  8385 21528     9     5    97
    515  1437     2]]"
2b52d481b30185d2c6e7b403d37277f70337d6ca,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a family of grammar formalisms built on a foundation of logic and type theory. Type-logical grammars originated when BIBREF4 introduced his Syntactic Calculus (called the Lambek calculus, L, by later authors).","[[    0   102   284     9 33055  4828 28599  1490    15    10  4811     9
  14578     8  1907  6680     4  7773    12 12376  3569 25187   119  2726
  19575    77   163  8863 45935   306  2942    39 33221 28201  2912 46283
     36  4155     5 13132  1951 41454     6   226     6    30   423  7601
    322     2]]"
61b0db2b5718d409b07f83f912bad6a788bfee5a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d2fbf34cf4b5b1fd82394124728b03003884409c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],IDEA,[[    0 14645   250     2]]
0c08af6e4feaf801185f2ec97c4da04c8b767ad6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
5cc937c2dcb8fd4683cb2298d047f27a05e16d43,0.0,Entailment,[[    2     0 30495  3760  1757     2]], continuous relaxation to top-k-argmax,[[    0 11152 26545     7   299    12   330    12  5384 29459     2]]
e6469135e0273481cf11a6c737923630bc7ccfca,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WikiSQL, SimpleQuestions, SequentialQA",[[    0 46929 46608     6 21375 46865     6 26400 12986  1864   250     2]]
f416c6818a7a8acb7ec4682ed424ecdbd7dd6df1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The multi-curricula learning scheme is scheduled according to the model's performance on the validation set, where the scheduling mechanism acts as the policy $\pi $ interacting with the dialogue model to acquire the learning status $s$. The reward of the multi-curricula learning mechanism $m_t$ indicates how well the current dialogue model performs.","[[    0   133  3228    12 17742  4063  5571  2239  3552    16  1768   309
      7     5  1421    18   819    15     5 26567   278     6   147     5
  19114  9562  4504    25     5   714 49959 19581    68 23140    19     5
   6054  1421     7  6860     5  2239  2194    68    29 48292    20  7970
      9     5  3228    12 17742  4063  5571  2239  9562    68   119  1215
     90  1629  8711   141   157     5   595  6054  1421 14023     4     2]]"
abc5836c54fc2ac8465aee5a83b9c0f86c6fd6f5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
1959e0ebc21fafdf1dd20c6ea054161ba7446f61,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.,"[[    0  7164   104    16 37213  9825   414    31  1131   557   414    36
    879 25384  4075   322 40090  9914  1209   250    12  7164   104  3685
     14  5026     7  8286   144  1330  2788    31  1461  2788     4     2]]"
212977344f4bf2ae8f060bdac0317db2d1801724,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
aceac4ad16ffe1af0f01b465919b1d4422941a6b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],we provide an extensive analysis of the state-of-the-art model,"[[   0 1694  694   41 4935 1966    9    5  194   12 1116   12  627   12
  2013 1421    2]]"
f37ed011e7eb259360170de027c1e8557371f002,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Humor in headlines (TitleStylist vs Multitask baseline):
Relevance: +6.53% (5.87 vs 5.51)
Attraction: +3.72% (8.93 vs 8.61)
Fluency: 1,98% (9.29 vs 9.11)","[[    0 40022   368    11  6337    36 46525 46083  8458  1954 14910   405
   4970 18043  3256 50118  9064  9525  2389    35  2055   401     4  4540
    207    36   245     4  5677  1954   195     4  4708    43 50118 28062
  22870    35  2055   246     4  4956   207    36   398     4  6478  1954
    290     4  5606    43 50118   597  6487  6761    35   112     6  5208
    207    36   466     4  2890  1954   361     4  1225    43     2]]"
37d829cd42db9ae3d56ab30953a7cf9eda050841,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
4266aacb575b4be7dbcdb8616766324f8790763c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"neural network-based models can outperform feature-based models with wide margins, contextualized representation learning can boost performance of NN models","[[    0   858  9799  1546    12   805  3092    64  9980  3899  1905    12
    805  3092    19  1810  5510     6 37617  1538  8985  2239    64  2501
    819     9   234   487  3092     2]]"
6e4505609a280acc45b0a821755afb1b3b518ffd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The BLEU metric ,[[    0   133   163  3850   791 14823  1437     2]]
95d98b2a7fbecd1990ec9a070f9d5624891a4f26,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a balanced dataset of 2,396 ironic and 2,396 non-ironic tweets is provided","[[    0   102  9320 41616     9   132     6 33275 25553     8   132     6
  33275   786    12 11680   636  6245    16  1286     2]]"
7488855f09b97eb6a027212fb7ace1d338f36a2b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
10ddc5caf36fe9d7438eb5a3936e24580c4ffe6a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],For relation prediction they test TransE and for relation extraction they test position aware neural sequence model,"[[    0  2709  9355 16782    51  1296  5428   717     8    13  9355 23226
     51  1296   737  2542 26739 13931  1421     2]]"
72122e0bc5da1d07c0dadb3401aab2acd748424d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],20K,[[  0 844 530   2]]
848ab388703c24faad79d83d254e4fd88ab27e2a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"'= ( , { ll k(h:, g:) if hV, gV

1 otherwise } )

where $_{h:}$ and $_{g:}$ denote the embedding representations of $h$ and $g$ , respectively.","[[    0   108  5214    36  2156 25522 19385   449  1640   298 46686   821
     35    43   114  1368   846     6   821   846 50118 50118   134  3680
  35524  4839 50118 50118  8569    68 49747   298    35 24303  1629     8
     68 49747   571    35 24303  1629 45722     5 33183 11303 30464     9
     68   298  1629     8    68   571  1629  2156  4067     4     2]]"
7486c9d9e6c407c0c3bc012405d689dbee072327,0.0,Entailment,[[    2     0 30495  3760  1757     2]],German,[[    0 27709     2]]
e96adf8466e67bd19f345578d5a6dc68fd0279a1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Even though natural language and image synthesis were part of several contributions on the supervised side of deep learning, unsupervised learning saw recently a tremendous rise in input from the research community specially on two subproblems: text-based natural language and image synthesis","[[    0  8170   600  1632  2777     8  2274 37423    58   233     9   484
   5694    15     5 20589   526     9  1844  2239     6   542 16101 25376
   2239   794   682    10  6839  1430    11  8135    31     5   557   435
  18041    15    80  2849  4892 42216    35  2788    12   805  1632  2777
      8  2274 37423     2]]"
3e0c9469821cb01a75e1818f2acb668d071fcf40,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"overall rating, mean number of turns",[[   0 2137 1250  691    6 1266  346    9 4072    2]]
73d657d6faed0c11c65b1ab60e553db57f4971ca,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
a8e0796c1ac353d428d84f4506a92b51bce51b87,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"D-IMDB (derived from large scale IMDB data snapshot), D-FB (derived from large scale Freebase data snapshot)","[[    0   495    12  3755 10842    36 38871    31   739  3189  9206 10842
    414 24512   238   211    12 22687    36 38871    31   739  3189  3130
  11070   414 24512    43     2]]"
a37e4a21ba98b0259c36deca0d298194fa611d2f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
8c981f8b992cb583e598f71741c322f522c6d2ad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],from the Database of Modern Icelandic Inflection (DMII) BIBREF1,"[[    0  7761     5 37875     9 13021 36539  7412 20576    36 25652 11194
     43   163  8863 45935   134     2]]"
1d860d7f615b9ca404c504f9df4231a702f840ef,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
db39a71080e323ba2ddf958f93778e2b875dcd24,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Our encoder-decoder framework employs separate encoding for different speakers in the dialog., We integrate semantic slot scaffold by performing delexicalization on original dialogs., We integrate dialog domain scaffold through a multi-task framework.","[[    0  2522  9689 15362    12 11127 15362  7208 13774  2559 45278    13
    430  6864    11     5 25730   482   166 13997 46195  8534 35737   279
     30  4655   263 14726  3569  1938    15  1461 25730    29   482   166
  13997 25730 11170 35737   279   149    10  3228    12 45025  7208     4
      2]]"
0e57a0983b4731eba9470ba964d131045c8c7ea7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
5cd5864077e4074bed01e3a611b747a2180088a0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],2000 images,[[    0 17472  3156     2]]
6c4cd8da5b4b298f29af3123b58d9a5d4b02180b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
928828544e38fe26c53d81d1b9c70a9fb1cc3feb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"29,500 documents",[[   0 2890    6 1497 2339    2]]
84a4a1f4695eba599d447e030c94f51e5f2f03bb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],our method still can improve the state-of-the-art accuracy BIBREF7 from 60.32% to 60.34%,"[[    0  2126  5448   202    64  1477     5   194    12  1116    12   627
     12  2013  8611   163  8863 45935   406    31  1191     4  2881   207
      7  1191     4  3079   207     2]]"
126e8112e26ebf8c19ca7ff3dd06691732118e90,0.0,Entailment,[[    2     0 30495  3760  1757     2]],There are 6 simulated datasets collected which is initialised with a corpus of size 550 and simulated by generating new documents from Wikipedia extracts and replacing existing documents,"[[    0   970    32   231 27361 42532  4786    61    16  2557  1720    19
     10 42168     9  1836 21903     8 27361    30 10846    92  2339    31
  28274 36635     8  8119  2210  2339     2]]"
ffb7a12dfe069ab7263bb7dd366817a9d22b8ef2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
81dc39ee6cdacf90d5f0f62134bf390a29146c65,0.0,Entailment,[[    2     0 30495  3760  1757     2]],contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks,"[[    0 46796  5564 33183   417  1033   109    45  3594  1122 46195 32242
  11401     8  3891    51    32    45  2024 33927    13  4276    12 10393
   2116    12  1527  5564  8558     2]]"
ec91b87c3f45df050e4e16018d2bf5b62e4ca298,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
1f63ccc379f01ecdccaa02ed0912970610c84b72,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The mixed objective improves EM by 2.5% and F1 by 2.2%,"[[    0   133  4281  4554 15296 14850    30   132     4   245   207     8
    274   134    30   132     4   176   207     2]]"
bd99aba3309da96e96eab3e0f4c4c8c70b51980a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"RNN-context, SRB, CopyNet, RNN-distract, DRGD","[[    0   500 20057    12 46796     6 13579   387     6 22279 15721     6
    248 20057    12 17165 15664     6 10994 38863     2]]"
3f46d8082a753265ec2a88ae8f1beb6651e281b6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CBT NE/CN, MR Movie reviews, IMDB Movie reviews, SUBJ","[[    0   347 13269 12462    73 29990     6 18838 13131  6173     6  9206
  10842 13131  6173     6 25272   863     2]]"
c34a15f1d113083da431e4157aceb11266e9a1b2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
74eb363ce30c44d318078cc1a46f8decf7db3ade,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
85d1831c28d3c19c84472589a252e28e9884500f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BERT-Base, QANet",[[    0 11126   565    12 34164     6  1209  1889   594     2]]
1fb73176394ef59adfaa8fc7827395525f9a5af7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],AmazonQA and ConciergeQA datasets,[[    0 25146  1864   250     8 21657   906  1899  1864   250 42532     2]]
0cd0755ac458c3bafbc70e4268c1e37b87b9721b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a93d4aa89ac3abbd08d725f3765c4f1bed35c889,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English , Chinese ",[[    0 35007  2156  1111  1437     2]]
c9c85eee41556c6993f40e428fa607af4abe80a9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],$\sim $ 8.7M annotated anonymised user utterances,"[[    0  1629 37457 13092    68   290     4   406   448 45068  1070 44563
   1720  3018 18672  5332     2]]"
ce6f6cd55ada011233a9dab4d99a94d7944d5388,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
00ef9cc1d1d60f875969094bb246be529373cb1d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Influential tweeters ( who they define as individuals certain to have a classifiable sentiment regarding the topic at hand) is used to label tweets in bulk in the absence of manually-labeled tweets.,"[[    0 31288  6487 12986  3545   268    36    54    51  9914    25  2172
   1402     7    33    10  1380 21584  5702  2624     5  5674    23   865
     43    16   341     7  6929  6245    11  8533    11     5  5171     9
  24704    12 33480   196  6245     4     2]]"
d76ecdc0743893a895bc9dc3772af47d325e6d07,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
f0946fb9df9839977f4d16c43476e4c2724ff772,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e1f5531ed04d0aae1dfcb0559f1512a43134c43a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
5a9f94ae296dda06c8aec0fb389ce2f68940ea88,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Their average improvement in Character Error Rate over the best MHA model was 0.33 percent points.,"[[    0 16837   674  3855    11 35177 37943 14064    81     5   275   256
   6826  1421    21   321     4  3103   135   332     4     2]]"
f6e5febf2ea53ec80135bbd532d6bb769d843dd8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
cf0085c1d7bd9bc9932424e4aba4e6812d27f727,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"FB24K, DBP24K, Game30K","[[    0 22687  1978   530     6 18496   510  1978   530     6  2436   541
    530     2]]"
bcec22a75c1f899e9fcea4996457cf177c50c4c5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"(i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero task-specific BiRNN layers of any kind","[[    0  1640   118    43  4276   234  2076    12 14175  6479   500 20057
  13171     6    36  4132    43  4276  4979    12 14175  6479   500 20057
  13171     6    50    36 31917    43  4276  3685    12 14175  6479   500
  20057 13171     9   143   761     2]]"
c4c9c7900a0480743acc7599efb359bc81cf3a4d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The increase is linearly from lowest on average 2.0 , medium around 3.5, and the largest is 6.0","[[    0   133   712    16 24248 23099    31  3912    15   674   132     4
    288  2156  4761   198   155     4   245     6     8     5  1154    16
    231     4   288     2]]"
20f7b359f09c37e6aaaa15c2cdbb52b031ab4809,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
77cf4379106463b6ebcb5eb8fa5bb25450fa5fb8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e7c0cdc05b48889905cc03215d1993ab94fb6eaa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
d4b9cdb4b2dfda1e0d96ab6c3b5e2157fd52685e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Two models will make the same predictions if and only if they use the same reasoning process., On similar inputs, the model makes similar decisions if and only if its reasoning is similar., Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other.","[[    0  9058  3092    40   146     5   276 12535   114     8   129   114
     51   304     5   276 20511   609   482   374  1122 16584     6     5
   1421   817  1122  2390   114     8   129   114    63 20511    16  1122
    482 25122  1667     9     5  8135    32    55   505     7     5  1421
  20511    87   643     4  7905     6     5  5694     9   430  1667     9
      5  8135    32  2222    31   349    97     4     2]]"
56836afc57cae60210fa1e5294c88e40bb10cc0e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"language identification, part-of-speech tagging, word segmentation, and preordering for statistical machine translation","[[    0 19527 10614     6   233    12  1116    12 40511 34694     6  2136
   2835  1258     6     8  1198 43976    13 17325  3563 19850     2]]"
86e3136271a7b93991c8de5d310ab15a6ac5ab8c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"(1) Good (3 points): The response is grammatical, semantically relevant to the query, and more importantly informative and interesting, (2) Acceptable (2 points): The response is grammatical, semantically relevant to the query, but too trivial or generic, (3) Failed (1 point): The response has grammar mistakes or irrelevant to the query","[[    0  1640   134    43  2497    36   246   332  3256    20  1263    16
  25187 45816     6  9031 38600  4249     7     5 25860     6     8    55
   7769 29749     8  2679     6    36   176    43 29081   868    36   176
    332  3256    20  1263    16 25187 45816     6  9031 38600  4249     7
      5 25860     6    53   350 30063    50 14569     6    36   246    43
  44216    36   134   477  3256    20  1263    34 33055  6160    50 21821
      7     5 25860     2]]"
bed527bcb0dd5424e69563fba4ae7e6ea1fca26a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],GermEval 2019 shared task,[[   0  534 8362  717 6486  954 1373 3685    2]]
8427988488b5ecdbe4b57b3813b3f981b07f53a5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Variety prediction task,[[    0 27717 25717 16782  3685     2]]
b7e419d2c4e24c40b8ad0fae87036110297d6752,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Text Similarity to Source Tweet, Text Similarity to Replied Tweet, Tweet Depth","[[    0 39645 17110  1571     7  4253 12244     6 14159 17110  1571     7
  26103  2550 12244     6 12244 43553     2]]"
b10632eaa0ca48f86522d8ec38b1d702cb0b8c01,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
a69af5937cab861977989efd72ad1677484b5c8c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the annotation machinery of BIBREF5,[[    0   627 47760 13922     9   163  8863 45935   245     2]]
2ea382c676e418edd5327998e076a8c445d007a5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
16f71391335a5d574f01235a9c37631893cd3bb0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Across 4 datasets, the best performing proposed model (CNN) achieved an average of 363% improvement over the state of the art method (LR-CNN)","[[    0 42945   204 42532     6     5   275  4655  1850  1421    36 16256
     43  4824    41   674     9 40801   207  3855    81     5   194     9
      5  1808  5448    36 33919    12 16256    43     2]]"
9c94ff8c99d3e51c256f2db78c34b2361f26b9c2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard.","[[    0   133 31893    64  5163    65     9   484 12574 38716  3731     7
   2142     6    50  1907    49   308  1579   114   956     4  3130  2788
   3731   109    45   464     5  6054   194    11     5   274 15153     6
     98    24    16   505     7 15970  1496    49   304    30  1976   615
   6054  1735     7     5 31893     4     2]]"
e84e80067b3343d136fd75300691c8b3d3efbdac,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"By choosing English (En) as the pivot language, we perform pivot alignments for identical English segments on Europarl Fr-En and En-De parallel corpora BIBREF18 , constructing a multi-parallel corpus of Fr-En-De. Then each of the Fr*-De and Fr-De* pseudo parallel corpora is established from the multi-parallel data by applying the pivot language-based translation described in the previous section.","[[    0  2765  8348  2370    36 16040    43    25     5 27475  2777     6
     52  3008 27475 12432  2963    13 14085  2370  5561    15  5122  5489
    462  4967    12 16040     8  2271    12 13365 12980 22997   102   163
   8863 45935  1366  2156 25886    10  3228    12  5489 44682 42168     9
   4967    12 16040    12 13365     4  1892   349     9     5  4967  3226
     12 13365     8  4967    12 13365  3226 38283 12980 22997   102    16
   2885    31     5  3228    12  5489 44682   414    30  9889     5 27475
   2777    12   805 19850  1602    11     5   986  2810     4     2]]"
7aa8375cdf4690fc3b9b1799b0f5a9ec1c1736ed,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
67131c15aceeb51ae1d3b2b8241c8750a19cca8e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Oracle ,[[    0 47003  1437     2]]
e86d381322c8db2b74a13a8e23082ddb010c1e40,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
0d7de323fd191a793858386d7eb8692cc924b432,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"current news, historical news, free time, sports, juridical news pieces, personal adverts, editorials.","[[    0 28311   340     6  4566   340     6   481    86     6  1612     6
  18603   808  3569   340  3745     6  1081  2329 24038     6  4474  9532
      4     2]]"
1a43df221a567869964ad3b275de30af2ac35598,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yelp Challenge dataset BIBREF2,[[    0   975   523   642 10045 41616   163  8863 45935   176     2]]
44c4bd6decc86f1091b5fc0728873d9324cdde4e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The ACP corpus has around 700k events split into positive and negative polarity ,"[[    0   133  7224   510 42168    34   198  7417   330  1061  3462    88
   1313     8  2430  8385 21528  1437     2]]"
cebf3e07057339047326cb2f8863ee633a62f49f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Arabic, German, Portuguese, Russian, Swedish",[[    0 33054   636     6  1859     6 13053     6  1083     6  9004     2]]
b1457feb6cdbf4fb19c8e87e1cd43981bc991c4c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
0f60864503ecfd5b048258e21d548ab5e5e81772,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
2b5dc3595dfc3d52a1525783d943b3dd0cc62473,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"It begins as ordinary tournament selection evolutionary architecture search with early stopping, with each child model training for a relatively small $s_0$ number of steps before being evaluated for fitness. However, after a predetermined number of child models, $m$ , have been evaluated, a hurdle, $h_0$ , is created by calculating the the mean fitness of the current population. For the next $m$ child models produced, models that achieve a fitness greater than $h_0$ after $s_0$ train steps are granted an additional $s_1$ steps of training and then are evaluated again to determine their final fitness. Once another $m$ models have been considered this way, another hurdle, $h_1$ , is constructed by calculating the mean fitness of all members of the current population that were trained for the maximum number of steps. For the next $m$ child models, training and evaluation continues in the same fashion, except models with fitness greater than $m$0 after $m$1 steps of training are granted an additional $m$2 number of train steps, before being evaluated for their final fitness. This process is repeated until a satisfactory number of maximum training steps is reached.","[[    0   243  3772    25  7945  1967  4230 29832  9437  1707    19   419
   8197     6    19   349   920  1421  1058    13    10  3487   650    68
     29  1215   288  1629   346     9  2402   137   145 15423    13  5704
      4   635     6    71    10 41679   346     9   920  3092     6    68
    119  1629  2156    33    57 15423     6    10 18263     6    68   298
   1215   288  1629  2156    16  1412    30 29770     5     5  1266  5704
      9     5   595  1956     4   286     5   220    68   119  1629   920
   3092  2622     6  3092    14  3042    10  5704  2388    87    68   298
   1215   288  1629    71    68    29  1215   288  1629  2341  2402    32
   4159    41   943    68    29  1215   134  1629  2402     9  1058     8
    172    32 15423   456     7  3094    49   507  5704     4  3128   277
     68   119  1629  3092    33    57  1687    42   169     6   277 18263
      6    68   298  1215   134  1629  2156    16 11236    30 29770     5
   1266  5704     9    70   453     9     5   595  1956    14    58  5389
     13     5  4532   346     9  2402     4   286     5   220    68   119
   1629   920  3092     6  1058     8 10437  1388    11     5   276  2734
      6  4682  3092    19  5704  2388    87    68   119  1629   288    71
     68   119  1629   134  2402     9  1058    32  4159    41   943    68
    119  1629   176   346     9  2341  2402     6   137   145 15423    13
     49   507  5704     4   152   609    16  6636   454    10 28173   346
      9  4532  1058  2402    16  1348     4     2]]"
bf6c14e9c5f476062cbaaf9179b0c9b751222c8f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the basic question generation module (Module 1) and co-attention visual question answering module (Module 2),"[[    0   627  3280   864  2706 20686    36 48720   112    43     8  1029
     12  2611 19774  7133   864 15635 20686    36 48720   132    43     2]]"
cca74448ab0c518edd5fc53454affd67ac1a201c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"198,112",[[    0 28773     6 17729     2]]
e63bde5c7b154fbe990c3185e2626d13a1bad171,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87","[[    0 40370   257    12   134    35  4431     4  1225     6 16463   257
     12   306    35   389     4  3897     6 30782   717  3411    35   973
      4  1558     6   248  5061  8800    12   574    35  5169     4  5677
      2]]"
9378b41f7e888e78d667e9763883dd64ddb48728,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
061682beb3dbd7c76cfa26f7ae650e548503d977,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
76405b76b930a5bbe895e9e96ce4a3cff1b0b1a1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"pre-trained text representations, transformer-based encoders together with GRU models, attention mechanisms are paramount for learning top performing networks, Top-Down is the preferred attention method","[[    0  5234    12 23830  2788 30464     6 40878    12   805  9689  1630
    268   561    19  8837   791  3092     6  1503 14519    32 24342    13
   2239   299  4655  4836     6  3107    12 17853    16     5  6813  1503
   5448     2]]"
3103502cf07726d3eeda34f31c0bdf1fc0ae964e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Zipf's law describes change of word frequency rate, while Heaps-Herdan describes different word number in large texts (assumed that Hepas-Herdan is consequence of Zipf's)","[[    0 47945   506    18   488  7448   464     9  2136 13135   731     6
    150    91  7527    12 13584 14856  7448   430  2136   346    11   739
  14301    36  2401 28817    14 21658   281    12 13584 14856    16 15180
      9 38277   506    18    43     2]]"
07f5e360e91b99aa2ed0284d7d6688335ed53778,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
29571867fe00346418b1ec36c3b7685f035e22ce,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"relation prediction, relation extraction, Open IE",[[    0 47114 16782     6  9355 23226     6  2117 18090     2]]
3b7798a6bce1a5faf411bb12e2e011dbab1e279d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
8d793bda51a53a4605c1c33e7fd20ba35581a518,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Confusion in recognizing the words that are active at a given node by a speech recognition solution developed for Indian Railway Inquiry System.,"[[    0 33295 15727    11 16257     5  1617    14    32  2171    23    10
    576 37908    30    10  1901  4972  2472  2226    13  1362 14977 29760
   5149     4     2]]"
98b11f70239ef0e22511a3ecf6e413ecb726f954,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
a276d5931b989e0a33f2a0bc581456cca25658d9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"3-gram and 4-gram conditional language model, Convolution, LSTM models BIBREF27 with and without attention BIBREF28, Transformer, GPT-2","[[    0   246    12 28526     8   204    12 28526 23431  2777  1421     6
  30505 23794     6   226  4014   448  3092   163  8863 45935  2518    19
      8   396  1503   163  8863 45935  2517     6  5428 22098     6   272
  10311    12   176     2]]"
7d4fad6367f28c67ad22487094489680c45f5062,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"window_size, alpha, sample, dm, hs, vector_size","[[    0 42996  1215 10799     6 32756     6  7728     6   385   119     6
   1368    29     6 37681  1215 10799     2]]"
63b0c93f0452d0e1e6355de1d0f3ff0fd67939fb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Quora duplicate question dataset BIBREF22,[[    0 12444  4330 33196   864 41616   163  8863 45935  2036     2]]
4f0f446bf4518af7f539f6283145135192d5c00b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Logistic Regression (LR), Random Forest (RF), Support Vector Machines (SVM)","[[    0 23345  5580  6304 21791    36 33919   238 34638  5761    36 30455
    238  7737 40419 28413    36   104 20954    43     2]]"
544b68f6f729e5a62c2461189682f9e4307a05c6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Google's Neural Machine Translation,[[    0 20441    18 44304 14969 41737     2]]
a7d72f308444616a0befc8db7ad388b3216e2143,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"in-house dataset, ACE05 dataset ",[[    0   179    12  3138 41616     6 36211  2546 41616  1437     2]]
0bd992a6a218331aa771d922e3c7bb60b653949a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
9776156fc93daa36f4613df591e2b49827d25ad2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"In terms of F1 score, the Hybrid approach improved by 23.47% and 1.39% on BiDAF and DCN respectively. The DCA approach improved by 23.2% and 1.12% on BiDAF and DCN respectively.","[[    0  1121  1110     9   274   134  1471     6     5 25367  1548  2782
     30   883     4  3706   207     8   112     4  3416   207    15  6479
   3134   597     8  5815   487  4067     4    20  5815   250  1548  2782
     30   883     4   176   207     8   112     4  1092   207    15  6479
   3134   597     8  5815   487  4067     4     2]]"
728c2fb445173fe117154a2a5482079caa42fe24,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach","[[    0  3479    12  9435 45774  7257  3569  5678     6   600   540  7690
     87 12142 45774  7257  3569  4158     6   429    28 31389 16230    31
     10  2007  2136 34703  1043  6761  1548     2]]"
ec62c4cdbeaafc875c695f2d4415bce285015763,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BERT, RoBERTa, DistilBERT, GPT, GPT2, Transformer-XL, XLNet, XLM","[[    0 11126   565     6  3830 11126 38495     6 11281   718 11126   565
      6   272 10311     6   272 10311   176     6  5428 22098    12 35826
      6 14402 15721     6 14402   448     2]]"
2a058f8f6bd6f8e80e8452e1dba9f8db5e3c7de8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"radial coordinate and the angular coordinates correspond to the modulus part and the phase part, respectively","[[    0  7822  2617 18251     8     5 42970 34721 20719     7     5 11134
  35223   233     8     5  4359   233     6  4067     2]]"
7a7e279170e7a2f3bc953c37ee393de8ea7bd82f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],cloze-style reading comprehension and user query reading comprehension questions,"[[    0  3998   139  2158    12  5827  2600 40494     8  3018 25860  2600
  40494  1142     2]]"
e4d16050f0b457c93e590261732a20401def9cde,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Levenshtein distance metric BIBREF8, diacritical swapping, Levenshtein distance is used in a weighted sum to cosine distance between word vectors, ELMo-augmented LSTM","[[    0 10350  2987  1193 37566  4472 14823   163  8863 45935   398     6
   2269  1043 46492 32093     6  1063  2987  1193 37566  4472    16   341
     11    10 19099  6797     7 12793   833  4472   227  2136 44493     6
  17678 17357    12  1180 10757   196   226  4014   448     2]]"
cf0085c1d7bd9bc9932424e4aba4e6812d27f727,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Freebase BIBREF0, DBpedia BIBREF1 and a self-construction game knowledge graph","[[    0 18074 11070   163  8863 45935   288     6 18496 47745   163  8863
  45935   134     8    10  1403    12 20836 31470   177  2655 20992     2]]"
41d3750ae666ea5a9cea498ddfb973a8366cccd6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"annotators are asked how attractive the headlines are, Likert scale from 1 to 10 (integer values)","[[    0 37250  3629    32   553   141  6043     5  6337    32     6   226
    967  2399  3189    31   112     7   158    36 49465  3266    43     2]]"
f809fd0d3acfaccbe6c8abb4a9d951a83eec9a32,0.0,Entailment,[[    2     0 30495  3760  1757     2]],labeled by experts,[[    0 33480   196    30  2320     2]]
fb76e994e2e3fa129f1e94f1b043b274af8fb84c,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," CWVAE is trained on an auxiliary dataset to learn the event background information by using the context-aware latent variable.  Then, in finetute stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target.","[[    0 17660  9788   717    16  5389    15    41 34590 41616     7  1532
      5   515  3618   335    30   634     5  5377    12 24590 42715 15594
      4  1437  1892     6    11  8746   594  4467  1289     6 17660  9788
    717    16  5389    15     5  3685    12 14175 41616     7  9037     5
    515  3618   335     7   349  2167  6659     9   318    12 12948  4047
  44125  1002     4     2]]"
4056ee2fd7a0a0f444275e627bb881134a1c2a10,0.0,Entailment,[[    2     0 30495  3760  1757     2]],We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the $4^\text{th}$ and $6^\text{th}$ columns of the file format) BIBREF13 . ,"[[    0   170   304     5 38675  9779 34694 42532  1286    30     5  9973
  43290 14768    36 13083    43  3907 27045    36   627 10146 26511  1258
      9     5    68   306 35227 37457 29015 45152   212 24303  1629     8
     68   401 35227 37457 29015 45152   212 24303  1629 18315     9     5
   2870  7390    43   163  8863 45935  1558   479  1437     2]]"
58f50397a075f128b45c6b824edb7a955ee8cba1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],1,[[  0 134   2]]
0b24b5a652d674d4694668d889643bc1accf18ef,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
7d59374d9301a0c09ea5d023a22ceb6ce07fb490,0.0,Entailment,[[    2     0 30495  3760  1757     2]],by number of distinct n-grams,[[    0  1409   346     9 11693   295    12 28526    29     2]]
1e59263f7aa7dd5acb53c8749f627cf68683adee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
20ec88c45c1d633adfd7bff7bbf3336d01fb6f37,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Precision, Recall, F1",[[    0 22763 37938     6 35109     6   274   134     2]]
08333e4dd1da7d6b5e9b645d40ec9d502823f5d7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"0.007 MAP on Task A, 0.032 MAP on Task B, 0.055 MAP on Task C","[[    0   288     4 35723 36890    15 12927    83     6   321     4 40935
  36890    15 12927   163     6   321     4 39558 36890    15 12927   230
      2]]"
a6419207d2299f25e2688517d1580b7ba07c8e4b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
2a0a44f169ad61774d77df65f8846bd57685bfcf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],hree years of online news articles from June 2016 to June 2019,"[[   0  298 5314  107    9  804  340 7201   31  502  336    7  502  954
     2]]"
54fe8f05595f2d1d4a4fd77f4562eac519711fa6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Systems do not perform well both in Facebook and Twitter texts,"[[    0 36383    29   109    45  3008   157   258    11   622     8   599
  14301     2]]"
753990d0b621d390ed58f20c4d9e4f065f0dc672,0.0,Entailment,[[    2     0 30495  3760  1757     2]],seed lexicon consists of positive and negative predicates,[[    0 28067 36912 17505 10726     9  1313     8  2430 12574 23020     2]]
9ae084e76095194135cd602b2cdb5fb53f2935c1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],word error rate,[[    0 14742  5849   731     2]]
524abe0ab77db168d5b2f0b68dba0982ac5c1d8e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
bcd6befa65cab3ffa6334c8ecedd065a4161028b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a form of wordplay jokes in which one sign (e.g. a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect","[[    0   102  1026     9  2136  5785 11248    11    61    65  1203    36
    242     4   571     4    10  2136    50    10 11054    43  3649    80
     50    55 39314    30 26005 11424  1090  4783     6  9486  6119  4783
      6    50 43676  9779 37015     7   277  1203     6    13    41  3833
  31214    50 38167  1683     2]]"
2480dfe2d996afef840a81bd920aeb9c26e5b31d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"exact string matching, inflectional string matching",[[    0  3463  7257  6755  8150     6  4047 20576   337  6755  8150     2]]
e8e6719d531e7bef5d827ac92c7b1ab0b8ec3c8e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
1d941d390c0ee365aa7d7c58963e646eea74cbd6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
642c4704a71fd01b922a0ef003f234dcc7b223cd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"irremediable annotation discrepancies, differences in choice of attributes to annotate, The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them, the two annotations encode distinct information, incorrectly applied UniMorph annotation, cross-lingual inconsistency in both resources","[[    0   853   241 34658   868 47760 35035     6  5550    11  2031     9
  16763     7 45068   877     6    20  1915  1235    74   240 18796     7
  46855     5  4249 38675   366 45122 28201   335     4   993 11991    56
     10   182   614   346     9 35642  4620     6     8   117  6694  2856
     50   583    12  9244  5559   227   106     6     5    80 47234 46855
  11693   335     6 27821  5049 22839   448 31724 47760     6  2116    12
   1527  5564 36849    11   258  1915     2]]"
569ad21441e99ae782d325d5f5e1ac19e08d5e76,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"title of the news article, screen name of the user","[[    0 14691     9     5   340  1566     6  2441   766     9     5  3018
      2]]"
48cc41c372d44b69a477998be449f8b81384786b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we achieve better results than GCN+PADG but without any use of domain-specific hand-crafted features,  RegSum achieves a similar ROUGE-2 score","[[    0  1694  3042   357   775    87 18397   487  2744   510  2606   534
     53   396   143   304     9 11170    12 14175   865    12 29496  1575
      6  1437  6304 38182 35499    10  1122   248  5061  8800    12   176
   1471     2]]"
dbdf13cb4faa1785bdee90734f6c16380459520b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"A combination of Minimum spanning trees, K-Nearest Neighbors and Markov Stability BIBREF15, BIBREF16, BIBREF17, BIBREF18","[[    0   250  4069     9 32600 19161  3980     6   229    12 14563 18759
  16853 19357     8  1190  1417 32946   163  8863 45935   996     6   163
   8863 45935  1549     6   163  8863 45935  1360     6   163  8863 45935
   1366     2]]"
8112d18681e266426cf7432ac4928b87f5ce8311,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English, Hindi",[[    0 35007     6 19840     2]]
363ddc06db5720786ed440927d7fbb7d0a8078ae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"stylometric, lexical, grammatical, and semantic","[[    0   620  4360 22356     6 36912  3569     6 25187 45816     6     8
  46195     2]]"
d72548fa4d29115252605d5abe1561a3ef2430ca,0.0,Entailment,[[    2     0 30495  3760  1757     2]],represent every sentence by their reduced n-gram set,[[    0 35213   358  3645    30    49  2906   295    12 28526   278     2]]
7b3d207ed47ae58286029b62fd0c160a0145e73d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
c4b5cc2988a2b91534394a3a0665b0c769b598bb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The reciprocal of the variance of the attention distribution,[[    0   133 36285     9     5 37832     9     5  1503  3854     2]]
06776b8dfd1fe27b5376ae44436b367a71ff9912,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Mandarin dataset, Cantonese dataset",[[    0 37136 15394 41616     6 10909  6909   242 41616     2]]
09f0dce416a1e40cc6a24a8b42a802747d2c9363,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Continuous Bag-of-Words (CBOW),"[[    0 28883 12685 13379    12  1116    12 38257    36 25392  4581    43
      2]]"
a883bb41449794e0a63b716d9766faea034eb359,0.0,Entailment,[[    2     0 30495  3760  1757     2]],images and text,[[    0 39472     8  2788     2]]
450a359d117bcfa2de4ffd987f787945f25b3b25,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"First, the embedding matrix INLINEFORM4 for all corpora is initialized, during the training phase, INLINEFORM9 can be used to bias the input feature, Next, we apply the language specific softmax to compute logits INLINEFORM4 and optimize them with the CTC objective","[[    0 10993     6     5 33183 11303 36173  2808 28302 38036   306    13
     70 22997   102    16 49271     6   148     5  1058  4359     6  2808
  28302 38036   466    64    28   341     7  9415     5  8135  1905     6
   4130     6    52  3253     5  2777  2167  3793 29459     7 37357  7425
   2629  2808 28302 38036   306     8 22016   106    19     5   230  6078
   4554     2]]"
1c68d18b4b65c4d75dc199d2043079490f6310f8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Entity identification with offset mapping and concept indexing,[[    0 49448 10614    19  6147 20410     8  4286  1965   154     2]]
71a7153e12879defa186bfb6dbafe79c74265e10,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Chinese general corpus,[[    0 24727   937 42168     2]]
29bdd1fb20d013b23b3962a065de3a564b14f0fb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
53640834d68cf3b86cf735ca31f1c70aa0006b72,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
b8dea4a98b4da4ef1b9c98a211210e31d6630cf3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"It has three sequentially connected units to output continue, act and slots generating multi-acts in a doble recurrent manner.","[[    0   243    34   130 16721 16722  3665  2833     7  4195   535     6
   1760     8 20063 10846  3228    12 19170    11    10   109  5225 35583
   4737     4     2]]"
60726d9792d301d5ff8e37fbb31d5104a520dea3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],MH17 Twitter dataset,[[    0 38092  1360   599 41616     2]]
279b633b90fa2fd69e84726090fadb42ebdf4c02,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the East Coast Bomb Cyclone,  the Mendocino, California wildfires, Hurricane Florence, Hurricane Michael, the California Camp Fires","[[    0   627   953  2565 14521  9384 31496     6  1437     5  8404  1975
   1696     6   886 12584     6  4370  9610     6  4370   988     6     5
    886  4746 38497     2]]"
45be26c01e82835d9949529003c6b64f90db3d1a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Twitter definition of hateful conduct,[[    0 22838  8515     9 26393  2883     2]]
387970ebc7ef99f302f318d047f708274c0e8f21,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
616c205142c7f37b3f4e81c0d1c52c79f926bcdc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SUMBT BIBREF17 is the current state-of-the-art model on WOZ 2.0, TRADE BIBREF3 is the current published state-of-the-art model","[[    0   104  5725 13269   163  8863 45935  1360    16     5   595   194
     12  1116    12   627    12  2013  1421    15   305   673  1301   132
      4   288     6  5758 15197   163  8863 45935   246    16     5   595
   1027   194    12  1116    12   627    12  2013  1421     2]]"
999b20dc14cb3d389d9e3ba5466bc3869d2d6190,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Kim et al. (2019),[[    0 18806  4400  1076     4    36 10626    43     2]]
2376c170c343e2305dac08ba5f5bda47c370357f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They crawled travel information from the Web to build a database, created a multi-domain goal generator from the database, collected dialogue between workers an automatically annotated dialogue acts. ","[[    0  1213 38475  1504   335    31     5  6494     7  1119    10  8503
      6  1412    10  3228    12 46400   724 22538    31     5  8503     6
   4786  6054   227  1138    41  6885 45068  1070  6054  4504     4  1437
      2]]"
36b5f0f62ee9be1ab50d1bb6170e98328d45997d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Word2Vec, Wang2Vec, and FastText","[[    0 44051   176   846  3204     6  9705   176   846  3204     6     8
   9612 39645     2]]"
6e2ad9ad88cceabb6977222f5e090ece36aa84ea,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254.,"[[    0   133 18043  1421    16    10  1844 13931    12   560    12 46665
   9689 15362    73 11127 15362  1421    19  1503     4    20  9689 15362
     16    10  2311 43606   337  2597    12 34256 25569 25940  1640   574
   4014   448    43  3551   163  8863 45935  1570     8     5  5044 15362
     10   881   226  4014   448  3551    19  1503  9562     4    20  1503
   9562    16 43547    25    11   163  8863 45935   466     8    52   304
     10 34405  1707    13 46133     4   166  2341   253    12   560    12
   1397   217     5  1617 33183   417  1033     4    20 33183 11303  1836
    341    16     9 13950     8     5  7397   194  1836     9     5   226
   4014   448  4590    16     9 35381     4     2]]"
69a7a6675c59a4c5fb70006523b9fe0f01ca415c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"link prediction , triplet classification",[[    0 12139 16782  2156  6436    90 20257     2]]
fd0ef5a7b6f62d07776bf672579a99c67e61a568,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," we measure our system's performance for datasets across various domains, evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs","[[    0    52  2450    84   467    18   819    13 42532   420  1337 30700
      6 30962    32   626    30  2312  4674    54  8832     5  2655  1542
      8   172  1679  3018 22680 21623     7     5  1209   250 15029     2]]"
e72a672f8008bbc52b93d8037a5fedf8956136af,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"E2ECM, CDM",[[   0  717  176 3586  448    6 7522  448    2]]
e854edcc5e9111922e6e120ae17d062427c27ec1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
7fe48939ce341212c1d801095517dc552b98e7b3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],gating mechanism acts upon each dimension of the word and character-level vectors,"[[    0   571  1295  9562  4504  2115   349 21026     9     5  2136     8
   2048    12  4483 44493     2]]"
5758ebff49807a51d080b0ce10ba3f86dcf71925,0.0,Entailment,[[    2     0 30495  3760  1757     2]],low-rank approximation of the co-occurrence matrix,"[[    0  5481    12 40081 46194     9     5  1029    12 23462 30904 36173
      2]]"
559c1307610a15427caeb8aff4d2c01ae5c9de20,8.3333,Entailment,[[    2     0 30495  3760  1757     2]],"For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 .","[[    0  2709     5 31648  1757  1380 24072    52  8933  1502  1075 11474
    868 35798   163  8863 45935   176  2156   163  8863 45935   246    25
   6264    11     5   781 18043     6 18366  3755   163  8863 45935   306
   2156     8    10 40878  1546    19  1198    12 23830 23341   163  8863
  45935   245   479     2]]"
35eb8464e934a2769debe14148667c61bf1da243,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"structured content analysis (also called closed coding) is used to turn qualitative or unstructured data of all kinds into structured and/or quantitative data, Projects usually involve teams of coders (also called annotators, labelers, or reviewers), with human labor required to code, annotate, or label a corpus of items.","[[    0 25384  4075  1383  1966    36 19726   373    44    48 25315 25776
     17    46    43    16   341     7  1004 29981    50   542 25384  4075
    414     9    70  6134    88 16697     8    73   368 17836   414     6
  23965  2333  6877   893     9    44    48 29659   268    17    46    36
  19726   373    44    48 37250  3629    17    46     6    44    48 33480
    268    17    46     6    50    44    48 22459   268    17    46   238
     19  1050  4178  1552     7    44    48 20414    17    46     6    44
     48 37250   877    17    46     6    50    44    48 33480    17    46
     10 42168     9  1964     4     2]]"
95d8368b1055d97250df38d1e8c4a2b283d2b57e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],pipeline that is used at Microsoft for production data,[[   0  642 1588 7012   14   16  341   23 3709   13  931  414    2]]
ef07ec34433221d4da79d5923fb00d8ac446b92c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],median cosine similarity,[[    0  4567   811 12793   833 37015     2]]
c348a8c06e20d5dee07443e962b763073f490079,0.0,Entailment,[[    2     0 30495  3760  1757     2]],evidence extraction and answer synthesis,[[    0 42655 23226     8  1948 37423     2]]
9785ecf1107090c84c57112d01a8e83418a913c1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"German, Spanish, Chinese",[[    0 27709     6  3453     6  1111     2]]
4a4b7c0d3e7365440b49e9e6b67908ea5cea687d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Majority, ESA, Word2Vec , Binary-BERT","[[    0 37038  1571     6 34479     6 15690   176   846  3204  2156 47466
     12 11126   565     2]]"
4ce3a6632e7d86d29a42bd1fcf325114b3c11d46,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
bb8f62950acbd4051774f1bfc50e3d424dd33b7c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
6a7370dd12682434248d006ffe0a72228c439693,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
52e8c9ed66ace1780e41815260af1309064d20de,0.0,Entailment,[[    2     0 30495  3760  1757     2]],WN18 and FB15k,[[    0 29722  1366     8 13042   996   330     2]]
2f9d30e10323cf3a6c9804ecdc7d5872d8ae35e4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SNAP (Stanford Network Analysis Project),[[    0 12436   591    36 36118  1891  3658  5213  3728    43     2]]
e9f868f22ae70c7681c28228b6019e155013f8d6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
c20b012ad31da46642c553ce462bc0aad56912db,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," movie sentence polarity dataset from BIBREF19, laptop and restaurant datasets collected from SemEval-201, we collected 2,000 reviews for each domain from the same review source","[[    0  1569  3645  8385 21528 41616    31   163  8863 45935  1646     6
   9972     8  2391 42532  4786    31 11202   717  6486    12 16295     6
     52  4786   132     6   151  6173    13   349 11170    31     5   276
   1551  1300     2]]"
6f2118a0c64d5d2f49eee004d35b956cb330a10e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Microsoft Research dataset containing movie, taxi and restaurant domains.","[[    0 35460  1624 41616  8200  1569     6  9955     8  2391 30700     4
      2]]"
81064bbd0a0d72a82d8677c32fb71b06501830a0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ROUGE-1 increases by 0.05, ROUGE-2 by 0.06 and ROUGE-L by 0.09","[[   0  500 5061 8800   12  134 3488   30  321    4 2546    6  248 5061
  8800   12  176   30  321    4 4124    8  248 5061 8800   12  574   30
   321    4 3546    2]]"
9e1bf306658ef2972159643fdaf149c569db524b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the Otomanguean language family,[[    0   627 10233  1075  1097  1780   260  2777   284     2]]
eeb6e0caa4cf5fdd887e1930e22c816b99306473,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The contexts are manually labelled with WordNet senses of the target words,"[[    0   133 38270    32 24704 22434    19 15690 15721 24074     9     5
   1002  1617     2]]"
b53efdbb9e53a65cd3828a3eb485c70f782a06e5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we fully connect nodes that represent sentences from the same passage, we fully connect nodes that represent the first sentence of each passage, we add an edge between the question and every node for each passage","[[    0  1694  1950  4686 32833    14  3594 11305    31     5   276  9078
      6    52  1950  4686 32833    14  3594     5    78  3645     9   349
   9078     6    52  1606    41  3543   227     5   864     8   358 37908
     13   349  9078     2]]"
a43c400ae37a8705ff2effb4828f4b0b177a74c4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],shared character embeddings for taggers in both languages together through optimization of a joint loss function,"[[    0 42502  2048 33183   417  1033    13  6694  7188    11   258 11991
    561   149 25212     9    10  2660   872  5043     2]]"
774ead7c642f9a6c59cfbf6994c07ce9c6789a35,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Amazon reviews,[[    0 25146  6173     2]]
770aeff30846cd3d0d5963f527691f3685e8af02,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
013a8525dbf7a9e1e69acc1cff18bb7b8261cbad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
b9c0049a7a5639c33efdb6178c2594b8efdefabb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],our method outperforms the baseline in both relevance and fluency significantly.,"[[    0  2126  5448  9980 33334     5 18043    11   258 21623     8  6626
   6761  3625     4     2]]"
626873982852ec83c59193dd2cf73769bf77b3ed,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"document-level accuracy, precision, recall, F-score","[[    0 43017    12  4483  8611     6 15339     6  6001     6   274    12
  31673     2]]"
cb8e2069218e30c643013c20e93ebe23525d9f55,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Adobe internal NLU tool, Pytext, Rasa","[[    0  9167 14004  3425 12817   791  3944     6 19972 29015     6   248
   8810     2]]"
e2a637f1d93e1ea9f29c96ff0fc6bc017209065b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],hand crafted by users,[[    0  4539 17626    30  1434     2]]
fbee81a9d90ff23603ee4f5986f9e8c0eb035b52,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Achieved the highest per-domain scores on Substance (F1  0.8) and the lowest scores on Interpersonal and Mood (F1  0.5), and show consistency in per-domain performance rankings between MLP and RBF models.","[[    0   250 17309  5202     5  1609   228    12 46400  4391    15 37506
     36   597   134 48981 23133   321     4   398    43     8     5  3912
   4391    15  3870 26892     8 39771    36   597   134 48981 23133   321
      4   245   238     8   311 12787    11   228    12 46400   819  8359
    227 10725   510     8 11191   597  3092     4     2]]"
4fdc707fae5747fceae68199851e3c3186ab8307,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d1909ce77d09983aa1b3ab5c56e2458caefbd442,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"entity match rate, BLEU score, Success F1 score","[[    0 46317   914   731     6   163  3850   791  1471     6 14361   274
    134  1471     2]]"
fed0785d24375ebbde51fb0503b93f14da1d8583,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"These models are likely to be deficient in encoding morphological features is that they are word level models, and do not have direct access sub-word information like inflectional endings, which indicates that these features are difficult to learn effectively purely from lexical distributions.","[[    0  4528  3092    32   533     7    28 38396    11 45278 38675  9779
   1575    16    14    51    32  2136   672  3092     6     8   109    45
     33  2228   899  2849    12 14742   335   101  4047 20576   337 41677
      6    61  8711    14   209  1575    32  1202     7  1532  4296 15430
     31 36912  3569 26070     4     2]]"
6d9fbd42b54313cfdc2665809886330f209e9286,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"IWSLT16, WMT15, NIST","[[    0   100   771 11160   565  1549     6   305 11674   996     6   234
  11595     2]]"
564dcaf8d0bcc274ab64c784e4c0f50d7a2c17ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"We apply this conversion to the 31 languages, Arabic, Hindi, Lithuanian, Persian, and Russian. , Dutch, Spanish","[[    0   170  3253    42 10012     7     5  1105 11991     6 19645     6
  19840     6 39714   811     6 27775     6     8  1083     4  2156  5979
      6  3453     2]]"
a8545f145d5ea2202cb321c8f93e75ad26fcf4aa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
8958465d1eaf81c8b781ba4d764a4f5329f026aa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"RIPA, Neighborhood Metric, WEAT",[[    0 29987   250     6 23579  4369  4063     6 10284  2571     2]]
5712a0b1e33484ebc6d71c70ae222109c08dede2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],VQA and GeoQA,[[    0   846  1864   250     8 29147  1864   250     2]]
736c74d2f61ac8d3ac31c45c6510a36c767a5d6d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
5328cc2588b2bf7b91f4e0f342e8cbfc6dc8ac00,0.0,Entailment,[[    2     0 30495  3760  1757     2]],LSTM-based RNN-T,[[    0   574  4014   448    12   805   248 20057    12   565     2]]
7917d44e952b58ea066dc0b485d605c9a1fe3dda,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
fb381a59732474dc71a413e25cac37e239547b55,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
02ce4c288df14a90a210cb39973c6ac0fb4cec59,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English,[[    0 35007     2]]
5be62428f973a08c303c66018b081ad140c559c8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Overall, our AMRAN outperforms all baselines, achieving 0.657 HR@10 and 0.410 NDCG@10.","[[    0 28965     6    84  3326   500  1889  9980 33334    70 11909 38630
      6  9499   321     4 38530 11683  1039   698     8   321     4 31132
    234  5949   534  1039   698     4     2]]"
3aa7173612995223a904cc0f8eef4ff203cbb860,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SLQA, Rusalka, HMA Model (single), TriAN (single), jiangnan (ensemble), MITRE (ensemble), TriAN (ensemble), HMA Model (ensemble)","[[    0 11160  1864   250     6 14762  9707   102     6   289  5273  7192
     36 25382   238  6892  1889    36 25382   238  1236 14607 10197    36
   9401 46831   238 20124  4629    36  9401 46831   238  6892  1889    36
   9401 46831   238   289  5273  7192    36  9401 46831    43     2]]"
a3c6acf900126bc9bd9c50ce99041ea00761da6a,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," workers were given a seed qualitative relation, asked to enter two objects, people, or situations to compare, created a question, guided by a large number of examples, LFs were elicited using a novel technique of reverse-engineering them from a set of follow-up questions","[[    0  1138    58   576    10  5018 29981  9355     6   553     7  2914
     80  8720     6    82     6    50  5458     7  8933     6  1412    10
    864     6 10346    30    10   739   346     9  7721     6   226 34417
     58 33340  4560   634    10  5808  9205     9  7213    12 36904   106
     31    10   278     9  1407    12   658  1142     2]]"
42d66726b5bf8de5b0265e09d76f5ab00c0e851a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Single-Turn, Multi-Turn",[[    0 44119    12 30093     6 19268    12 30093     2]]
10f560fe8e1c0c7dea5e308ee4cec16d07874f1d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"selects the next summary sentence based not only on properties of the source text, but also on the previously selected sentences in the summary","[[    0 38450    29     5   220  4819  3645   716    45   129    15  3611
      9     5  1300  2788     6    53    67    15     5  1433  3919 11305
     11     5  4819     2]]"
d484a71e23d128f146182dccc30001df35cdf93f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Perplexity of the best model is 65.58 compared to best baseline 105.79.
Bleu of the best model is 6.57 compared to best baseline 5.50.","[[    0 20823 26028  1571     9     5   275  1421    16  3620     4  4432
   1118     7   275 18043 11061     4  5220     4 50118 40370   257     9
      5   275  1421    16   231     4  4390  1118     7   275 18043   195
      4  1096     4     2]]"
73bb8b7d7e98ccb88bb19ecd2215d91dd212f50d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],comparing the summary with the text instead of the reference and labeling the candidate bad if it is incorrect or irrelevant,"[[    0 11828  5867     5  4819    19     5  2788  1386     9     5  5135
      8 27963     5  1984  1099   114    24    16 17401    50 21821     2]]"
edb2d24d6d10af13931b3a47a6543bd469752f0c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language.,"[[    0  1213  3919    70     5 48300    31   644  1014     7   719   777
     19    23   513  1764  1617    11     5 32644     8    23   513   204
    377     9     5 45757    18   750     4   252    67  2928  1822    19
      5  8533     9     5  5694    32    11  1093  2777     4     2]]"
182b6d77b51fa83102719a81862891f49c23a025,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"deciding publisher partisanship, risk annotator bias because of short description text provided to annotators","[[    0 11127  8231 10710 32459  4128     6   810 45068  2630  9415   142
      9   765  8194  2788  1286     7 45068  3629     2]]"
ba56afe426906c4cfc414bca4c66ceb4a0a68121,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Datasets used are Celex (English, Dutch), Festival (Italian), OpenLexuque (French), IIT-Guwahati (Manipuri), E-Hitz (Basque)","[[    0 43703   281  2580   341    32 15858 14726    36 35007     6  5979
    238  3502    36 39251   238  2117 43551   257  3407    36 28586   238
     38  2068    12 14484   605   895  3648    36  6407  1588  6151   238
    381    12   725  4494    36 40258  3407    43     2]]"
d40662236eed26f17dd2a3a9052a4cee1482d7d6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],a vector of frame-level acoustic features,[[    0   102 37681     9  5120    12  4483 21979  1575     2]]
7e34501255b89d64b9598b409d73f96489aafe45,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," dataset on Mechanical Turk involving human perception, action and communication","[[    0 41616    15 35644 19683  3329  1050 10518     6   814     8  4358
      2]]"
880a76678e92970791f7c1aad301b5adfc41704f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ML  logistic regression classifier combined with a Convolutional Neural Network (CNN) to identify self-reported diagnostic tweets.
NLP methods:  tweet conversion to numeric word vector,  removing tweets containing hyperlinks, removing ""retweets"", removing all tweets containing horoscope indicators,  lowercasing and  removing punctuation.","[[    0 10537  1437  7425  5580 39974  1380 24072  2771    19    10 30505
  23794   337 44304  3658    36 16256    43     7  3058  1403    12 28422
  20862  6245     4 50118   487 21992  6448    35  1437  3545 10012     7
  46325  2136 37681     6  1437  8201  6245  8200  8944 40822     6  8201
     22  4903  1694  2580  1297  8201    70  6245  8200 17211 31468 13038
      6  1437   795   438  7913     8  1437  8201 15760  9762     4     2]]"
9282cf80265a914a13053ab23b77d1a8ed71db1b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English, Russian",[[    0 35007     6  1083     2]]
1870f871a5bcea418c44f81f352897a2f53d0971,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"DOSPERT, BSSS and VIAS",[[    0 47198   510 18854     6   163  8108   104     8 11663  2336     2]]
2b61893b22ac190c94c2cb129e86086888347079,0.0,Entailment,[[    2     0 30495  3760  1757     2]],DBpedia,[[    0 10842 47745     2]]
b43fa27270eeba3e80ff2a03754628b5459875d6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Alarm, Banks, Buses, Calendar, Events, Flights, Homes, Hotels, Media, Messaging, Movies, Music, Payment, Rental Cars, Restaurants, Ride Sharing, Services, Train, Travel, Weather","[[    0  7083  4526     6  7489     6   163  9764     6 29419     6 13634
      6  4150  6183     6 13609     6  6003  2507     6  2454     6 15212
   6257     6 27568     6  3920     6 22807     6   248 13589 17714     6
  26210  3277     6 17499 24277     6  1820     6 15507     6  8696     6
   5842     2]]"
dc5ff2adbe1a504122e3800c9ca1d348de391c94,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The unsupervised tasks include five tasks from SemEval Semantic Textual Similarity (STS) in 2012-2016 BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 and the SemEval2014 Semantic Relatedness task (SICK-R) BIBREF35 .

The cosine similarity between vector representations of two sentences determines the textual similarity of two sentences, and the performance is reported in Pearson's correlation score between human-annotated labels and the model predictions on each dataset., Supervised Evaluation
It includes Semantic relatedness (SICK) BIBREF35 , SemEval (STS-B) BIBREF36 , paraphrase detection (MRPC) BIBREF37 , question-type classification (TREC) BIBREF38 , movie review sentiment (MR) BIBREF39 , Stanford Sentiment Treebank (SST) BIBREF40 , customer product reviews (CR) BIBREF41 , subjectivity/objectivity classification (SUBJ) BIBREF42 , opinion polarity (MPQA) BIBREF43 .","[[    0   133   542 16101 25376  8558   680   292  8558    31 11202   717
   6486 11202 26970 14159  5564 17110  1571    36  4014   104    43    11
   1125    12  9029   163  8863 45935   541  2156   163  8863 45935  2983
   2156   163  8863 45935  2881  2156   163  8863 45935  3103  2156   163
   8863 45935  3079     8     5 11202   717  6486 16310 11202 26970  3283
   1825  3685    36   104 11839    12   500    43   163  8863 45935  2022
    479 50118 50118   133 12793   833 37015   227 37681 30464     9    80
  11305 23483     5 46478 37015     9    80 11305     6     8     5   819
     16   431    11 16116    18 22792  1471   227  1050    12 37250  1070
  14105     8     5  1421 12535    15   349 41616   482  1582 25376 37766
  50118   243  1171 11202 26970  1330  1825    36   104 11839    43   163
   8863 45935  2022  2156 11202   717  6486    36  4014   104    12   387
     43   163  8863 45935  3367  2156 40127 34338 12673    36 12642  4794
     43   163  8863 45935  3272  2156   864    12 12528 20257    36   565
  40698    43   163  8863 45935  3170  2156  1569  1551  5702    36 12642
     43   163  8863 45935  3416  2156  8607 12169  8913 11077  5760    36
    104  4014    43   163  8863 45935  1749  2156  2111  1152  6173    36
   9822    43   163  8863 45935  4006  2156  2087  9866    73 40412  9866
  20257    36   104 12027   863    43   163  8863 45935  3714  2156  2979
   8385 21528    36  7629  1864   250    43   163  8863 45935  3897   479
      2]]"
bd9930a613dd36646e2fc016b6eb21ab34c77621,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1,006 fake reviews and 994 real reviews",[[    0   134     6 38049  4486  6173     8   361  6405   588  6173     2]]
9d9d84822a9c42eb0257feb7c18086d390dae3be,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
9c2de35d07f0d536bfdefe4828d66dd450de2b61,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
be73a88d5b695200e2ead4c2c24e2a977692970e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a0fbf90ceb520626b80ff0f9160b3cd5029585a5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BIBREF16,[[    0  5383   387 45935  1549     2]]
3cca26a9474d3b0d278e4dd57e24b227e7c2cd41,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Brent corpus, PTB , Beijing University Corpus, Penn Chinese Treebank","[[    0   387  9854 42168     6  7008   387  2156  3332   589 28556     6
   5953  1111 11077  5760     2]]"
73bbe0b6457423f08d9297a0951381098bd89a2b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"2008 Punyakanok et al. 
2009 Zhao et al. + ME 
2008 Toutanova et al. 
2010 Bjorkelund et al.  
2015 FitzGerald et al. 
2015 Zhou and Xu 
2016 Roth and Lapata 
2017 He et al. 
2017 Marcheggiani et al.
2017 Marcheggiani and Titov 
2018 Tan et al. 
2018 He et al. 
2018 Strubell et al. 
2018 Cai et al. 
2018 He et al. 
2018 Li et al. 
","[[    0 27418 14687   219   677   260  1638  4400  1076     4  1437 50118
  23301 34219  4400  1076     4  2055 12341  1437 50118 27418   255   995
  18980  4400  1076     4  1437 50118 24789 20542   368  5576  3194  4400
   1076     4  1437  1437 50118 14420 21872   534 26755  4400  1076     4
   1437 50118 14420 28279     8 30226  1437 50118  9029 13880     8 18485
   2186  1437 50118  3789    91  4400  1076     4  1437 50118  3789  1127
   2871  6149 20909  4400  1076     4 50118  3789  1127  2871  6149 20909
      8 18125  1417  1437 50118  2464  7650  4400  1076     4  1437 50118
   2464    91  4400  1076     4  1437 50118  2464  5997  1792  1641  4400
   1076     4  1437 50118  2464   230  1439  4400  1076     4  1437 50118
   2464    91  4400  1076     4  1437 50118  2464  5991  4400  1076     4
   1437 50118     2]]"
579941de2838502027716bae88e33e79e69997a6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.
For number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. 
For date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1.","[[    0  2709   881    12 36407  1142     6     5  1850 41211  8800    12
    104 15513  2606  1477   819     9     5 14077 48508 11802 18043    13
    132     4   134 14850     8   112     4  3118   274   134     4 50118
   2709   346  1907   864     6  1437 14077 48508 11802 18043  1437    33
   3855    81 41211  8800    12   104 15513  2606    13   155     6  1225
   1437 14850     8  1437   132     6  5208   274   134     4  1437 50118
   2709  1248   864     6  1437 41211  8800    12   104 15513  2606    33
   5139    11   132     6  4197 14850    53 14077 48508 11802    33  3855
      9   204     6  3416   274   134     4     2]]"
285858416b1583aa3d8ba0494fd01c0d4332659f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1) investigate errors produced by the end-to-end methods and explore several approaches to correct common errors done in French, 2) compare the end-to-end methods in a SLU context and evaluate the semantic value of the partially correct produced words","[[    0   134    43  4830  9126  2622    30     5   253    12   560    12
   1397  6448     8  5393   484  8369     7  4577  1537  9126   626    11
   1515     6   132    43  8933     5   253    12   560    12  1397  6448
     11    10 10962   791  5377     8 10516     5 46195   923     9     5
   8531  4577  2622  1617     2]]"
e09dcb6fc163bba7d704178e7edba2e630b573c2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
eae13c9693ace504eab1f96c91b16a0627cd1f75,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
74cd51a5528c6c8e0b634f3ad7a9ce366dfa5706,0.0,Entailment,[[    2     0 30495  3760  1757     2]],it is less expensive and quantifies interpretability using continuous values rather than binary evaluations,"[[    0   405    16   540  3214     8 24934 10687 18107  4484   634 11152
   3266  1195    87 32771 30962     2]]"
63c0128935446e26eacc7418edbd9f50cba74455,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"440 sentences, 2247 triples extracted from those sentences, and 11262 judgements on those triples.","[[    0 26634 11305     6   132 27011  7182 12349 27380    31   167 11305
      6     8 12730  5379 21392 41330    15   167  7182 12349     4     2]]"
07d7652ad4a0ec92e6b44847a17c378b0d9f57f5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],10.37 BLEU,[[   0  698    4 3272  163 3850  791    2]]
91c81807374f2459990e5f9f8103906401abc5c2,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," The basic idea of the visualization, drawing on Isaac Newtons visualization of the color spectrum BIBREF8 , is to express a mixture in terms of its constituents as represented in barycentric coordinates.","[[    0    20  3280  1114     9     5 38228     6  5523    15 12370 10793
     17    27    29 38228     9     5  3195  8576   163  8863 45935   398
   2156    16     7  5486    10 12652    11  1110     9    63 13990    25
   4625    11   741  1766 19483 34721     4     2]]"
b3fcab006a9e51a0178a1f64d1d084a895bd8d5c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"S2VT, RGB (VGG), RGB (VGG)+Flow (AlexNet), LSTM-E (VGG), LSTM-E (C3D) and Yao et al.","[[    0   104   176 35707     6 38523    36   846 24592   238 38523    36
    846 24592 49197 41779    36 16804 15721   238   226  4014   448    12
    717    36   846 24592   238   226  4014   448    12   717    36   347
    246   495    43     8 38748  4400  1076     4     2]]"
010e3793eb1342225857d3f95e147d8f8467192a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The Dutch section consists of 2,333,816 sentences and 53,487,257 words., The SONAR500 corpus consists of more than 500 million words obtained from different domains.","[[    0   133  5979  2810 10726     9   132     6 25631     6   398  1549
  11305     8  4268     6 37795     6 31297  1617   482    20   208  2191
   2747  1497 42168 10726     9    55    87  1764   153  1617  4756    31
    430 30700     4     2]]"
4e9684fd68a242cb354fa6961b0e3b5c35aae4b6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Unimodal LSTM vs Best Multimodal (FCM)
- F score: 0.703 vs 0.704
- AUC: 0.732 vs 0.734 
- Mean Accuracy: 68.3 vs 68.4 ","[[    0  9685   757  1630   337   226  4014   448  1954  2700 14910   757
   1630   337    36  5268   448    43 50118    12   274  1471    35   321
      4 37106  1954   321     4 37871 50118    12    83 12945    35   321
      4   406  2881  1954   321     4   406  3079  1437 50118    12 30750
  42688    35  5595     4   246  1954  5595     4   306  1437     2]]"
6090d3187c41829613abe785f0f3665d9ecd90d9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Only in the context of a sentence does a word have a meaning.,"[[    0 19933    11     5  5377     9    10  3645   473    10  2136    33
     10  3099     4     2]]"
74fe054f5243c8593ddd2c0628f91657246b7dfa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
8c0a0747a970f6ea607ff9b18cfeb738502d9a95,0.0,Entailment,[[    2     0 30495  3760  1757     2]],ERR of 19.05 with i-vectors and 15.52 with x-vectors,"[[    0  2076   500     9   753     4  2546    19   939    12   548 38636
      8   379     4  4429    19  3023    12   548 38636     2]]"
4c854d33a832f3f729ce73b206ff90677e131e48,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"tried many configurations of our network models, but report results with only three configurations, Transformer Type 1, Transformer Type 2, Transformer Type 3","[[    0    90 10382   171 33843     9    84  1546  3092     6    53   266
    775    19   129   130 33843     6  5428 22098  7773   112     6  5428
  22098  7773   132     6  5428 22098  7773   155     2]]"
65d34041ffa4564385361979a08706b10b92ebc7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
3da9a861dfa25ed486cff0ef657d398fdebf8a93,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Noun WordNet Semantic Text Exchange Model (NWN-STEM), General WordNet Semantic Text Exchange Model (GWN-STEM), Word2Vec Semantic Text Exchange Model (W2V-STEM)","[[    0   487  7928 15690 15721 11202 26970 14159  3080  7192    36 24430
    487    12 43896   238  1292 15690 15721 11202 26970 14159  3080  7192
     36   534 29722    12 43896   238 15690   176   846  3204 11202 26970
  14159  3080  7192    36   771   176   846    12 43896    43     2]]"
4db3c2ca6ddc87209c31b20763b7a3c1c33387bc,0.0,Entailment,[[    2     0 30495  3760  1757     2]], from a collection of advanced persistent threats (APT) reports which are published from 2008 to 2018,"[[    0    31    10  2783     9  3319 13109  3455    36   591   565    43
    690    61    32  1027    31  2266     7   199     2]]"
a4e66e842be1438e5cd8d7cb2a2c589f494aee27,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Depeche + SVM,[[    0 13365  2379  2871  2055   208 20954     2]]
d0c79f4a5d5c45fe673d9fcb3cd0b7dd65df7636,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"New best results of accuracy (P@1) on Vecmap:
Ours-GeoMMsemi: EN-IT 50.00 IT-EN 42.67 EN-DE 51.60 DE-EN 47.22 FI-EN 39.62 EN-ES 39.47 ES-EN 36.43","[[    0  4030   275   775     9  8611    36   510  1039   134    43    15
  39800 32557    35 50118   673  4668    12 20981   139 16261  1090  5408
     35 13245    12  2068   654     4   612  3779    12  2796  3330     4
   4111 13245    12 10089  4074     4  2466  5885    12  2796  4034     4
   2036 20027    12  2796  3191     4  5379 13245    12  1723  3191     4
   3706 18366    12  2796  2491     4  3897     2]]"
e28a6e3d8f3aa303e1e0daff26b659a842aba97b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
71f135be79341e61c28c3150b1822d0c4d0ca8d6,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," improves the F1 score by almost 2%, which corresponds to a 12.3% error rate reduction","[[    0 15296     5   274   134  1471    30   818   132  4234    61 40189
      7    10   316     4   246   207  5849   731  4878     2]]"
9c0cf1630804366f7a79a40934e7495ad9f32346,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
7358a1ce2eae380af423d4feeaa67d2bd23ae9dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The embeddings are learned several times using the training set, then the average is taken.","[[    0   133 33183   417  1033    32  2435   484   498   634     5  1058
    278     6   172     5   674    16   551     4     2]]"
77db56fee07b01015a74413ca31f19bea7203f0b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"F$_1$, precision, and recall",[[    0   597  1629  1215   134 47110 15339     6     8  6001     2]]
b02d2d351bd2e49d4d59db0a8a6ef23cb90bfbc4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
40f87db3a8d1ac49b888ce3358200f7d52903ce7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
cf15c4652e23829d8fb4cf2a25e64408c18734c1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the image can play the role of a pivot language"" to bridge the two languages without paralleled corpus","[[    0   627  2274    64   310     5   774     9    10 27475    44    48
  19527   113     7  4081     5    80 11991   396 46515  1329 42168     2]]"
d5105a6a6d5d1931b0729dcf15ca862d6eac770f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],62,[[   0 5379    2]]
5699996a7a2bb62c68c1e62e730cabf1e3186eef,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
4cbc56d0d53c4c03e459ac43e3c374b75fd48efe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LSTM, SCIBERT",[[    0   574  4014   448     6  4998  8863 18854     2]]
6cdd61ebf84aa742155f4554456cc3233b6ae2bf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SVM with RBF kernel,[[    0   104 20954    19 11191   597 34751     2]]
551f77b58c48ee826d78b4bf622bb42b039eca8c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],can be biased by dataset used and may generate categories which are suboptimal compared to human designed categories,"[[    0  7424    28 21099    30 41616   341     8   189  5368  6363    61
     32  2849 19693 16980  1118     7  1050  1887  6363     2]]"
345f65eaff1610deecb02ff785198aa531648e75,0.0,Entailment,[[    2     0 30495  3760  1757     2]], automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15,"[[    0  6885  4756   149     5  5674 23226     8  5702  1966  4116  2942
     11    84  2052   173   163  8863 45935   996     2]]"
fbabde18ebec5852e3d46b1f8ce0afb42350ce62,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
4b8257cdd9a60087fa901da1f4250e7d910896df,0.0,Entailment,[[    2     0 30495  3760  1757     2]],typos in spellings or ungrammatical words,[[    0  2553 11474    11  8921  1033    50   542 28526 45816  1617     2]]
8f838ec579f2609b01227da3d8c77860ac1b39d2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
180047e1ccfc7c98f093b8d1e1d0479a4cca99cc,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," sequence-to-sequence model (denoted as S2S), HRAN","[[    0 13931    12   560    12 46665  1421    36  3898 11367    25   208
    176   104   238 11683  1889     2]]"
059acc270062921ad27ee40a77fd50de6f02840a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
d51069595f67a3a53c044c8a37bae23facbfa45d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d71937fa5da853f7529f767730547ccfb70e5908,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"News Articles, Twitter",[[    0  5532 15219     6   599     2]]
f27502c3ece9ade265389d5ace90ca9ca42b46f3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],separate set of Turkers to rate the stories for overall quality and the three improvement areas,"[[    0 39114   877   278     9 19683   268     7   731     5  1652    13
   1374  1318     8     5   130  3855   911     2]]"
a30958c7123d1ad4723dcfd19d8346ccedb136d5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
68794289ed6078b49760dc5fdf88618290e94993,0.0,Entailment,[[    2     0 30495  3760  1757     2]],A sequence of logical statements represented in a computational graph,[[    0   250 13931     9 16437  1997  4625    11    10 38163 20992     2]]
d1ff6cba8c37e25ac6b261a25ea804d8e58e09c0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"F-score, Area Under the ROC Curve (AUC), mean accuracy (ACC), Precision vs Recall plot, ROC curve (which plots the True Positive Rate vs the False Positive Rate)","[[    0   597    12 31673     6  4121  2096     5   248  4571 41596    36
    250 12945   238  1266  8611    36 21678   238 29484  1954 35109  6197
      6   248  4571  9158    36  5488 21258     5  7447 25968 14064  1954
      5 35297 25968 14064    43     2]]"
529dabe7b4a8a01b20ee099701834b60fb0c43b0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement","[[    0  1342 23801  1757     6  1194     6  6970     6   310     6  1569
      6   748 12376     6   697  2308     6  1901     6  4149     6  3872
  12257     8  6859     2]]"
8b71ede8170162883f785040e8628a97fc6b5bcb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"it is necessary to evaluate the performance of the above mentioned part of the pipeline before proceeding further. The evaluation of the performance is summarised in Table TABREF11. It shows that organising the two models into the pipeline boosted the performance of the reference recognition model, leading to a higher F1 measure in the initial recognition of the text spans and their classification.","[[    0   405    16  2139     7 10516     5   819     9     5  1065  2801
    233     9     5  4116   137 19635   617     4    20 10437     9     5
    819    16 39186  1720    11  9513   255  4546 45935  1225     4    85
    924    14 22821     5    80  3092    88     5  4116  5934     5   819
      9     5  5135  4972  1421     6   981     7    10   723   274   134
   2450    11     5  2557  4972     9     5  2788 23645     8    49 20257
      4     2]]"
f10325d022e3f95223f79ab00f8b42e3bb7ca040,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They derive entity grid with grammatical relations and RST discourse relations and concatenate them with pooling vector for the char-bigrams before feeding to the resulting vector to the softmax layer.,"[[    0  1213 34882 10014  7961    19 25187 45816  3115     8   248  4014
  19771  3115     8 10146 26511   877   106    19  3716   154 37681    13
      5 16224    12  8527 27809   137 10943     7     5  5203 37681     7
      5  3793 29459 10490     4     2]]"
33d2919f3400cd3c6fbb6960d74187ec80b41cd6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The selection model selects the best answer from the set $\lbrace a_i\rbrace _{i=1}^N$ observed during the interaction by predicting the difference of the F1 score to the average F1 of all variants.,"[[    0   133  4230  1421 38845     5   275  1948    31     5   278 49959
    462 37123    10  1215   118 37457   338 37123 18134 45152   118  5214
    134 24303 35227   487  1629  6373   148     5 10405    30 15924     5
   2249     9     5   274   134  1471     7     5   674   274   134     9
     70 21740     4     2]]"
df25dd9004a3b367202d7731ee912a8052a35780,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
8abb96b2450ebccfcc5c98772cec3d86cd0f53e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
9ebb2adf92a0f8db99efddcade02a20a219ca7d9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They used 6 indicators for proficiency (same for written and spoken) each marked by bad, medium or good by one expert.","[[    0  1213   341   231 13038    13 35897    36 41690    13  1982     8
   5826    43   349  4760    30  1099     6  4761    50   205    30    65
   3827     4     2]]"
7883a52f008f3c4aabfc9f71ce05d7c4107e79bb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
a996b6aee9be88a3db3f4127f9f77a18ed10caba,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"0.8320 on semantic typing, 0.7194 on entity matching","[[    0   288     4  6361   844    15 46195 29691     6   321     4   406
  30548    15 10014  8150     2]]"
e215fa142102f7f9eeda9c9eb8d2aeff7f2a33ed,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"for each multiple-choice question $(q,A) \in Q_\mathit {tr}$ and each choice $a \in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S, take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \in A$ and over all questions in $Q_\mathit {tr}$","[[    0  1990   349  1533    12 24335   864 44612  1343     6   250    43
  44128   179  1209  1215 37457 40051   405 25522  4328 24303  1629     8
    349  2031    68   102 44128   179    83  1629  2156    52   304    70
    786    12  8287 14742 22121    11    68  1343  1629     8    68   102
   1629    25    41 45609 39954 25860   136   208     6   185     5   299
   1878  2323     6   422  2117 18090   748   306     6     8 13884     5
   5203 13145 12349    81    70    68   102 44128   179    83  1629     8
     81    70  1142    11    68  1864  1215 37457 40051   405 25522  4328
  24303  1629     2]]"
0c09ffb337be0feb25e2fd14164b35a0969d7b4c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],word2vec BIBREF0,[[    0 14742   176 25369   163  8863 45935   288     2]]
0117aa1266a37b0d2ef429f1b0653b9dde3677fe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
b5d6357d3a9e3d5fdf9b344ae96cddd11a407875,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"PCFGLA-based parser, viz. Berkeley parser BIBREF5, minimal span-based neural parser BIBREF6","[[    0  4794   597 10020   250    12   805 48946     6 27155     4 10817
  48946   163  8863 45935   245     6  9865  8968    12   805 26739 48946
    163  8863 45935   401     2]]"
9fe4a2a5b9e5cf29310ab428922cc8e7b2fc1d11,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"FTLM++, BERT-large, XLNet","[[    0 11615 21672 42964     6   163 18854    12 11802     6 14402 15721
      2]]"
36b25021464a9574bf449e52ae50810c4ac7b642,0.0,Entailment,[[    2     0 30495  3760  1757     2]],From Twitter profile descriptions of the users.,[[    0  7605   599  4392 24173     9     5  1434     4     2]]
8b11bc3a23932afe7d52c19deffd9dec4830f2e9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e78a47aec37d9a3bec5a18706b0a462c148c118b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
62a6382157d5f9c1dce6e6c24ac5994442053002,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"accuracy, normalized mutual information",[[    0  7904 45386     6 35247  7628   335     2]]
23d32666dfc29ed124f3aa4109e2527efa225fbc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They use it as addition to previous model - they add new edge between words if word embeddings are similar.,"[[    0  1213   304    24    25  1285     7   986  1421   111    51  1606
     92  3543   227  1617   114  2136 33183   417  1033    32  1122     4
      2]]"
cacb83e15e160d700db93c3f67c79a11281d20c5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"No, there has been previous work on recognizing social norm violation.","[[    0  3084     6    89    34    57   986   173    15 16257   592 13071
   4565     4     2]]"
bbeb74731b9ac7f61e2d74a7d9ea74caa85e62ef,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
10091275f777e0c2890c3ac0fd0a7d8e266b57cf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e829f008d62312357e0354a9ed3b0827c91c9401,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, TF-IDF Emoticon features","[[    0 16750 19187 34066 31967     6  4657     9 27242     6 14687  3894
   9762     6 12169  8913  5213     6  3676 22609     6 35690    12  2688
    597  3676  1242 17505  1575     2]]"
35915166ab2fd3d39c0297c427d4ac00e8083066,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
4d4550533edb19c38cb876b1640e62e34e2b88e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],hatespeechdata.com,[[    0   298  1626  2379  7529 23687     4   175     2]]
05e3b831e4c02bbd64a6e35f6c52f0922a41539a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
9e805020132d950b54531b1a2620f61552f06114,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CNN-mean, CNN-avgmax",[[    0 16256    12 43348     6  3480    12  1469   571 29459     2]]
53b02095ba7625d85721692fce578654f66bbdf0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
b34c60eb4738e0439523bcc679fe0fe70ceb8bde,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"in the OpenBookQA setup the open book part is much larger, the open book part is much larger (than a small paragraph) and is not complete as additional common knowledge may be required","[[    0   179     5  2117 24751  1864   250 11808     5   490  1040   233
     16   203  2514     6     5   490  1040   233    16   203  2514    36
   5652    10   650 17818    43     8    16    45  1498    25   943  1537
   2655   189    28  1552     2]]"
8c0621016e96d86a7063cb0c9ec20c76a2dba678,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
ffbd6f583692db66b719a846ba2b7f6474df481a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],model is composed of an encoder (SECREF5) and a decoder with the attention mechanism (SECREF7) that are both implemented using recurrent neural networks (RNNs),"[[    0 21818    16 14092     9    41  9689 15362    36 47847 21536 45935
    245    43     8    10  5044 15362    19     5  1503  9562    36 47847
  21536 45935   406    43    14    32   258  6264   634 35583 26739  4836
     36   500 20057    29    43     2]]"
a5418e4af99a2cbd6b7a2b8041388a2d01b8efb2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Loss analysis. To conduct a more thorough evaluation, we further investigate model behaviours in terms of both reconstruction loss and KL loss, as shown in Figure FIGREF14. These plots were obtained based on the E2E training set using the inputless setting.","[[    0   574  5434  1966     4   598  2883    10    55 10675 10437     6
     52   617  4830  1421 30723    11  1110     9   258 18228   872     8
  26544   872     6    25  2343    11 17965 37365 45935  1570     4  1216
  21258    58  4756   716    15     5   381   176   717  1058   278   634
      5  8135  1672  2749     4     2]]"
52b113e66fd691ae18b9bb8a8d17e1ee7054bb81,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Vagalume website,[[   0  846 1073  337 9271  998    2]]
b799936d6580c0e95102027175d3fe184f0ee253,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The given corpus is traversed, and for each element INLINEFORM6 , its successor INLINEFORM7 , together with a given element, forms a directed edge INLINEFORM8 . Finally, such edges are weighted according to the number of times they appear in a given corpus. Thus the graph, constructed after traversing a given corpus, consists of all local neighborhoods (order one), merged into a single joint structure. Global contextual information is potentially kept intact (via weights), even though it needs to be detected via network analysis","[[    0   133   576 42168    16 34038   196     6     8    13   349  7510
   2808 28302 38036   401  2156    63 10359  2808 28302 38036   406  2156
    561    19    10   576  7510     6  4620    10  3660  3543  2808 28302
  38036   398   479  3347     6   215 15716    32 19099   309     7     5
    346     9   498    51  2082    11    10   576 42168     4 10623     5
  20992     6 11236    71 34038   154    10   576 42168     6 10726     9
     70   400  9100    36 10337    65   238 21379    88    10   881  2660
   3184     4  1849 37617   335    16  2905  1682 13762    36 11409 23341
    238   190   600    24   782     7    28 12333  1241  1546  1966     2]]"
94bee0c58976b58b4fef9e0adf6856fe917232e5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Switchboard-2000 contains 1700 more hours of speech data.,"[[    0 45112  4929    12 17472  6308 33236    55   722     9  1901   414
      4     2]]"
38b527783330468bf6c4829f7d998e6f17c615f0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
62613aca3d7c7d534c9f6d8cb91ff55626bb8695,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Argus Dataset, AI2-8grade/CK12 Dataset, MCTest Dataset","[[    0 45621   687 16673   281   594     6  4687   176    12   398  8425
     73   347   530  1092 16673   281   594     6   256  7164   990 16673
    281   594     2]]"
c30c3e0f8450b1c914d29f41c17a22764fa078e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The system extends BiDAF BIBREF4 with self-attention,"[[    0   133   467 14269  6479  3134   597   163  8863 45935   306    19
   1403    12  2611 19774     2]]"
6f8386ad64dce3a20bc75165c5c7591df8f419cf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],We thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space.,"[[    0   170  4634   860     7  3438     5  2777    12 14175   335    31
      5 30464    30   715  2961     5 30464     9 11305    11   349  2777
     98    14    49   674  5738    23     5  9813     9     5 37681   980
      4     2]]"
ad16c8261c3a0b88c685907387e1a6904eb15066,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"how to incorporate affective information into chatbots, what are resources that available and can be used to build EAC, and how to evaluate EAC performance","[[    0  9178     7 14518  3327  2088   335    88  7359 26478     6    99
     32  1915    14   577     8    64    28   341     7  1119   381  2562
      6     8   141     7 10516   381  2562   819     2]]"
547be35cff38028648d199ad39fb48236cfb99ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
0cd90e5b79ea426ada0203177c28812a7fc86be5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],varied the number of experts between models,[[    0 10806  2550     5   346     9  2320   227  3092     2]]
0457242fb2ec33446799de229ff37eaad9932f2a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"handling large volume incoming data, sentiment analysis on tweets and predictive online learning","[[    0  4539  1527   739  3149 11433   414     6  5702  1966    15  6245
      8 27930   804  2239     2]]"
0cd0755ac458c3bafbc70e4268c1e37b87b9721b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
57388bf2693d71eb966d42fa58ab66d7f595e55f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],A BPE model is applied to the stem after morpheme segmentation.,"[[    0   250   163 16035  1421    16  5049     7     5 10114    71 14628
  42464  1794  2835  1258     4     2]]"
d6a27c41c81f12028529e97e255789ec2ba39eaa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"accuracy can change up to 5 percentage points, whereas BLEU can vary up to 8 points","[[    0  7904 45386    64   464    62     7   195  3164   332     6  9641
    163  3850   791    64 10104    62     7   290   332     2]]"
317a6f211ecf48c58f008c12fbd5d41901db3e36,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
f63519bb5e116671cebd65cc78880c5cb573c570,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e98d331faacd50f8ec588d2466b5a85da1f37e6f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
af1439c68b28c27848203f863675946380d28943,0.0,Entailment,[[    2     0 30495  3760  1757     2]],RoBERTa baseline,[[    0 27110 11126 38495 18043     2]]
42394c54a950bae8cebecda9de68ee78de69dc0d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],counts of predicate-argument tuples from English Wikipedia,"[[    0 11432    29     9 48206    12 46494 13145 12349    31  2370 28274
      2]]"
12c7d79d2a26af2d445229d0c8ba3ba1aab3f5b5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
3b995a7358cefb271b986e8fc6efe807f25d60dc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],GloVE; SGNS,[[    0   534  4082  8856   131 16324  6949     2]]
edb2d24d6d10af13931b3a47a6543bd469752f0c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They collect subreddits from January 2013 to December 2014,2 for which there are at
least 500 words in the vocabulary used to estimate the measures,
in at least 4 months of the subreddits history. They compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language.","[[    0  1213  5555 48300    31   644  1014     7   719   777     6   176
     13    61    89    32    23 50118   459  1988  1764  1617    11     5
  32644   341     7  3278     5  1797     6 50118   179    23   513   204
    377     9     5 45757    17    27    29   750     4   252 37357    84
   1797    81     5  1450  1982    30  1434    11    10   435    11    86
   6410     9   377     6    13   349 21547  2171   353     6     8 24704
   3438  1822   147     5  8533     9     5  5694    32    11    10  1093
   2777     4     2]]"
92412a449c28b9121a4a4f4acca996563f107131,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"pre-trained word embeddings BIBREF11, BIBREF12, recurrent BIBREF13, transformer-based sentence encoders BIBREF14, distinct convolutional neural networks, standard fusion strategies,  two main attention mechanisms BIBREF18, BIBREF19","[[    0  5234    12 23830  2136 33183   417  1033   163  8863 45935  1225
      6   163  8863 45935  1092     6 35583   163  8863 45935  1558     6
  40878    12   805  3645  9689  1630   268   163  8863 45935  1570     6
  11693 15380 23794   337 26739  4836     6  2526 24904  4964     6  1437
     80  1049  1503 14519   163  8863 45935  1366     6   163  8863 45935
   1646     2]]"
cfffc94518d64cb3c8789395707e4336676e0345,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"classification, regression, neural methods",[[    0  4684  5000     6 39974     6 26739  6448     2]]
1c85a25ec9d0c4f6622539f48346e23ff666cd5f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],5 questions per image,[[   0  245 1142  228 2274    2]]
557d1874f736d9d487eb823fe8f6dab4b17c3c42,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Bantu,[[  0 387 927 257   2]]
64ab2b92e986e0b5058bf4f1758e849f6a41168b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
a592498ba2fac994cd6fad7372836f0adb37e22a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],100 million sentences,[[    0  1866   153 11305     2]]
63723c6b398100bba5dc21754451f503cb91c9b8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"POS and DP task: CONLL 2018
NER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF
NLI task: mBERT or XLM (not clear from text)","[[    0 42740     8 24931  3685    35  8748  6006   199 50118 20196  3685
     35    36  2362  4935   173    43  8776 11909 38630  4307   597     8
   6479   574  4014   448    12  9822   597 50118   487 27049  3685    35
    475 11126   565    50 14402   448    36  3654   699    31  2788    43
      2]]"
a60030cfd95d0c10b1f5116c594d50cb96c87ae6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
351510da69ab6879df5ff5c7c5f49a8a7aea4632,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d28260b5565d9246831e8dbe594d4f6211b60237,0.0,Entailment,[[    2     0 30495  3760  1757     2]],We empirically provide a formula to measure the richness in the scenario of machine translation.,"[[    0   170 46533  3435   694    10  9288     7  2450     5 38857    11
      5  5665     9  3563 19850     4     2]]"
05238d1fad2128403577822aa4822ef8ca9570ac,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
decb07f9be715de024236e50dc7011a132363480,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CNN model can be trained in a purely online setting. We first initialize the model parameters $\theta _0$ (line 1), which can be a trained model from other disaster events or it can be initialized randomly to start from scratch.

As a new batch of labeled tweets $B_t= \lbrace \mathbf {s}_1 \ldots \mathbf {s}_n \rbrace $ arrives, we first compute the log-loss (cross entropy) in Equation 11 for $B_t$ with respect to the current parameters $\theta _t$ (line 2a). Then, we use backpropagation to compute the gradients $f^{\prime }(\theta _{t})$ of the loss with respect to the current parameters (line 2b). Finally, we update the parameters with the learning rate $\eta _t$ and the mean of the gradients (line 2c). We take the mean of the gradients to deal with minibatches of different sizes. Notice that we take only the current minibatch into account to get an updated model. ","[[    0 16256  1421    64    28  5389    11    10 15430   804  2749     4
    166    78 49161     5  1421 17294 49959   627  4349 18134   288  1629
     36  1902   112   238    61    64    28    10  5389  1421    31    97
   4463  1061    50    24    64    28 49271 22422     7   386    31 12272
      4 50118 50118  1620    10    92 14398     9 16274  6245    68   387
   1215    90  5214 44128   462 37123 44128 40051 36920 25522    29 24303
   1215   134 44128  4779  5992 44128 40051 36920 25522    29 24303  1215
    282 44128   338 37123    68  8842     6    52    78 37357     5  7425
     12 13242    36 15329 47382    43    11  8510  1258   365    13    68
    387  1215    90  1629    19  2098     7     5   595 17294 49959   627
   4349 18134    90  1629    36  1902   132   102   322  1892     6    52
    304   124 27128  1073  1258     7 37357     5 12003 20676    68   506
  35227 49918 28752 35524 49921   627  4349 18134 45152    90 49424  1629
      9     5   872    19  2098     7     5   595 17294    36  1902   132
    428   322  3347     6    52  2935     5 17294    19     5  2239   731
  49959  8152 18134    90  1629     8     5  1266     9     5 12003 20676
     36  1902   132   438   322   166   185     5  1266     9     5 12003
  20676     7   432    19  5251  1452 24700     9   430 10070     4 22873
     14    52   185   129     5   595  5251  1452 11175    88  1316     7
    120    41  4752  1421     4  1437     2]]"
d3092cd32cd581a57fa4844f80fe18d6b920e903,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LSTM with text embedding, LSTM with emoji embedding, Attention-based LSTM with emojis","[[    0   574  4014   448    19  2788 33183 11303     6   226  4014   448
     19 21554 33183 11303     6 35798    12   805   226  4014   448    19
   2841  4203   354     2]]"
6976296126e4a5c518e6b57de70f8dc8d8fde292,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), Textual Kernels Model (TKM)","[[    0 47702  2585  8729   225  1258  7192    36  5268   448   238  2064
  37438  2585  8729   225  1258  7192    36  3632   448   238 14159  5564
    229 47909  7192    36   565   530   448    43     2]]"
66125cfdf11d3bf8e59728428e02021177142c3a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Table TABREF15 shows that word-alignment based on mBERT representations surpasses the outputs of the standard FastAlign tool even if it was provided large parallel corpus. This suggests that word-level semantics are well captured by mBERT contextual embeddings. For this task, learning an explicit projection had a negligible effect on the performance.","[[    0 41836   255  4546 45935   996   924    14  2136    12   337 15645
    716    15   475 11126   565 30464 14874   293     5 39512     9     5
   2526  9612  7083  4932  3944   190   114    24    21  1286   739 12980
  42168     4   152  3649    14  2136    12  4483 46264    32   157  4705
     30   475 11126   565 37617 33183   417  1033     4   286    42  3685
      6  2239    41 16045 18144    56    10 36334  1683    15     5   819
      4     2]]"
79d999bdf8a343ce5b2739db3833661a1deab742,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"No extraction, No annotation, Wrong range, Wrong tag, Wrong range and tag","[[    0  3084 23226     6   440 47760     6 31273  1186     6 31273  6694
      6 31273  1186     8  6694     2]]"
79443bf3123170da44396b0481364552186abb91,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"sequence classification, sequence labeling",[[    0 46665 20257     6 13931 27963     2]]
4698298d506bef02f02c80465867f2cd12d29182,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BIBREF35 for VQA dataset, BIBREF5, BIBREF36","[[    0  5383   387 45935  2022    13   468  1864   250 41616     6   163
   8863 45935   245     6   163  8863 45935  3367     2]]"
1ef5fc4473105f1c72b4d35cf93d312736833d3d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
3ccd337f77c5d2f7294eb459ccc1770796c2eaef,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Model Confidence, Continuity, Query-relatedness, Repetitiveness, Specificity","[[    0 45149 10303 17444     6 21803 15147     6 44489    12  3368  1825
      6  2825   594 43468     6 37699  1571     2]]"
707db46938d16647bf4b6407b2da84b5c7ab4a81,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Simple Skip improves F1 from 74.34 to 74.81
Transformer Skip improes F1 from 74.34 to 74.95 ","[[    0 45093  6783 15296   274   134    31  6657     4  3079     7  6657
      4  6668 50118 19163 22098  6783 44354   293   274   134    31  6657
      4  3079     7  6657     4  4015  1437     2]]"
fa3663567c48c27703e09c42930e51bacfa54905,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"deep convolutional networks BIBREF53 , BIBREF54","[[    0 13637 15380 23794   337  4836   163  8863 45935  4540  2156   163
   8863 45935  4283     2]]"
100cf8b72d46da39fedfe77ec939fb44f25de77f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"dataset that contains article-comment parallel contents INLINEFORM0 , and an unpaired dataset that contains the documents (articles or comments) INLINEFORM1","[[    0 36146   281   594    14  6308  1566    12 44951 12980 13654  2808
  28302 38036   288  2156     8    41 23089 35016 41616    14  6308     5
   2339    36 32315    50  1450    43  2808 28302 38036   134     2]]"
68edb6a483cdec669c9130c928994654f1c19839,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"NDCG, MRR, recall@k, mean rank","[[    0 13457 40099     6 18838   500     6  6001  1039   330     6  1266
   7938     2]]"
31ee92e521be110b6a5a8d08cc9e6f90a3a97aae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d491ee69db39ec65f0f6da9ec03450520389699a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"32.25% of gang members in our dataset have chained together the police and the pistol emoji, compared to just 1.14% of non-gang members, only 1.71% of non-gang members have used the hundred points emoji and pistol emoji together in tweets while 53% of gang members have used them, gang members have a penchant for using just a small set of emoji symbols that convey their anger and violent behavior","[[    0  2881     4  1244   207     9  5188   453    11    84 41616    33
  38938   561     5   249     8     5 18481 21554     6  1118     7    95
    112     4  1570   207     9   786    12 21251   453     6   129   112
      4  5339   207     9   786    12 21251   453    33   341     5  6317
    332 21554     8 18481 21554   561    11  6245   150  4268   207     9
   5188   453    33   341   106     6  5188   453    33    10 28914    13
    634    95    10   650   278     9 21554 19830    14 15451    49  6378
      8  4153  3650     2]]"
347e86893e8002024c2d10f618ca98e14689675f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],only high-quality data helps,[[   0 8338  239   12 8634  414 2607    2]]
5d85d7d4d013293b4405beb4b53fa79ac7c03401,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"human preference annotation is available, $Q(x_1, x_2) \in \lbrace >,<,\approx \rbrace $ is the true label for the pair","[[    0 19003 12832 47760    16   577     6    68  1864  1640  1178  1215
    134     6  3023  1215   176    43 44128   179 44128   462 37123  8061
      6 41552     6 37457  3340 31726 44128   338 37123    68    16     5
   1528  6929    13     5  1763     2]]"
c806e891324af5d10a72c3b4b9b91177ae6446fb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"TextCNN, TextRNN, SASE, DPCNN, and BERT, $LS \cup KLD \cup CONN$ and $KLD \cup LS \cup LS_{inter}$ are the best systems with the highest recall and F1-score respectively","[[    0 39645 16256     6 14159   500 20057     6   208 24199     6   211
   4794 20057     6     8   163 18854     6    68 10463 44128 21033   229
  18048 44128 21033  8748   487  1629     8    68   530 18048 44128 21033
  29881 44128 21033 29881 49747  8007 24303  1629    32     5   275  1743
     19     5  1609  6001     8   274   134    12 31673  4067     2]]"
6548db45fc28e8a8b51f114635bad14a13eaec5b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"weGAN, deGAN",[[    0  1694 38416     6   263 38416     2]]
16af38f7c4774637cf8e04d4b239d6d72f0b0a3a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],over 104k documents,[[    0  2137 13259   330  2339     2]]
0062ad4aed09a57d0ece6aa4b873f4a4bf65d165,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we define the similarity between INLINEFORM7 and INLINEFORM8 by, DISPLAYFORM0","[[    0  1694  9914     5 37015   227  2808 28302 38036   406     8  2808
  28302 38036   398    30     6 15421 43784 38036   288     2]]"
a7adb63db5066d39fdf2882d8a7ffefbb6b622f0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],There is no baseline.,[[    0   970    16   117 18043     4     2]]
e67d2266476abd157fc8c396b3dfb70cb343471e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
30dad5d9b4a03e56fa31f932c879aa56e11ed15b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Appreciation, Satisfied, Peripheral complaint, Demanded inquiry, Corruption, Lagged response, Unresponsive, Medicine payment, Adverse behavior, Grievance ascribed and Obnoxious/irrelevant","[[    0 19186 29605     6 41802  2550     6  2595 38914   337  3674     6
   5245 13833  6422     6 28756     6   226 11290  1263     6  1890 23401
      6  8029  3207     6  1614 15189  3650     6 27964  3623  2389 24690
  33273     8  5816 48165    73   853 41017     2]]"
5efed109940bf74ed0a9d4a5e97a535502b23d27,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
1bdf7e9f3f804930b2933ebd9207a3e000b27742,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
3c0eaa2e24c1442d988814318de5f25729696ef5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
6c8bd7fa1cfb1b2bbeb011cc9c712dceac0c8f06,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Our baseline model is composed of the following typical components: word embedding, input encoder, alignment, aggregation, and prediction.","[[    0  2522 18043  1421    16 14092     9     5   511  6097  6411    35
   2136 33183 11303     6  8135  9689 15362     6 22432     6 40796     6
      8 16782     4     2]]"
90159e143487505ddc026f879ecd864b7f4f479e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Little overlap except common basic Latin alphabet and that Hindi and Marathi languages use same script.,"[[    0 23675 27573  4682  1537  3280  5862 34555     8    14 19840     8
   1127 20206 11991   304   276  8543     4     2]]"
cfffc94518d64cb3c8789395707e4336676e0345,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," Support Vector Regression (SVR) and Support Vector Classification (SVC), deep learning regression models of BIBREF2 to convert them to classification models","[[    0  7737 40419  6304 21791    36   104 13055    43     8  7737 40419
  40509    36   104 14858   238  1844  2239 39974  3092     9   163  8863
  45935   176     7 10304   106     7 20257  3092     2]]"
5c4c8e91d28935e1655a582568cc9d94149da2b2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],About the same performance,[[    0 21674     5   276   819     2]]
dad8cc543a87534751f9f9e308787e1af06f0627,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"AIDA-B, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI","[[    0   250 28887    12   387     6 36211 34972     6 20054     6    83
  15513   250 17831     6   305   487  1691    12   347  9112   387     6
    305   487  1691    12   771 20458   100     2]]"
479fc9e6d6d80e69f425d9e82e618e6b7cd12764,0.0,Entailment,[[    2     0 30495  3760  1757     2]],intra-sequential and intra-word,[[    0  2544   763    12 33430 12986     8 18592    12 14742     2]]
476d0b5579deb9199423bb843e584e606d606bc7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BIBREF13, majority baseline",[[    0  5383   387 45935  1558     6  1647 18043     2]]
3af9156b95a4c2d67cc54b80b92cc7b918fea2a9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"identify the boundaries of timexes and assign them to one of the following classes: date, time, duration, set,  Then we evaluated these results using more detailed measures for timexes","[[    0  8009  4591     5 10156     9    86 43849     8 28467   106     7
     65     9     5   511  4050    35  1248     6    86     6 13428     6
    278     6  1437  1892    52 15423   209   775   634    55  4271  1797
     13    86 43849     2]]"
3ac30bd7476d759ea5d9a5abf696d4dfc480175b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],LSTM LMs,[[    0   574  4014   448   226 13123     2]]
0460019eb2186aef835f7852fc445b037bd43bb7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],two,[[   0 7109    2]]
b7291845ccf08313e09195befd3c8030f28f6a9e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BERT-Base BIBREF2 to provide the state-of-the-art contextualized modeling,"[[    0 11126   565    12 34164   163  8863 45935   176     7   694     5
    194    12  1116    12   627    12  2013 37617  1538 19039     2]]"
d39c911bf2479fdb7af339b59acb32073242fab3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Car, Phone, Notebook, Camera",[[    0  9518     6 12091     6  6068  6298     6 11149     2]]
6bcff3ef61aad6bf1280ea26ed79585e1b838e64,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
dc4096b8bab0afcbbd4fbb015da2bea5d38251cd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we compressed English BERT using magnitude weight pruning BIBREF8 and observed the results on transfer learning to the General Language Understanding Evaluation (GLUE) benchmark BIBREF9, a diverse set of natural language understanding tasks including sentiment analysis, NLI, and textual similarity evaluation. ","[[    0  1694 34307  2370   163 18854   634 11259  2408  3349 37215   163
   8863 45935   398     8  6373     5   775    15  2937  2239     7     5
   1292 22205 22513 37766    36 10020  9162    43  5437   163  8863 45935
    466     6    10  5544   278     9  1632  2777  2969  8558   217  5702
   1966     6 12817   100     6     8 46478 37015 10437     4  1437     2]]"
975a4ac9773a4af551142c324b64a0858670d06e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"17,833 sentences, 826,987 characters and 2,714 question-answer pairs","[[    0  1360     6 39134 11305     6   290  2481     6 40366  3768     8
    132     6 37901   864    12 27740 15029     2]]"
6b4de7fef3a543215f16042ce6a29186bf84fea4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BERT, ERNIE, and BERT-wwm","[[    0 11126   565     6 13895   487  7720     6     8   163 18854    12
  33130   119     2]]"
9c4a4dfa7b0b977173e76e2d2f08fa984af86f0e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Full Testing Set accuracy: 84.02
Cleaned Testing Set accuracy: 93.48","[[    0 31440 25980  8504  8611    35  7994     4  4197 50118 40827   196
  25980  8504  8611    35  8060     4  3818     2]]"
e3981a11d3d6a8ab31e1b0aa2de96f253653cfb2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English,[[    0 35007     2]]
4e4d377b140c149338446ba69737ea191c4328d9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],ACL Anthology Reference Corpus,[[    0  2562   574 38188  4383 34177 28556     2]]
54fa5196d0e6d5e84955548f4ef51bfd9b707a32,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English to French and English to German,[[    0 35007     7  1515     8  2370     7  1859     2]]
65e72ad72a9cbfc379f126b10b0ce80cfe44579b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"NAT w/ Fertility, NAT-IR, NAT-REG, LV NAR, CTC Loss, CMLM","[[    0 28693   885    73   274 41748     6 36233    12  5216     6 36233
     12 32239     6 38669   234  2747     6   230  6078 19700     6   230
  10537   448     2]]"
cf93a209c8001ffb4ef505d306b6ced5936c6b63,0.0,Entailment,[[    2     0 30495  3760  1757     2]],late 2014,[[    0 19593   777     2]]
cdf65116a7c50edddcb115e9afd86b2b6accb8ad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"verb/preposition-based relation, nominal attribute, descriptive phrase and hyponymy relation.","[[    0 37644    73  5234 14413    12   805  9355     6 26255 21643     6
  42690 11054     8 15671  6119  4783  9355     4     2]]"
5a2c0c55a43dcc0b9439d330d2cbe1d5d444bf36,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
6c0f97807cd83a94a4d26040286c6f89c4a0f8e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],finite sequence of terms,[[    0   506 42524 13931     9  1110     2]]
f42d470384ca63a8e106c7caf1cb59c7b92dbc27,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"exact match, f1 score, edit distance and goal match","[[    0  3463  7257   914     6   856   134  1471     6 17668  4472     8
    724   914     2]]"
2c88b46c7e3a632cfa10b7574276d84ecec7a0af,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the model proposed in BIBREF3,[[    0   627  1421  1850    11   163  8863 45935   246     2]]
47726be8641e1b864f17f85db9644ce676861576,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"We compare this method of bias mitigation with the no bias mitigation (""Orig""), geometric bias mitigation (""Geo""), the two pieces of our method alone (""Prob"" and ""KNN"") and the composite method (""KNN+Prob""). We note that the composite method performs reasonably well according the the RIPA metric, and much better than traditional geometric bias mitigation according to the neighborhood metric, without significant performance loss according to the accepted benchmarks. To our knowledge this is the first bias mitigation method to perform reasonably both on both metrics.","[[    0   170  8933    42  5448     9  9415 23336    19     5   117  9415
  23336  6697 47837 16844 38985  9415 23336  6697 20981   139 16844     5
     80  3745     9    84  5448  1937  6697 10653   428   113     8    22
    530 20057  8070     8     5 14464  5448  6697   530 20057  2744 10653
    428 18653   166  1591    14     5 14464  5448 14023 15646   157   309
      5     5 31059   250 14823     6     8   203   357    87  2065 38985
   9415 23336   309     7     5  3757 14823     6   396  1233   819   872
    309     7     5  3903 22485     4   598    84  2655    42    16     5
     78  9415 23336  5448     7  3008 15646   258    15   258 12758     4
      2]]"
8255f74cae1352e5acb2144fb857758dda69be02,0.0,Entailment,[[    2     0 30495  3760  1757     2]],by calculating log ratio of grammatical phrase over ungrammatical phrase,"[[    0  1409 29770  7425  1750     9 25187 45816 11054    81   542 28526
  45816 11054     2]]"
7c792cda220916df40edb3107e405c86455822ed,0.0,Entailment,[[    2     0 30495  3760  1757     2]],METEOR,[[    0 43543   717  3411     2]]
c47f593a5b92abc2e3c536fe2baaca226913688b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],See Figure FIGREF3,[[    0 19224 17965 37365 45935   246     2]]
fd08dc218effecbe5137a7e3b73d9e5e37ace9c1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
1f6666c2c1d1d5f66208a6fa7da3b3442a577dbc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"unigram, bigram and trigram",[[    0   879  1023  4040     6   380  4040     8 46681  4040     2]]
ad1f230f10235413d1fe501e414358245b415476,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT","[[    0 37426   574  4014   448    12 29459     6 22783  7629     6 18366
   3755     6   229  3755     6 18366  3755  2055 17678 17357     6     8
    163 18854     2]]"
b8fdc600f9e930133bb3ec8fbcc9c600d60d24b0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
a516b37ad9d977cb9d4da3897f942c1c494405fe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"DocQA, SAN, QANet, ASReader, LM, Random Guess","[[    0 42291  1864   250     6 14055     6  1209  1889   594     6  6015
  46347     6 31160     6 34638 31239     2]]"
827b5bd215599623a3125afe331b56b89b42bf09,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The De7 database,[[   0  133  926  406 8503    2]]
11dde2be9a69a025f2fc29ce647201fb5a4df580,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Proposed method achieves 94.5 UAS and 92.4 LAS  compared to 94.3 and 92.2 of best state-of-the -art greedy based parser. Best state-of-the art parser overall achieves 95.8 UAS and 94.6 LAS.,"[[    0 41895  7878  5448 35499  8940     4   245   121  2336     8  8403
      4   306   226  2336  1437  1118     7  8940     4   246     8  8403
      4   176     9   275   194    12  1116    12   627   111  2013 34405
    716 48946     4  2700   194    12  1116    12   627  1808 48946  1374
  35499  6164     4   398   121  2336     8  8940     4   401   226  2336
      4     2]]"
0a050658d09f3c6e21e9ab828dc18e59b147cf7c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
280f863cfd63b711980ca6c7f1409c0306473de7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
8829f738bcdf05b615072724223dbd82463e5de6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
326e08a0f5753b90622902bd4a9c94849a24b773,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"17,833 sentences, 826,987 characters and 2,714 question-answer pairs","[[    0  1360     6 39134 11305     6   290  2481     6 40366  3768     8
    132     6 37901   864    12 27740 15029     2]]"
1763a029daca7cab10f18634aba02a6bd1b6faa7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"AGDT improves the performance by 2.4% and 1.6% in the DS part of the two dataset, Our AGDT surpasses GCAE by a very large margin (+11.4% and +4.9% respectively) on both datasets, In the HDS part, the AGDT model obtains +3.6% higher accuracy than GCAE on the restaurant domain and +4.2% higher accuracy on the laptop domain","[[    0  3450 23858 15296     5   819    30   132     4   306   207     8
    112     4   401   207    11     5    44    48  5433    17    46   233
      9     5    80 41616     6  1541  5680 23858 14874   293   272  4054
    717    30    10   182   739  2759 16998  1225     4   306   207     8
   2055   306     4   466   207  4067    43    15   258 42532     6    96
      5    44    48   725  5433    17    46   233     6     5  5680 23858
   1421  6168 30759  2055   246     4   401   207   723  8611    87   272
   4054   717    15     5  2391 11170     8  2055   306     4   176   207
    723  8611    15     5  9972 11170     2]]"
aecb485ea7d501094e50ad022ade4f0c93088d80,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
e4cc2e73c90e568791737c97d77acef83588185f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],8000,[[    0 42412     2]]
2d274c93901c193cf7ad227ab28b1436c5f410af,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D","[[    0 37426  3134   597     6  8248 34853  1209   250     6   208    12
  15721  2744   347  1723   176   104     6   163 18854  2744 46064    12
   8332 15721     6 10908   368  2744  3376   534     6   468 13548     6
  16718   591   500  5733     6 24294  8332   448  2744 13449  2371     6
   2585  1301 15721     6   248 12642  2744   250   176   495     2]]"
acd05f31e25856b9986daa1651843b8dc92c2d99,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," 9,892 stories of sexual harassment incidents",[[   0  361    6 5046  176 1652    9 1363 4331 4495    2]]
5b2480c6533696271ae6d91f2abe1e3a25c4ae73,0.0,Entailment,[[    2     0 30495  3760  1757     2]],It is not completely valid for natural languages because of diversity of language - this is called smoothing requirement.,"[[    0   243    16    45  2198  8218    13  1632 11991   142     9  5845
      9  2777   111    42    16   373 35444  8223  7404     4     2]]"
c13fe4064df0cfebd0538f29cb13e917fc5c3be0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The network architecture has a multi-task Bi-Directional Recurrent Neural Network, with an unsupervised sequence labeling task and a low-dimensional embedding layer between tasks. There is a hidden layer after each successive task with skip connections to the senior supervised layers.","[[    0   133  1546  9437    34    10  3228    12 45025  6479    12   495
  43606   337  7382 41937 44304  3658     6    19    41   542 16101 25376
  13931 27963  3685     8    10   614    12 23944 33183 11303 10490   227
   8558     4   345    16    10  7397 10490    71   349 12565  3685    19
  14514  7070     7     5   949 20589 13171     4     2]]"
21663d2744a28e0d3087fbff913c036686abbb9a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Their model does not differ from BERT.,[[    0 16837  1421   473    45 10356    31   163 18854     4     2]]
f62c78be58983ef1d77049738785ec7ab9f2a3ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Kaggle
Subversive Kaggle
Wikipedia
Subversive Wikipedia
Reddit
Subversive Reddit ","[[    0   530  7165   459 50118 23055 47804   229  7165   459 50118 47681
  50118 23055 47804 28274 50118 47758 50118 23055 47804  6844  1437     2]]"
565d668947ffa6d52dad019af79289420505889b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
27c1c678d3862c7676320ca493537b03a9f0c77a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],KVRET,[[    0   530   846 36995     2]]
41174d8b176cb8549c2d83429d94ba8218335c84,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
ed7985e733066cd067b399c36a3f5b09e532c844,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They use a left-to-right attention mask so that the input tokens can only attend to other input tokens, and the target tokens can only attend to the input tokens and already generated target tokens.","[[    0  1213   304    10   314    12   560    12  4070  1503 11445    98
     14     5  8135 22121    64   129  2725     7    97  8135 22121     6
      8     5  1002 22121    64   129  2725     7     5  8135 22121     8
    416  5129  1002 22121     4     2]]"
827464c79f33e69959de619958ade2df6f65fdee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
0b9021cefca71081e617a362e7e3995c5f1d2a88,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CNN-C, CNN-W, CNN-Lex-C, CNN-Lex-W, Bi-LSTM-C , Bi-LSTM-W, Lex-rule, BOW","[[    0 16256    12   347     6  3480    12   771     6  3480    12 43551
     12   347     6  3480    12 43551    12   771     6  6479    12   574
   4014   448    12   347  2156  6479    12   574  4014   448    12   771
      6 14786    12 28523     6   163  4581     2]]"
a1d422cb2e428333961370496ca281a1be99fdff,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"coherence, logical consistency, fluency and diversity",[[    0   876 40584     6 16437 12787     6  6626  6761     8  5845     2]]
9bffc9a9c527e938b2a95ba60c483a916dbd1f6b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
5816ebf15e31bdf70e1de8234132e146d64e31eb,0.0,Entailment,[[    2     0 30495  3760  1757     2]], multinomial logistic regression,[[    0  7268   179 48866  7425  5580 39974     2]]
2cc42d14c8c927939a6b8d06f4fdee0913042416,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
7011b26ffc54769897e4859e4932aeddfab82c9f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],YouTube ASR system ,[[    0 36169  6015   500   467  1437     2]]
964705a100e53a9181d1a5ac8150696de12ecaf0,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," training dataset contains 2,815 examples, 761 testing examples","[[    0  1058 41616  6308   132     6 38828  7721     6   262  5606  3044
   7721     2]]"
43761478c26ad65bec4f0fd511ec3181a100681c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a7f07ae48eed084c3144214228f4ecb72bc0a0e3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"IMG-only, QUES-only, SAN, SANDY,  VOES-Oracle, VOES","[[    0  3755   534    12  8338     6 16944  1723    12  8338     6 14055
      6   208  5945   975     6  1437 36584  1723    12 47003     6 36584
   1723     2]]"
b3ff166bd480048e099d09ba4a96e2e32b42422b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
1a1293e24f4924064e6fb9998658f5a329879109,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SEQ2SEQ, CVAE, Transformer, HRED, DialogWAE","[[    0  3388  1864   176  3388  1864     6   230  9788   717     6  5428
  22098     6 11683  1691     6 28985  2154   771 16329     2]]"
69a88b6be3b34acc95c5e36acbe069c0a0bc67d6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"8.1 million scientific documents, 154K computer science articles, 622K citing sentences","[[    0   398     4   134   153  6441  2339     6 25502   530  3034  2866
   7201     6   231  2036   530  4319 11305     2]]"
fb3687ea05d38b5e65fdbbbd1572eacd82f56c0b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
58a340c338e41002c8555202ef9adbf51ddbb7a1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SST-2 dataset,[[    0   104  4014    12   176 41616     2]]
33f72c8da22dd7d1378d004cbd8d2dcd814a5291,0.0,Entailment,[[    2     0 30495  3760  1757     2]],error rate in a minimal pair ABX discrimination task,[[    0 44223   731    11    10  9865  1763  6266  1000  6886  3685     2]]
3a6559dc6eba7f5abddf3ac27376ba0b9643a908,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
068dbcc117c93fa84c002d3424bafb071575f431,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Inter-annotator agreement, comparison against expert annotation, agreement with PropBank Data annotations.","[[    0 26267    12 37250  2630  1288     6  6676   136  3827 47760     6
   1288    19 13695 13532  5423 47234     4     2]]"
fe6bb55b28f14ed8ac82c122681905397e31279d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],avoids the need for expensive cross-validation for hyperparameter selection,"[[    0  1469 22926     5   240    13  3214  2116    12 42679  1258    13
   8944 46669  5906  4230     2]]"
dd155f01f6f4a14f9d25afc97504aefdc6d29c13,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Quality measures using perplexity and recall, and performance measured using latency and energy usage. ","[[    0 45065  1797   634 33708  1571     8  6001     6     8   819  9550
    634 35940     8  1007  9453     4  1437     2]]"
6e2899c444baaeb0469599f65722780894f90f29,0.0,Entailment,[[    2     0 30495  3760  1757     2]],we use mean average precision (MAP) as the main evaluation metric,"[[    0  1694   304  1266   674 15339    36 43861    43    25     5  1049
  10437 14823     2]]"
26b5c090f72f6d51e5d9af2e470d06b2d7fc4a98,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256","[[    0   204    12 39165  9689 15362     6   204    12 39165  5044 15362
      6 30848    12 21618  1421     6    19 33183 11303     8  7397  1836
    278    25 22078     2]]"
3a1bd3ec1a7ce9514da0cb2dfcaa454ba8c0ed14,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," five subtasks which involve standard classification, ordinal classification and distributional estimation. For a more detailed description see BIBREF0","[[    0   292 30757 40981    61  6877  2526 20257     6 22474  6204 20257
      8  3854   337 32809     4   286    10    55  4271  8194   192   163
   8863 45935   288     2]]"
f3e96c5487d87557a661a65395b0162033dc05b3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Zulu,[[    0  1301 12709     2]]
5067e5eb2cddbb34b71e8b74ab9210cd46bb09c5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Matching features from matching sentences from various perspectives.,[[    0   448 20262  1575    31  8150 11305    31  1337 17403     4     2]]
b3ec918827cd22b16212265fcdd5b3eadee654ae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
471d624498ab48549ce492ada9e6129da05debac,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Concat
Turn
Gate
Action Copy
Tree Copy
SQL Attn
Concat + Action Copy
Concat + Tree Copy
Concat + SQL Attn
Turn + Action Copy
Turn + Tree Copy
Turn + SQL Attn
Turn + SQL Attn + Action Copy","[[    0  9157  8729 50118 30093 50118 37155 50118 36082 22279 50118 33731
  22279 50118 46608  7279   282 50118  9157  8729  2055  5828 22279 50118
   9157  8729  2055 11077 22279 50118  9157  8729  2055 41614  7279   282
  50118 30093  2055  5828 22279 50118 30093  2055 11077 22279 50118 30093
   2055 41614  7279   282 50118 30093  2055 41614  7279   282  2055  5828
  22279     2]]"
9714cb7203c18a0c53805f6c889f2e20b4cab5dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],video sequence is first fed into the VGG model BIBREF9 to extract visual feature,"[[    0 14406 13931    16    78  9789    88     5   468 24592  1421   163
   8863 45935   466     7 14660  7133  1905     2]]"
4c026715ee365c709381c5da770bdc8297eed19f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],candidates who have been liked or shortlisted are considered part of the hirable class,"[[    0 32503 24143    54    33    57  6640    50   765 11301    32  1687
    233     9     5  1368 45770  1380     2]]"
c80669cb444a6ec6249b971213b0226f59940a82,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
4542a4e7eabb8006fb7bcff2ca6347cfb3fbc56b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
4cab33c8dd46002e0ccafda3916b37366a24a394,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
c78f18606524539e4c573481e5bf1e0a242cc33c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],1001,[[    0 45792     2]]
2a6469f8f6bf16577b590732d30266fd2486a72e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They use self-play learning , optimize the model for specific metrics, train separate models per user, use model  and response classification predictors, and filter the dataset to obtain higher quality training data.","[[    0  1213   304  1403    12  5785  2239  2156 22016     5  1421    13
   2167 12758     6  2341  2559  3092   228  3018     6   304  1421  1437
      8  1263 20257  7006   994     6     8 14929     5 41616     7  6925
    723  1318  1058   414     4     2]]"
3c3cb51093b5fd163e87a773a857496a4ae71f03,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history","[[    0     5  2314  1421  1239     5  2136 33183 11303 13931    25  8135
      6  2314    81   349  1736  1984  2136    31    80 17403    35    36
    134    43     5  3302    14     5  1984  2136  1495    64    28 11394
     25    10  1030  2136   131    36   176    43     5 45374     9     5
   3104    14     5  1984  2136  2024  3905   986  2835  1258   750     2]]"
da2b43d7d048f3f59adf26a67ce66bd2d8a06326,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Training and testing are done in alternating steps: In each epoch, for training, we first present to an LSTM network 1000 samples in a given language, which are generated according to a certain discrete probability distribution supported on a closed finite interval. We then freeze all the weights in our model, exhaustively enumerate all the sequences in the language by their lengths, and determine the first $k$ shortest sequences whose outputs the model produces inaccurately. , experimented with 1, 2, 3, and 36 hidden units for $a^n b^n$ ; 2, 3, 4, and 36 hidden units for $a^n b^n c^n$ ; and 3, 4, 5, and 36 hidden units for $a^n b^n c^n d^n$ . , Following the traditional approach adopted by BIBREF7 , BIBREF12 , BIBREF9 and many other studies, we train our neural network as follows. At each time step, we present one input character to our model and then ask it to predict the set of next possible characters, based on the current character and the prior hidden states. Given a vocabulary $\mathcal {V}^{(i)}$ of size $d$ , we use a one-hot representation to encode the input values; therefore, all the input vectors are $d$ -dimensional binary vectors. The output values are $(d+1)$ -dimensional though, since they may further contain the termination symbol $\dashv $ , in addition to the symbols in $\mathcal {V}^{(i)}$ . The output values are not always one-hot encoded, because there can be multiple possibilities for the next character in the sequence, therefore we instead use a $k$ -hot representation to encode the output values. Our objective is to minimize the mean-squared error (MSE) of the sequence predictions.","[[    0 44466     8  3044    32   626    11 36830  2402    35    96   349
  43660     6    13  1058     6    52    78  1455     7    41   226  4014
    448  1546 10775  7931    11    10   576  2777     6    61    32  5129
    309     7    10  1402 34014 18102  3854  2800    15    10  1367 40001
  22455     4   166   172 11346    70     5 23341    11    84  1421     6
  19379  6608 41949   877    70     5 26929    11     5  2777    30    49
  18915     6     8  3094     5    78    68   330  1629 28617 26929  1060
  39512     5  1421  9108 38225  7223     4  2156 37672    19   112     6
    132     6   155     6     8  2491  7397  2833    13    68   102 35227
    282   741 35227   282  1629 25606   132     6   155     6   204     6
      8  2491  7397  2833    13    68   102 35227   282   741 35227   282
    740 35227   282  1629 25606     8   155     6   204     6   195     6
      8  2491  7397  2833    13    68   102 35227   282   741 35227   282
    740 35227   282   385 35227   282  1629   479  2156  3515     5  2065
   1548  5091    30   163  8863 45935   406  2156   163  8863 45935  1092
   2156   163  8863 45935   466     8   171    97  3218     6    52  2341
     84 26739  1546    25  3905     4   497   349    86  1149     6    52
   1455    65  8135  2048     7    84  1421     8   172  1394    24     7
   7006     5   278     9   220   678  3768     6   716    15     5   595
   2048     8     5  2052  7397   982     4  6211    10 32644 49959 40051
  11762 25522   846 24303 49688  1640   118 48950  1629     9  1836    68
    417  1629  2156    52   304    10    65    12 10120  8985     7 46855
      5  8135  3266   131  3891     6    70     5  8135 44493    32    68
    417  1629   111 23944 32771 44493     4    20  4195  3266    32 44612
    417  2744   134    43  1629   111 23944   600     6   187    51   189
    617  5585     5 17829  7648 49959 42360   705    68  2156    11  1285
      7     5 19830    11 49959 40051 11762 25522   846 24303 49688  1640
    118 48950  1629   479    20  4195  3266    32    45   460    65    12
  10120 45320     6   142    89    64    28  1533 11550    13     5   220
   2048    11     5 13931     6  3891    52  1386   304    10    68   330
   1629   111 10120  8985     7 46855     5  4195  3266     4  1541  4554
     16     7 15925     5  1266    12 30919  6537  5849    36   448  3388
     43     9     5 13931 12535     4     2]]"
bde6fa2057fa21b38a91eeb2bb6a3ae7fb3a2c62,0.0,Entailment,[[    2     0 30495  3760  1757     2]],51.5,[[   0 4708    4  245    2]]
ebeedbb8eecdf118d543fdb5224ae610eef212c8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Procrustes, GPA, GeoMM, GeoMM$_{semi}$, Adv-C-Procrustes, Unsup-SL, Sinkhorn-BT","[[    0 10653  8344  4193   293     6 34408     6 29147 16261     6 29147
  16261  1629 49747  1090  5408 24303 47110 17638    12   347    12 10653
   8344  4193   293     6  1890 35901    12 11160     6   208  4291 20968
     12 13269     2]]"
d325a3c21660dbc481b4e839ff1a2d37dcc7ca46,40.0,Entailment,[[    2     0 30495  3760  1757     2]],"Detection, Direction, Graded Entailment",[[    0 43170 20970     6 29332     6  2974  7560  9860  3760  1757     2]]
eddb18109495976123e10f9c6946a256a55074bd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MVCNN, a novel CNN architecture for sentence classification. It combines multichannel initialization  diverse versions of pretrained word embeddings are used  and variable-size filters  features of multigranular phrases are extracted with variable-size convolution filters. ","[[    0   448 14858 20057     6    10  5808  3480  9437    13  3645 20257
      4    85 15678  7268  1725 28189 49164   126  5544  7952     9 11857
  26492  2136 33183   417  1033    32   341   126     8 15594    12 10799
  19214   126  1575     9  7268  1023  3917  8244 22810    32 27380    19
  15594    12 10799 15380 23794 19214     4  1437     2]]"
3a62dd5fece70f8bf876dcbb131223682e3c54b7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ttention vector sequence is segmented into several subsequences and each subsequence represents the attention of one word, we devise an appropriate aggregation module to fuse the inner-word character attention","[[    0  5967 19774 37681 13931    16  2835   196    88   484 49734  8457
      8   349 49734  4086  3372     5  1503     9    65  2136     6    52
  34901    41  3901 40796 20686     7 38689     5  8725    12 14742  2048
   1503     2]]"
31b6544346e9a31d656e197ad01756813ee89422,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
dcd6f18922ac5c00c22cef33c53ff5ae08b42298,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"First preference is given to the labels that are perfectly matching in all the neural annotators., In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models., When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one. , Finally, when none the above conditions are fulfilled, we leave out the label with an unknown category.","[[    0 10993 12832    16   576     7     5 14105    14    32  6683  8150
     11    70     5 26739 45068  3629   482    96   403    80    66     9
    130  5377  3092    32  4577     6   172    24    16   145  7869   114
     14  6929    16    67  2622    30    23   513    65     9     5   786
     12 46796  3092   482   520    52   192    14  4146     9     5  5377
   3092    16  5591     5   276   775     6   172    52  7938     5 14105
     19    49  7091  2123  3266  2622    25    10 18102  3854   634     5
     68 24810 29459  1629  5043     4    20 14105    32 24713    11 35602
    645   309     7  2123  3266     4  1892    52  1649   114     5    78
    130    36 11173    77    65  5377  1421     8   258   786    12 46796
   3092  2592     5   276  6929    43    50    23   513    80 14105    32
   8150     6   172    52  1157     7  1339    14    65     4  2156  3347
      6    77  4146     5  1065  1274    32 20218     6    52   989    66
      5  6929    19    41  4727  4120     4     2]]"
084fb7c80a24b341093d4bf968120e3aff56f693,0.0,Entailment,[[    2     0 30495  3760  1757     2]], represent the state using natural language,[[   0 3594    5  194  634 1632 2777    2]]
aa7decee4e3006c2c99b1f331a5b32d44a565ef6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
fc65f19a30150a0e981fb69c1f5720f0136325b0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
13149342ccbb7a6df9b4b1bed890cfbdc1331c1f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a total of 2,560 pseudo-tweets in three different languages: Japanese (ja), English (en) and Chinese (zh)","[[    0   102   746     9   132     6 30207 38283    12    90  1694  2580
     11   130   430 11991    35  2898    36  1910   238  2370    36   225
     43     8  1111    36 13808    43     2]]"
5e5460ea955d8bce89526647dd7c4f19b173ab34,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Total number of transcribed utterances including Train and Test for both Eng and Ger language is 5562 (2188 cleaned),"[[    0 37591   346     9 27472 33273 18672  5332   217 15507     8  4500
     13   258  8227     8  7965  2777    16   195 36950    36   176 28871
  17317    43     2]]"
af073d84b8a7c968e5822c79bef34a28655886de,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"1.34 and 1.12 BLEU score on top of the strong baselines in BIBREF3, perplexity scores are also better, On the Google Production dataset, our model achieved 1.01 higher test BLEU score","[[    0   134     4  3079     8   112     4  1092   163  3850   791  1471
     15   299     9     5   670 11909 38630    11   163  8863 45935   246
      6 33708  1571  4391    32    67   357     6   374     5  1204  9850
  41616     6    84  1421  4824   112     4  2663   723  1296   163  3850
    791  1471     2]]"
18e915b917c81056ceaaad5d6581781c0168dac9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],all annotators that a triple extraction was incorrect,[[    0  1250 45068  3629    14    10  6436 23226    21 17401     2]]
42bc4e0cd0f3e238a4891142f1b84ebcd6594bf1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"RNN-based Seq2Seq, Variational Seq2Seq, VRNMT , CWVAE-Unpretrained","[[    0   500 20057    12   805  1608  1343   176 14696  1343     6 41058
   5033  1608  1343   176 14696  1343     6  8311   487 11674  2156 17660
   9788   717    12  9685 42354 26492     2]]"
31b20a4bab09450267dfa42884227103743e3426,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"entity types or concepts BIBREF13, relations paths BIBREF17,  textual descriptions BIBREF11, BIBREF12, logical rules BIBREF23, deep neural network models BIBREF24","[[    0 46317  3505    50 14198   163  8863 45935  1558     6  3115 14561
    163  8863 45935  1360     6  1437 46478 24173   163  8863 45935  1225
      6   163  8863 45935  1092     6 16437  1492   163  8863 45935  1922
      6  1844 26739  1546  3092   163  8863 45935  1978     2]]"
b3ac67232c8c7d5a759ae025aee85e9c838584eb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
fc54736e67f748f804e8f66b3aaaea7f5e55b209,0.0,Entailment,[[    2     0 30495  3760  1757     2]],conduct experiments using artificially constructed unnormalized text by corrupting words in the normal dev set,"[[    0 34746 15491   634 30768 11236   542 21113  1538  2788    30 10334
    154  1617    11     5  2340  8709   278     2]]"
fb5ce11bfd74e9d7c322444b006a27f2ff32a0cf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],96-97.6% using the objects color or shape and 79% using shape alone,"[[   0 5607   12 6750    4  401  207  634    5 8720 3195   50 3989    8
  7589  207  634 3989 1937    2]]"
e292676c8c75dd3711efd0e008423c11077938b1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],LSTM and BERT ,[[    0   574  4014   448     8   163 18854  1437     2]]
5a06f11aa75a8affde3d595c40fb03e06769e368,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
9a4aa0e4096c73cd2c3b1eab437c1bf24ae7bf03,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"abstracts, sentences",[[    0   873 47796    29     6 11305     2]]
737397f66751624bcf4ef891a10b29cfc46b0520,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Google N-grams
COHA
Moral Foundations Dictionary (MFD)
","[[    0 20441   234    12 28526    29 50118  6335  6826 50118   448 15010
  11911  1635 41243    36   448 24667    43 50118     2]]"
96ee62407b1ca2a6538c218781e73e8fbf45094a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
b546f14feaa639e43aa64c799dc61b8ef480fb3d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
56e58bdf0df76ad1599021801f6d4c7b77953e29,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"influence of each word on the score of the correct relation, that impact should and does still correlate with human judgments","[[    0  9433 31759     9   349  2136    15     5  1471     9     5  4577
   9355     6    14   913   197     8   473   202 42407    19  1050 28728
      2]]"
ae95a7d286cb7a0d5bc1a8283ecbf803e9305951,0.0,Entailment,[[    2     0 30495  3760  1757     2]], recurrent neural network (RNN)-based sequence-to-sequence (Seq2Seq) models for NATS,"[[    0 35583 26739  1546    36   500 20057 19281   805 13931    12   560
     12 46665    36 14696  1343   176 14696  1343    43  3092    13 36233
    104     2]]"
f2c5da398e601e53f9f545947f61de5f40ede1ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The coefficients are projected back to the dummy variable space.,"[[    0   133 48550    32  5635   124     7     5 34759 15594   980     4
      2]]"
26012f57cba21ba44b9a9f7ed8b1ed9e8ee7625d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],PV-DM,[[    0   510   846    12 25652     2]]
771b373d09e6eb50a74fffbf72d059ad44e73ab0,0.0,Entailment,[[    2     0 30495  3760  1757     2]], we were looking for original and uncommon sentence change suggestions,[[    0    52    58   546    13  1461     8 18186  3645   464  9622     2]]
a85698f19a91ecd3cd3a90a93a453d2acebae1b7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
7081b6909cb87b58a7b85017a2278275be58bf60,0.0,Entailment,[[    2     0 30495  3760  1757     2]],210,[[    0 22146     2]]
191d4fe8a37611b2485e715bb55ff1a30038ad6a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
8d8300d88283c73424c8f301ad9fdd733845eb47,0.0,Entailment,[[    2     0 30495  3760  1757     2]],confusion matrices of labels between annotators,[[    0 17075 15727  7821 45940     9 14105   227 45068  3629     2]]
5d70c32137e82943526911ebdf78694899b3c28a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
ad67ca844c63bf8ac9fdd0fa5f58c5a438f16211,0.0,Entailment,[[    2     0 30495  3760  1757     2]],1000 hours of WSJ audio data,[[    0 20078   722     9 18483   863  6086   414     2]]
27f575e90487ef68298cfb6452683bb977e39e43,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
aeab5797b541850e692f11e79167928db80de1ea,0.0,Entailment,[[    2     0 30495  3760  1757     2]],all three representations are concatenated and passed into a MLP,"[[    0  1250   130 30464    32 10146 26511  1070     8  1595    88    10
  10725   510     2]]"
2815bac42db32d8f988b380fed997af31601f129,0.0,Entailment,[[    2     0 30495  3760  1757     2]],It had the highest accuracy comparing to all datasets 0.986% and It had the highest improvement comparing to previous methods on the same dataset by 8%,"[[    0   243    56     5  1609  8611 12818     7    70 42532   321     4
  40558   207     8    85    56     5  1609  3855 12818     7   986  6448
     15     5   276 41616    30   290   207     2]]"
cee29acec4da1b247795daa4e2e82ef8a7b25a64,0.0,Entailment,[[    2     0 30495  3760  1757     2]],DL-61,[[    0 26109    12  5606     2]]
b25e7137f49f77e7e67ee2f40ca585d3a377f8b5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"spellchecking mammography reports and tweets BIBREF7 , BIBREF4","[[    0 47370 33851 32751 10486   690     8  6245   163  8863 45935   406
   2156   163  8863 45935   306     2]]"
e755fb599690d0d0c12ddb851ac731a0a7965797,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Dutch, French, Russian, Spanish , Turkish, English ","[[    0 40877     6  1515     6  1083     6  3453  2156  4423     6  2370
   1437     2]]"
c2da598346b74541c78ecff5c9586b3857dd01b5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d9c26c1bfb3830c9f3dbcccf4c8ecbcd3cb54404,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English-Japanese,[[    0 35007    12 35655     2]]
e26e7e9bcd7e2cea561af596c59b98e823653a4b,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," four different companies in the telecommunication, electronics, and insurance industries","[[    0   237   430   451    11     5  8327 24741     6  8917     6     8
   1911  4510     2]]"
3d583a0675ad34eb7a46767ef5eba5f0ea898aa9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],LSTM,[[   0  574 4014  448    2]]
9846f84747b89f5c692665c4ea7111671ad9839a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
74261f410882551491657d76db1f0f2798ac680f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (3 Experimental Setup) We experiment with six target languages: French (FR), Brazilian Portuguese (PT), Italian (IT), Polish (PL), Croatian (HR), and Finnish (FI).","[[    0 33683    19  1383  1716    35    36   246 41701 45524    43   166
   9280    19   411  1002 11991    35  1515    36  5499   238  6606 13053
     36 10311   238  3108    36  2068   238 11145    36  7205   238 27248
     36 16271   238     8 21533    36 14071   322     2]]"
7c9c73508da628d58aaadb258f3a9d4cc2a8a9b3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
05bb75a1e1202850efa9191d6901de0a34744af0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],documents from the CommonCrawl dataset that has the most overlapping n-grams with the question,"[[    0 37447 30179    31     5  9732   347 33889 41616    14    34     5
    144 35642   295    12 28526    29    19     5   864     2]]"
53d6cbee3606dd106494e2e98aa93fdd95920375,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"test accuracy of 88.9%, which exceeds the previous best by 16.9%","[[    0 21959  8611     9  7953     4   466  4234    61 23984     5   986
    275    30   545     4   466   207     2]]"
4d5f112874250d48eb49649c4abe31d6c9236700,0.0,Entailment,[[    2     0 30495  3760  1757     2]],GPT-2,[[    0   534 10311    12   176     2]]
c029deb7f99756d2669abad0a349d917428e9c12,0.0,Entailment,[[    2     0 30495  3760  1757     2]],3%,[[  0 246 207   2]]
226ae469a65611f041de3ae545be0e386dba7d19,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Wikipedea Corpus and BooksCorpus,[[    0 45445 34740 13184 28556     8 16206 25313   687     2]]
199bdb3a6b1f7c89d95ea6c6ddbbb5eff484fa1f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
36383971a852d1542e720d3ea1f5adeae0dbff18,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"holistic, TraAtt, RegAtt, ConAtt, ConAtt, iBOWIMG , VQA, VQA, WTL , NMN , SAN , AMA , FDA , D-NMN, DMN+","[[    0  9649  5580     6  8221 28062     6  6304 28062     6  2585 28062
      6  2585 28062     6   939   387  4581  3755   534  2156   468  1864
    250     6   468  1864   250     6   305 22290  2156 31736   487  2156
  14055  2156 33614  2156  7985  2156   211    12 31156   487     6 18695
    487  2744     2]]"
2a57fdc7e985311989b6829c1ceb201096e5c809,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Parts of Speech (POS) tags, Prior polarity of the words, Capitalization, Negation, Text Feature","[[    0 45305     9 27242    36 42740    43 19445     6  7987  8385 21528
      9     5  1617     6  1867  1938     6 13912  1258     6 14159 31967
      2]]"
92cfac12d9583747bd9be8604275b4a9ddd8afe6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we suggest to estimate the interpretability of $k$ th component as $ \operatorname{interp}_k W = \sum _{i,j=1}^N W_{i,k} W_{j,k} \left(W_i \cdot W_j \right). $","[[    0  1694  3608     7  3278     5 18107  4484     9    68   330  1629
   3553  7681    25    68 44128  8428   415  4244  4344 45152  8007   642
  24303  1215   330   305  5457 44128 18581 18134 45152   118     6   267
   5214   134 24303 35227   487   305 49747   118     6   330 24303   305
  49747   267     6   330 24303 44128  6960  1640   771  1215   118 44128
  28690  1242   305  1215   267 44128  4070   322    68     2]]"
496b4ae3c0e26ec95ff6ded5e6790f24c35f0f5b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],by converting human advice to first-order logic format and use as an input to calculate gradient,"[[    0  1409 18841  1050  2949     7    78    12 10337 14578  7390     8
    304    25    41  8135     7 15756 43141     2]]"
a7829abed2186f757a59d3da44893c0172c7012b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"number of coattention blocks, the batch size, and the number of epochs trained and ensembled our three best networks","[[    0 30695     9  1029  2611 19774  5491     6     5 14398  1836     6
      8     5   346     9 43660    29  5389     8  1177 26660  1329    84
    130   275  4836     2]]"
18288c7b0f8bd7839ae92f9c293e7fb85c7e146a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],weak correlation with p-value of 0.08,[[    0 25785 22792    19   181    12 19434     9   321     4  3669     2]]
65e6a1cc2590b139729e7e44dce6d9af5dd2c3b5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"do not follow a particular plan or pursue a particular fixed information need,  integrating content found via search with content from structured data, at each system turn, there are a large number of conversational moves that are possible, most other domains do not have such high quality structured data available, live search may not be able to achieve the required speed and efficiency","[[    0  5016    45  1407    10  1989   563    50  5445    10  1989  4460
    335   240     6  1437 22688  1383   303  1241  1707    19  1383    31
  16697   414     6    23   349   467  1004     6    89    32    10   739
    346     9 28726  5033  3136    14    32   678     6   144    97 30700
    109    45    33   215   239  1318 16697   414   577     6   697  1707
    189    45    28   441     7  3042     5  1552  2078     8  5838     2]]"
e659ceb184777015c12db2da5ae396635192f0b0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
9544cc0244db480217ce9174aa13f1bf09ba0d94,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English, German",[[    0 35007     6  1859     2]]
e8e00b4c0673af5ab02ec82563105e4157cc54bb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],transformer model achieves higher BLEU score than both Attention encoder-decoder and sequence-sequence model,"[[    0  9981 22098  1421 35499   723   163  3850   791  1471    87   258
  35798  9689 15362    12 11127 15362     8 13931    12 46665  1421     2]]"
0d1408744651c3847469c4a005e4a9dccbd89cf1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
8847f2c676193189a0f9c0fe3b86b05b5657b76a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],1593 annotations,[[    0   996  6478 47234     2]]
04012650a45d56c0013cf45fd9792f43916eaf83,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"comparing to the results from reducing the number of layers in the decoder, the BLEU score was 69.93 which is less than 1% in case of test2016 and in case of test2017 it was less by 0.2 %. In terms of TER it had higher score by 0.7 in case of test2016 and 0.1 in case of test2017. ","[[    0 11828  5867     7     5   775    31  4881     5   346     9 13171
     11     5  5044 15362     6     5   163  3850   791  1471    21  5913
      4  6478    61    16   540    87   112   207    11   403     9  1296
   9029     8    11   403     9  1296  3789    24    21   540    30   321
      4   176  7606     4    96  1110     9 28107    24    56   723  1471
     30   321     4   406    11   403     9  1296  9029     8   321     4
    134    11   403     9  1296  3789     4  1437     2]]"
1af4d56eeaf74460ca2c621a2ad8a5d8dbac491c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
f37026f518ab56c859f6b80b646d7f19a7b684fa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"our model requires 100k parameters , while BIBREF8 requires 250k parameters","[[    0  2126  1421  3441   727   330 17294  2156   150   163  8863 45935
    398  3441  5773   330 17294     2]]"
1a7d2ade16149630c0028339a816fcafa8192408,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"7,507",[[    0   406     6 38235     2]]
53807f435d33fe5ce65f5e7bda7f77712194f6ab,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," only 1 in 9 qualitative papers in Human-Computer Interaction reported inter-rater reliability metrics, low-effort responses from crowdworkers","[[    0   129   112    11   361 29981  6665    11  3861    12 45942  3870
  10845   431  3222    12   338  5109 13677 12758     6   614    12 29678
   2723  8823    31  2180 16941     2]]"
4904ef32a8f84cf2f53b1532ccf7aa77273b3d19,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
23d0637f8ae72ae343556ab135eedc7f4cb58032,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"unsupervised morphological analyzer capable of using lattices improved the accuracy of phoneme recognition and word segmentation, Consequently, this result suggests that this word segmentation method considers the multiple hypothesis of speech recognition as a whole and reduces uncertainty such as variability in recognition","[[    0   879 16101 25376 38675  9779 23815  6403  4453     9   634 39832
   6355  2782     5  8611     9 43676 17185  4972     8  2136  2835  1258
      6 25613     6    42   898  3649    14    42  2136  2835  1258  5448
   9857     5  1533 31098     9  1901  4972    25    10  1086     8 13458
   4983   215    25 36049    11  4972     2]]"
bbc58b193c08ccb2a1e8235a36273785a3b375fb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
19608e727b527562b750949e41e763908566b58e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a12a08099e8193ff2833f79ecf70acf132eda646,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
fc3f0eb297b2308b99eb4661a510c9cdbb6ffba2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],3029,[[   0  541 2890    2]]
9174aded45bc36915f2e2adb6f352f3c7d9ada8b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SST-2, Snips",[[   0  104 4014   12  176    6 7500 7418    2]]
d5fa26a2b7506733f3fa0973e2fe3fc1bbd1a12d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Yes, as new sentences.",[[    0  9904     6    25    92 11305     4     2]]
1fdcc650c65c11908f6bde67d5052087245f3dde,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
50c8b821191339043306fd28e6cda2db400704f9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],We collected Japanese fictional stories from the Web,[[    0   170  4786  2898 18588  1652    31     5  6494     2]]
3c16d4cf5dc23223980d9c0f924cb9e4e6943f13,0.0,Entailment,[[    2     0 30495  3760  1757     2]],AMS method.,[[    0 31864  5448     4     2]]
3b391cd58cf6a61fe8c8eff2095e33794e80f0e3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"historical S&P 500 component stocks
 306242 news articles","[[    0 32001 30340   208   947   510  1764  7681  1815 50118 32360 28698
    340  7201     2]]"
c20bb0847ced490a793657fbaf6afb5ef54dad81,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
0300cf768996849cab7463d929afcb0b09c9cf2a,0.0,Entailment,[[    2     0 30495  3760  1757     2]], extraction-then-synthesis framework,[[    0 23226    12 13040    12  8628  3999 35571  7208     2]]
3bfdbf2d4d68e01bef39dc3371960e25489e510e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Measuring three aspects: argumentation, specificity and knowledge domain.","[[    0  5096 40786   130  5894    35  4795  1258     6 42561     8  2655
  11170     4     2]]"
bab8c69e183bae6e30fc362009db9b46e720225e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Marcheggiani and Titov (2017) and Cai et al. (2018),"[[    0 10169  2871  6149 20909     8 18125  1417    36  3789    43     8
    230  1439  4400  1076     4    36  2464    43     2]]"
aa1f605619b2487cc914fc2594c8efe2598d8555,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a1885f807753cff7a59f69b5cf6d0fdef8484057,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English wikipedia dataset has more than 18 million, a dump of 15 million English news articles ","[[    0 35007 47764 47410 41616    34    55    87   504   153     6    10
  12371     9   379   153  2370   340  7201  1437     2]]"
7920f228de6ef4c685f478bac4c7776443f19f39,0.0,Entailment,[[    2     0 30495  3760  1757     2]],English,[[    0 35007     2]]
4e2e19a58e1f2cc5a7b1bc666c1577922454d8c8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
af60462881b2d723adeb4acb5fbc07ea27b6bde2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we demonstrate that harassment occurred more frequently during the night time than the day time, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) , We also found that the majority of young perpetrators engaged in harassment behaviors on the streets, we found that adult perpetrators of sexual harassment are more likely to act alone, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location , commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers.","[[    0  1694  8085    14  4331  2756    55  5705   148     5   363    86
     87     5   183    86     6    24   924    14 12035 20022 15595    36
   3654  2343    11     5  1955   238  2883   994     8  2377    32   299
      5   889     9  2006  3505     9 27395   268     6  1432    30   964
      8  6774     6    52 13526    14    89  5152   670 43879   227     5
   1046     9 16989     8     5  2259     9  4331     6   227     5   881
     73 34654 27395   254  1640    29    43     8  2259     6     8   227
   1046     8   881    73 34654 27395   254  1640    29    43  2156   166
     67   303    14     5  1647     9   664 16989  4009    11  4331 17156
     15     5  2827     6    52   303    14  4194 16989     9  1363  4331
     32    55   533     7  1760  1937     6    52    67   303    14     5
  43879   227     5  4620     9  4331    19     5  1046     6   881    73
  34654 27395   254     6  1907     9 27395   254     6     8  2259  2156
  18240  1102    55  5705    77 27395   268    58    11  1134     4  1426
     53    45   513     6   285  4264    16   147    82   300 32227  7240
   6699   144  5705   258    30  2598  3670     8    30  2883   994     8
   2377     4     2]]"
9893c5f36f9d503678749cb0466eeaa0cfc9413f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],five-character window context,[[    0  9579    12 23375  2931  5377     2]]
b124137e62178a2bd3b5570d73b1652dfefa2457,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," analogy query, analogy browsing",[[    0 33460 25860     6 33460 24033     2]]
f887d5b7cf2bcc1412ef63bff4146f7208818184,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
5a02a3dd26485a4e4a77411b50b902d2bda3731b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"To model an answer which is a collection of spans, the multi-span head uses the $\mathtt {BIO}$ tagging format BIBREF8: $\mathtt {B}$ is used to mark the beginning of a span, $\mathtt {I}$ is used to mark the inside of a span and $\mathtt {O}$ is used to mark tokens not included in a span","[[    0  3972  1421    41  1948    61    16    10  2783     9 23645     6
      5  3228    12 36407   471  2939     5 49959 40051  5967 25522  5383
    673 24303  1629 34694  7390   163  8863 45935   398    35 49959 40051
   5967 25522   387 24303  1629    16   341     7  2458     5  1786     9
     10  8968     6 49959 40051  5967 25522   100 24303  1629    16   341
      7  2458     5  1025     9    10  8968     8 49959 40051  5967 25522
    673 24303  1629    16   341     7  2458 22121    45  1165    11    10
   8968     2]]"
07ee4e0277ad1083270131d32a71c3fe062a916d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Author profiling and deception detection in Arabic, LAMA+DINA Emotion detection, Sentiment analysis in Arabic tweets","[[    0 42779 25993     8 29244 12673    11 19645     6   226 19455  2744
    495 16712  3676 19187 12673     6 12169  8913  1966    11 19645  6245
      2]]"
f858031ebe57b6139af46ee0f25c10870bb00c3c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SParC BIBREF2 and CoSQL BIBREF6,"[[    0  4186   271   347   163  8863 45935   176     8   944 46608   163
   8863 45935   401     2]]"
3d49b678ff6b125ffe7fb614af3e187da65c6f65,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The framework jointly learns parametrized QA and QG models subject to the constraint in equation 2. In more detail, they minimize QA and QG loss functions, with a third dual loss for regularization.","[[    0   133  7208 13521 25269 40206   594  1069 16317  1209   250     8
   1209   534  3092  2087     7     5 39816    11 19587   132     4    96
     55  4617     6    51 15925  1209   250     8  1209   534   872  8047
      6    19    10   371  6594   872    13  1675  1938     4     2]]"
18fbf9c08075e3b696237d22473c463237d153f5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Moderate agreement of 0.64-0.68 Fleiss Kappa over event type labels, 0.77 Fleiss Kappa over participant labels, and good agreement of 90.5% over coreference information.","[[    0 30597 45329  1288     9   321     4  4027    12   288     4  4671
  12602  3006    17    27 37772    81   515  1907 14105     6   321     4
   4718 12602  3006    17    27 37772    81 16076 14105     6     8   205
   1288     9  1814     4   245   207    81  2731 23861   335     4     2]]"
fc29bb14f251f18862c100e0d3cd1396e8f2c3a1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
70afd28b0ecc02eb8e404e7ff9f89879bf71a670,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
4fcc668eb3a042f60c4ce2e7d008e7923b25b4fc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
98ba7a7aae388b1a77dd6cab890977251d906359,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
578d0b23cb983b445b1a256a34f969b34d332075,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Arap-Tweet BIBREF19 , an in-house Twitter dataset for gender, the MADAR shared task 2 BIBREF20, the LAMA-DINA dataset from BIBREF22, LAMA-DIST, Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34","[[    0   250  8645    12 44445   163  8863 45935  1646  2156    41    11
     12  3138   599 41616    13  3959     6     5 27557  2747  1373  3685
    132   163  8863 45935   844     6     5   226 19455    12   495 16712
  41616    31   163  8863 45935  2036     6   226 19455    12   495 11595
      6 19645  6245   703    30  4576  2571  1039 14071  4629 10626  1373
     12 45025   163  8863 45935  1978     6   163  8863 45935  1244     6
    163  8863 45935  2481     6   163  8863 45935  2518     6   163  8863
  45935   134     6   163  8863 45935  2517     6   163  8863 45935  2890
      6   163  8863 45935   541     6   163  8863 45935  2983     6   163
   8863 45935  2881     6   163  8863 45935  3103     6   163  8863 45935
   3079     2]]"
d8bf4a29c7af213a9a176eb1503ec97d01cc8f51,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
1eeabfde99594b8d9c6a007f50b97f7f527b0a17,0.0,Entailment,[[    2     0 30495  3760  1757     2]],validation data,[[    0 42679  1258   414     2]]
21df76462c76d6e2d52fb7dce573ee5336627cb5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],participants in group one very rarely gave different answers to questions one and two (18 of 500 instances or 3.6%),"[[    0 42038  3277    11   333    65   182  7154   851   430  5274     7
   1142    65     8    80    36  1366     9  1764 10960    50   155     4
    401  8871     2]]"
46ee1cbbfbf0067747b28bdf4c8c2f7dc8955650,0.0,Entailment,[[    2     0 30495  3760  1757     2]],LSTMs,[[    0   574  4014 13123     2]]
578d0b23cb983b445b1a256a34f969b34d332075,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," Arap-Tweet , UBC Twitter Gender Dataset, MADAR , LAMA-DINA , IDAT@FIRE2019, 15 datasets related to sentiment analysis of Arabic, including MSA and dialects","[[    0    83  8645    12 44445  2156   121  3573   599 25262 16673   281
    594     6 27557  2747  2156   226 19455    12   495 16712  2156  4576
   2571  1039 14071  4629 10626     6   379 42532  1330     7  5702  1966
      9 19645     6   217   256  3603     8 37508    29     2]]"
ee27e5b56e439546d710ce113c9be76e1bfa1a3d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
cc5d8e12f6aecf6a5f305e2f8b3a0c67f49801a9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],36%,[[   0 3367  207    2]]
bd7a95b961af7caebf0430a7c9f675816c9c527f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"DSTC2, M2M-sim-M, M2M-sim-R","[[    0   495  4014   347   176     6   256   176   448    12 13092    12
    448     6   256   176   448    12 13092    12   500     2]]"
51aaec4c511d96ef5f5c8bae3d5d856d8bc288d3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search,"[[    0   627 18043   147  2718  2706  2939    10  2526 13931    12   560
     12 46665  1421 21621    19  1503  9562     8  2718 14925  2939  5581
     12  9502  1707     2]]"
83f24e4bbf9de82d560cdde64b91d6d672def6bf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
bd74452f8ea0d1d82bbd6911fbacea1bf6e08cab,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
458f3963387de57fdc182875c9ca3798b612b633,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"GPT2, SciBERT model of BIBREF11","[[    0   534 10311   176     6 22640 11126   565  1421     9   163  8863
  45935  1225     2]]"
5c90e1ed208911dbcae7e760a553e912f8c237a5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"In-house dataset consists of  3716 documents 
ACE05 dataset consists of  1635 documents","[[    0  1121    12  3138 41616 10726     9  1437  2908  1549  2339  1437
  50118 15949  2546 41616 10726     9  1437   545  2022  2339     2]]"
4519afe91b1042876d7c021487d98e2d72a09861,0.0,Entailment,[[    2     0 30495  3760  1757     2]],dominant temporal associations can be learned from training data,[[    0 12623 40542 41853 14697    64    28  2435    31  1058   414     2]]
60cb756d382b3594d9e1f4a5e2366db407e378ae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
c554a453b6b99d8b59e4ef1511b1b506ff6e5aa4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Questions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified: we found selecting queries where the first word is in a manually constructed set of indicator words and are of sufficient length, to be effective. Questions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing. Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as not answerable"" if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question's answer is yes"" or no""","[[    0 46865    32  4366    31 44563  1538     6 26683  1070 22680     7
      5  1204  1707  3819     4  3232  9709    14    32   533     7    28
   4420    73  2362  1142    32    37   710 18281  2006    35    52   303
  18099 22680   147     5    78  2136    16    11    10 24704 11236   278
      9  9189  1617     8    32     9  7719  5933     6     7    28  2375
      4 18964    32   129  1682   114    10 28274  1842    16  1835    25
     65     9     5    78   292   775     6    11    61   403     5   864
      8 28274  1842    32   576     7    10  1050 45068  2630    13   617
   5774     4   660  3654  3629  6929   864    73 31154 15029    11    10
    130    12 13975   609     4  1234     6    51  2845   114     5   864
     16   205     6  3099    24    16 32616  4748     6 40624 41777     6
      8 14030 21833   335     4   152  7579    16   156   137     5 45068
   2630  3681     5 28274  1842     4  4130     6    13   205  1142     6
  45068  3629   465    10  9078   624     5  3780    14  6308   615   335
      7  1948     5   864     4   660  3654  3629    64  2458  1142    25
     44    48  3654  1948   868   113   114     5 28274  1566   473    45
   5585     5  5372   335     4  3347     6 45068  3629  2458   549     5
    864    18  1948    16    44    48 10932   113    50    44    48  2362
    113     2]]"
a883bb41449794e0a63b716d9766faea034eb359,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"context is a procedural text, the question and the multiple choice answers are composed of images","[[    0 46796    16    10 24126  2788     6     5   864     8     5  1533
   2031  5274    32 14092     9  3156     2]]"
69e678666d11731c9bfa99953e2cd5a5d11a4d4f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SParC BIBREF2 and CoSQL BIBREF6,"[[    0  4186   271   347   163  8863 45935   176     8   944 46608   163
   8863 45935   401     2]]"
da068b20988883bc324e55c073fb9c1a5c39be33,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," correct information substantially improves it - we see an increase of up to 2.3 BLEU over the baseline, Finally, the She said prefixes substantially increase the number of feminine-marked verbs, bringing the proportion much closer to that of the reference","[[    0  4577   335 12246 15296    24   111    52   192    41   712     9
     62     7   132     4   246   163  3850   791    81     5 18043     6
   3347     6     5    44    48  2515    26    17    46 46622   293 12246
    712     5   346     9 27360    12 18584 47041     6  2406     5 10301
    203  2789     7    14     9     5  5135     2]]"
f011d6d5287339a35d00cd9ce1dfeabb1f3c0563,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
64b65687b82ddb17c3d068381aaee56eb7fc02cd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Twitter dataset obtained from the authors of BIBREF12,"[[    0 22838 41616  4756    31     5  7601     9   163  8863 45935  1092
      2]]"
b161febf86cdd58bd247a934120410068b24b7d1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, question, other","[[    0  1073 43563     6  1948     6  9819     6 20628     6 25499  1258
      6 12073     6  2430  4289     6   864     6    97     2]]"
ca4b66ffa4581f9491442dcec78ca556253c8146,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
c9e9c5f443649593632656a5934026ad8ccc1712,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," We first encode text inputs using bidirectional LSTMs, then compute summaries using self-attention and conditional summaries using attention. We concatenate text summaries into text features, which, along with visual features, are processed through consecutive layers. In this case of a textual environment, we consider the grid of word embeddings as the visual features for . The final output is further processed by MLPs to compute a policy distribution over actions and a baseline for advantage estimation.","[[    0   166    78 46855  2788 16584   634  2311 43606   337   226  4014
  13123     6   172 37357 32933  5119   634  1403    12  2611 19774     8
  23431 32933  5119   634  1503     4   166 10146 26511   877  2788 32933
   5119    88  2788  1575     6    61     6   552    19  7133  1575     6
     32 12069   149  3396 13171     4    96    42   403     9    10 46478
   1737     6    52  1701     5  7961     9  2136 33183   417  1033    25
      5  7133  1575    13   479    20   507  4195    16   617 12069    30
  10725 13438     7 37357    10   714  3854    81  2163     8    10 18043
     13  2093 32809     4     2]]"
352c081c93800df9654315e13a880d6387b91919,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
85912b87b16b45cde79039447a70bd1f6f1f8361,0.0,Entailment,[[    2     0 30495  3760  1757     2]],449050,[[   0 3305 3248 1096    2]]
372fbf2d120ca7a101f70d226057f9639bf1f9f2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Crowd workers were asked to mark whether a triple was correct, namely, did the triple reflect the consequence of the sentence.","[[    0   347 34298  1138    58   553     7  2458   549    10  6436    21
   4577     6 13953     6   222     5  6436  4227     5 15180     9     5
   3645     4     2]]"
735f58e28d84ee92024a36bc348cfac2ee114409,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
ad0a7fe75db5553652cd25555c6980f497e08113,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By treating logical forms as a latent variable and training a discriminative log-linear model over logical form y given x.,"[[    0  2765  8959 16437  4620    25    10 42715 15594     8  1058    10
  40846   179  3693  7425    12 43871  1421    81 16437  1026  1423   576
   3023     4     2]]"
1d74fd1d38a5532d20ffae4abbadaeda225b6932,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"F1 score and Recall are 68.66, 80.08 with Traditional NERs as reference and 59.56, 69.76 with Wikipedia titles as reference.","[[    0   597   134  1471     8 35109    32  5595     4  4280     6  1812
      4  3669    19 22530   234  2076    29    25  5135     8  5169     4
   4419     6  5913     4  5067    19 28274  4867    25  5135     4     2]]"
2e6b7afaf14871ed6db674782b93709910020b06,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"original models were better in some tasks (CR, MPQA, MRPC), utilizing self-attentive sentence representation further improves performances in 5 out of 8 tasks","[[    0 34545  3092    58   357    11   103  8558    36  9822     6  3957
   1864   250     6 18838  4794   238 21437  1403    12  2611  1342  2088
   3645  8985   617 15296  4476    11   195    66     9   290  8558     2]]"
868c69c8f623e30b96df5b5c8336070994469f60,0.0,Entailment,[[    2     0 30495  3760  1757     2]], CoLA contains example sentences from linguistics publications labeled by experts,"[[    0   944  8272  6308  1246 11305    31 38954 16307 16043 16274    30
   2320     2]]"
abc5836c54fc2ac8465aee5a83b9c0f86c6fd6f5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
4170ed011b02663f5b1b1a3c1f0415b7abfaa85d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we observe a positive correlation between retweeting and co-voting, strongest positive correlations are in the areas Area of freedom, security and justice, External relations of the Union, and Internal markets, Weaker, but still positive, correlations are observed in the areas Economic, social and territorial cohesion, European citizenship, and State and evolution of the Union, significantly negative coefficient, is the area Economic and monetary system","[[    0  1694 14095    10  1313 22792   227 24352   154     8  1029    12
    705 12653     6  8260  1313 43879    32    11     5   911  4121     9
   3519     6   573     8  2427     6 25468  3115     9     5  1332     6
      8 18387  1048     6   166  4218     6    53   202  1313     6 43879
     32  6373    11     5   911  4713     6   592     8 15752 31657     6
    796  8860     6     8   331     8 10795     9     5  1332     6  3625
   2430 45979     6    16     5   443  4713     8  5775   467     2]]"
4b624064332072102ea674254d7098038edad572,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
45893f31ef07f0cca5783bd39c4e60630d6b93b3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They derive it from Wordnet,[[    0  1213 34882    24    31 15690  4135     2]]
cfb5ab893ed77f9df7eeb4940b6bacdef5acccea,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
c82e945b43b2e61c8ea567727e239662309e9508,0.0,Entailment,[[    2     0 30495  3760  1757     2]],distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort,"[[    0 17165 35308  6234   227 34948  1313     8  2430 32242   624   349
    810  3724 11170     8  6846    13 16697   414  4786    15     5  1002
  26268     2]]"
3b090b416c4ad7d9b5b05df10c5e7770a4590f6a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
709a4993927187514701fe3cc491ac3030da1215,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"information retrieval system, word-association method,  CCG-style rule-based semantic parser written specifically for friction questions, state-of-the-art neural semantic parser","[[    0 31480 43372   467     6  2136    12  2401 41156  5448     6  1437
   9841   534    12  5827  2178    12   805 46195 48946  1982  4010    13
  20737  1142     6   194    12  1116    12   627    12  2013 26739 46195
  48946     2]]"
0c7823b27326b3f5dff51f32f45fc69c91a4e06d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],in open-ended task esp. for counting-type questions ,"[[    0   179   490    12  6228  3685 21179     4    13 10581    12 12528
   1142  1437     2]]"
0e9c08b635c1ebfd36472550d619095541bb5af1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"when the gate has high value, more information flows from the word-level representation; otherwise, char-level will take the dominating place,  for unfamiliar noun entities, the gates tend to bias towards char-level representation in order to care richer morphological structure","[[    0 14746     5  8751    34   239   923     6    55   335  7964    31
      5  2136    12  4483  8985   131  3680     6 16224    12  4483    40
    185     5 17349   317     6  1437    13 21942 44875  8866     6     5
  14213  3805     7  9415  1567 16224    12  4483  8985    11   645     7
    575 24230 38675  9779  3184     2]]"
f5eac66c08ebec507c582a2445e99317a83e9ebe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
7997b9971f864a504014110a708f215c84815941,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text","[[    0 45659  2580 28269  2574     6   304     9  3904 24684     8 15760
   9762     6  2649 26827  1033     6 41046     6    92  1617     6 44163
      6     8 11581    12 14175 33090     8 40993  1635     6   765    36
  16096  1804    43  2788     2]]"
29772ba04886bee2d26b7320e1c6d9b156078891,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a9b10e3db5902c6142e7d6a83253ad2a6cee77fc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
bdc6664cec2b94b0b3769bc70a60914795f39574,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"average INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 values","[[    0 20365  2808 28302 38036   288  2156  2808 28302 38036   134  2156
      8  2808 28302 38036   176  3266     2]]"
2fea3c955ff78220b2c31a8ad1322bc77f6706f8,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," our method enables to control the morphological realization of first and second-person pronouns, together with verbs and adjectives related to them","[[    0    84  5448  9849     7   797     5 38675  9779 24179     9    78
      8   200    12  5970 43083     6   561    19 47041     8 45272  3699
   1330     7   106     2]]"
e1b36927114969f3b759cba056cfb3756de474e4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Improved AECNN-T by 2.1 and AECNN-T-SM BY 0.9,"[[    0 48313    83  3586 20057    12   565    30   132     4   134     8
     83  3586 20057    12   565    12 15153 10786   321     4   466     2]]"
7e328cc3cffa521e73f111d6796aaa9661c8eb07,0.0,Entailment,[[    2     0 30495  3760  1757     2]],predicting the word given its context,[[    0   642 39733   154     5  2136   576    63  5377     2]]
bbdb2942dc6de3d384e3a1b705af996a5341031b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],A bi-LSTM with max-pooling on top of it,"[[    0   250  4003    12   574  4014   448    19 19220    12 10416   154
     15   299     9    24     2]]"
ffa4d4bfb226382ca4ecde65ecdc44a3d9e0ce81,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Paraphrase Identification,[[    0 22011  8258 34338 36309     2]]
6aee16c4f319a190c2a451c1c099b66162299a28,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"(1) grammatical correctness, (2) contextual coherence, (3) emotional appropriateness","[[    0  1640   134    43 25187 45816 34726     6    36   176    43 37617
   1029 40584     6    36   246    43  3722 40194   415 14186     2]]"
d3d4eef047aa01391e3e5d613a0f1f786ae7cfc7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa","[[    0  2629   819   202   784  8299   639  3092  5389    15     5  1461
   2370  1058   278    11     5 26933 11160  8625    12   565  4923  2749
      6  7694     4   176  1954     4  7383     4  6468    13  3830 11126
  38495     2]]"
b6c235d5986914b380c084d9535a7b01310c0278,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"correct class can be directly inferred from the text content easily, even without background knowledge, correct class can be inferred from the text content, given that event-specific knowledge is provided, orrect class can be inferred from the text content if the text is interpreted correctly","[[    0 36064  1380    64    28  2024 42870    31     5  2788  1383  2773
      6   190   396  3618  2655     6  4577  1380    64    28 42870    31
      5  2788  1383     6   576    14   515    12 14175  2655    16  1286
      6    50 41087  1380    64    28 42870    31     5  2788  1383   114
      5  2788    16 19912 12461     2]]"
f5cf8738e8d211095bb89350ed05ee7f9997eb19,0.0,Entailment,[[    2     0 30495  3760  1757     2]],up to four percentage points in accuracy,[[   0  658    7  237 3164  332   11 8611    2]]
7f2fd7ab968de720082133c42c2052d351589a67,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"word2vec, 200 as the dimension of the obtained word vectors","[[    0 14742   176 25369     6  1878    25     5 21026     9     5  4756
   2136 44493     2]]"
77f04cd553df691e8f4ecbe19da89bc32c7ac734,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
c4a6b727769328333bb48d59d3fc4036a084875d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN","[[    0 33837     6  9612  1864   250     6  6479  3134   597     6  9025
    506    12 11621   791     6 24294  8332   448     6 19462  1589   344
   3314   102     6 24294  1864   250    12 11621   487     2]]"
8ea664a72e6d6eca73c1b3e1f75a72a677474ab1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
3513682d4ee2e64725b956c489cd5b5995a6acf2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"monte-carlo, sequential sampling",[[    0  5806   859    12  5901  4082     6 29698 20424     2]]
0f928732f226185c76ad5960402e9342c0619310,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"feedforward neural networks (DNNs), convolutional neural networks (CNNs)","[[    0 18790 16135 26739  4836    36   495 20057    29   238 15380 23794
    337 26739  4836    36 16256    29    43     2]]"
0da6cfbc8cb134dc3d247e91262f5050a2200664,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Clusters of Twitter user ids from accounts of American or German political actors, musicians, media websites or sports club","[[    0 11428 37406     9   599  3018  1437  7823    31  2349     9   470
     50  1859   559  5552     6  8884     6   433  7656    50  1612   950
      2]]"
cc5d3903913fa2e841f900372ec74b0efd5e0c71,0.0,Entailment,[[    2     0 30495  3760  1757     2]],12 binary-class classification and multi-class classification of reviews based on rating,"[[    0  1092 32771    12  4684 20257     8  3228    12  4684 20257     9
   6173   716    15   691     2]]"
fa572f1f3f3ce6e1f9f4c9530456329ffc2677ca,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
989271972b3176d0a5dabd1cc0e4bdb671269c96,0.0,Entailment,[[    2     0 30495  3760  1757     2]],from Arabic WikiNews site https://ar.wikinews.org/wiki,"[[    0  7761 19645 45569  5532  1082  1205   640   271     4 39224   179
  10269     4  1957    73 47214     2]]"
a1917232441890a89b9a268ad8f987184fa50f7a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Trinomials are likely to appear in exactly one order,"[[    0 12667   179  1075  9532    32   533     7  2082    11  2230    65
    645     2]]"
fab4ec639a0ea1e07c547cdef1837c774ee1adb8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
3a9d391d25cde8af3334ac62d478b36b30079d74,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
fd5412e2784acefb50afc3bfae1e087580b90ab9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
7889ec45b996be0b8bf7360d08f84daf3644f115,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"pal-EtAl:2018:WMT proposed an APE model that uses three self-attention-based encoders, tebbifakhr-EtAl:2018:WMT, the NMT-subtask winner of WMT 2018 ($wmt18^{nmt}_{best}$), employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics., shin-lee:2018:WMT propose that each encoder has its own self-attention and feed-forward layer to process each input separately. , The APE PBSMT-subtask winner of WMT 2018 ($wmt18^{smt}_{best}$) BIBREF11 also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross-attention component for $src \rightarrow pe$ above the previous cross-attention for $mt \rightarrow pe$.","[[    0 18239    12   717    90  7083    35  2464    35   771 11674  1850
     41  1480   717  1421    14  2939   130  1403    12  2611 19774    12
    805  9689  1630   268     6  3055 14141  1594  7352   338    12   717
     90  7083    35  2464    35   771 11674     6     5   234 11674    12
  10936 45025  1924     9   305 11674   199  1358   605 16100  1366 49688
    282 16100 24303 49747  7885 24303  1629   238 12735 13931    12  4483
    872  8047    11   645     7  1877  4895  9415   148  1058     8     7
     28  4292    19     5  8408 10437 12758   482 38802    12  7445    35
   2464    35   771 11674 15393    14   349  9689 15362    34    63   308
   1403    12  2611 19774     8  3993    12 16135 10490     7   609   349
   8135 12712     4  2156    20  1480   717 24098 11674    12 10936 45025
   1924     9   305 11674   199  1358   605 16100  1366 49688  9426    90
  24303 49747  7885 24303  1629    43   163  8863 45935  1225    67  2633
    277 40878    12   805  3228    12 17747  1480   717    61  2939    80
   9689  1630   268     8 32201    41   943  2116    12  2611 19774  7681
     13    68 45692 44128  4070 17214  3723  1629  1065     5   986  2116
     12  2611 19774    13    68 16100 44128  4070 17214  3723 48292     2]]"
9349acbfce95cb5d6b4d09ac626b55a9cb90e55e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Background, extends, uses, motivation, compare/contrast, and future work for the ACL-ARC dataset. Background, method, result comparison for the SciCite dataset.","[[    0 48277     6 14269     6  2939     6 10563     6  8933    73 10800
  16136     6     8   499   173    13     5 21147    12 11969 41616     4
  32070     6  5448     6   898  6676    13     5 22640   347  1459 41616
      4     2]]"
a24a7a460fd5e60d71a7e787401c68caa4702df6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"AraVec for Arabic, FastText for French, and Word2vec Google News for English.","[[    0   250   763   846  3204    13 19645     6  9612 39645    13  1515
      6     8 15690   176 25369  1204   491    13  2370     4     2]]"
1ce26783f0ff38925bfc07bbbb65d206e52c2d21,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a9def7958eac7b9a780403d4f136927f756bab83,0.0,Entailment,[[    2     0 30495  3760  1757     2]],MTMSN BIBREF4,[[    0 11674 48508   163  8863 45935   306     2]]
e854edcc5e9111922e6e120ae17d062427c27ec1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
3371d586a3a81de1552d90459709c57c0b1a2594,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks., Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves.","[[    0 42945  3092     8  8558    35    20  3093    36   281  4435  4113
  34012    43     9  3975 21843    23     5   672     9  2167  3092     8
   8558   482 15581  8135   980    35    20  3093     9  3975 21843    23
      5   672     9  2849  4182 11667     9     5  8135   980     6   215
     25  9100     9  1122 16584     6    50 23429 16584  1235     4     2]]"
36d892460eb863220cd0881d5823d73bbfda172c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"DREAM, MCTest, TOEFL, and SemEval-2018 Task 11","[[    0   495 28057     6   256  7164   990     6  3842   717  7613     6
      8 11202   717  6486    12  2464 12927   365     2]]"
95bb3ea4ebc3f2174846e8d422abc076e1407d6a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],between 1900s and 2010s,[[    0 25784 23137    29     8  1824    29     2]]
ac54a9c30c968e5225978a37032158a6ffd4ddb8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],This seems to indicate that the downstream QA module relies more on the upstream paragraph-level retrieval whereas the verification module relies more on the upstream sentence-level retrieval.,"[[    0   713  1302     7  6364    14     5 18561  1209   250 20686 12438
     55    15     5 21561 17818    12  4483 43372  9641     5 14925 20686
  12438    55    15     5 21561  3645    12  4483 43372     4     2]]"
4d05a264b2353cff310edb480a917d686353b007,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The HMM can identify punctuation or pick up on vowels.,"[[    0   133   289 16261    64  3058 15760  9762    50  1339    62    15
  27578  2507     4     2]]"
39492338e27cb90bf1763e4337c2f697cf5082ba,0.0,Entailment,[[    2     0 30495  3760  1757     2]],10 CNNs and 10 LSTMs,[[    0   698  3480    29     8   158   226  4014 13123     2]]
ad6415f4351c44ffae237524696a3f76f383bfd5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
0038b073b7cca847033177024f9719c971692042,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The relation R(x,y) is mapped onto a question q whose answer is y","[[    0   133  9355   248  1640  1178     6   219    43    16 33493  2500
     10   864  2231  1060  1948    16  1423     2]]"
0f2403fa77738bf05534d7f9d83c9dbb0a0d6140,0.0,Entailment,[[    2     0 30495  3760  1757     2]],level of agreement (Krippendorff's INLINEFORM0 ),"[[    0  4483     9  1288    36   530 22872 25883  3145    18  2808 28302
  38036   288  4839     2]]"
83f567489da49966af3dc5df2d9d20232bb8cb1e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
79cfd1b82c72d18e2279792c66a042c0e9dfa6b7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],DyNet,[[    0   495   219 15721     2]]
0619fc797730a3e59ac146a5a4575c81517cc618,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN., we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. ","[[    0   170  8933    84  1421   819    19     5  8369     9   163  8863
  45935   288   163  8863 45935   245    15  4062   104 28556     4   163
   8863 45935   288   431     5   775     9 35540  9860 47145    36 19854
  30495   238 28868     6   208 20954    15  4062   104 28556   519   205
    819    11   986    86     4    20  1421     9   163  8863 45935   245
     16    10   194    12  1116    12   627    12  2013    98   444    30
    634    10  7275  3632 20057   482    52  8933   775    19     5  1421
      9   163  8863 45935  1570    14   341    10 12547     9  1533  1542
   1380 27368    36 12743    43   215    25 28868     6 34638  5761    36
  30455   238   208 20954     8  9359  5580  6304 21791    36 33919   322
     20   381  6949  1421    16  2771    19  3298    12  1116    12 30938
     36 18935   771   238  1905 41505    36 43316    43     8 36912 46043
      4    20  1421     9   163  8863 45935  1570    16    10   194    12
   1116    12   627    12  2013    15  5316     8   289  9822 42532     4
   1437     2]]"
9f89bff89cea722debc991363f0826de945bc582,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WS353S, SimLex999, SimVerb3500","[[    0 13691 27044   104     6  6202 43551 16692     6  6202 21119   428
   2022   612     2]]"
efe49829725cfe54de01405c76149a4fe4d18747,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For example, in QuasarT, it improves 16.8% in EM score and 20.4% in F1 score. , For example, in QuasarT, it improves 4.6% in EM score and 3.5% in F1 score.","[[    0  2709  1246     6    11  3232 31126   565     6    24 15296   545
      4   398   207    11 14850  1471     8   291     4   306   207    11
    274   134  1471     4  2156   286  1246     6    11  3232 31126   565
      6    24 15296   204     4   401   207    11 14850  1471     8   155
      4   245   207    11   274   134  1471     4     2]]"
df510c85c277afc67799abcb503caa248c448ad2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
79a28839fee776d2fed01e4ac39f6fedd6c6a143,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Proposing an improved RNN model, the phonetic temporal neural LID approach, based on phonetic features that results in better performance","[[    0 41895 10174    41  2782   248 20057  1421     6     5 43676 15557
  41853 26739   226  2688  1548     6   716    15 43676 15557  1575    14
    775    11   357   819     2]]"
0137ecebd84a03b224eb5ca51d189283abb5f6d9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BERTNLU from ConvLab-2, a rule-based model (RuleDST) , TRADE (Transferable Dialogue State Generator) , a vanilla policy trained in a supervised fashion from ConvLab-2 (SL policy)","[[    0 11126   565 27027   791    31 30505 30906    12   176     6    10
   2178    12   805  1421    36 47181   495  4014    43  2156  5758 15197
     36 46552   868 33854   331 44750    43  2156    10 21857   714  5389
     11    10 20589  2734    31 30505 30906    12   176    36 11160   714
     43     2]]"
1231934db6adda87c1b15e571468b8e9d225d6fe,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Excluding the embedding weights, our model requires 100k parameters","[[    0  9089 19586     5 33183 11303 23341     6    84  1421  3441   727
    330 17294     2]]"
2c8d5e3941a6cc5697b242e64222f5d97dba453c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BERT on Quora drops from 94.6% to 24.1%,"[[    0 11126   565    15  3232  4330  9305    31  8940     4   401   207
      7   706     4   134   207     2]]"
13ca4bf76565564c8ec3238c0cbfacb0b41e14d2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"14 TDs, BIBREF15",[[    0  1570 18130     6   163  8863 45935   996     2]]
9cc0fd3721881bd8e246d20fff5d15bd32365655,0.0,Entailment,[[    2     0 30495  3760  1757     2]],image,[[    0 20094     2]]
f258ada8577bb71873581820a94695f4a2c223b3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"70,000",[[   0 3083    6  151    2]]
03b939ad70593f6475c56e9be73ba409d33faa62,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LEAD, QUERY_SIM, MultiMR, SVR, DocEmb, ISOLATION","[[    0  3850  2606     6 16944 22570  1215 37266     6 19268 12642     6
    208 13055     6 19761 42578     6  3703  3384  6034     2]]"
5c059a13d59947f30877bed7d0180cca20a83284,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
fd80a7162fde83077ed82ae41d521d774f74340a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Burckhardt et al. BIBREF22, Liu et al. BIBREF18, Dernoncourt et al. BIBREF9, Yang et al. BIBREF10","[[    0 30593  2420 15553  4400  1076     4   163  8863 45935  2036     6
  13768  4400  1076     4   163  8863 45935  1366     6   211  3281   261
   8953  4400  1076     4   163  8863 45935   466     6 13262  4400  1076
      4   163  8863 45935   698     2]]"
896e99d7f8f957f6217185ff787e94f84c136087,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Unsupervised CLWEs. These methods first induce a seed dictionary $D^{(1)}$ leveraging only two unaligned monolingual spaces (C1). While the algorithms for unsupervised seed dictionary induction differ, they all strongly rely on the assumption of similar topological structure between the two pretrained monolingual spaces. Once the seed dictionary is obtained, the two-step iterative self-learning procedure (C2) takes place: 1) a dictionary $D^{(k)}$ is first used to learn the joint space $\mathbf {Y}^{(k)} = \mathbf {X{W}}^{(k)}_x \cup \mathbf {Z{W}}^{(k)}_z$ ; 2) the nearest neighbours in $\mathbf {Y}^{(k)}$ then form the new dictionary $D^{(k+1)}$ . We illustrate the general structure in Figure 1 .","[[    0  9685 16101 25376  5289  9112    29     4  1216  6448    78 28944
     10  5018 36451    68   495 49688  1640   134 48950  1629 21221   129
     80   542 36967  6154 31992  5564  5938    36   347   134   322   616
      5 16964    13   542 16101 25376  5018 36451 26076 10356     6    51
     70  5025  5864    15     5 15480     9  1122   299  9779  3184   227
      5    80 11857 26492  6154 31992  5564  5938     4  3128     5  5018
  36451    16  4756     6     5    80    12 13975 41393  3693  1403    12
  29888  7089    36   347   176    43  1239   317    35   112    43    10
  36451    68   495 49688  1640   330 48950  1629    16    78   341     7
   1532     5  2660   980 49959 40051 36920 25522   975 24303 49688  1640
    330 48950  5457 44128 40051 36920 25522  1000 45152   771 46961 49688
   1640   330 48950  1215  1178 44128 21033 44128 40051 36920 25522  1301
  45152   771 46961 49688  1640   330 48950  1215   329  1629 25606   132
     43     5 14712 10689    11 49959 40051 36920 25522   975 24303 49688
   1640   330 48950  1629   172  1026     5    92 36451    68   495 49688
   1640   330  2744   134 48950  1629   479   166 23168     5   937  3184
     11 17965   112   479     2]]"
41844d1d1ee6d6d38f31b3a17a2398f87566ed92,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"two parallel convolutional networks, INLINEFORM0 , that share the same set of weights","[[    0  7109 12980 15380 23794   337  4836     6  2808 28302 38036   288
   2156    14   458     5   276   278     9 23341     2]]"
1ba28338d3f993674a19d2ee2ec35447e361505b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Chowdhury BIBREF14 and Thomas et al. BIBREF11, FBK-irst BIBREF10, Liu et al. BIBREF9, Sahu et al. BIBREF12","[[    0  4771  1722 16593  6801   163  8863 45935  1570     8  1813  4400
   1076     4   163  8863 45935  1225     6 13042   530    12 17698   163
   8863 45935   698     6 13768  4400  1076     4   163  8863 45935   466
      6   208 17421  4400  1076     4   163  8863 45935  1092     2]]"
310e61b9dd4d75bc1bebbcb1dae578f55807cd04,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"LDC corpus, NIST 2003(MT03), NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06), NIST 2008(MT08)","[[    0   574  5949 42168     6   234 11595  4999  1640 11674  3933   238
    234 11595  4482  1640 11674  3387   238   234 11595  4013  1640 11674
   2546   238   234 11595  3503  1640 11674  4124   238   234 11595  2266
   1640 11674  3669    43     2]]"
86abeff85f3db79cf87a8c993e5e5aa61226dc98,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"negative, positive",[[    0 33407     6  1313     2]]
751aa2b1531a17496536887288699cc8d5c3cec9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Hence WordPiece tokenizer tokenizes noisy words into subwords. However, it ends up breaking them into subwords whose meaning can be very different from the meaning of the original word. Often, this changes the meaning of the sentence completely, therefore leading to substantial dip in the performance.","[[    0   725  4086 15690   510 39426 19233  6315 19233  7396 28269  1617
     88  2849 30938     4   635     6    24  3587    62  3433   106    88
   2849 30938  1060  3099    64    28   182   430    31     5  3099     9
      5  1461  2136     4 17482     6    42  1022     5  3099     9     5
   3645  2198     6  3891   981     7  6143 10645    11     5   819     4
      2]]"
aa0d67c2a1bc222d1f2d9e5d51824352da5bb6dc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"TransE, TransR and TransH, PTransE, and ALL-PATHS, R-GCN BIBREF24 and KR-EAR BIBREF26","[[    0 19163   717     6  5428   500     8  5428   725     6   221 19163
    717     6     8 12389    12   510  2571  6391     6   248    12 11961
    487   163  8863 45935  1978     8 20858    12 27757   163  8863 45935
   2481     2]]"
8740c3000e740ac5c0bc8f329d908309f7ffeff6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MojiTalk , PersonaChat , Empathetic-Dialogues","[[    0 17357  5186 27743  2156 46107 29665  2156  3676 11632 18667    12
  48873  3663     2]]"
7b35593033e4c6b9dccba98f22a7eeaa3385df38,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
c9ee70c481c801892556eb6b9fd8ee38197923be,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"language-independent (e.g., punctuation marks, positive and negative emoticons, quotations, personal pronouns, tweet's length, named entities),  language-dependent relying on dedicated lexicons (e.g., negation, opinion lexicons, opposition words)","[[    0 19527    12 32982    36   242     4   571   482 15760  9762  4863
      6  1313     8  2430 36355 46043     6 42727     6  1081 43083     6
   3545    18  5933     6  1440  8866   238  1437  2777    12 30231 13304
     15  3688 36912 46043    36   242     4   571   482 15183  1258     6
   2979 36912 46043     6  1756  1617    43     2]]"
01123a39574bdc4684aafa59c52d956b532d2e53,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"AE-HCN outperforms by 17%, AE-HCN-CNN outperforms by 20% on average","[[    0 16329    12 13459   487  9980 33334    30   601  4234 35722    12
  13459   487    12 16256  9980 33334    30   291   207    15   674     2]]"
3a8d65eb8e1dbb995981a0e02d86ebf3feab107a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"an adversarial loss ($\ell _{adv}$) for each model as in the baseline, a cycle consistency loss ($\ell _{cycle}$) on each side","[[    0   260 37930 27774   872  1358 37457  1641 18134 45152 28006 24303
   1629    43    13   349  1421    25    11     5 18043     6    10  4943
  12787   872  1358 37457  1641 18134 45152 23060 24303  1629    43    15
    349   526     2]]"
b2b0321b0aaf58c3aa9050906ade6ef35874c5c1,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," $150,000$ tweets",[[   0   68 6115    6  151 1629 6245    2]]
221e9189a9d2431902d8ea833f486a38a76cbd8e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The average number of utterances per dialog is about 23 ,"[[    0   133   674   346     9 18672  5332   228 25730    16    59   883
   1437     2]]"
57fdb0f6cd91b64a000630ecb711550941283091,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
41e300acec35252e23f239772cecadc0ea986071,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Multilingual Neural Machine Translation Models,[[    0 45287 41586 44304 14969 41737 32146     2]]
bf2ebc9bbd4cbdf8922c051f406effc97fd16e54,0.0,Entailment,[[    2     0 30495  3760  1757     2]],two,[[   0 7109    2]]
7aaaf7bff9947c6d3b954ae25be87e6e1c49db6d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
85a7dbf6c2e21bfb7a3a938381890ac0ec2a19e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"English$\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)","[[    0 35007  1629 37457  4070 17214    68 39251    73 27709 14566     9
      5  6186  4014    12   347 42168     6   287   943   414     6    52
    304    10  3344     9   285     8 14101   414    13    59   545   153
   3645 15029    13  2370    12 39251    36 16040    12   243    43     8
     68   306     4   306  1629   153   305 11674  1570  3645 15029    13
      5  2370    12 27709    36 16040    12 13365    43     2]]"
41bff17f7d7e899c03b051e20ef01f0ebc5c8bb1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],ROUGE BIBREF29 and METEOR BIBREF30,"[[    0   500  5061  8800   163  8863 45935  2890     8 30782   717  3411
    163  8863 45935   541     2]]"
1083ec9a2a33f7fe2b6b51bbcebd2d9aec4b4de2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],is to minimize the negative likelihood of the aligned unanswerable question $\tilde{q}$ given the answerable question $q$ and its corresponding paragraph $p$ that contains the answer ,"[[    0   354     7 15925     5  2430 11801     9     5 14485   542 27740
    868   864 49959    90 30880 45152  1343 24303  1629   576     5  1948
    868   864    68  1343  1629     8    63 12337 17818    68   642  1629
     14  6308     5  1948  1437     2]]"
0c29d08f766b06ceb2421aa402e71a2d65a5a381,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Convolutional Neural Network (CNN),[[    0  9157   705 23794   337 44304  3658    36 16256    43     2]]
e015d033d4ee1c83fe6f192d3310fb820354a553,0.0,Entailment,[[    2     0 30495  3760  1757     2]],BIBREF8 a refined collection of tweets gathered from twitter,"[[    0  5383   387 45935   398    10 15616  2783     9  6245  4366    31
   7409     2]]"
33957fde72f9082a5c11844e7c47c58f8029c4ae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Freebase,[[    0 18074 11070     2]]
356e462f7966e30665a387ed7a9ad2e830479da6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The dataset was created by using translations provided by Tatoeba and OpenSubtitles BIBREF16.,"[[    0   133 41616    21  1412    30   634 41762  1286    30   255  3938
   3209   102     8  2117 23055    90 46913   163  8863 45935  1549     4
      2]]"
a5b67470a1c4779877f0d8b7724879bbb0a3b313,0.0,Entailment,[[    2     0 30495  3760  1757     2]],micro-averaged F1,[[    0 35228    12  9903  4628   274   134     2]]
ac148fb921cce9c8e7b559bba36e54b63ef86350,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The same 2K set from Gigaword used in BIBREF7,"[[    0   133   276   132   530   278    31 14448  1584  3109   341    11
    163  8863 45935   406     2]]"
447eb98e602616c01187960c9c3011c62afd7c27,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Table TABREF10 displays the twenty resulting topics,[[    0 41836   255  4546 45935   698  8612     5 10328  5203  7614     2]]
04cab3325e20c61f19846674bf9a2c46ea60c449,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Wav2vec BIBREF22, a fully-supervised system using all labeled data","[[    0   771  1469   176 25369   163  8863 45935  2036     6    10  1950
     12 16101 25376   467   634    70 16274   414     2]]"
64c7545ce349265e0c97fd6c434a5f8efdc23777,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization","[[    0   574   991  9244  1938    16   626    30    41  3827 19645 38954
    661   147 24684 21349    32  4760     6     8  2084  5471   281    32
   1286    19   455  2269  1043  3961  1938     2]]"
cd1034c183edf630018f47ff70b48d74d2bb1649,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
b686e10a725254695821e330a277c900792db69f,4.1667,Entailment,[[    2     0 30495  3760  1757     2]]," represent each word with an expressive multimodal distribution, for multiple distinct meanings, entailment, heavy tailed uncertainty, and enhanced interpretability. For example, one mode of the word `bank' could overlap with distributions for words such as `finance' and `money', and another mode could overlap with the distributions for `river' and `creek'.","[[    0  3594   349  2136    19    41 36340 27250  1630   337  3854     6
     13  1533 11693 39314     6 31648  1757     6  2016   326 13355  4983
      6     8  9094 18107  4484     4   286  1246     6    65  5745     9
      5  2136 22209  5760   108   115 27573    19 26070    13  1617   215
     25 22209   506 13598   108     8 22209 17479  3934     8   277  5745
    115 27573    19     5 26070    13 22209 28199   108     8 22209   438
  18230  2652     2]]"
16fa6896cf4597154363a6c9a98deb49fffef15f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
e0b7acf4292b71725b140f089c6850aebf2828d2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Word alignments are generated for parallel text, and aligned words are assumed to also share AMR node alignments.","[[    0 44051 12432  2963    32  5129    13 12980  2788     6     8 14485
   1617    32  9159     7    67   458  3326   500 37908 12432  2963     4
      2]]"
48fb76ae9921c9d181f65afc63a42af8ba3bc519,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"large raw Czech corpora available from the LINDAT/CLARIN repository, Czech Wikipedia","[[    0 11802  6087  9096 22997   102   577    31     5   226 13796  2571
     73  7454  2747  2444 30076     6  9096 28274     2]]"
9c2f306044b3d1b3b7fdd05d1c046e887796dd7a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SemEval 2016 Task 6 BIBREF7, Stance Sentiment Emotion Corpus (SSEC) BIBREF15","[[    0 37504   717  6486   336 12927   231   163  8863 45935   406     6
    312  2389 12169  8913  3676 19187 28556    36  8108  3586    43   163
   8863 45935   996     2]]"
785c054f6ea04701f4ab260d064af7d124260ccc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SearchSnippets, StackOverflow, Biomedical","[[    0 39954 37790  5600  2580     6 31197 10777 19322     6  6479 39820
      2]]"
40c2bab4a6bf3c0628079fcf19e8b52f27f51d98,0.0,Entailment,[[    2     0 30495  3760  1757     2]],using generative process,[[    0 10928 20181  3693   609     2]]
400efd1bd8517cc51f217b34cbf19c75d94b1874,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e1132564b0dd916a522e7690bc7719d2bba3fe68,0.0,Entailment,[[    2     0 30495  3760  1757     2]],we integrate object-pairs with global representation and make a pair-wise inference to detect the relationship among the segments,"[[    0  1694 13997  7626    12   642 18022    19   720  8985     8   146
     10  1763    12 10715 42752     7 10933     5  1291   566     5  5561
      2]]"
30db81df46474363d5749d7f6a94b7ef95cd3e01,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Twitter, Yelp reviews and movie reviews",[[    0 22838     6 29730  6173     8  1569  6173     2]]
5e9732ff8595b31f81740082333b241d0a5f7c9a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],on diversity 6.87 and on relevance 4.6 points higher,"[[    0   261  5845   231     4  5677     8    15 21623   204     4   401
    332   723     2]]"
4547818a3bbb727c4bb4a76554b5a5a7b5c5fedb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words),"[[    0  6070   763    12  5481   414  1881    36  1866   330  1617     9
   1058   414    43     8     5   455    38   771 11160   565   501  1058
  42168    36   246     4   176   448  1617    43     2]]"
071bcb4b054215054f17db64bfd21f17fd9e1a80,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
87357448ce4cae3c59d4570a19c7a9df4c086bd8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The most simple one is to directly apply a CNN layer after the embedding layer to obtain blended contextual representations. Then a GRU layer is applied afterward.,"[[    0   133   144  2007    65    16     7  2024  3253    10  3480 10490
     71     5 33183 11303 10490     7  6925 25630 37617 30464     4  1892
     10  8837   791 10490    16  5049 14723     4     2]]"
14e78db206a8180ea637774aa572b073e3ffa219,0.0,Entailment,[[    2     0 30495  3760  1757     2]],RNN encoders,[[    0   500 20057  9689  1630   268     2]]
975085e3b6679cc644fdd6ad11b7c2d1261a2dc6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
b2fab9ffbcf1d6ec6d18a05aeb6e3ab9a4dbf2ae,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL).","[[    0 42702     6    84  3685  3441  3092     7 22209 18790  1235   108
   1195    87 25409    12 35151   106    19   335     4   152 28052   256
   5199    25    10 29698   568    12  5349   936   524 30743     7 37700
   2239    36 17868   322     2]]"
ff307b10e56f75de6a32e68e25a69899478a13e4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"logistic regression (LR), recurrent neural network (RNN) BIBREF35, convolutional neural network (CNN) BIBREF36 and Google BERT BIBREF37","[[    0 12376  5580 39974    36 33919   238 35583 26739  1546    36   500
  20057    43   163  8863 45935  2022     6 15380 23794   337 26739  1546
     36 16256    43   163  8863 45935  3367     8  1204   163 18854   163
   8863 45935  3272     2]]"
42be49b883eba268e3dbc5c3ff4631442657dcbb,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," At last, our model is evaluated on two benchmark treebanks for both constituent and dependency parsing. The empirical results show that our parser reaches new state-of-the-art for all parsing tasks.","[[    0   497    94     6    84  1421    16 15423    15    80  5437  3907
  27045    13   258 31350     8 31492 46563     4    20 40150   775   311
     14    84 48946 11541    92   194    12  1116    12   627    12  2013
     13    70 46563  8558     4     2]]"
70c2dc170a73185c9d1a16953f85aca834ead6d3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Mean Average Precision,[[    0  5096   260  8317 29484     2]]
21cbcd24863211b02b436f21deaf02125f34da4c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Couples Therapy Corpus (CoupTher) BIBREF21,"[[    0   347  1438 12349 25889 28556    36   347 18615 43958    43   163
   8863 45935  2146     2]]"
e74ba39c35af53d3960be5a6c86eddd62cef859f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],IR methods perform better than the best neural models,[[    0  5216  6448  3008   357    87     5   275 26739  3092     2]]
abebf9c8c9cf70ae222ecb1d3cabf8115b9fc8ac,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"political events such as elections, corruption cases or justice decisions","[[    0 17522  1061   215    25  1727     6  3198  1200    50  2427  2390
      2]]"
da4d25dd9de09d16168788bb02ad600f5b0b3ba4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"single head, disabling a whole layer, that is, all 12 heads in a given layer","[[    0 25382   471     6 41625    10  1086 10490     6    14    16     6
     70   316  3885    11    10   576 10490     2]]"
2fa0b9d0cb26e1be8eae7e782ada6820bc2c037f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],97.32%,[[   0 6750    4 2881  207    2]]
4d30c2223939b31216f2e90ef33fe0db97e962ac,0.0,Entailment,[[    2     0 30495  3760  1757     2]],11'248,[[    0  1225   108 28654     2]]
3c34187a248d179856b766e9534075da1aa5d1cf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the results obtained on development and test set (F1 = 89.60, F1 = 87.82) and the results on the supplemental test set (F1 = 71.49)","[[    0   627   775  4756    15   709     8  1296   278    36   597   134
   5457  8572     4  2466     6   274   134  5457  8176     4  6551    43
      8     5   775    15     5 27634  1296   278    36   597   134  5457
   6121     4  3414    43     2]]"
0b92fb692feb35d4b4bf4665f7754d283d6ad5f3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"results that for the in-domain training case, our models BERT-ADA Lapt and BERT-ADA Rest achieve performance close to state-of-the-art on the laptops dataset, new state-of-the-art on the restaurants dataset with accuracies of $79.19\%$ and $87.14\%$, respectively.","[[    0 38547    14    13     5    11    12 46400  1058   403     6    84
   3092   163 18854    12 15779   226 14455     8   163 18854    12 15779
   8426  3042   819   593     7   194    12  1116    12   627    12  2013
     15     5 15962 41616     6    92   194    12  1116    12   627    12
   2013    15     5  4329 41616    19 49060 15668     9    68  5220     4
   1646 37457   207  1629     8    68  5677     4  1570 37457   207 47110
   4067     4     2]]"
38c74ab8292a94fc5a82999400ee9c06be19f791,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"It contains 106,350 documents",[[    0   243  6308 13442     6 10056  2339     2]]
17988d65e46ff7d756076e9191890aec177b081e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
442f8da2c988530e62e4d1d52c6ec913e3ec5bf1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Cebuano and Mandarin, Tagalog and Cantonese","[[    0   347  3209   257  2601     8 33830     6 12650 31263     8 10909
   6909   242     2]]"
2a1e6a69e06da2328fc73016ee057378821e0754,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Exact matches to the entity string and predictions from a coreference resolution system,"[[    0  9089  7257  2856     7     5 10014  6755     8 12535    31    10
   2731 23861  3547   467     2]]"
fe578842021ccfc295209a28cf2275ca18f8d155,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"RNNs, CNNs, Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel","[[    0   500 20057    29     6  3480    29     6  7300  2088  1501   293
     19  1587  6406  4966  9210  8223     6   449    12  3998  4193  2961
      6   208 20954    19 26956 34751     2]]"
14c0328e8ec6360a913b8ecb3e50cb27650ff768,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"all of our models outperform the random baseline by a wide margin, he best F1 score in content words more than doubles that of the random baseline (0.286 vs. 0.116)","[[    0  1250     9    84  3092  9980  3899     5  9624 18043    30    10
   1810  2759     6    37   275   274   134  1471    11  1383  1617    55
     87  8862    14     9     5  9624 18043    36   288     4 30168  1954
      4   321     4 22418    43     2]]"
f01a88e15ef518a68d8ca2bec992f27e7a3a6add,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"DISPLAYFORM0, DISPLAYFORM0 DISPLAYFORM1","[[    0 37056 43784 38036   288     6 15421 43784 38036   288 15421 43784
  38036   134     2]]"
b8cee4782e05afaeb9647efdb8858554490feba5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
b464bc48f176a5945e54051e3ffaea9a6ad886d7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Slot filling, we consider the actions that a user might perform via apps on their phone, The corresponding actions are booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant","[[    0 48969  8794     6    52  1701     5  2163    14    10  3018   429
   3008  1241  3798    15    49  1028     6    20 12337  2163    32 12666
     10  2524     6 22175    10   184     6  2159  2353  3308     6     8
    442    10 17246    23    10  2391     2]]"
4d8ca3f7aa65dcb42eba72acf3584d37b416b19c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"A mechanism ${f}$ is a random function that takes a dataset $\mathcal {N}$ as input, and outputs a random variable ${f}(\mathcal {N})$.","[[    0   250  9562 48982   506 24303  1629    16    10  9624  5043    14
   1239    10 41616 49959 40051 11762 25522   487 24303  1629    25  8135
      6     8 39512    10  9624 15594 48982   506 24303 49921 40051 11762
  25522   487 49424 48292     2]]"
9d336c4c725e390b6eba8bb8fe148997135ee981,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
22c36082b00f677e054f0f0395ed685808965a02,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
1546356a8c5893dc2d298dcbd96d0307731dd54d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The baseline model BIBREF5 we compare with regards the output space of the model as a subset INLINEFORM2 where INLINEFORM3 is the set of all tag sets seen in this training data.,"[[    0   133 18043  1421   163  8863 45935   245    52  8933    19 11246
      5  4195   980     9     5  1421    25    10 37105  2808 28302 38036
    176   147  2808 28302 38036   246    16     5   278     9    70  6694
   3880   450    11    42  1058   414     4     2]]"
e1c681280b5667671c7f78b1579d0069cba72b0e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Ternary Trans-CNN , Hybrid multi-channel CNN and LSTM","[[    0   565  3281  1766  5428    12 16256  2156 25367  3228    12 27681
   3480     8   226  4014   448     2]]"
753990d0b621d390ed58f20c4d9e4f065f0dc672,0.0,Entailment,[[    2     0 30495  3760  1757     2]],a vocabulary of positive and negative predicates that helps determine the polarity score of an event,"[[    0   102 32644     9  1313     8  2430 12574 23020    14  2607  3094
      5  8385 21528  1471     9    41   515     2]]"
6cd874c4ae8e70f3c98c7176191c13a7decfbc45,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BERT-ADA, BERT-PT, AEN-BERT, SDGCN-BERT","[[    0 11126   565    12 15779     6   163 18854    12 10311     6    83
   2796    12 11126   565     6 12723 11961   487    12 11126   565     2]]"
293e9a0b30670f4f0a380e574a416665a8c55bc2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
49b38189b8336ce41d0f0b4c5c9459722736e15b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
bb97537a0a7c8f12a3f65eba73cefa6abcd2f2b2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.
","[[    0 47515  1822    33 12246   723  1162     9  3708  3018 16580    87
     55  4375  1822     4   901 16141  1822  8483 30389   723  3708 16580
   1162    87    55 14569  1822     4   345    16    67    10   670  1313
   1291   227    10   435    18 29713 24414     8     5   674   346     9
    377    14    10  3018    40  1095    11    14   435   111    10   765
     12  1279  2904  6373    13  3708 16580 19303    88  1181    12  1279
   4921     8  3649    14   251    12  1279  3018 16580   429    28  5025
   3185    30     5  5239     7    61    10   435 15342  1639  5808  1383
      4 50118     2]]"
2bc0bb7d3688fdd2267c582ca593e2ce72718a91,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Wiktionary,[[    0 45445 24659  1766     2]]
fa5357c56ea80a21a7ca88a80f21711c5431042c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],36,[[   0 3367    2]]
a725246bac4625e6fe99ea236a96ccb21b5f30c6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Amazon Conversational Bot Toolkit, natural language understanding (NLU) (nlu) module, dialog manager, knowledge bases, natural language generation (NLG) (nlg) module, text to speech (TTS) (tts)","[[    0 25146 38119  5033 13954 25251 23199     6  1632  2777  2969    36
  27027   791    43    36   282  6487    43 20686     6 25730  1044     6
   2655  7531     6  1632  2777  2706    36 27027   534    43    36 35760
    571    43 20686     6  2788     7  1901    36   565  2685    43    36
     90  1872    43     2]]"
22b8836cb00472c9780226483b29771ae3ebdc87,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data.,"[[    0  1213 49161    49  2136     8 10014 33183   417  1033    19 44493
   1198    12 23830    81    10   739 42168     9 35237 14286   196   414
      4     2]]"
922f1b740f8b13fdc8371e2a275269a44c86195e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
ddd6ba43c4e1138156dd2ef03c25a4c4a47adad0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
3f5f74c39a560b5d916496e05641783c58af2c5d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Random perturbation of Wikipedia sentences using mask-filling with BERT, backtranslation and randomly drop out","[[    0 45134 32819 13157  1258     9 28274 11305   634 11445    12   506
   7491    19   163 18854     6   124 48235     8 22422  1874    66     2]]"
4a61260d6edfb0f93100d92e01cf655812243724,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"machine translation, statistical machine, sentiment analysis",[[    0 37556 19850     6 17325  3563     6  5702  1966     2]]
eced6a6dffe43c28e6d06ab87eed98c135f285a3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
e737cfe0f6cfc6d3ac6bec32231d9c893bfc3fc9,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," filter deletes all KB triples where the correct answer (e.g., Apple) is a case-insensitive substring of the subject entity name (e.g., Apple Watch), person name filter uses cloze-style questions to elicit name associations inherent in BERT, and deletes KB triples that correlate with them","[[    0 14929 44714   293    70 29006  7182 12349   147     5  4577  1948
     36   242     4   571   482  1257    43    16    10   403    12  1344
  43634 32493  4506     9     5  2087 10014   766    36   242     4   571
    482  1257  3075   238   621   766 14929  2939 42771  2158    12  5827
   1142     7 39581   766 14697 17886    11   163 18854     6     8 44714
    293 29006  7182 12349    14 42407    19   106     2]]"
974868e4e22f14766bcc76dc4927a7f2795dcd5e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
82bcacad668351c0f81bd841becb2dbf115f000e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures","[[    0   102  1907     9 20511   716    15  2136  5010     6  3441     5
   1460     7  5604     5 10405   227 36912  3569     8 45774 28201  6609
      2]]"
f6380c60e2eb32cb3a9d3bca17cf4dc5ae584eca,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Training embeddings from small-corpora can increase the performance of some tasks,"[[    0 44466 33183   417  1033    31   650    12  7215 20523    64   712
      5   819     9   103  8558     2]]"
acc8d9918d19c212ec256181e51292f2957b37d7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],This approach considers related images,[[   0  713 1548 9857 1330 3156    2]]
9e391c8325b48f6119ca4f3d428b1b2b037f5c13,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WER can reflect our model's effectiveness in generating questions that are similar to those of SQuAD, WER can be used for initial analyses","[[    0 45117    64  4227    84  1421    18 12833    11 10846  1142    14
     32  1122     7   167     9   208 12444  2606     6   305  2076    64
     28   341    13  2557 20070     2]]"
eecf62e18a790bcfdd8a56f0c4f498927ff2fb47,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"softly augments a randomly chosen word in a sentence by its contextual mixture of multiple related words, replacing the one-hot representation of a word by a distribution provided by a language model over the vocabulary","[[    0 24810   352 26713  2963    10 22422  4986  2136    11    10  3645
     30    63 37617 12652     9  1533  1330  1617     6  8119     5    65
     12 10120  8985     9    10  2136    30    10  3854  1286    30    10
   2777  1421    81     5 32644     2]]"
516626825e51ca1e8a3e0ac896c538c9d8a747c8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
39f8db10d949c6b477fa4b51e7c184016505884f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity,"[[    0  1409 26005 19771  3115     7 43384  8385 21528    31  5018 12574
  23020     7   507  5702  8385 21528     2]]"
993b896771c31f3478f28112a7335e7be9d03f21,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"A network, whose learned functions satisfy a certain equation. The  network contains RNN cells with either nested internal memories or dependencies that extend temporally beyond the immediately previous hidden state.","[[    0   250  1546     6  1060  2435  8047 15332    10  1402 19587     4
     20  1437  1546  6308   248 20057  4590    19  1169 46902  3425  6180
     50 45371    14  4442 39216  2368  1684     5  1320   986  7397   194
      4     2]]"
a2a3af59f3f18a28eb2ca7055e1613948f395052,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Twitter,[[    0 22838     2]]
265c9b733e4dfffb76acfbade4c0c9b14d3ccde1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male), data was aligned at the phone-level, 121fps with a 135 field of view, single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames)","[[    0 38972 30630  1538 21979     8 29976   414    31  4893  3700  2623
    408     6  5180   195    12  1092   107   793    36  2983  2182     6
    974  2943   238   414    21 14485    23     5  1028    12  4483     6
  18872 36151    19    10 16157   882     9  1217     6   881 29976  5120
  10726     9 38554 23930  2886    31   349     9     5  5549 14194  2301
     36  5449  1178 31185  6087 16420    43     2]]"
0a4e82dc3728be0bd0325bfe944e7e7de0b98b22,0.0,Entailment,[[    2     0 30495  3760  1757     2]],human representative to review the IVA chat history and resume the failed task,"[[    0 19003  4915     7  1551     5 10831   250  7359   750     8  6654
      5  1447  3685     2]]"
43878a6a8fc36aaae29d95815355aaa7d25c3b53,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the personalized bAbI dialog dataset,[[    0   627 17159   741 13112   100 25730 41616     2]]
f4238f558d6ddf3849497a130b3a6ad866ff38b3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (formula 1) bias(q, a, b) = cos(a, q)  cos(b, q)
Bias is calculated as substraction of cosine similarities of question and some answer for two opposite answers.","[[    0 33683    19  1383  1716    35    36  3899  5571   112    43  9415
   1640  1343     6    10     6   741    43  5457 12793  1640   102     6
   2231    43 42736 12793  1640   428     6  2231    43 50118   387  5003
     16  9658    25 32493 22870     9 12793   833 20097     9   864     8
    103  1948    13    80  5483  5274     4     2]]"
bdc1f37c8b5e96e3c29cc02dae4ce80087d83284,0.0,Entailment,[[    2     0 30495  3760  1757     2]],unweighted average recall (UAR) metric,[[    0   879  4301   196   674  6001    36   791  2747    43 14823     2]]
66d743b735ba75589486e6af073e955b6bb9d2a4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Attn seq2seq, Ptr-UNK, KV Net, Mem2Seq, DSR","[[    0 28062   282 48652   176 47762     6   221  4328    12  4154   530
      6   229   846  5008     6 20207   176 14696  1343     6   211 17973
      2]]"
ba48c095c496d01c7717eaa271470c3406bf2d7c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Chinese,[[    0 24727     2]]
58edc6ed7d6966715022179ab63137c782105eaf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the hybrid model MinAvgOut + RL,[[    0   627  9284  1421  3635 48929 14944  2055 28483     2]]
f229069bcb05c2e811e4786c89b0208af90d9a25,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
9ac923be6ada1ba2aa20ad62b0a3e593bb94e085,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For evaluating the interpretability, we use $Coherence@k$ (Equation 6 ) , automated and manual word intrusion tests.","[[    0  2709 15190     5 18107  4484     6    52   304    68  8739 40584
   1039   330  1629    36 28568  1258   231  4839  2156 11554     8 12769
   2136 32028  3457     4     2]]"
c8b9b962e4d40c50150b2f8873a4004f25398464,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"High scores to semantically opposite translations/summaries, Low scores to semantically related translations/summaries and High scores to unintelligible translations/summaries.","[[    0 18522  4391     7  9031 38600  5483 41762    73    29 16598  5119
      6  6207  4391     7  9031 38600  1330 41762    73    29 16598  5119
      8   755  4391     7 45467 37448  4748 41762    73    29 16598  5119
      4     2]]"
6c80bc3ed6df228c8ca6e02c0a8a1c2889498688,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
993ee7de848ab6adfe02fa728b3a2c896238859b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
dfb0351e8fa62ceb51ce77b0f607885523d1b8e8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
9174aded45bc36915f2e2adb6f352f3c7d9ada8b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SST-2 (Stanford Sentiment Treebank, version 2), Snips","[[    0   104  4014    12   176    36 36118  1891 12169  8913 11077  5760
      6  1732   132   238  7500  7418     2]]"
7182f6ed12fa990835317c57ad1ff486282594ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"it systematically holds out inputs in the training set containing basic primitive verb, ""jump"", and tests on sequences containing that verb.","[[    0   405 28210  3106    66 16584    11     5  1058   278  8200  3280
  36005 33760     6    22 43750  1297     8  3457    15 26929  8200    14
  33760     4     2]]"
0154d8be772193bfd70194110f125813057413a4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"mean-pooling their outputs (AVG), concatenating the entity and its name with a slash symbol (CONCAT)","[[    0 43348    12 10416   154    49 39512    36 10612   534   238 10146
  26511  1295     5 10014     8    63   766    19    10 18304  7648    36
  15299   347  2571    43     2]]"
fb96c0cd777bb2961117feca19c6d41bfd8cfd42,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"idebate.com, debatewise.org, procon.org","[[    0  1949   428   877     4   175     6  2625 10715     4  1957     6
   1759  3865     4  1957     2]]"
d9eacd965bbdc468da522e5e6fe7491adc34b93b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Support Vector Machines (SVM), Gaussian Naive Bayes, Multinomial Naive Bayes, Decision Trees, Random Forests and a Maximum Entropy classifier","[[    0 38873 40419 28413    36   104 20954   238 10160 42472  7300  2088
   1501   293     6 14910   179 48866  7300  2088  1501   293     6 30300
  32073     6 34638  6311  5019     8    10 35540  9860 47145  1380 24072
      2]]"
c8031c1629d270dedc3b0c16dcb7410524ff1bab,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"restricted copy mechanism to ensure literally honestness, coverage mechanism to alleviate the under extraction and over extraction problem, and gated dependency attention mechanism to incorporate dependency information","[[    0 42751  5375  9562     7  1306  5909  5322  1825     6  1953  9562
      7 19893     5   223 23226     8    81 23226   936     6     8   821
   1070 31492  1503  9562     7 14518 31492   335     2]]"
a1064307a19cd7add32163a70b6623278a557946,0.0,Entailment,[[    2     0 30495  3760  1757     2]],908456 unique words are available in collected corpus.,"[[    0   466  3669 34781  2216  1617    32   577    11  4786 42168     4
      2]]"
4bdad5a20750c878d1a891ef255621f6172b6a79,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we connect each latent variable with a word in the vocabulary, thus each latent variable has an exact semantic meaning.","[[    0  1694  4686   349 42715 15594    19    10  2136    11     5 32644
      6  4634   349 42715 15594    34    41  6089 46195  3099     4     2]]"
4140d8b5a78aea985546aa1e323de12f63d24add,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
ed11b4ff7ca72dd80a792a6028e16ba20fccff66,0.0,Entailment,[[    2     0 30495  3760  1757     2]],they are available in the Visual Genome dataset,[[    0 10010    32   577    11     5 25878  4380  4399 41616     2]]
4dad15fee1fe01c3eadce8f0914781ca0a6e3f23,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They exclude slot-specific parameters and incorporate better feature representation of user utterance and dialogue states using syntactic information and convolutional neural networks (CNN).,"[[    0  1213 18717  8534    12 14175 17294     8 14518   357  1905  8985
      9  3018 18672  2389     8  6054   982   634 45774 28201   335     8
  15380 23794   337 26739  4836    36 16256   322     2]]"
526ae24fa861d52536b66bcc2d2ddfce483511d6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],relative WER improvement of 10%.,[[    0 46624   305  2076  3855     9   158  2153     2]]
efc65e5032588da4a134d121fe50d49fe8fe5e8c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Multitask learning is used for the task of predicting relevance of a comment on a different question to a given question, where the supplemental tasks are predicting relevance between the questions, and between the comment and the corresponding question","[[    0 45287   405  4970  2239    16   341    13     5  3685     9 15924
  21623     9    10  1129    15    10   430   864     7    10   576   864
      6   147     5 27634  8558    32 15924 21623   227     5  1142     6
      8   227     5  1129     8     5 12337   864     2]]"
b6a4ab009e6f213f011320155a7ce96e713c11cf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
c383fa9170ae00a4a24a8e39358c38395c5f034b,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," words found in the control word lists are then removed, The remaining words, which represent the content","[[   0 1617  303   11    5  797 2136 8204   32  172 2928    6   20 2405
  1617    6   61 3594    5 1383    2]]"
8a871b136ccef78391922377f89491c923a77730,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Stanford NER, BiLSTM+CRF, LSTM+CNN+CRF, T-NER and BiLSTM+CNN+Co-Attention","[[    0 36118  1891   234  2076     6  6479   574  4014   448  2744  9822
    597     6   226  4014   448  2744 16256  2744  9822   597     6   255
     12 20196     8  6479   574  4014   448  2744 16256  2744  8739    12
  28062 19774     2]]"
9dc844f82f520daf986e83466de0c84d93953754,0.0,Entailment,[[    2     0 30495  3760  1757     2]],MultiNLI BIBREF15 and SNLI BIBREF16 ,"[[    0 46064   487 27049   163  8863 45935   996     8 13687 27049   163
   8863 45935  1549  1437     2]]"
f33236ebd6f5a9ccb9b9dbf05ac17c3724f93f91,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"balanced accuracy, i.e., the average of the three accuracies on each class","[[    0 29683  8611     6   939     4   242   482     5   674     9     5
    130 49060 15668    15   349  1380     2]]"
785eb3c7c5a5c27db14006ac357299ed1216313a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],LASSO optimization problem,[[    0   574 17042   673 25212   936     2]]
fbe22e133fa919f06abd8afbed3395af51d2bfef,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
2a950ede24b26a45613169348d5db9176fda4f82,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
18412237f7faafc6befe975d5bcd348e2b499b55,0.0,Entailment,[[    2     0 30495  3760  1757     2]],$4th$,[[   0 1629  306  212 1629    2]]
d0c636fa9ef99c4f44ab39e837a680217b140269,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
8ea4bd4c1d8a466da386d16e4844ea932c44a412,0.0,Entailment,[[    2     0 30495  3760  1757     2]],A parallel corpus where the source is an English expression of code and the target is Python code.,"[[    0   250 12980 42168   147     5  1300    16    41  2370  8151     9
   3260     8     5  1002    16 31886  3260     4     2]]"
6b2fbc1c083491a774233f9edf8f76bd879418df,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
6adde6bc3e27a32eac5daa57d30ab373f77690be,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
7fbbe191f4d877cc6af89c00fcfd5b5774d2a2bb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
6bff681f1f6743ef7aa6c29cc00eac26fafdabc2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
7aae4533dbf097992f23fb2e0574ec5c891ca236,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BTEC corpus, the CSTAR03 and IWSLT04 held out sets, the NIST2008 Open Machine Translation Campaign","[[    0 13269  3586 42168     6     5 24425  2747  3933     8    38   771
  11160   565  3387   547    66  3880     6     5   234 11595 27418  2117
  14969 41737 11068     2]]"
982d375378238d0adbc9a4c987d633ed16b7f98f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Twitter, Reddit, Online Dialogues",[[    0 22838     6  6844     6  5855 28985  2154  3663     2]]
b5e883b15e63029eb07d6ff42df703a64613a18a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],using topic modeling model Latent Dirichlet Allocation (LDA),"[[    0 10928  5674 19039  1421  9882  1342 32024  1725  2716   404 15644
     36   574  3134    43     2]]"
4a2248e1c71c0b0183ab0d225440cae2da396b8d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Evaluation Dataset of Japanese Lexical Simplification kodaira,"[[    0   717  6486  9762 16673   281   594     9  2898 14786  3569 39753
   5000   449  1630  2456   102     2]]"
fd7f13b63f6ba674f5d5447b6114a201fe3137cb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],english language,[[    0 47120  2777     2]]
61272b1d0338ed7708cf9ed9c63060a6a53e97a2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],accuracy of 82.6%,[[    0  7904 45386     9  7383     4   401   207     2]]
a4ff1b91643e0c8a0d4cc1502d25ca85995cf428,0.0,Entailment,[[    2     0 30495  3760  1757     2]],two surveys by two groups - school students and meteorologists to draw on a map a polygon representing a given geographical descriptor,"[[    0  7109 12092    30    80  1134   111   334   521     8 12505 10974
      7  2451    15    10  5456    10 11424 14601  4561    10   576 20456
  47220     2]]"
5e41516a27c587aa2f80dba8cf4c3f616174099b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],same sentences after applying character level perturbations,[[    0 41690 11305    71  9889  2048   672 32819 13157  1635     2]]
938cf30c4f1d14fa182e82919e16072fdbcf2a82,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the average volatility of all utterances,[[    0   627   674  4975     9    70 18672  5332     2]]
11c5b12e675cfd8d1113724f019d8476275bd700,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
73633afbefa191b36cca594977204c6511f9dad4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Not at the moment, but summaries can be additionaly extended with this annotations.","[[    0  7199    23     5  1151     6    53 32933  5119    64    28   943
    219  3112    19    42 47234     4     2]]"
c431c142f5b82374746a2b2f18b40c6874f7131d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WMT18 EnDe bitext, WMT16 EnRo bitext, WMT15 EnFr bitext, We perform our experiments on WMT18 EnDe bitext, WMT16 EnRo bitext, and WMT15 EnFr bitext respectively. We use WMT Newscrawl for monolingual data (2007-2017 for De, 2016 for Ro, 2007-2013 for En, and 2007-2014 for Fr). For bitext, we filter out empty sentences and sentences longer than 250 subwords. We remove pairs whose whitespace-tokenized length ratio is greater than 2. This results in about 5.0M pairs for EnDe, and 0.6M pairs for EnRo. We do not filter the EnFr bitext, resulting in 41M sentence pairs.","[[    0   771 11674  1366  2271 13365 10970 11483     6   305 11674  1549
   2271 27110 10970 11483     6   305 11674   996  2271 29220 10970 11483
      6   166  3008    84 15491    15   305 11674  1366  2271 13365 10970
  11483     6   305 11674  1549  2271 27110 10970 11483     6     8   305
  11674   996  2271 29220 10970 11483  4067     4   166   304   305 11674
    188  3866 33889    13  6154 31992  5564   414    36 30741    12  3789
     13   926     6   336    13  3830     6  3010    12 10684    13  2271
      6     8  3010    12 16310    13  4967   322   286 10970 11483     6
     52 14929    66  5802 11305     8 11305  1181    87  5773  2849 30938
      4   166  3438 15029  1060 17942 18851    12 46657  1538  5933  1750
     16  2388    87   132     4   152   775    11    59   195     4   288
    448 15029    13  2271 13365     6     8   321     4   401   448 15029
     13  2271 27110     4   166   109    45 14929     5  2271 29220 10970
  11483     6  5203    11  3492   448  3645 15029     4     2]]"
9cba2ee1f8e1560e48b3099d0d8cf6c854ddea2e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The system benefits from filters of each size., features of multigranular phrases are extracted with variable-size convolution filters.","[[    0   133   467  1795    31 19214     9   349  1836   482  1575     9
   7268  1023  3917  8244 22810    32 27380    19 15594    12 10799 15380
  23794 19214     4     2]]"
c1f4d632da78714308dc502fe4e7b16ea6f76f81,0.0,Entailment,[[    2     0 30495  3760  1757     2]],French-English,[[    0 28586    12 35007     2]]
f4e17b14318b9f67d60a8a2dad1f6b506a10ab36,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Comparing BLEU score of model with and without attention,"[[    0 24699  5867   163  3850   791  1471     9  1421    19     8   396
   1503     2]]"
e3dc8689d8db31f04797f515fe224f6075f5cb16,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
9595bf228c9e859b0dc745e6c74070be2468d2cf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
b39f2249a1489a2cef74155496511cc5d1b2a73d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (Table 1)
Previous state-of-the art on same dataset: ResNet50 89% (6 languages), SVM-HMM 70% (4 languages)","[[    0 33683    19  1383  1716    35    36 41836   112    43 50118 44716
    194    12  1116    12   627  1808    15   276 41616    35  4787 15721
   1096  8572   207    36   401 11991   238   208 20954    12   725 16261
   1510   207    36   306 11991    43     2]]"
5065ff56d3c295b8165cb20d8bcfcf3babe9b1b8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU-3/4, ROUGE-2/L, CIDEr, SPICE, BERTScore","[[    0 30876   791    12   246    73   306     6   248  5061  8800    12
    176    73   574     6   230  2688 28012     6  6178  9292     6   163
  18854 36088     2]]"
2ec9c1590c96f17a66c7d4eb95dc5d3a447cb973,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"sampled all papers published in the Computer Science subcategories of Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Social and Information Networks (cs.SI), Computational Linguistics (cs.CL), Computers and Society (cs.CY), Information Retrieval (cs.IR), and Computer Vision (CS.CV), the Statistics subcategory of Machine Learning (stat.ML), and Social Physics (physics.soc-ph), filtered for papers in which the title or abstract included at least one of the words machine learning, classif*, or supervi* (case insensitive), filtered to papers in which the title or abstract included at least twitter or tweet (case insensitive)","[[    0 33243 18331    70  6665  1027    11     5 16772  4662  2849   438
  45885     9 27332  6558    36 11365     4 15238   238 14969 13807    36
  11365     4 38820   238  3574     8  3522 14641    36 11365     4  6850
    238 34710  5033   226 35308 16307    36 11365     4  7454   238 34710
    268     8  3930    36 11365     4   347   975   238  3522  9944  1069
  28017    36 11365     4  5216   238     8 16772 13396    36  6842     4
  35433   238     5 12064  2849 42747     9 14969 13807    36 24344     4
  10537   238     8  3574 31590    36  3792 33823     4 32460    12  3792
    238 33459    13  6665    11    61     5  1270    50 20372  1165    23
    513    65     9     5  1617    44    48 37556  2239    17    46     6
     44    48  4684  1594  3226    17    46     6    50    44    48 16101
   6873  3226    17    46    36 11173 29401   238 33459     7  6665    11
     61     5  1270    50 20372  1165    23   513    44    48  1556    17
     46    50    44    48    90 21210    17    46    36 11173 29401    43
      2]]"
b367b823c5db4543ac421d0057b02f62ea16bf9f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
da8bda963f179f5517a864943dc0ee71249ee1ce,0.0,Entailment,[[    2     0 30495  3760  1757     2]],4 layers,[[    0   306 13171     2]]
819d2e97f54afcc7cdb3d894a072bcadfba9b747,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CNN, TIME, 20 Newsgroups, and Reuters-21578","[[    0 16256     6 18034     6   291   491 36378     6     8  1201    12
  24355  5479     2]]"
8486e06c03f82ebd48c7cfbaffaa76e8b899eea5,0.0,Entailment,[[    2     0 30495  3760  1757     2]], hand-curated collection of complete inflection tables for 198 lemmata,"[[    0   865    12 17742  1070  2783     9  1498  4047 20576  9248    13
  30858  2084  5471  2186     2]]"
090f2b941b9c5b6b7c34ae18c2cc97e9650f1f0b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Information Retrieval,[[    0 38741  9944  1069 28017     2]]
3288a50701a80303fd71c8c5ede81cbee14fa2c7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
c58e60b99a6590e6b9a34de96c7606b004a4f169,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"dependency relation between two words, word sense",[[    0 45950  6761  9355   227    80  1617     6  2136  1472     2]]
26faad6f42b6d628f341c8d4ce5a08a591eea8c2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],508,[[    0 36911     2]]
aacb0b97aed6fc6a8b471b8c2e5c4ddb60988bf5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],one,[[   0 1264    2]]
5c0b8c1b649df1b07d9af3aa9154ac340ec8b81c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
e5ae8ac51946db7475bb20b96e0a22083b366a6d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
4d062673b714998800e61f66b6ccbf7eef5be2ac,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Moral Choice Machine computes the cosine similarity in a sentence embedding space of an arbitrary action embedded in question/answer pairs,"[[    0   448 15010 14431 14969  7753 11282     5 12793   833 37015    11
     10  3645 33183 11303   980     9    41 23501   814 14224    11   864
     73 27740 15029     2]]"
73abb173a3cc973ab229511cf53b426865a2738b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a deep neural network (DNN) architecture proposed in BIBREF24 , maximum entropy (MaxEnt) proposed in BIBREF23 type of discriminative model","[[    0   102  1844 26739  1546    36   495 20057    43  9437  1850    11
    163  8863 45935  1978  2156  4532 47382    36 19854 30495    43  1850
     11   163  8863 45935  1922  1907     9 40846   179  3693  1421     2]]"
7ce7edd06925a943e32b59f3e7b5159ccb7acaf6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],consistent increase in the validation loss after about 15 epochs,"[[    0 10998 21464   712    11     5 26567   872    71    59   379 43660
     29     2]]"
601e58a3d2c03a0b4cd627c81c6228a714e43903,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CAEVO,[[    0  4054 19896   673     2]]
1a678d081f97531d54b7122254301c20b3531198,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Wikidata, ReVerb, FB15K, TACRED","[[    0 45445   808  2186     6  1223 21119   428     6 13042   996   530
      6   255  2562 36015     2]]"
d3bb06d730efbedd30ec226fe8cf828a4773bf5c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR), Resource Description Framework (RDF)","[[    0 13716 12183  7732  9612  9301  3870  8428  4484  5187    36  8064
    406   274   725  5216   238 13877 33425 30763    36   500  9380    43
      2]]"
0d4aa05eb00d9dee74000ea5b21b08f693ba1e62,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Although cursing is frequent in tweets, they represent just 1.15% of all words used BIBREF21 . In contrast, we found 5.72% of all words posted by gang member accounts to be classified as a curse word, gang members talk about material things with terms such as got, money, make, real, need whereas ordinary users tend to vocalize their feelings with terms such as new, like, love, know, want, look, make, us","[[    0 13863 41142    16  7690    11  6245     6    51  3594    95   112
      4   996   207     9    70  1617   341   163  8863 45935  2146   479
     96  5709     6    52   303   195     4  4956   207     9    70  1617
   1278    30  5188   919  2349     7    28  8967    25    10 26020  2136
      6  5188   453  1067    59  1468   383    19  1110   215    25   300
      6   418     6   146     6   588     6   240  9641  7945  1434  3805
      7  7578  2072    49  6453    19  1110   215    25    92     6   101
      6   657     6   216     6   236     6   356     6   146     6   201
      2]]"
3fae289ab1fc023bce2fa4f1ce4d9f828074f232,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," restrict the content of each text to the abstract and conclusion of the original work, considered other parts of the original works such as introduction or discussion sections, extracted text portions are appropriate for the AV task, each original work was preprocessed manually, removed tables, formulas, citations, quotes and sentences that include non-language content such as mathematical constructs or specific names of researchers, systems or algorithms","[[    0 14058     5  1383     9   349  2788     7     5 20372     8  6427
      9     5  1461   173     6  1687    97  1667     9     5  1461  1364
    215    25  7740    50  3221  9042     6 27380  2788 14566    32  3901
     13     5 17307  3685     6   349  1461   173    21  1198 31931   196
  24704     6  2928  9248     6 37130     6 31173     6 11495     8 11305
     14   680   786    12 19527  1383   215    25 30412 43978    50  2167
   2523     9  2634     6  1743    50 16964     2]]"
1db37e98768f09633dfbc78616992c9575f6dba4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
cf8edc6e8c4d578e2bd9965579f0ee81f4bf35a9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WMT2014, WMT2016 and IWSLT-2014","[[    0   771 11674 16310     6   305 11674  9029     8    38   771 11160
    565    12 16310     2]]"
ead5dc1f3994b2031a1852ecc4f97ac5760ea977,0.0,Entailment,[[    2     0 30495  3760  1757     2]],14 categories,[[   0 1570 6363    2]]
3460393d6888dd34113fa0813a1b3a1514c66aa6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
5679fabeadf680e35a4f7b092d39e8638dca6b4d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
b921a1771ed0ba9dbeff9da000336ecf2bb38322,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
7e51490a362135267e75b2817de3c38dfe846e21,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," Local, shallow features based mostly on orthographic, word shape and n-gram features plus their context","[[    0  4004     6 16762  1575   716  2260    15 25691 25510     6  2136
   3989     8   295    12 28526  1575  2704    49  5377     2]]"
5c26388a2c0b0452d529d5dd565a5375fdabdb70,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Lurking Horror, Afflicted, Anchorhead, 9:05, TextWorld games","[[    0   574   710  7037 28719     6 11176 29637     6 29860   368  3628
      6   361    35  2546     6 14159 10988   426     2]]"
c0a11ba0f6bbb4c69b5a0d4ae9d18e86a4a8f354,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
1ccfd288f746c35006f5847297ab52020729f523,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"abuse, social, anxiety, PTSD, and financial",[[    0 33045     6   592     6  6882     6 24679     6     8   613     2]]
5fb348b2d7b012123de93e79fd46a7182fd062bd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"NELL-One, Wiki-One",[[    0  9009  6006    12  3762     6 45569    12  3762     2]]
994ac7aa662d16ea64b86510fcf9efa13d17b478,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
b093b440ae3cd03555237791550f3224d159d85b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"EMPATHETICDIALOGUES dataset, a dataset containing 1.5 million Twitter conversation, gathered by using Twitter API from customer care account of 62 brands across several industries, SEMAINE corpus BIBREF30","[[    0  5330 48468  3935  2371   495 11694 10207  8888 41616     6    10
  41616  8200   112     4   245   153   599  1607     6  4366    30   634
    599 21013    31  2111   575  1316     9  5356  3595   420   484  4510
      6   208 26674 13974 42168   163  8863 45935   541     2]]"
98515bd97e4fae6bfce2d164659cd75e87a9fc89,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Sociability from ego-network on Twitter,[[    0   104 17641  4484    31 21450    12 34728    15   599     2]]
8c78b21ec966a5e8405e8b9d3d6e7099e95ea5fb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM),"[[    0   267 15494  2239   234 21992  3092    14   304 15380 23794   337
  26739  1546    36 16256    43   163  8863 45935   398     8  4003    12
  42184   337   251   765    12  1279  3783    36 37426   574  4014   448
     43     2]]"
7dce1b64c0040500951c864fce93d1ad7a1809bc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"a masking speech enhancement model BIBREF11, BIBREF12, BIBREF13","[[    0   102 11445   154  1901 25387  1421   163  8863 45935  1225     6
    163  8863 45935  1092     6   163  8863 45935  1558     2]]"
d79d897f94e666d5a6fcda3b0c7e807c8fad109e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Average reward across 5 seeds show that NLP representations are robust to changes in the environment as well task-nuisances,"[[    0 46315  7970   420   195  9775   311    14   234 21992 30464    32
   6295     7  1022    11     5  1737    25   157  3685    12 18373   354
   5332     2]]"
c84590ba32df470a7c5343d8b99e541b217f10cf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The Wikipedia Toxic Comments dataset,[[    0   133 28274 39304  9353 41616     2]]
42a61773aa494f7b12838f71a949034c12084de1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MemN2N BIBREF12, Attentive and Impatient Readers BIBREF6","[[    0 35038   487   176   487   163  8863 45935  1092     6  7279  1342
   2088     8  5902 23846 23148   163  8863 45935   401     2]]"
e51d0c2c336f255e342b5f6c3cf2a13231789fed,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They collected tweets in Russian language using a heuristic query specific to Russian,"[[    0  1213  4786  6245    11  1083  2777   634    10    37 46177 25860
   2167     7  1083     2]]"
5ea87432b9166d6a4ab8806599cd2b1f9178622f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"best results were obtained using new word embeddings, best group of word embeddings is EC, The highest type F1-score was obtained for EC1 model, built using binary FastText Skip-gram method utilising subword information, ability of the model to provide vector representation for the unknown words seems to be the most important","[[    0  7885   775    58  4756   634    92  2136 33183   417  1033     6
    275   333     9  2136 33183   417  1033    16 11270     6    20  1609
   1907   274   134    12 31673    21  4756    13 11270   134  1421     6
   1490   634 32771  9612 39645  6783    12 28526  5448 14258  3009  2849
  14742   335     6  1460     9     5  1421     7   694 37681  8985    13
      5  4727  1617  1302     7    28     5   144   505     2]]"
db9021ddd4593f6fadf172710468e2fdcea99674,0.0,Entailment,[[    2     0 30495  3760  1757     2]],incorporating coding syntax tree model,[[    0  3976 36370  1295 25776 45362  3907  1421     2]]
601f96770726a0063faf9bacd5db01c4af5add1f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],rule-based and dictionary-based methods ,[[    0 28523    12   805     8 36451    12   805  6448  1437     2]]
982979cb3c71770d8d7d2d1be8f92b66223dec85,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," For example, one metric could consist in checking whether for any given word, all words that are known to belong to the same class are closer than any words belonging to different classes, independently of the actual cosine","[[    0   286  1246     6    65 14823   115 19438    11  8405   549    13
    143   576  2136     6    70  1617    14    32   684     7  9943     7
      5   276  1380    32  2789    87   143  1617 11441     7   430  4050
      6 12672     9     5  3031 12793   833     2]]"
af7a9b56596f90c84f962098f7e836309161badf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],1338 pairs for training,[[    0  1558  3170 15029    13  1058     2]]
7ce213657f7ee792148988c5a3578b24cd2f9c62,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The different attention distributions suggest that the proposed senti-emoji embedding is capable of recognizing words with strong sentiments that are closely related to the true sentiment even with the presence of words with conflicting sentiments,"[[    0   133   430  1503 26070  3608    14     5  1850  1051   118    12
    991 16873 33183 11303    16  4453     9 16257  1617    19   670 18316
     14    32  3615  1330     7     5  1528  5702   190    19     5  2621
      9  1617    19 22757 18316     2]]"
a726046eec1e2efa5fe3926963863bf755e64682,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"German, English, Chinese",[[    0 27709     6  2370     6  1111     2]]
390aa2d733bd73699899a37e65c0dee4668d2cd8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
77cf4379106463b6ebcb5eb8fa5bb25450fa5fb8,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," three types of questions, namely tumor size, proximal resection margin and distal resection margin","[[    0   130  3505     9  1142     6 13953 16570  1836     6 43860 16980
  25806 14970  2759     8  7018   337 25806 14970  2759     2]]"
689d1d0c4653a8fa87fd0e01fa7e12f75405cd38,0.0,Entailment,[[    2     0 30495  3760  1757     2]],biLSTM-networks,[[    0  5605   574  4014   448    12  4135 11655     2]]
0fd7d12711dfe0e35467a7ee6525127378a1bacb,0.0,Entailment,[[    2     0 30495  3760  1757     2]], listening comprehension task ,[[    0  6288 40494  3685  1437     2]]
1ef8d1cb1199e1504b6b0daea52f2e4bd2ef7023,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
c10f38ee97ed80484c1a70b8ebba9b1fb149bc91,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SVMRank,[[    0   104   846 12642  3153     2]]
c3a9732599849ba4a9f07170ce1e50867cf7d7bf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"2) Nave Bayes with SVM (NBSVM), 3) Extreme Gradient Boosting (XGBoost), 4) FastText algorithm with Bidirectional LSTM (FastText-BiLSTM)","[[    0   176    43  7300 48350  1501   293    19   208 20954    36   487
   3297 20954   238   155    43 24495 24257  4843 25802   154    36  1000
   4377   139  2603   238   204    43  9612 39645 17194    19 12658 43606
    337   226  4014   448    36 35515 39645    12 37426   574  4014   448
     43     2]]"
9003c7041d3d2addabc2c112fa2c7efe5fab493c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"inappropriate, discriminating",[[    0   179 23360     6 38303     2]]
d94ac550dfdb9e4bbe04392156065c072b9d75e1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
03895bc75e4d01c359cd269a9eb3b6ea57039817,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CNN model which additionally learns intermediate hidden layer representations and convolutional filters. Moreover the CNN model can take advantage of the semantic similarity encoded in the distributed word2vec representations,"[[    0 16256  1421    61 31151 25269 21398  7397 10490 30464     8 15380
  23794   337 19214     4  7905     5  3480  1421    64   185  2093     9
      5 46195 37015 45320    11     5  7664  2136   176 25369 30464     2]]"
249f2a9bd9d59679cbe82b3fa01572fc7a04f81b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d664054c8d1f8e84169d4ab790f2754274353685,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the UBC database BIBREF14,[[    0   627   121  3573  8503   163  8863 45935  1570     2]]
f5e571207d9f4701b4d01199ef7d0bfcfa2c0316,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Each class has different patterns in adjectives, adverbs and verbs for sarcastic and non-sarcastic classes","[[    0 20319  1380    34   430  8117    11 45272  3699     6  2329 44713
      8 47041    13 39580     8   786    12    29   271  5182   636  4050
      2]]"
a6950c22c7919f86b16384facc97f2cf66e5941d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"INLINEFORM0 (SemEval 2014) contains reviews of the laptop domain and those of INLINEFORM1 (SemEval 2014), INLINEFORM2 (SemEval 2015) and INLINEFORM3 (SemEval 2016) are for the restaurant domain.","[[    0  2444 28302 38036   288    36 37504   717  6486   777    43  6308
   6173     9     5  9972 11170     8   167     9  2808 28302 38036   134
     36 37504   717  6486   777   238  2808 28302 38036   176    36 37504
    717  6486   570    43     8  2808 28302 38036   246    36 37504   717
   6486   336    43    32    13     5  2391 11170     4     2]]"
63403ffc0232ff041f3da8fa6c30827cfd6404b7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Accuracy,[[    0 36984 45386     2]]
e9209ebf38c4ae4d93884f68c7b5b3444e0604f3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
e1f559da7fa501d3190073bca9ce4d4a12149e80,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
83251fd4a641cea8b180b49027e74920bca2699a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style","[[    0  5827     9    10  3645    16  4625    25    10 37681     9  3948
      9  1367  2136  4050    36  3341  1081 43083    43    25   157    25
   3948     9 45774 28201  1575   101     5   346     9 13944  2747   786
     12 42985  1536    11    63 12329 43756     6   187 13166  3184    34
     57  2343     7    28 22206     9  2496     2]]"
2870fbce43a3cf6daf982f720137c008b30c60dc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"nouns, verbs, pronouns, subjects, objects, negation words, special BERT tokens","[[    0   282  7928    29     6 47041     6 43083     6  9352     6  8720
      6 15183  1258  1617     6   780   163 18854 22121     2]]"
3efc0981e7f959d916aa8bb32ab1c347b8474ff8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Our lexical features include 1-, 2-, and 3-grams in both word and character levels., number of characters and the number of words, POS tags, 300-dimensional pre-trained word embeddings from GloVe, latent semantic indexing, tweet representation by applying the Brown clustering algorithm, positive words (e.g., love), negative words (e.g., awful), positive emoji icon and negative emoji icon, boolean features that check whether or not a negation word is in a tweet","[[    0  2522 36912  3569  1575   680   112 20551   132 20551     8   155
     12 28526    29    11   258  2136     8  2048  1389   482   346     9
   3768     8     5   346     9  1617     6 29206 19445     6  2993    12
  23944  1198    12 23830  2136 33183   417  1033    31  4573   139 30660
      6 42715 46195  1965   154     6  3545  8985    30  9889     5  1547
  46644  2961 17194     6  1313  1617    36   242     4   571   482   657
    238  2430  1617    36   242     4   571   482 11522   238  1313 21554
   9360     8  2430 21554  9360     6 49378  1575    14  1649   549    50
     45    10 15183  1258  2136    16    11    10  3545     2]]"
a49832c89a2d7f95c1fe6132902d74e4e7a3f2d0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],CoNLL 2014,[[   0 8739  487 6006  777    2]]
4bf4374135c39d10dafece4bed8ef547dc3bf9f0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
bb71a638668a21c2d446b44cbf51676c839658f7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"A maximum of three contributors will listen to any audio clip. If an $<$audio,transcript$>$ pair first receives two up-votes, then the clip is marked as valid. If instead the clip first receives two down-votes, then it is marked as invalid.","[[    0   250  4532     9   130 17233    40  4161     7   143  6086  7200
      4   318    41    68 41552  1629 24846     6  9981 26083  1629 15698
   1629  1763    78  9524    80    62    12 45547     6   172     5  7200
     16  4760    25  8218     4   318  1386     5  7200    78  9524    80
    159    12 45547     6   172    24    16  4760    25 21567     4     2]]"
b8137eb0fa0b41f871c899a54154f640f0e9aca1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"relational entities, general text-based attributes, descriptive text of images, nodes in graph structure of networks, queries","[[    0  5982  5033  8866     6   937  2788    12   805 16763     6 42690
   2788     9  3156     6 32833    11 20992  3184     9  4836     6 22680
      2]]"
980568848cc8e7c43f767da616cf1e176f406b05,0.0,Entailment,[[    2     0 30495  3760  1757     2]],27 ,[[   0 2518 1437    2]]
bed527bcb0dd5424e69563fba4ae7e6ea1fca26a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],2019 GermEval shared task on hierarchical text classification,[[    0 10626 24063   717  6486  1373  3685    15 44816  2788 20257     2]]
ba973b13f26cd5eb1da54663c0a72842681d5bf5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"news publications, wine reviews, and Reddit",[[    0  2926 16043     6  3984  6173     6     8  6844     2]]
093dd1e403eac146bcd19b51a2ace316b36c6264,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
faa4f28a2f2968cecb770d9379ab2cfcaaf5cfab,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Speaker's Gender Effects, Interlocutors' Gender and Number Effects","[[    0 29235  4218    18 25262 37759     6  3870 26516  1182   994   108
  25262     8 12270 37759     2]]"
8ec94313ea908b6462e1f5ee809a977a7b6bdf01,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
d824f837d8bc17f399e9b8ce8b30795944df0d51,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By visualizing syntactic distance estimated by the parsing network,"[[    0  2765  7133  2787 45774 28201  4472  2319    30     5 46563  1546
      2]]"
9efd025cfa69c6ff2777528bd158f79ead9353d1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
ee279ace5bc69d15e640da967bd4214fe264aa1a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"mean rank (MR), mean reciprocal rank (MRR), as well as Hits@1, Hits@3, and Hits@10","[[    0 43348  7938    36 12642   238  1266 36285  7938    36 12642   500
    238    25   157    25 35865  1039   134     6 35865  1039   246     6
      8 35865  1039   698     2]]"
e12674f0466f8c0da109b6076d9939b30952c7da,0.0,Entailment,[[    2     0 30495  3760  1757     2]],FastText,[[    0 35515 39645     2]]
63eb31f613a41a3ddd86f599e743ed10e1cd07ba,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Hindi-English,[[    0   725  2028   118    12 35007     2]]
5d83b073635f5fd8cd1bdb1895d3f13406583fbd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Hasty Student, Impatient Reader, BiDAF, BiDAF w/ static memory","[[    0   725 19626  9067     6  5902 23846 27019     6  6479  3134   597
      6  6479  3134   597   885    73 25156  3783     2]]"
d6e2b276390bdc957dfa7e878de80cee1f41fbca,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Only Bert base and Bert large are compared to proposed approach.,"[[    0 19933 12975  1542     8 12975   739    32  1118     7  1850  1548
      4     2]]"
bcce5eef9ddc345177b3c39c469b4f8934700f80,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
42af0472e6895eaf7b9392674b0d956e64e86b03,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"German$\leftrightarrow $English, German$\leftrightarrow $French, Chinese$\leftrightarrow $English, English$\rightarrow $Lithuanian, English$\rightarrow $Finnish, and Russian$\rightarrow $English, Lithuanian$\rightarrow $English, Finnish$\rightarrow $English, and English$\rightarrow $Kazakh","[[    0 27709  1629 37457  6960  4070 17214    68 35007     6  1859  1629
  37457  6960  4070 17214    68 28586     6  1111  1629 37457  6960  4070
  17214    68 35007     6  2370  1629 37457  4070 17214    68   574  3432
   7372   811     6  2370  1629 37457  4070 17214    68   597  5246  1173
      6     8  1083  1629 37457  4070 17214    68 35007     6 39714   811
   1629 37457  4070 17214    68 35007     6 21533  1629 37457  4070 17214
     68 35007     6     8  2370  1629 37457  4070 17214    68   530  1222
   7352     2]]"
97757a69d9fc28b260e68284fd300726fbe358d0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Bias feature, Token feature, Uppercase feature (y/n), Titlecase feature (y/n), Character trigram feature, Quotation feature (y/n), Word suffix feature (last three characters), POS tag (provided by spaCy utilities), Word shape (provided by spaCy utilities), Word embedding (see Table TABREF26)","[[    0   387  5003  1905     6 29464  1905     6   121 39609  3175  1905
     36   219    73   282   238 13497 11173  1905    36   219    73   282
    238 35177 46681  4040  1905     6  3232 35915  1905    36   219    73
    282   238 15690 47503  1905    36 13751   130  3768   238 29206  6694
     36 39355    30 18179 25826  9987   238 15690  3989    36 39355    30
  18179 25826  9987   238 15690 33183 11303    36  7048  9513   255  4546
  45935  2481    43     2]]"
8720c096c8b990c7b19f956ee4930d5f2c019e2b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
f7789313a804e41fcbca906a4e5cf69039eeef9f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Reuters-21578 BIBREF30,  LabelMe BIBREF31, 20-Newsgroups benchmark corpus BIBREF29 ","[[    0  1251    12 24355  5479   163  8863 45935   541     6  1437 36870
   5096   163  8863 45935  2983     6   291    12  5532 36378  5437 42168
    163  8863 45935  2890  1437     2]]"
2159062595f24ec29826d517429e1b809ba068b3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a0876fcbcb5a5944b412613e885703f14732676c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
de2d33760dc05f9d28e9dabc13bab2b3264cadb7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
129c03acb0963ede3915415953317556a55f34ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"First, the GRU only allows sentences to have context from sentences before them, but not after them. This prevents information propagation from future sentences. Second, the supporting sentences may be too far away from each other on a word level to allow for these distant sentences to interact through the word level GRU.","[[    0 10993     6     5  8837   791   129  2386 11305     7    33  5377
     31 11305   137   106     6    53    45    71   106     4   152 17410
    335 44441    31   499 11305     4  4665     6     5  3117 11305   189
     28   350   444   409    31   349    97    15    10  2136   672     7
   1157    13   209 13258 11305     7 10754   149     5  2136   672  8837
    791     4     2]]"
1e4450e23ec81fdd59821055f998fd9db0398b16,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
61fba3ab10f7b6906e27b028fb1d42ec601c3fb8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
f1b738a7f118438663f9d77b4ccd3a2c4fd97c01,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"precision , recall , Hamming loss, micro averaged precision and recall ","[[    0  5234 37938  2156  6001  2156  3600  7059   872     6  5177  8329
  15339     8  6001  1437     2]]"
8a94766f8251fa0bce0e09e5c69ce05761811a62,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"plan-to-DFS mapping to perform the correct sequence of traversals, and train a neural classifier to act as a controller","[[    0 11181    12   560    12  9380   104 20410     7  3008     5  4577
  13931     9 34038  1536     6     8  2341    10 26739  1380 24072     7
   1760    25    10 17554     2]]"
48c3e61b2ed7b3f97706e2a522172bf9b51ec467,0.0,Entailment,[[    2     0 30495  3760  1757     2]],correctness of all the question answer pairs are verified by at least two annotators,"[[    0 36064  1825     9    70     5   864  1948 15029    32 13031    30
     23   513    80 45068  3629     2]]"
9c9e90ceaba33242342a5ae7568e89fe660270d5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"0.021 higher MRR, a 2.4% higher H@1, and a 2.4% higher H@3 against RotatE, respectively, doesn't outperform the previous state-of-the-art as much as that of WN18RR and YAGO3-10, HAKE gains a 0.050 higher MRR, 6.0% higher H@1 and 4.6% higher H@3 than RotatE, respectively","[[    0   288     4 41656   723 18838   500     6    10   132     4   306
    207   723   289  1039   134     6     8    10   132     4   306   207
    723   289  1039   246   136  9104   415   717     6  4067     6   630
     75  9980  3899     5   986   194    12  1116    12   627    12  2013
     25   203    25    14     9   305   487  1366 25733     8   854  3450
    673   246    12   698     6 27842  6961  3077    10   321     4 25423
    723 18838   500     6   231     4   288   207   723   289  1039   134
      8   204     4   401   207   723   289  1039   246    87  9104   415
    717     6  4067     2]]"
f8f13576115992b0abb897ced185a4f9d35c5de9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
88e9e5ad0e4c369b15d81a4e18f7d12ff8fa9f1b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
2d536961c6e1aec9f8491e41e383dc0aac700e0a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"- paraphrase 1
- paraphrase 2
- different meaning
- opposite meaning
- nonsense
- minimal change
- generalization
- gossip
- formal sentence
- non-standard sentence
- simple sentence
- possibility
- ban
- future
- past","[[    0    12 40127 34338   112 50118    12 40127 34338   132 50118    12
    430  3099 50118    12  5483  3099 50118    12 20175 50118    12  9865
    464 50118    12   937  1938 50118    12 20445 50118    12  4828  3645
  50118    12   786    12 21993  3645 50118    12  2007  3645 50118    12
   3302 50118    12  2020 50118    12   499 50118    12   375     2]]"
018ef092ffc356a2c0e970ae64ad3c2cf8443288,0.0,Entailment,[[    2     0 30495  3760  1757     2]],8757 news records,[[    0 29271   406   340  2189     2]]
8b99767620fd4efe51428b68841cc3ec06699280,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
5fb6a21d10adf4e81482bb5c1ec1787dc9de260d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence,"[[    0  2765 14364   154 28404  4249  5018  1617    19    10   278     9
  28404 21821  5018  1617   716    15     5  9976     9  7398  4086     2]]"
11e6b79f1f48ddc6c580c4d0a3cb9bcb42decb17,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"40 mel-scaled log filterbank enegries (FBanks) computed every 10 ms with 25 ms window, deltas and delta-deltas (120 features in vector), spectrogram","[[    0  1749 15352    12  3866  9389  7425 14929  5760  1177  3733  4458
     36 22687 10950    43 43547   358   158 43601    19   564 43601  2931
      6  2424    90   281     8 38744    12   417  6607   281    36 10213
   1575    11 37681   238 19416 42189     2]]"
247e1fe052230458ce11b98e3637acf0b86795cd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
0716b481b78d80b012bca17c897c62efbe7f3731,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
3703433d434f1913307ceb6a8cfb9a07842667dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Considering ""What"" and ""How"" separately versus jointly optimizing for both.","[[    0 41562    22  2264   113     8    22  6179   113 12712  4411 13521
  36226    13   258     4     2]]"
aee1af55d39145f609da95116ab1b154adb5fa7e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],allows models that are consistently performing well to train for more steps,"[[    0 37984  3092    14    32  6566  4655   157     7  2341    13    55
   2402     2]]"
579a0603ec56fc2b4aa8566810041dbb0cd7b5e7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],perform experiments to utilize ASR $n$-best hypotheses during evaluation,"[[    0  1741  3899 15491     7 16085  6015   500    68   282  1629    12
   7885 44850   148 10437     2]]"
d0b005cb7ed6d4c307745096b2ed8762612480d2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Transformer generation model,[[    0 19163 22098  2706  1421     2]]
b0fd686183b056ea3f63a7ab494620df1d598c24,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"train the parser on six other languages in the Google universal dependency treebanks version 2.0 (de, en, es, fr, it, pt, sv, excluding whichever is the target language), and we use gold coarse POS tags","[[    0 21714     5 48946    15   411    97 11991    11     5  1204 10547
  31492  3907 27045  1732   132     4   288    36  2794     6  1177     6
   2714     6  6664     6    24     6 46238     6 23119     6 12556 29047
     16     5  1002  2777   238     8    52   304  1637 38667 29206 19445
      2]]"
ff814793387c8f3b61f09b88c73c00360a22a60e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d5ff8fc4d3996db2c96cb8af5a6d215484991e62,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments,"[[    0   133   256 11579 26118  3042    10  3625   723 10624 37178    23
     10  1122 37113 37178    77  1118     7     5    97 15491     2]]"
cc2b98b46497c71e955e844fb36e9ef6e2784640,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BLEU BIBREF8, RIBES BIBREF9, token-level delay","[[    0 30876   791   163  8863 45935   398     6   248  8863  1723   163
   8863 45935   466     6 19233    12  4483  4646     2]]"
63d9b12dc3ff3ceb1aed83ce11371bca8aac4e8f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
3bfb8c12f151dada259fbd511358914c4b4e1b0e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"precision, recall, F-measure",[[    0  5234 37938     6  6001     6   274    12  1794 24669     2]]
53377f1c5eda961e438424d71d16150e669f7072,0.0,Entailment,[[    2     0 30495  3760  1757     2]],pure summarization model NHG,[[    0 38627 39186  1938  1421 18886   534     2]]
18d8b52b4409c718bf1cc90ce9e013206034bbd9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],average 12.8 min per recording,[[    0 20365   316     4   398  5251   228  5492     2]]
56123dd42cf5c77fc9a88fc311ed2e1eb672126e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SPTree, Tagging, CopyR, HRL, GraphR, N-gram Attention","[[    0  4186 33731     6   255 12771     6 22279   500     6   289 17868
      6 20919   500     6   234    12 28526 35798     2]]"
4319a13a6c4a9494ccb465509c9d4265f63dc9b5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the generated sentences often did not include all desired attributes.,"[[    0   627  5129 11305   747   222    45   680    70 12762 16763     4
      2]]"
61aac406b648865f007a400dcd69f28e44efc636,0.0,Entailment,[[    2     0 30495  3760  1757     2]],computationally inexpensive means to understand what happened at the stopping point,"[[    0   175  9179 29688 23107   839     7  1346    99  1102    23     5
   8197   477     2]]"
203d322743353aac8a3369220e1d023a78c2cae3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
f6937199e4b06bfbaa22edacc7339410de9703db,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"PersonaChat BIBREF12, DailyDialog BIBREF13, OpenSubtitles BIBREF7","[[    0 41761   102 29665   163  8863 45935  1092     6  1681 48873   163
   8863 45935  1558     6  2117 23055    90 46913   163  8863 45935   406
      2]]"
e6c872fea474ea96ca2553f7e9d5875df4ef55cd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
bbcd77aac74989f820e84488c52f3767d0405d51,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
327e06e2ce09cf4c6cc521101d0aecfc745b1738,0.0,Entailment,[[    2     0 30495  3760  1757     2]],accuracy with standard deviation,[[    0  7904 45386    19  2526 38369     2]]
3582fac4b2705db056f75a14949db7b80cbc3197,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
c9c85eee41556c6993f40e428fa607af4abe80a9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],on $\sim $ 8.7M annotated anonymised user utterances,"[[    0   261 49959 13092    68   290     4   406   448 45068  1070 44563
   1720  3018 18672  5332     2]]"
7a70fb11cb3449749f0c2c06101965bf5d02c54a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
11ed8c4d98a4e8994990edba54319efe9c6745f2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],NELL,[[   0 9009 6006    2]]
649d6dc076251547aece6532f75d00fc99081d2b,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," some configurations used in some architectures (e.g., additional RNN layers) are actually irrelevant","[[    0   103 33843   341    11   103 41885    36   242     4   571   482
    943   248 20057 13171    43    32   888 21821     2]]"
d3a1a53521f252f869fdae944db986931d9ffe48,0.0,Entailment,[[    2     0 30495  3760  1757     2]],political pundits of the Washington Post,[[    0 17522 23663     9     5   663  1869     2]]
84d36bca06786070e49d3db784e42a51dd573d36,0.0,Entailment,[[    2     0 30495  3760  1757     2]],conceptualization task,[[    0 38498  5564  1938  3685     2]]
5787ac3e80840fe4cf7bfae7e8983fa6644d6220,0.0,Entailment,[[    2     0 30495  3760  1757     2]],We collected a corpus of poems and a corpus of vernacular literature from online resources,"[[    0   170  4786    10 42168     9 25833     8    10 42168     9  1437
  12170 28049 13144    31   804  1915     2]]"
b0376a7f67f1568a7926eff8ff557a93f434a253,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Comparing with the highest performing baseline: 1.3 points on ACE2004 dataset, 0.6 points on CWEB dataset, and 0.86 points in the average of all scores.","[[    0 24699  5867    19     5  1609  4655 18043    35   112     4   246
    332    15 36211 34972 41616     6   321     4   401   332    15   230
   9112   387 41616     6     8   321     4  5334   332    11     5   674
      9    70  4391     4     2]]"
29983f4bc8a5513a198755e474361deee93d4ab6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],five-minute reuse and one-day return,[[    0  9579    12  4530 32344     8    65    12  1208   671     2]]
86f24ecc89e743bb1534ac160d08859493afafe9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"dependency head and dependency relation label, denoted as Dep and Rel for short, Tree-based Position Feature (TPF) as Dependency Path (DepPath), Shortest Dependency Path (SDP) as Relation Path (RelPath)","[[    0 45950  6761   471     8 31492  9355  6929     6  3069 11367    25
   6748     8  8136    13   765     6 11077    12   805 21355 31967    36
  15993   597    43    25 43290  6761 16234    36 29774 42119   238  7787
    990 43290  6761 16234    36   104  5174    43    25  8136  1258 16234
     36 29806 42119    43     2]]"
c5171daf82107fce0f285fa18f19e91fbd1215c5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the evaluation metrics include BLEU and ROUGE (1, 2, L) scores","[[    0   627 10437 12758   680   163  3850   791     8   248  5061  8800
     36   134     6   132     6   226    43  4391     2]]"
ff83eea2df9976c1a01482818340871b17ad4f8c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
186ccc18c6361904bee0d58196e341a719fb31c2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Sociodemographics: gender, age, marital status, etc., Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc., Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc.","[[    0   104 17641  1630   991 43685    35  3959     6  1046     6 30615
   2194     6  4753   482 11182  1131   750    35   346     9   986 18054
      6   750     9  2628   636   808  6948     6   674  5933     9  1095
     36   658   454    14  7988   238  4753   482  3522    31     5   595
   7988    35  5933     9  1095    36 14502   238 23630   810     6   346
      8  5933     9  2775     6    86     9 15462     6 10437  4391     6
   4753     4     2]]"
dd5c9a370652f6550b4fd13e2ac317eaf90973a8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],0.9098 correlation,[[    0   288     4 39517   398 22792     2]]
81cee2fc6edd9b7bc65bbf6b4aa35782339e6cff,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Variety of formats supported (PDF, Word...), user can define content elements of document","[[    0 27717 25717     9 19052  2800    36 38506     6 15690   734   238
   3018    64  9914  1383  4785     9  3780     2]]"
afa94772fca7978f30973c43274ed826c40369eb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
4c71ed7d30ee44cf85ffbd7756b985e32e8e07da,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"document categorization, regression tasks",[[    0 43017 18072  1938     6 39974  8558     2]]
98b11f70239ef0e22511a3ecf6e413ecb726f954,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
30b5e5293001f65d2fb9e4d1fdf4dc230e8cf320,0.0,Entailment,[[    2     0 30495  3760  1757     2]],To classify a text as belonging to one of the ten possible classes.,"[[    0  3972 36029    10  2788    25 11441     7    65     9     5  2724
    678  4050     4     2]]"
f052444f3b3bf61a3f226645278b780ebd7774db,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
12c6ca435f4fcd4ad5ea5c0d76d6ebb9d0be9177,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d5498d16e8350c9785782b57b1e5a82212dbdaad,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Relative error is less than 5%,[[    0 29806  3693  5849    16   540    87   195   207     2]]
6743c1dd7764fc652cfe2ea29097ea09b5544bc3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
c06b5623c35b6fa7938340fa340269dc81d061e1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"noun-noun subset of bless, leds BIBREF13, bless, wbless, bibless, hyperlex BIBREF20","[[    0   282  7928    12   282  7928 37105     9 22212     6   669    29
    163  8863 45935  1558     6 22212     6   885   428  1672     6   741
   1452  1672     6  8944 14726   163  8863 45935   844     2]]"
ea6764a362bac95fb99969e9f8c773a61afd8f39,0.0,Entailment,[[    2     0 30495  3760  1757     2]],82.0%,[[   0 6551    4  288  207    2]]
9fe6339c7027a1a0caffa613adabe8b5bb6a7d4a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
bd6cec2ab620e67b3e0e7946fc045230e6906020,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"F1 score of 0.71 for this task without any specific training, simply by choosing a threshold below which all sentence pairs are considered duplicates, distances between duplicate and non-duplicate questions using different embedding systems","[[    0   597   134  1471     9   321     4  5339    13    42  3685   396
    143  2167  1058     6  1622    30  8348    10 11543   874    61    70
   3645 15029    32  1687 30501 23020     6 21459   227 33196     8   786
     12  6588  2911 14263  1142   634   430 33183 11303  1743     2]]"
df8cc1f395486a12db98df805248eb37c087458b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SST (Stanford Sentiment Treebank), Subj (Subjectivity dataset), MPQA Opinion Corpus, RT is another movie review sentiment dataset, TREC is a dataset for classification of the six question types","[[    0   104  4014    36 36118  1891 12169  8913 11077  5760   238  4052
    267    36 47159  9866 41616   238  3957  1864   250 20253 28556     6
  10541    16   277  1569  1551  5702 41616     6   255 40698    16    10
  41616    13 20257     9     5   411   864  3505     2]]"
7f5ab9a53aef7ea1a1c2221967057ee71abb27cb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
f6a1125c5621a2f32c9bcdd188dff14efa096083,0.0,Entailment,[[    2     0 30495  3760  1757     2]],2.2 BLEU gains,[[   0  176    4  176  163 3850  791 3077    2]]
69b41524dc5820143e45f2f3545cd5c0a70e2922,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"SumBasic, Lsa, LexRank, TextRank, Bayes, Hmm, MaxEnt, NeuralSum, Lead-N","[[    0 38182 46255     6   226 11146     6 14786 46052     6 14159 46052
      6  1501   293     6 44258     6  4471 30495     6 44304 38182     6
  14243    12   487     2]]"
cb1126992a39555e154bedec388465b249a02ded,0.0,Entailment,[[    2     0 30495  3760  1757     2]],using a mixture of manual and semi-automatic techniques,[[    0 10928    10 12652     9 12769     8  4126    12 27106  7373     2]]
7065e6140dbaffadebe62c9c9d3863ca0f829d52,0.0,Entailment,[[    2     0 30495  3760  1757     2]],seven,[[    0 17723     2]]
0d755ff58a7e22eb4d02fca45d4a7a3920f4e725,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
4c4f76837d1329835df88b0921f4fe8bda26606f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
a25c1883f0a99d2b6471fed48c5121baccbbae82,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models","[[    0 14229  3044    35  5545     4   401    13   881  1421   396  2731
  23861     6  5138     4   306    13   881  1421    19  2731 23861     6
   6121     4   176    13 12547     9   195  3092     2]]"
fc8bc6a3c837a9d1c869b7ee90cf4e3c39bcd102,0.0,Entailment,[[    2     0 30495  3760  1757     2]],There were hierarchical and non-hierarchical baselines; BERT was one of those baselines,"[[    0   970    58 44816     8   786    12   298   906 13161  3569 11909
  38630   131   163 18854    21    65     9   167 11909 38630     2]]"
0106bd9d54e2f343cc5f30bb09a5dbdd171e964b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],twitter ,[[   0 1556 1437    2]]
4b41f399b193d259fd6e24f3c6e95dc5cae926dd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For the question generation model 15,000 images with 75,000 questions. For the chatbot model, around 460k utterances over 230k dialogues.","[[    0  2709     5   864  2706  1421   379     6   151  3156    19  3337
      6   151  1142     4   286     5  7359 12749  1421     6   198 33888
    330 18672  5332    81 16242   330 25730  3663     4     2]]"
d4ce220dcdabcf9ebf4da9bdd3ad778e2d79fc07,0.0,Entailment,[[    2     0 30495  3760  1757     2]],increases F1-score by 10.2% and 3% compared with the existing best systems $LS \cup KLD \cup CONN$ and $KLD \cup LS \cup LS_{inter}$,"[[    0 33008  9354   274   134    12 31673    30   158     4   176   207
      8   155   207  1118    19     5  2210   275  1743    68 10463 44128
  21033   229 18048 44128 21033  8748   487  1629     8    68   530 18048
  44128 21033 29881 44128 21033 29881 49747  8007 24303  1629     2]]"
0bffc3d82d02910d4816c16b390125e5df55fd01,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
4a8bceb3b6d45f14c4749115d6aa83912f0b0a6e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
89b9a2389166b992c42ca19939d750d88c5fa79b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
0c3924214572579ddbc1b4a87c7f7842ef20ff1b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Cuneiform,[[    0   347  4438 38263     2]]
3f7a7e81908a763e5ca720f90570c5f224ac64f6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
537a786794604ecc473fb3ef6222e0c3cb81f772,0.0,Entailment,[[    2     0 30495  3760  1757     2]],How2,[[   0 6179  176    2]]
ac3c88ace59bf75788370062db139f60499c2056,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The D2V model has been rated 80 times as ""bad relevance"" while the pmra returned only 24 times badly relevant documents.","[[    0   133   211   176   846  1421    34    57  5211  1812   498    25
     22 10999 21623   113   150     5  4751   763  1835   129   706   498
   7340  4249  2339     4     2]]"
9f89bff89cea722debc991363f0826de945bc582,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"MEN, MTurk287, MTurk771, RG, RW, SimLex999, SimVerb3500, WS353, WS353R, WS353S","[[    0 42717     6   256 38110   330 30928     6   256 38110   330 40652
      6 38281     6 34573     6  6202 43551 16692     6  6202 21119   428
   2022   612     6 18483 27044     6 18483 27044   500     6 18483 27044
    104     2]]"
9b2b063e8a9938da195c9c0d6caa3e37a4a615a8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
2c3b2c3bab6d18cb0895462e3cfd91cd0dee7f7d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BiDAF, BERT ",[[    0 37426  3134   597     6   163 18854  1437     2]]
a61732774faf30bab15bf944b2360ec4710870c1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
6a099dfe354a79936b59d651ba0887d9f586eaaf,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
a8e4522ce2ce7336e731286654d6ad0931927a4e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],existing visual features aren't sufficient enough to expect benefits from the visual modality in NMT,"[[    0 19865  7133  1575  2025    75  7719   615     7  1057  1795    31
      5  7133 11134  6948    11   234 11674     2]]"
c96a6b30d71c6669592504e4ee8001e9d1eb1fba,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
663b36f99ad2422f4d3a8c6398ebf55ceab7770d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],words extracted from YouTube video comments and descriptions for all YouTube videos shared in the user's timeline,"[[    0 30938 27380    31  4037   569  1450     8 24173    13    70  4037
   3424  1373    11     5  3018    18 10589     2]]"
29d917cc38a56a179395d0f3a2416fca41a01659,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," Generate a query out of the claim and querying a search engine, rank the words by means of TF-IDF, use IBM's AlchemyAPI to identify named entities, generate queries of 510 tokens, which execute against a search engine, and collect the snippets and the URLs in the results, skipping any result that points to a domain that is considered unreliable.","[[    0 15745   877    10 25860    66     9     5  2026     8 31212  4048
     10  1707  3819     6  7938     5  1617    30   839     9 35690    12
   2688   597     6   304 11510    18 47002 40104     7  3058  1440  8866
      6  5368 22680     9   195  2383   698 22121     6    61 11189   136
     10  1707  3819     6     8  5555     5 39976     8     5 44163    11
      5   775     6 26756   143   898    14   332     7    10 11170    14
     16  1687 31298     4     2]]"
c97a4a1c0e3d00137a9ae8d6fbb809ba6492991d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
02348ab62957cb82067c589769c14d798b1ceec7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BiGRU s with attention, ROUGE, Language model (LM), Next sentence prediction","[[    0 37426 11621   791   579    19  1503     6   248  5061  8800     6
  22205  1421    36 21672   238  4130  3645 16782     2]]"
6e8c587b6562fafb43a7823637b84cd01487059a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Ranges from 44.22 to 100.00 depending on K and the sequence length.,"[[    0   500 33094    31  3550     4  2036     7   727     4   612  6122
     15   229     8     5 13931  5933     4     2]]"
9ef182b61461d0d8b6feb1d6174796ccde290a15,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Use an existing one,[[    0 34447    41  2210    65     2]]
a978a1ee73547ff3a80c66e6db3e6c3d3b6512f4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"0.08 points on the 2011 test set, 0.44 points on the 2012 test set, 0.42 points on the 2013 test set for IWSLT-CE.","[[    0   288     4  3669   332    15     5  1466  1296   278     6   321
      4  3305   332    15     5  1125  1296   278     6   321     4  3714
    332    15     5  1014  1296   278    13    38   771 11160   565    12
   8041     4     2]]"
6916596253d67f74dba9222f48b9e8799581bad9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
6e97c06f998f09256be752fa75c24ba853b0db24,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Accuracy across six datasets,[[    0 36984 45386   420   411 42532     2]]
f53be1266be1fea5598a671080226c9c983b69e3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
22c802872b556996dd7d09eb1e15989d003f30c0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They compute Pearsons correlation between NED measure for patient-to-therapist and patient-perceived emotional bond rating and NED measure for therapist-to-patient and patient-perceived emotional bond rating,"[[    0  1213 37357 16116    17    27    29 22792   227   234  1691  2450
     13  3186    12   560    12 12968 33688     8  3186    12  1741 31597
   3722  2175   691     8   234  1691  2450    13 18931    12   560    12
  23846     8  3186    12  1741 31597  3722  2175   691     2]]"
ef8099e2bc0ac4abc4f8216740e80f2fa22f41f6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Spanish, English, Italian, Arabic, German, Portuguese, Russian and Swedish","[[    0 41226     6  2370     6  3108     6 19645     6  1859     6 13053
      6  1083     8  9004     2]]"
99d7bef0ef395360b939a3f446eff67239551a9d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
4e841138f307839fd2c212e9f02489e27a5f830c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],DA recognition is aimed to assign a label to each utterance in a conversation. It can be formulated as a supervised classification problem. ,"[[    0  3134  4972    16  3448     7 28467    10  6929     7   349 18672
   2389    11    10  1607     4    85    64    28 34359    25    10 20589
  20257   936     4  1437     2]]"
d3014683dff9976b7c56b72203df99f0e27e9989,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"we report P@1, which is equivalent to accuracy, we also provide results with P@5 and P@10 in the Appendix","[[    0  1694   266   221  1039   134     6    61    16  6305     7  8611
      6    52    67   694   775    19   221  1039   245     8   221  1039
    698    11     5 40643     2]]"
983c2fe7bdbf471bb8b15db858fd2cbec86b96a5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
ef872807cb0c9974d18bbb886a7836e793727c3d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.,"[[    0   133  1617    14    64  6364     5 12720     9     5  8523  1617
     25 37617 32712     8  5368    24    31     5  6885 27380 37617 32712
      4     2]]"
bfce2afe7a4b71f9127d4f9ef479a0bfb16eaf76,0.0,Entailment,[[    2     0 30495  3760  1757     2]],rating questions on a scale of 1-5 based on fluency of language used and relevance of the question to the context,"[[    0 21172  1142    15    10  3189     9   112    12   245   716    15
   6626  6761     9  2777   341     8 21623     9     5   864     7     5
   5377     2]]"
9df4a7bd0abb99ae81f0ebb29c488f1caa0f268f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],We treated the neural semantic retrieval at both the paragraph and sentence level as binary classification problems with models' parameters updated by minimizing binary cross entropy loss.,"[[    0   170  3032     5 26739 46195 43372    23   258     5 17818     8
   3645   672    25 32771 20257  1272    19  3092   108 17294  4752    30
  34655 32771  2116 47382   872     4     2]]"
fbe5e513745d723aad711ceb91ce0c3c2ceb669e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],None,[[    0 29802     2]]
c4a0c7b6f1a00f3233a5fe16240a98d9975701c0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
6e962f1f23061f738f651177346b38fd440ff480,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BERTje BIBREF8, an ULMFiT model (Universal Language Model Fine-tuning for Text Classification model) BIBREF19., mBERT","[[    0 11126   565  2359   163  8863 45935   398     6    41   121 21672
   9474   565  1421    36 35011 22205  7192 14321    12 24641   154    13
  14159 40509  1421    43   163  8863 45935  1646   482   475 11126   565
      2]]"
10045d7dac063013a8447b5a4bc3a3c2f18f9e82,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
5a23f436a7e0c33e4842425cf86d5fd8ba78ac92,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"553,451 documents",[[    0 39252     6 36860  2339     2]]
ecfb2e75eb9a8eba8f640a039484874fa0d2fceb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
ff2bcf2d8ffee586751ce91cf15176301267b779,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Lexicon of the cities tend to use most forms of a particular concept,"[[    0 43551 17505     9     5  1947  3805     7   304   144  4620     9
     10  1989  4286     2]]"
6ce057d3b88addf97a30cb188795806239491154,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"BERT, XLNET RoBERTa, ALBERT, DistilBERT","[[    0 11126   565     6 14402 13548  3830 11126 38495     6  6019 11126
    565     6 11281   718 11126   565     2]]"
55588ae77496e7753bff18763a21ca07d9f93240,0.0,Entailment,[[    2     0 30495  3760  1757     2]],It uses particular forms of a concept rather than all of them uniformly,"[[    0   243  2939  1989  4620     9    10  4286  1195    87    70     9
    106 41919     2]]"
ebae0cd1fe0e7ba877d4b3055190e8b1dfcaeb53,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"User location (uloc), User language (ulang), Timezone (tz), Tweet language (tlang), Offset (offset), User name (name), User description (description), Tweet content (content)","[[    0 44518  2259    36   922  1975   238 27913  2777    36   922  1097
    238  3421 13930    36 12527   238 12244  2777    36    90 32373   238
   4995  8738    36 48025   238 27913   766    36 13650   238 27913  8194
     36 42739   238 12244  1383    36 10166    43     2]]"
75043c17a2cddfce6578c3c0e18d4b7cf2f18933,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"audiences wanted products more and more contemporary, intense and a little bit novel or sophisticated, but less and less mellow and (surprisingly) unpretentious","[[    0  5247 27797   770   785    55     8    55  8708     6  5676     8
     10   410   828  5808    50 10364     6    53   540     8   540 34384
   1722     8    36 33258    43   542 42354 40058     2]]"
f42e61f9ad06fb782d1574eb973c880add4f76d2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"type of recurrent unit, type of attention, choice of sequential vs. tree-based model structure","[[    0 12528     9 35583  1933     6  1907     9  1503     6  2031     9
  29698  1954     4  3907    12   805  1421  3184     2]]"
544e29937e0c972abcdd27c953dc494b2376dd76,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Two different BERT models were developed,[[    0  9058   430   163 18854  3092    58  2226     2]]
aeda22ae760de7f5c0212dad048e4984cd613162,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For each source sentence, transformation sentences that are transformed according to some criteria (paraphrase, minimal change etc.)","[[    0  2709   349  1300  3645     6  7791 11305    14    32 11229   309
      7   103  8608    36  5489  8258 34338     6  9865   464  4753  1592
      2]]"
54415efa91566d5d7135fa23bce3840d41a6389e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],300-dimensional vectors,[[    0  2965    12 23944 44493     2]]
a381ba83a08148ce0324b48b8ff35128e66f580a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"High-order CNN, Tree-LSTM, DRNN, DCNN, CNN-MC, NBoW and SVM ","[[    0 18522    12 10337  3480     6 11077    12   574  4014   448     6
  10994 20057     6  5815 20057     6  3480    12  6018     6   234 18935
    771     8   208 20954  1437     2]]"
09a993756d2781a89f7ec5d7992f812d60e24232,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
feedddb7ae4998b6a3eaa2d6323017ba278748cc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Our search space consists of two stackable cells, one for the model encoder and one for the decoder , Each cell contains NASNet-style blocks, which receive two hidden state inputs and produce new hidden states as outputs, Our search space contains five branch-level search fields (input, normalization, layer, output dimension and activation), one block-level search field (combiner function) and one cell-level search field (number of cells).","[[    0  2522  1707   980 10726     9    80 17709   868  4590     6    65
     13     5  1421  9689 15362     8    65    13     5  5044 15362  2156
   4028  3551  6308 14680 15721    12  5827  5491     6    61  1325    80
   7397   194 16584     8  2592    92  7397   982    25 39512     6  1541
   1707   980  6308   292  6084    12  4483  1707  5447    36 46797     6
   2340  1938     6 10490     6  4195 21026     8 29997   238    65  1803
     12  4483  1707   882    36 15302  5101  5043    43     8    65  3551
     12  4483  1707   882    36 30695     9  4590   322     2]]"
1951cde612751410355610074c3c69cec94824c2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],autoencoders,[[    0  4255 18057 29659   268     2]]
3786164eaf3965c11c9969c4463b8c3223627067,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"8 levels and degrees, ranging from `not hot' to `luke warm' to `hot +'. Every utterance was labeled with one of these discrete labels by a single annotator","[[    0   398  1389     8  4176     6  6272    31 22209  3654  2131   108
      7 22209  6487  1071  3279   108     7 22209 10120  2055  2652  4337
  18672  2389    21 16274    19    65     9   209 34014 14105    30    10
    881 45068  2630     2]]"
92240eeab107a4f636705b88f00cefc4f0782846,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
9190c56006ba84bf41246a32a3981d38adaf422c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila)","[[    0 16435 33598    31    10 12371     9     5  3108 28274    36 26064
    954     4  3387     4  2663   238    31     5  1049  6363     9  3108
   1204   491    36 42204 18048     6   234  6034     6 29915 28275     6
  34945   487 41819     6 37924 23177 12613     6 22204     6  4998 41499
      6 12925 33086    43     8    31   103 44563  1538 28975   227  1434
      8    10  2111   575  7359 12749    36   574 31692    43     2]]"
2122bd05c03dde098aa17e36773e1ac7b6011969,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Fill-in-the-blank natural language questions,[[    0 46448    12   179    12   627    12 35763  1632  2777  1142     2]]
7b2bf0c1a24a2aa01d49f3c7e1bdc7401162c116,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"By using a Bayesian approach  and by using word-pairs, where they extract all the pairs of co-occurring words within each tweet.  They search for the words that achieve the highest number of spikes matching the days of events.","[[    0  2765   634    10  1501 44871  1548  1437     8    30   634  2136
     12   642 18022     6   147    51 14660    70     5 15029     9  1029
     12 23462 16715  1617   624   349  3545     4  1437   252  1707    13
      5  1617    14  3042     5  1609   346     9 27572  8150     5   360
      9  1061     4     2]]"
9aa52b898d029af615b95b18b79078e9bed3d766,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Proposed vs best baseline:
Decoding: 8541 vs 8532 tokens/sec
Training: 8h vs 8h","[[    0 41895  7878  1954   275 18043    35 50118 15953 19519    35  5663
   4006  1954  5663  2881 22121    73  8584 50118 44466    35   290   298
   1954   290   298     2]]"
2fffff59e57b8dbcaefb437a6b3434fc137f813b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
3334f50fe1796ce0df9dd58540e9c08be5856c23,0.0,Entailment,[[    2     0 30495  3760  1757     2]]," For each user, we calculate the proportion of tweets scored positively by each LIWC category.","[[    0   286   349  3018     6    52 15756     5 10301     9  6245  1008
  13541    30   349 25709 17314  4120     4     2]]"
00d6228bcd6b839529e52d0d622bf787a9356158,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"precision ($P.$), recall ($R.$) and $F_1$ scores for target recognition and targeted sentiment","[[    0  5234 37938  1358   510 46992   238  6001  1358   500 46992    43
      8    68   597  1215   134  1629  4391    13  1002  4972     8  3656
   5702     2]]"
540e9db5595009629b2af005e3c06610e1901b12,0.0,Entailment,[[    2     0 30495  3760  1757     2]],The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia.,"[[    0   133  7601   679    14     5 40823   718 12935 42168  1437  6308
   1255  3157 47234   150   145 28269     4   252 37952 19197    14  1395
     33  1255    12 36014 13031    30  6676    19 28274     4     2]]"
32a232310babb92991c4b1b75f7aa6b4670ec447,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
313087c69caeab2f58e7abd62664d3bd93618e4e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"manually labeled and tell exactly if one sentence should be extracted (assuming our annotations are in agreement), to further verify that FAR correlates with human preference,","[[    0   397 13851 16274     8  1137  2230   114    65  3645   197    28
  27380    36 34730    84 47234    32    11  1288   238     7   617 12881
     14 34122 44555    19  1050 12832     6     2]]"
b6fb72437e3779b0e523b9710e36b966c23a2a40,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WikiSQL - 2 rules (SELECT, WHERE)
SimpleQuestions - 1 rule
SequentialQA - 3 rules (SELECT, WHERE, COPY)","[[    0 46929 46608   111   132  1492    36 49179     6 29919    43 50118
  45093 46865   111   112  2178 50118 48245 12986  1864   250   111   155
   1492    36 49179     6 29919     6 14637   975    43     2]]"
dd2f21d60cfca3917a9eb8b192c194f4de85e8b2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"interdependence between rate and distortion, impact of KL on the sharpness of the approximated posteriors, demonstrate how certain generative behaviours could be imposed on VAEs via a range of maximum channel capacities, some experiments to find if any form of syntactic information is encoded in the latent space","[[    0  8007   417 41575   227   731     8 36075     6   913     9 26544
     15     5  4406  1825     9     5 36612 24985 11566 10327     6  8085
    141  1402 20181  3693 30723   115    28  5713    15 11790 15286  1241
     10  1186     9  4532  4238 23549     6   103 15491     7   465   114
    143  1026     9 45774 28201   335    16 45320    11     5 42715   980
      2]]"
2d3bf170c1647c5a95abae50ee3ef3b404230ce4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],standard parametrized attention and a non-attention baseline,"[[    0 21993 40206   594  1069 16317  1503     8    10   786    12  2611
  19774 18043     2]]"
6a566095e25cbb56330456d7a1f3471693817712,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
b24767fe7e6620369063e646fd3048dc645a8348,0.0,Entailment,[[    2     0 30495  3760  1757     2]],overlapping dialogue acts,[[    0  2137   462 12040  6054  4504     2]]
7af01e2580c332e2b5e8094908df4e43a29c8792,0.0,Entailment,[[    2     0 30495  3760  1757     2]],By computing number of unique responses and number of responses divided by the number of unique responses to that question for each of the questions,"[[    0  2765 11730   346     9  2216  8823     8   346     9  8823  6408
     30     5   346     9  2216  8823     7    14   864    13   349     9
      5  1142     2]]"
ef7b62a705f887326b7ebacbd62567ee1f2129b3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Siamese neural network consisting of an embedding layer, a LSTM layer and a feed-forward layer with ReLU activations","[[    0 35684 12336   242 26739  1546 17402     9    41 33183 11303 10490
      6    10   226  4014   448 10490     8    10  3993    12 16135 10490
     19  1223 39826 30264  1635     2]]"
5bcc12680cf2eda2dd13ab763c42314a26f2d993,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For sentence-level prediction they used tolerance accuracy, for segment retrieval accuracy and MRR and for the pipeline approach they used overall accuracy","[[    0  2709  3645    12  4483 16782    51   341 12352  8611     6    13
   2835 43372  8611     8 18838   500     8    13     5  4116  1548    51
    341  1374  8611     2]]"
1d1ab5d8a24dfd15d95a5a7506ac0456d1192209,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
9439430ff97c6e927d919860b1cb86a0dcff0038,0.0,Entailment,[[    2     0 30495  3760  1757     2]],10-fold cross validation,[[    0   698    12 12851  2116 26567     2]]
e8ca81d5b36952259ef3e0dbeac7b3a622eabe8e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],IEMOCAP,[[   0  100 5330 4571  591    2]]
f6202100cfb83286dc51f57c68cffdbf5cf50a3f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Step-Wise Decoder Fusion, Multimodal Attention Modulation, Visual-Semantic (VS) Regularizer","[[    0 25093    12   771  1496 49227 20914     6 14910   757  1630   337
  35798 10274 11264     6 25878    12 37504 26970    36 11859    43 24306
   6315     2]]"
4f243056e63a74d1349488983dc1238228ca76a7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
32537fdf0d4f76f641086944b413b2f756097e5e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],improving the score for WNLaMPro-medium by 50% compared to BERT$_\text{base}$ and 31% compared to Attentive Mimicking,"[[    0 33491  6645     5  1471    13   305   487 10766   448 10653    12
  37661    30   654   207  1118     7   163 18854  1629  1215 37457 29015
  45152 11070 24303  1629     8  1105   207  1118     7  7279  1342  2088
  38123 10176     2]]"
a1e07c7563ad038ee2a7de5093ea08efdd6077d4,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"(about 4 million sentences, 138 million word tokens), one trained on the Billion Word benchmark","[[    0  1640  9006   204   153 11305     6 21436   153  2136 22121   238
     65  5389    15     5 17102 15690  5437     2]]"
b13cf4205f3952c3066b9fb81bd5c4277e2bc7f5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
1024f22110c436aa7a62a1022819bfe62dc0d336,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"To verify the reliability of the transformed semantic space, we propose an evaluation benchmark on the basis of word similarity datasets. Given an enriched space INLINEFORM0 and a similarity dataset INLINEFORM1 , we compute the similarity of each word pair INLINEFORM2 as the cosine similarity of their corresponding transformed vectors INLINEFORM3 and INLINEFORM4 from the two spaces, where INLINEFORM5 and INLINEFORM6 for LS and INLINEFORM7 and INLINEFORM8 for CCA. ","[[    0  3972 12881     5 13677     9     5 11229 46195   980     6    52
  15393    41 10437  5437    15     5  1453     9  2136 37015 42532     4
   6211    41 33389   980  2808 28302 38036   288     8    10 37015 41616
   2808 28302 38036   134  2156    52 37357     5 37015     9   349  2136
   1763  2808 28302 38036   176    25     5 12793   833 37015     9    49
  12337 11229 44493  2808 28302 38036   246     8  2808 28302 38036   306
     31     5    80  5938     6   147  2808 28302 38036   245     8  2808
  28302 38036   401    13 29881     8  2808 28302 38036   406     8  2808
  28302 38036   398    13   230  4054     4  1437     2]]"
141dab98d19a070f1ce7e7dc384001d49125d545,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"For the DMN+, we propose replacing this single GRU with two different components. The first component is a sentence reader, The second component is the input fusion layer","[[    0  2709     5 18695   487 30787    52 15393  8119    42   881  8837
    791    19    80   430  6411     4    20    78  7681    16    10  3645
  10746     6    20   200  7681    16     5  8135 24904 10490     2]]"
f7d67d6c6fbc62b2953ab74db6871b122b3c92cc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"It is an order of magnitude more efficient in terms of training time., his model requires pre-training and mutual-learning and requires days of training time, whereas the simple architecture we propose requires on the order of an hour","[[    0   243    16    41   645     9 11259    55  5693    11  1110     9
   1058    86   482    39  1421  3441  1198    12 32530     8  7628    12
  29888     8  3441   360     9  1058    86     6  9641     5  2007  9437
     52 15393  3441    15     5   645     9    41  1946     2]]"
5a0841cc0628e872fe473874694f4ab9411a1d10,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"on SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI","[[    0   261 12180 37790  5600  2580 41616    30   231     4  4956   207
     11 10018     6    30   231     4  6405   207    11   234  7539   131
     15  6479 39820 41616    30   195     4  4718   207    11 10018     6
    155     4  6468   207    11   234  7539     2]]"
f7817b949605fb04b1e4fec9dd9ca8804fb92ae9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Because, unlike other languages, English does not mark grammatical genders","[[    0 10105     6  7328    97 11991     6  2370   473    45  2458 25187
  45816 36231     2]]"
f7070b2e258beac9b09514be2bfcc5a528cc3a0e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
6a9eb407be6a459dc976ffeae17bdd8f71c8791c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"reward 1 for successfully completing the task, with a discount by the number of turns, and reward 0 when fail","[[   0  241 7767  112   13 5116 8796    5 3685    6   19   10 6720   30
     5  346    9 4072    6    8 7970  321   77 5998    2]]"
602396d1f5a3c172e60a10c7022bcfa08fa6cbc9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Proposed RCRN outperforms ablative baselines BiLSTM by +2.9% and 3L-BiLSTM by +1.1% on average across 16 datasets.,"[[    0 41895  7878   248  9822   487  9980 33334 32573  3693 11909 38630
   6479   574  4014   448    30  2055   176     4   466   207     8   155
    574    12 37426   574  4014   448    30  2055   134     4   134   207
     15   674   420   545 42532     4     2]]"
8df89988adff57279db10992846728ec4f500eaa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Typical implementations of dynamic programming algorithms are serial in the length of the sequence, Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized, Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient","[[    0 47572  3569 42993     9  6878  8326 16964    32 13603    11     5
   5933     9     5 13931     6 34710  5033 13879    16   190    55     9
     41   696    13 46563 16964     6    61  1395    28    25  2773 12980
   1538     6  6802    13    97  9031   853  1033     6   215    25  7425
      8 19220     6   209  1414    32  1169  2635    50   182  3783 29958
      2]]"
f062723bda695716aa7cb0f27675b7fc0d302d4d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"judged by 10 raters on a [0,10] scale","[[    0 12864  4462    30   158 12378   268    15    10   646   288     6
    698   742  3189     2]]"
da4535b75e360604e3ce4bb3631b0ba96f4dadd3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"in an ME game there are typically several interpretive biases at work: each player has her own bias, as does the Jury","[[    0   179    41 12341   177    89    32  3700   484 18107  2088 31681
     23   173    35   349   869    34    69   308  9415     6    25   473
      5 28492     2]]"
9d5df9022cc9eb04b9f5c5a9d8308a332ebdf50c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],They use a two-stage labeling strategy where in the first stage single annotators label a large number of short texts with relatively pure sentiment orientations and in the second stage multiple annotators label few text samples with mixed sentiment orientations,"[[    0  1213   304    10    80    12 10850 27963  1860   147    11     5
     78  1289   881 45068  3629  6929    10   739   346     9   765 14301
     19  3487  8309  5702 33379  1635     8    11     5   200  1289  1533
  45068  3629  6929   367  2788  7931    19  4281  5702 33379  1635     2]]"
0a3a8d1b0cbac559f7de845d845ebbfefb91135e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Accuracy not available: WER results are reported 42.6 German, 35.9 English","[[    0 36984 45386    45   577    35   305  2076   775    32   431  3330
      4   401  1859     6  1718     4   466  2370     2]]"
b1e90a546dc92e96b657fff5dad8e89f4ac6ed5e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
43ee69902a5fc1e3c7bacc4456d3f779c45a911d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
cb4727cd5643dabc3f5c95e851d5313f5d979bdc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],our Open model achieves more than 3 points of f1-score than the state-of-the-art result,"[[    0  2126  2117  1421 35499    55    87   155   332     9   856   134
     12 31673    87     5   194    12  1116    12   627    12  2013   898
      2]]"
e8c0fabae0d29491471e37dec34f652910302928,0.0,Entailment,[[    2     0 30495  3760  1757     2]],beyond localized features and have access to the entire sequence,"[[    0  1610 32565 35668  1575     8    33   899     7     5  1445 13931
      2]]"
28b2a20779a78a34fb228333dc4b93fd572fda15,0.0,Entailment,[[    2     0 30495  3760  1757     2]],supervised learning,[[    0 16101 25376  2239     2]]
a78a6fd6ca36413586836838e38f3fa9282646ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
d5256d684b5f1b1ec648d996c358e66fe51f4904,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Improve existing NLP methods. Improve linguistic analysis. Measure impact of word normalization tools.,"[[    0 47302  2210   234 21992  6448     4 40736 39608  1966     4 22328
    913     9  2136  2340  1938  3270     4     2]]"
8985ead714236458a7496075bc15054df0e3234e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Overall accuracy per model is: 5-gram (60.5), LSTM (68.9), TXL (68.7), GPT-2 (80.1)","[[    0 28965  8611   228  1421    16    35   195    12 28526    36  2466
      4   245   238   226  4014   448    36  4671     4   466   238 12031
    574    36  4671     4   406   238   272 10311    12   176    36  2940
      4   134    43     2]]"
405964517f372629cda4326d8efadde0206b7751,0.0,Entailment,[[    2     0 30495  3760  1757     2]],they use ROC curves and cross-validation,[[    0 10010   304   248  4571 23739     8  2116    12 42679  1258     2]]
b573b36936ffdf1d70e66f9b5567511c989b46b2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],unshuffled version of the French OSCAR corpus,"[[    0   879  1193  5865  1329  1732     9     5  1515   384  3632  2747
  42168     2]]"
eb95af36347ed0e0808e19963fe4d058e2ce3c9f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],51.7 and 51.6 on 4th and 8th grade question sets with no curated knowledge. 47.5 and 48.0 on 4th and 8th grade question sets when both solvers are given the same knowledge,"[[    0  4708     4   406     8  4074     4   401    15   204   212     8
    290   212  4978   864  3880    19   117 23132  2655     4  4034     4
    245     8  2929     4   288    15   204   212     8   290   212  4978
    864  3880    77   258  9281  3697    32   576     5   276  2655     2]]"
66bf0d61ffc321f15e7347aaed191223f4ce4b4a,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"2,060 workers",[[    0   176     6 37143  1138     2]]
9653c89a93ac5c717a0a26cf80e9aa98a5ccf910,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"WDAqua BIBREF0 , QAKiS BIBREF7 , gAnswer BIBREF6 and Platypus BIBREF8","[[    0   771  3134 23869   163  8863 45935   288  2156  1209  7140   118
    104   163  8863 45935   406  2156   821 33683   163  8863 45935   401
      8 23654 18198   687   163  8863 45935   398     2]]"
a5dd569e6d641efa86d2c2b2e970ce5871e0963f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods","[[    0   530    12  1794  1253     6  6783    12 26086   468  9041   994
      6  7382 47779 44304  3658     8  2884 44947 40419   716 46644  2961
   6448     2]]"
b6b5f92a1d9fa623b25c70c1ac67d59d84d9eec8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Their best average precision tops previous best result by 0.202,"[[    0 16837   275   674 15339 13657   986   275   898    30   321     4
  25155     2]]"
4e2cb1677df949ee3d1d3cd10962b951da907105,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Decoder that generates question using an LSTM-based language model,"[[    0 15953 15362    14 17382   864   634    41   226  4014   448    12
    805  2777  1421     2]]"
e84ba95c9a188fda4563f45e53fbc8728d8b5dab,0.0,Entailment,[[    2     0 30495  3760  1757     2]],One model per topic.,[[   0 3762 1421  228 5674    4    2]]
2a859e80d8647923181cb2d8f9a2c67b1c3f4608,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Be explicit in what you evaluate., Faithfulness evaluation should not involve human-judgement on the quality of interpretation., Faithfulness evaluation should not involve human-provided gold labels., Do not trust inherent interpretability claims., Faithfulness evaluation of IUI systems should not rely on user performance.","[[    0  9325 16045    11    99    47 10516   482 14161 21843 10437   197
     45  6877  1050    12 12864 43790    15     5  1318     9 13794   482
  14161 21843 10437   197    45  6877  1050    12 39355  1637 14105   482
   1832    45  2416    44    48   179 33591 18107  4484    17    46  1449
    482 14161 21843 10437     9    38 18157  1743   197    45  5864    15
   3018   819     4     2]]"
6aed1122050b2d508dc1790c13cdbe38ff126089,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"attention-based sequence-to-sequence model , CVAE","[[    0  2611 19774    12   805 13931    12   560    12 46665  1421  2156
    230  9788   717     2]]"
0b54032508c96ff3320c3db613aeb25d42d00490,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Tweets related to a Bank of America DDos attack were used as training data. The test datasets contain tweets related to attacks to Bank of America, PNC and Wells Fargo.","[[    0 45659  2580  1330     7    10   788     9   730 27932   366   908
     58   341    25  1058   414     4    20  1296 42532  5585  6245  1330
      7  1912     7   788     9   730     6   221  6905     8  6093  8044
      4     2]]"
d4d771bcb59bab4f3eb9026cda7d182eb582027d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d151327c93b67928313f8fad8079a4ff9ef89314,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
f92ee3c5fce819db540bded3cfcc191e21799cb1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],cannot be disclosed due to licensing restrictions,[[    0   438 37250    28  4638   528     7 10765  5165     2]]
b8b588ca1e876b3094ae561a875dd949c8965b2e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue,"[[    0  2362  1637  2526    13  6885 15190    80    36   368    55    43
   6054  1743    77  2811     5 11658     9     5  1050     8     5  6626
   6761     9     5  5129  6054     2]]"
c5ac07528cf99d353413c9d9ea61a1a699dd783e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"unigrams, bigrams, and trigrams, including sequences of punctuation, Word2Vec word embeddings","[[    0   879  1023 27809     6   380 27809     6     8 46681 27809     6
    217 26929     9 15760  9762     6 15690   176   846  3204  2136 33183
    417  1033     2]]"
7bc993b32484d6ae3c86d0b351a68e59fd2757a5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Spanish,[[    0 41226     2]]
a56fbe90d5d349336f94ef034ba0d46450525d19,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Author's own DCG rules are defined from scratch.,"[[    0 42779    18   308  5815   534  1492    32  6533    31 12272     4
      2]]"
f52ec4d68de91dba66668f0affc198706949ff90,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Women-Yoga,[[    0 19814    12   975 10447     2]]
9508e9ec675b6512854e830fa89fa6a747b520c5,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
222b2469eede9a0448e0226c6c742e8c91522af3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
7d841b98bcee29aaa9852ef7ceea1213d703deba,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
37e8f5851133a748c4e3e0beeef0d83883117a98,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Proposed model achive 66+-22 win rate, baseline CNN 13+-1  and baseline FiLM 32+-3 .","[[    0 41895  7878  1421    10   611  2088  5138  2744    12  2036   339
    731     6 18043  3480   508  2744    12   134  1437     8 18043 19643
  21672  2107  2744    12   246   479     2]]"
d8cecea477dfc5163dca6e2078a2fe6bc94ce09f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],accuracy,[[    0  7904 45386     2]]
71fca845edd33f6e227eccde10db73b99a7e157b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the baseline provided by BIBREF8, the baselines provided by the ABSA organizers","[[    0   627 18043  1286    30   163  8863 45935   398     6     5 11909
  38630  1286    30     5  6266  3603  9921     2]]"
f41c401a4c6e1be768f8e68f774af3661c890ffd,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
5937ebbf04f62d41b48cbc6b5c38fc309e5c2328,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Quotation (q) dialogue acts, on the other hand, are mostly used with `Anger' and `Frustration', Action Directive (ad) dialogue act utterances, which are usually orders, frequently occur with `Anger' or `Frustration' although many with `Happy' emotion in case of the MELD dataset, Acknowledgements (b) are mostly with positive or neutral, Appreciation (ba) and Rhetorical (bh) backchannels often occur with a greater number in `Surprise', `Joy' and/or with `Excited' (in case of IEMOCAP), Questions (qh, qw, qy and qyd) are mostly asked with emotions `Surprise', `Excited', `Frustration' or `Disgust' (in case of MELD) and many are neutral, No-answers (nn) are mostly `Sad' or `Frustrated' as compared to yes-answers (ny)., Forward-functions such as Apology (fa) are mostly with `Sadness' whereas Thanking (ft) and Conventional-closing or -opening (fc or fp) are usually with `Joy' or `Excited'","[[    0 12444 35915    36 24987 14285   862  1343    43  6054  4504     6
     15     5    97   865     6    32  2260   341    19 22209 25441   254
    108     8 22209 29220 36852  3934  5828 41185    36   625    43  6054
   1760 18672  5332     6    61    32  2333  3365     6  5705  5948    19
  22209 25441   254   108    50 22209 29220 36852   108  1712   171    19
  22209 21136   108 11926    11   403     9     5   256 34816 41616     6
     83 42852 41330    36   428    43    32  2260    19  1313    50  7974
      6  3166 29605    36  3178    43     8   248  4626 30340    36 22549
     43   124   611 34735   747  5948    19    10  2388   346    11 22209
  27526 22627  3934 22209 37252   108     8    73   368    19 22209 42765
   4560   108    36   179   403     9    38  5330  4571   591   238 18964
     36  1343   298     6  2231   605     6  2231   219     8  2231   219
  24987 14285   862   417    43    32  2260   553    19  8597 22209 27526
  22627  3934 22209 42765  4560  3934 22209 29220 36852   108    50 22209
  26402   571  4193   108    36   179   403     9   256 34816    43     8
    171    32  7974     6   440    12  1253   605   268    36 15688    43
     32  2260 22209 37188   108    50 22209 29220  4193  8358   108    25
   1118     7  4420    12  1253   605   268    36  2855   322     6 10038
     12 18317 16849   215    25  8712  4383    36 12010    43    32  2260
     19 22209 37188  1825   108  9641  3837   154    36  2543    43     8
   2585 31370    12  3998 10174    50   111 12211    36 25484    50   856
    642    43    32  2333    19 22209 37252   108    50 22209 42765  4560
    108     2]]"
445e792ce7e699e960e2cb4fe217aeacdd88d392,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Demographic information is predicted using weighted lexicon of terms.,"[[    0 24658 25510   335    16  6126   634 19099 36912 17505     9  1110
      4     2]]"
e40df8c685a28b98006c47808f506def68f30e26,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
d3a1a53521f252f869fdae944db986931d9ffe48,0.0,Entailment,[[    2     0 30495  3760  1757     2]],the experts in the field,[[   0  627 2320   11    5  882    2]]
2c20426c003f7e3053f8e6c333f8bb744f6f31f8,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Emotion Classification (EC), Named Entity Recognition (NER), Sentence Pair Matching (SPM), Natural Language Inference (NLI)","[[    0 16750 19187 40509    36  3586   238 30436 46718 23288  7469    36
  20196   238 12169  4086 34587  9018   154    36  4186   448   238  7278
  22205    96 23861    36   487 27049    43     2]]"
20eb673b01d202b731e7ba4f84efc10a18616dd3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"the number of speakers of each gender category, their speech duration","[[    0   627   346     9  6864     9   349  3959  4120     6    49  1901
  13428     2]]"
d71cb7f3aa585e256ca14eebdc358edfc3a9539c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"CELEX (Dutch and English) - SVM-HMM
Festival, E-Hitz and OpenLexique - Liang hyphenation
IIT-Guwahat - Entropy CRF","[[    0  8041  3850  1000    36 40877     8  2370    43   111   208 20954
     12   725 16261 50118   597 30474     6   381    12   725  4494     8
   2117 43551  5150   111 38400 15671  2457  1258 50118   100  2068    12
  14484   605   895   415   111  9860 47145  4307   597     2]]"
f3d0e6452b8d24b7f9db1fd898d1fbe6cd23f166,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
f103789b85b00ec973076652c639bd31c605381e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Senseval-2 (SE2), Senseval-3 (SE3), SemEval 2013 task 12 (SE13), and SemEval 2015 task 13 (SE15), OntoNotes Release 5.0","[[    0 38239  6486    12   176    36  3388   176   238 19484  6486    12
    246    36  3388   246   238 11202   717  6486  1014  3685   316    36
   3388  1558   238     8 11202   717  6486   570  3685   508    36  3388
    996   238 13302   139 44691 15781   195     4   288     2]]"
8951fde01b1643fcb4b91e51f84e074ce3b69743,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They  evaluate newly proposed models in several low-resource settings across different languages with real, distantly supervised data with non-synthetic noise","[[    0  1213  1437 10516  3862  1850  3092    11   484   614    12 44814
   9629   420   430 11991    19   588     6  7018 12106 20589   414    19
    786    12  8628  3999 18667  6496     2]]"
af8d3ee6a282aaa885e9126aa4bcb08ac68837e0,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"over 41,250 videos and 825,000 captions in both English and Chinese., over 206,000 English-Chinese parallel translation pairs","[[    0  2137  3492     6  5714  3424     8   290  1244     6   151 13363
   2485    11   258  2370     8  1111   482    81 27637     6   151  2370
     12 24727 12980 19850 15029     2]]"
8f882f414d7ea12077930451ae77c6e5f093adbc,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ncorporating NNGLM and NNJM both independently and jointly into, baseline system","[[    0   282  7215 12150  1295   234   487 10020   448     8   234 25471
    448   258 12672     8 13521    88     6 18043   467     2]]"
6147846520a3dc05b230241f2ad6d411d614e24c,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"paper acceptance prediction, Named Entity Recognition (NER), author stance prediction","[[    0 33331 10502 16782     6 30436 46718 23288  7469    36 20196   238
   2730  6663 16782     2]]"
79bd2ad4cb5c630ce69d5a859ed118132cae62d7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
2898e4aa7a3496c628e7ddf2985b48fb11aa3bba,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"test-set perplexity, likelihood convergence and clustering measures, visualizing the topic summaries, authors' topic distributions and by performing an automatic labeling task","[[    0 21959    12  8738 33708  1571     6 11801 33345     8 46644  2961
   1797     6  7133  2787     5  5674 32933  5119     6  7601   108  5674
  26070     8    30  4655    41  8408 27963  3685     2]]"
0fce128b8aaa327ac0d58ec30cd2ecbea2019baa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Seq2Seq, HLSTM, HLSTM+Copy, HLSTM+Graph Attention, HLSTM+Contextual Attention","[[    0 14696  1343   176 14696  1343     6 38690  4014   448     6 38690
   4014   448  2744 48233     6 38690  4014   448  2744 45288 35798     6
  38690  4014   448  2744 48522  5564 35798     2]]"
00e6324ecd454f5d4b2a4b27fcf4104855ff8ee2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],we use t-SNE tool BIBREF27 to visualize the learned embedding,"[[    0  1694   304   326    12   104  9009  3944   163  8863 45935  2518
      7 39086     5  2435 33183 11303     2]]"
38a5cc790f66a7362f91d338f2f1d78f48c1e252,0.0,Entailment,[[    2     0 30495  3760  1757     2]],SVM,[[    0   104 20954     2]]
89b9e298993dbedd3637189c3f37c0c4791041a1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"embedding of the claim, Web evidence",[[    0 35804 11303     9     5  2026     6  6494  1283     2]]
40b9f502f15e955ba8615822e6fa08cb5fd29c81,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Corpus 5KL, Corpus 8KF",[[    0 25313   687   195   530   574     6 28556   290   530   597     2]]
4ca0d52f655bb9b4bc25310f3a76c5d744830043,0.0,Entailment,[[    2     0 30495  3760  1757     2]],1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation,"[[    0   134 12096 16274 25730  3663    13  1058     8 15452 35237 14286
    196 25730  3663    13 10437     2]]"
f9de9ddea0c70630b360167354004ab8cbfff041,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
6b7d76c1e1a2490beb69609ba5652476dde8831b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],adding context causes speakers to focus on broader semantic and pragmatic issues of discourse coherence,"[[    0 30547  5377  4685  6864     7  1056    15  5153 46195     8 26794
    743     9 19771  1029 40584     2]]"
7d5ba230522df1890619dedcfb310160958223c1,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
b9f852256113ef468d60e95912800fab604966f6,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Camrest, InCar Assistant",[[    0 28542  7110     6    96  9518  6267     2]]
ce6201435cc1196ad72b742db92abd709e0f9e8d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
d5f8707ddc21741d52b3c2a9ab1af2871dc6c90b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ROUGE and CIC, relevance, conciseness and readability on a 1 to 5 scale, and rank the summary pair","[[    0   500  5061  8800     8   230  2371     6 21623     6 10146   354
  14186     8  1166  4484    15    10   112     7   195  3189     6     8
   7938     5  4819  1763     2]]"
045dbdbda5d96a672e5c69442e30dbf21917a1ee,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
191107cd112f7ee6d19c1dc43177e6899452a2c7,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
ae8354e67978b7c333094c36bf9d561ca0c2d286,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"datasets from the NIST DUC-05, DUC-06 and DUC-07 shared tasks","[[    0 36146   281  2580    31     5   234 11595   211 12945    12  2546
      6   211 12945    12  4124     8   211 12945    12  3570  1373  8558
      2]]"
d20d6c8ecd7cb0126479305d27deb0c8b642b09f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"FBanks with cepstral mean normalization (CMN), variance with mean normalization (CMVN)","[[    0 22687 10950    19   740  2462   620  7085  1266  2340  1938    36
  18814   487   238 37832    19  1266  2340  1938    36 18814   846   487
     43     2]]"
441886f0497dc84f46ed8c32e8fa32983b5db42e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],partisan news detector,[[    0 29247   340 30956     2]]
14684ad200915ff1e3fc2a89cb614e472a1a2854,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
be7b375b22d95d1f6c68c48f57ea87bf82c72123,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"ROUGE F1, METEOR",[[    0   500  5061  8800   274   134     6 30782   717  3411     2]]
33ccbc401b224a48fba4b167e86019ffad1787fb,0.0,Entailment,[[    2     0 30495  3760  1757     2]],from 50K to 4.8M,[[   0 7761  654  530    7  204    4  398  448    2]]
2677b88c2def3ed94e25a776599555a788d197f2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],6-layer bLSTM with 1024 hidden units,"[[    0   401    12 39165   741   574  4014   448    19 46959  7397  2833
      2]]"
aa800b424db77e634e82680f804894bfa37f2a34,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
ea51aecd64bd95d42d28ab3f1b60eecadf6d3760,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Books, DVDs, Electronics, Kitchen appliances",[[    0 38108     6 37206     6 12057     6 11580 14636     2]]
dbfce07613e6d0d7412165e14438d5f92ad4b004,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"affective features provided by different emotion models such as Emolex, EmoSenticNet, Dictionary of Affect in Language, Affective Norms for English Words and Linguistics Inquiry and Word Count","[[    0  3707 38089  1575  1286    30   430 11926  3092   215    25  3676
   4104  1178     6  3676   139 35212   636 15721     6 41243     9 37089
     11 22205     6 37089  2088 20336    29    13  2370 27341     8   226
  35308 16307 29760     8 15690 12440     2]]"
51d03f0741b72ae242c380266acd2321baf43444,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
915cf3d481164217290d7b1eb9d48ed3e249196d,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"A request for information act should be issued early in a conversation, followed by an answer, informative statement, or apology towards the end of the conversation,  offering extra help at the end of a conversation, or thanking the customer yields more satisfied customers, and more resolved problems , asking yes-no questions early-on in a conversation is highly associated with problem resolution (ratio 3:1), but asking them at the end of a conversation has as similarly strong association with unsatisfied customers, Giving elaborate answers that are not a simple affirmative, negative, or response acknowledgement (i.e. Answer (Other)) towards the middle of a conversation leads to satisfied customers that are not frustrated, requesting information towards the end of a conversation (implying that more information is still necessary at the termination of the dialogue) leads to unsatisfied and unresolved customers","[[    0   250  2069    13   335  1760   197    28  1167   419    11    10
   1607     6  1432    30    41  1948     6 29749   445     6    50  9664
   1567     5   253     9     5  1607     6  1437  1839  1823   244    23
      5   253     9    10  1607     6    50 22787     5  2111  5167    55
  10028   916     6     8    55  8179  1272  2156  1996  4420    12  2362
   1142   419    12   261    11    10  1607    16  2200  3059    19   936
   3547    36  6528  1020   155    35   134   238    53  1996   106    23
      5   253     9    10  1607    34    25 11401   670  5259    19 36010
   2550   916     6 19506 10986  5274    14    32    45    10  2007 33003
      6  2430     6    50  1263 32068    36   118     4   242     4 31652
     36 24989 35122  1567     5  1692     9    10  1607  3315     7 10028
    916    14    32    45  8164     6 14030   335  1567     5   253     9
     10  1607    36   757 26308   154    14    55   335    16   202  2139
     23     5 17829     9     5  6054    43  3315     7 36010  2550     8
  29909   916     2]]"
e4024db40f4b8c1ce593f53b28718e52d5007cd2,0.0,Entailment,[[    2     0 30495  3760  1757     2]],using mean opinion score (MOS) naturalness judgments produced by a crowd-sourced pool of raters,"[[    0 10928  1266  2979  1471    36   448  3196    43  1632  1825 28728
   2622    30    10  2180    12    29 22241  3716     9 12378   268     2]]"
8c0846879771c8f3915cc2e0718bee448f5cb007,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"80 excerpts from scientific works, collection of 1,645 chat conversations, collection of 200 aggregated postings","[[    0  2940 30336    31  6441  1364     6  2783     9   112     6 33611
   7359  5475     6  2783     9  1878 26683  1070 32378     2]]"
3213529b6405339dfd0c1d2a0f15719cdff0fa93,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"The baseline models used are DrQA modified to support answering no answer questions, DrQA+CoQA which is pre-tuned on CoQA dataset, vanilla BERT, BERT+review tuned on domain reviews, BERT+CoQA tuned on the supervised CoQA data","[[    0   133 18043  3092   341    32   925  1864   250 10639     7   323
  15635   117  1948  1142     6   925  1864   250  2744  8739  1864   250
     61    16  1198    12 24641   196    15   944  1864   250 41616     6
  21857   163 18854     6   163 18854  2744 22459 14536    15 11170  6173
      6   163 18854  2744  8739  1864   250 14536    15     5 20589   944
   1864   250   414     2]]"
3748787379b3a7d222c3a6254def3f5bfb93a60e,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Grammaticality, non-redundancy, referential clarity, focus, structure & coherence","[[    0   534  4040 45816  1571     6   786    12  2050  3194  9875     6
  30114  3999  2617 10498     6  1056     6  3184   359  1029 40584     2]]"
df79d04cc10a01d433bb558d5f8a51bfad29f46b,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"Answer with content missing: (Applications section) We use Wikipedia articles
in five languages
(Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English) as well as the Na dataset of Adams
et al. (2017).
Select:
Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English","[[    0 33683    19  1383  1716    35    36 46407  2810    43   166   304
  28274  7201 50118   179   292 11991 50118  1640   530 11287   271   605
   5219     6  1587   139     6   221  1671   560     6 25282 29973   179
      6     8    10 37105     9  2370    43    25   157    25     5  7300
  41616     9  5710 50118   594  1076     4    36  3789   322 50118 45356
     35 50118   530 11287   271   605  5219     6  1587   139     6   221
   1671   560     6 25282 29973   179     6     8    10 37105     9  2370
      2]]"
028910d643c103abd90045ccb07ee8adc5a3e177,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Unanswerable,[[    0  9685 27740   868     2]]
79b174d20ea5dd4f35e25c9425fb97f40e27cd6f,0.0,Entailment,[[    2     0 30495  3760  1757     2]],No,[[   0 3084    2]]
269b05b74d5215b09c16e95a91ae50caedd9e2aa,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"agreement rates, Kappa value",[[    0  1073 43563  1162     6 37772   923     2]]
157b9f6f8fb5d370fa23df31de24ae7efb75d6f3,0.0,Entailment,[[    2     0 30495  3760  1757     2]],"They achieved best result in the PAN 2017 shared task with accuracy for Variety prediction task 0.0013 more than the 2nd best baseline, accuracy for Gender prediction task 0.0029 more than 2nd best baseline and accuracy for Joint prediction task 0.0101 more than the 2nd best baseline","[[    0  1213  4824   275   898    11     5 29975   193  1373  3685    19
   8611    13 17064 16782  3685   321     4   612  1558    55    87     5
    132  1187   275 18043     6  8611    13 25262 16782  3685   321     4
    612  2890    55    87   132  1187   275 18043     8  8611    13  9490
  16782  3685   321     4  2663  2663    55    87     5   132  1187   275
  18043     2]]"
d427e3d41c4c9391192e249493be23926fc5d2e9,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Yes,[[   0 9904    2]]
f33a21c6a9c75f0479ffdbb006c40e0739134716,0.0,Entailment,[[    2     0 30495  3760  1757     2]],syntax-based system may generate correct syntactic analyses for partial grammatical fragments,"[[    0  8628 49529    12   805   467   189  5368  4577 45774 28201 20070
     13  9801 25187 45816 27958     2]]"
1ed6acb88954f31b78d2821bb230b722374792ed,0.0,Entailment,[[    2     0 30495  3760  1757     2]],Private dashboard is leaderboard where competitors can see results after competition is finished - on hidden part of test set (private test set).,"[[    0 38076 24246    16   884  4929   147  6117    64   192   775    71
   1465    16  1550   111    15  7397   233     9  1296   278    36 22891
   1296   278   322     2]]"
