{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import PreTrainedModel, AutoModelForSequenceClassification, AutoConfig, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from transformers import BertForSequenceClassification\n",
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, m, w):\n",
    "    loss = (x * 2).mean() + (m * 3).mean() + w.mean()\n",
    "    new_mem = 3 * x[:len(m)] + m\n",
    "    return loss, new_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, mem = torch.ones(10, requires_grad=True), torch.ones(2, requires_grad=True)\n",
    "x2 = torch.ones(10, requires_grad=True)\n",
    "w = torch.Tensor([2]* 10)\n",
    "w.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
       "         0.2000]),\n",
       " tensor([1.5000, 1.5000]),\n",
       " tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memc = mem.clone()\n",
    "loss1, new_mem1 = model(x1, memc, w)\n",
    "new_mem1.retain_grad()\n",
    "loss1.backward()\n",
    "x1.grad, mem.grad, w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5345/1414870003.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  memc.grad\n"
     ]
    }
   ],
   "source": [
    "memc.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss2, new_mem2 = model(x2, new_mem1, w)\n",
    "# loss2.backward(retain_graph=True)\n",
    "# x1.grad, mem.grad, x2.grad, w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.7000, 4.7000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
       "         0.2000]),\n",
       " tensor([3., 3.]),\n",
       " tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
       "         0.2000]),\n",
       " tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
       "         0.2000]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2, new_mem2 = model(x2, new_mem1, w)\n",
    "loss2.backward()\n",
    "x1.grad, mem.grad, x2.grad, w.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m loss2, new_mem2 \u001b[38;5;241m=\u001b[39m model(x2, new_mem1, w)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mloss2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m x1\u001b[38;5;241m.\u001b[39mgrad, mem\u001b[38;5;241m.\u001b[39mgrad, x2\u001b[38;5;241m.\u001b[39mgrad, w\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/tensor.py:221\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Tensor \u001b[38;5;129;01mand\u001b[39;00m has_torch_function(relevant_args):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    215\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    216\u001b[0m         relevant_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m         retain_graph\u001b[38;5;241m=\u001b[39mretain_graph,\n\u001b[1;32m    220\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph)\n\u001b[0;32m--> 221\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/torch/autograd/__init__.py:130\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 130\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "loss2, new_mem2 = model(x2, new_mem1, w)\n",
    "loss2.backward()\n",
    "x1.grad, mem.grad, x2.grad, w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_mem1 grad: tensor([1.5000, 1.5000])\n"
     ]
    }
   ],
   "source": [
    "x1, mem = torch.ones(10, requires_grad=True), torch.ones(2, requires_grad=True)\n",
    "x2 = torch.ones(10, requires_grad=True)\n",
    "\n",
    "loss1, new_mem1 = model(x1, mem, w)\n",
    "loss2, new_mem2 = model(x2, new_mem1, w)\n",
    "\n",
    "new_mem1.register_hook(lambda x: print(f'new_mem1 grad: {x}'))\n",
    "new_mem2.register_hook(lambda x: print(f'new_mem2 grad: {x}'))\n",
    "loss2.backward()\n",
    "# x1.grad, mem.grad, x2.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " tensor([3., 3.]),\n",
       " tensor([0.4000, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000, 0.4000,\n",
       "         0.4000]))"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, mem = torch.ones(10, requires_grad=True), torch.ones(2, requires_grad=True)\n",
    "x2 = torch.ones(10, requires_grad=True)\n",
    "\n",
    "# loss1, new_mem1 = model(x1, mem, w)\n",
    "loss2, new_mem2 = model(x2, mem, w)\n",
    "loss2.backward()\n",
    "loss2, new_mem2 = model(x2, mem, w)\n",
    "loss2.backward()\n",
    "\n",
    "x1.grad, mem.grad, x2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
      "        0.2000]) tensor([1.5000, 1.5000]) None\n",
      "tensor([4.7000, 4.7000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
      "        0.2000]) tensor([3., 3.]) tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000,\n",
      "        0.2000])\n"
     ]
    }
   ],
   "source": [
    "x1, mem = torch.ones(10, requires_grad=True), torch.ones(2, requires_grad=True)\n",
    "x2 = torch.ones(10, requires_grad=True)\n",
    "\n",
    "loss1, new_mem1 = model(x1, mem, w)\n",
    "loss2, new_mem2 = model(x2, new_mem1, w)\n",
    "loss1.backward()\n",
    "print(x1.grad, mem.grad, x2.grad)\n",
    "loss2.backward()\n",
    "print(x1.grad, mem.grad, x2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from modeling_rmt import RMTEncoderForSequenceClassification\n",
    "from modeling_rmt_enc_dec import RMTEncoderDecoderForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cephfs/home/bulatov/bulatov/hvdenv/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 't5-base'\n",
    "\n",
    "# experiment_path = \"/home/bulatov/bulatov/runs/finetune/fix_tok2/qasper/t5-base/lr2e-04_linear_adamw_wd1e-03_1002-1024_mem10_bs32_iters1600_sl/run_1/\"\n",
    "experiment_path = \"/home/bulatov/bulatov/runs/finetune/debug/qasper/t5-base/lr2e-04_linear_adamw_wd1e-03_499-1024_mem10_bs32_iters1600_sl/run_1\"\n",
    "\n",
    "cpt_path = os.path.join(experiment_path, \"model_best.pth\")\n",
    "config_path = os.path.join(experiment_path, \"config.json\")\n",
    "cpt = torch.load(cpt_path, map_location='cpu')\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    d = json.load(f)\n",
    "\n",
    "base_model = transformers.T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "# rmt = RMTEncoderDecoderForConditionalGeneration.from_pretrained(model_name, num_labels=3)\n",
    "rmt = RMTEncoderDecoderForConditionalGeneration(base_model=base_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "set_params_kwargs = {k:v for k,v in d.items() if k in rmt.set_params.__code__.co_varnames}\n",
    "set_params_kwargs['backbone_cls'] = None#transformers.T5ForConditionalGeneration\n",
    "set_params_kwargs['segment_ordering'] = 'regular'\n",
    "set_params_kwargs['inter_layer_memory'] = False\n",
    "set_params_kwargs['tokenizer'] = tokenizer        \n",
    "\n",
    "rmt.set_params(**set_params_kwargs)\n",
    "rmt.load_state_dict(cpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 't5-base'\n",
    "\n",
    "# # experiment_path = \"/home/bulatov/bulatov/runs/finetune/debug/qasper/t5-base/lr2e-04_linear_adamw_wd1e-03_-1024_bl_bs_iters_sl/run_2\"\n",
    "# experiment_path = \"/home/bulatov/bulatov/runs/finetune/debug/qasper/t5-base/lr2e-04_linear_adamw_wd1e-03_1024-1024_bl_bs_iters_sl/run_2\"\n",
    "\n",
    "# cpt_path = os.path.join(experiment_path, \"model_best.pth\")\n",
    "# config_path = os.path.join(experiment_path, \"config.json\")\n",
    "# cpt = torch.load(cpt_path, map_location='cpu')\n",
    "\n",
    "# with open(config_path, 'r') as f:\n",
    "#     d = json.load(f)\n",
    "\n",
    "# baseline = T5ForConditionalGeneration.from_pretrained(model_name, num_labels=3)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# baseline.load_state_dict(cpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holder:\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_len = d['input_seq_len']\n",
    "target_seq_len = 1024\n",
    "batch_size = 1\n",
    "\n",
    "args = Holder\n",
    "args.target_seq_len = target_seq_len\n",
    "args.input_seq_len = input_seq_len\n",
    "args.input_prefix = ''\n",
    "\n",
    "\n",
    "device = torch.device(2)\n",
    "\n",
    "global_attention_first_token = False  # should be True for LED\n",
    "encode_plus_kwargs = {'truncation': True, 'padding': 'longest', 'pad_to_multiple_of': 1}\n",
    "# generate_kwargs = {'max_length': args.target_seq_len, 'min_length': args.target_seq_len}\n",
    "generate_kwargs = {}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # cut too long strings because they may slow down tokenization\n",
    "    inputs = [b['input'][:args.input_seq_len * 10] for b in batch]\n",
    "    if 'outputs' in batch[0]:\n",
    "        # if we have more than 1 label per example (only in valid) take only one of them\n",
    "        # to compute loss on valid\n",
    "        labels = [b['outputs'][0][:args.target_seq_len * 10] for b in batch]\n",
    "    else:\n",
    "        labels = [b['output'][:args.target_seq_len * 10] for b in batch]\n",
    "    if args.input_prefix:\n",
    "        inputs = [args.input_prefix + inp for inp in inputs]\n",
    "    features = tokenizer.batch_encode_plus(list(inputs), max_length=args.input_seq_len, return_tensors='pt',\n",
    "                                           **encode_plus_kwargs)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer.batch_encode_plus(list(labels), max_length=args.target_seq_len, return_tensors='pt',\n",
    "                                             **encode_plus_kwargs).input_ids\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    features['labels'] = labels\n",
    "    features['id'] = [b['id'] for b in batch]\n",
    "    if 'outputs' in batch[0]:\n",
    "        features['target_text'] = [b['outputs'] for b in batch]\n",
    "    else:\n",
    "        features['target_text'] = [b['output'] for b in batch]\n",
    "    if 'global_attention_mask' in features:\n",
    "        raise RuntimeError('What global attention mask for Longformer and LongformerEncoder-Decoder should be?')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset scrolls (/home/bulatov/.cache/huggingface/datasets/tau___scrolls/qasper/1.0.0/672021d5d8e1edff998a6ea7a5bff35fdfd0ae243e7cf6a8c88a57a04afb46ac)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32fb03c0021404dba765f98cc65c52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_name = 'qasper'\n",
    "dataset = datasets.load_dataset('tau/scrolls', task_name)\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset,)\n",
    "kwargs = {'pin_memory': True, 'num_workers': 0}\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "valid_dataset = dataset['validation']\n",
    "valid_sampler = RandomSampler(valid_dataset)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=train_sampler,\n",
    "                                collate_fn=collate_fn, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2567, 1726)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset['pid']), len(valid_dataset['pid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictions from all segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(self, input_ids, return_all_outputs=False, **kwargs):\n",
    "    memory = self.set_memory()\n",
    "    mem_start_ind = 1 if self.bos_token is not None else 0\n",
    "    min_length, max_length = None, None\n",
    "    if 'min_length' in kwargs:\n",
    "        min_length = kwargs.pop('min_length')\n",
    "    if 'max_length' in kwargs:\n",
    "        max_length = kwargs.pop('max_length')\n",
    "\n",
    "    segmented = self.pad_and_segment(input_ids)\n",
    "    segmented = list(zip(*segmented))\n",
    "\n",
    "    if self.segment_ordering in {'regular', 'last_memory_only'}:\n",
    "        pass\n",
    "    elif self.segment_ordering == 'reversed':\n",
    "        segmented = segmented[::-1]\n",
    "    elif self.segment_ordering == 'bidirectional':\n",
    "        segmented = segmented + segmented[::-1][1:]\n",
    "    elif self.segment_ordering == 'repeat_first':\n",
    "        segmented = segmented + segmented[:1]\n",
    "    else:\n",
    "        raise ValueError(f'Unknown segment ordering: {self.segment_ordering}')\n",
    "\n",
    "    outputs = []\n",
    "    for seg_num, segment_data in enumerate(segmented):\n",
    "        input_ids, attention_mask, token_type_ids = segment_data\n",
    "        if memory.ndim == 2:\n",
    "            memory = memory.repeat(input_ids.shape[0], 1, 1)\n",
    "        if (self.bptt_depth > -1) and (len(segmented) - seg_num > self.bptt_depth): \n",
    "            memory = memory.detach()\n",
    "\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "        if self.drop_empty_segments:\n",
    "            non_empty_mask = [not torch.equal(input_ids[i], self.empty) for i in range(len(input_ids))]\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            input_ids = input_ids[non_empty_mask]\n",
    "            attention_mask = attention_mask[non_empty_mask]\n",
    "            token_type_ids = token_type_ids[non_empty_mask]\n",
    "\n",
    "            inputs_embeds = self.embeddings(input_ids)\n",
    "            inputs_embeds[:, mem_start_ind:mem_start_ind+self.num_mem_tokens] = memory[non_empty_mask]\n",
    "\n",
    "        else:\n",
    "            inputs_embeds = self.embeddings(input_ids)\n",
    "            inputs_embeds[:, mem_start_ind:mem_start_ind+self.num_mem_tokens] = memory\n",
    "\n",
    "        seg_kwargs['inputs_embeds'] = inputs_embeds\n",
    "        seg_kwargs['attention_mask'] = attention_mask\n",
    "        if seg_num < len(segmented)-1:\n",
    "            labels = torch.zeros(inputs_embeds.shape[0], inputs_embeds.shape[1], device=inputs_embeds.device, dtype=input_ids.dtype)\n",
    "            gen_out = self.model.generate(**seg_kwargs, output_hidden_states=True, min_length=min_length, max_length=max_length)\n",
    "            outputs.append(gen_out)\n",
    "            out = self.model.forward(**seg_kwargs, output_hidden_states=True, labels=labels)\n",
    "            if self.drop_empty_segments:\n",
    "                memory[non_empty_mask] = out.encoder_hidden_states[-1][:, mem_start_ind:mem_start_ind+self.num_mem_tokens]\n",
    "            else:\n",
    "                memory = out.encoder_hidden_states[-1][:, mem_start_ind:mem_start_ind+self.num_mem_tokens]\n",
    "        else:\n",
    "            out = self.model.generate(**seg_kwargs, output_hidden_states=True, min_length=min_length, max_length=max_length)\n",
    "            outputs.append(out)\n",
    "\n",
    "    if return_all_outputs:\n",
    "        return out, outputs\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def evaluate(output, sample):\n",
    "    labels = sample['labels']\n",
    "    logits = o['logits']\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    correct_mask = preds == labels[output['non_empty_mask']]\n",
    "    return correct_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "def download_metric():\n",
    "    scrolls_metric_path = hf_hub_download(repo_id=\"datasets/tau/scrolls\", filename=\"metrics/scrolls.py\")\n",
    "    updated_scrolls_metric_path = (\n",
    "        os.path.dirname(scrolls_metric_path) + os.path.basename(scrolls_metric_path).replace(\".\", \"_\") + \".py\"\n",
    "    )\n",
    "    shutil.copy(scrolls_metric_path, updated_scrolls_metric_path)\n",
    "    return updated_scrolls_metric_path\n",
    "\n",
    "\n",
    "scrolls_metric_path = download_metric()\n",
    "scrolls_metric = datasets.load_metric(scrolls_metric_path, task_name, keep_in_memory=True)\n",
    "\n",
    "def metrics_fn(labels, generation_outputs, verbose=True):\n",
    "    # compute metrics based on stored labels, predictions, ...\n",
    "        # replace -100 with pad token in labels\n",
    "    y = labels\n",
    "    p = tokenizer.batch_decode(generation_outputs, skip_special_tokens=True)\n",
    "    if verbose:\n",
    "        for i in range(len(y)):\n",
    "            print(f'y: {y[i]}')\n",
    "            print(f'p: {p[i]}')\n",
    "            # print(f'p ids: {generation_outputs[i]}')\n",
    "            print('-' * 50)\n",
    "        # todo: do we need to better clean P to remove tokens after eos? not remove special tokens only\n",
    "    if y is not None and p is not None:\n",
    "        if not isinstance(y[0], list):\n",
    "            y = [[_y] for _y in y]\n",
    "        result = scrolls_metric.compute(predictions=p, references=y)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'pid', 'input', 'output'],\n",
       "    num_rows: 2567\n",
       "})"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label_train = dict(zip(train_dataset['id'], train_dataset['output']))\n",
    "id2label_valid = dict(zip(valid_dataset['id'], valid_dataset['output']))\n",
    "\n",
    "id2text_train = dict(zip(train_dataset['id'], train_dataset['input']))\n",
    "id2text_valid = dict(zip(valid_dataset['id'], valid_dataset['input']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%time\n",
    "# it = 0\n",
    "# max_it =  30\n",
    "\n",
    "# baseline.to(device=device)\n",
    "# sampler = RandomSampler(train_dataset)\n",
    "# dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler,\n",
    "#                                 collate_fn=collate_fn, **kwargs)\n",
    "# # sampler = RandomSampler(valid_dataset)\n",
    "# # dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=sampler,\n",
    "# #                                 collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "\n",
    "# it = 0\n",
    "    \n",
    "# res_df = pd.DataFrame()\n",
    "# gen = iter(dataloader)\n",
    "# for sample in gen:\n",
    "#     ids, target_text, labels = sample.pop('id'), sample.pop('target_text'), sample.pop('labels')\n",
    "#     for key in sample:\n",
    "#         sample[key] = sample[key].to(device)\n",
    "    \n",
    "#     # out, outputs = generate(rmt, return_all_outputs=True, **sample)  \n",
    "#     out = baseline.generate(**sample)  \n",
    "\n",
    "#     res_dict = {'ids': ids}\n",
    "\n",
    "#     f1 = metrics_fn(target_text, generation_outputs=out, verbose=False)['f1']\n",
    "#     res_dict[f'f1_seg_1'] = f1\n",
    "\n",
    "#     preds = tokenizer.batch_decode(out, skip_special_tokens=True)\n",
    "#     res_dict['preds'] = preds\n",
    "#     res_dict['preds_tokens'] = str(out.cpu().numpy())\n",
    "\n",
    "#     res_dict['target_text'] = target_text\n",
    "#     res_dict['labels'] = str(labels.cpu().numpy())\n",
    "\n",
    "#     res_df = res_df.append(pd.DataFrame(res_dict), ignore_index=True)\n",
    "\n",
    "\n",
    "#     it += 1\n",
    "#     if it > max_it:\n",
    "#         break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_df.to_csv('tables/qasper-valid-t5-base-1024.csv', index=False)\n",
    "# res_df.to_csv('tables/qasper-train-t5-base-1024.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# it = 0\n",
    "# max_it =  3000\n",
    "\n",
    "# rmt.to(device=device)\n",
    "# rmt.drop_empty_segments = False\n",
    "# # sampler = RandomSampler(train_dataset)\n",
    "# # dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler,\n",
    "# #                                 collate_fn=collate_fn, **kwargs)\n",
    "# sampler = RandomSampler(valid_dataset)\n",
    "# dataloader = DataLoader(valid_dataset, batch_size=batch_size, sampler=sampler,\n",
    "#                                 collate_fn=collate_fn, **kwargs)\n",
    "\n",
    "\n",
    "# it = 0\n",
    "    \n",
    "# res_df = pd.DataFrame()\n",
    "# gen = iter(dataloader)\n",
    "# predictions = []\n",
    "# for sample in gen:\n",
    "#     ids, target_text, labels = sample.pop('id'), sample.pop('target_text'), sample.pop('labels')\n",
    "#     for key in sample:\n",
    "#         sample[key] = sample[key].to(device)\n",
    "    \n",
    "#     # out, outputs = generate(rmt, return_all_outputs=True, **sample)  \n",
    "#     out = rmt.generate(**sample)\n",
    "\n",
    "#     res_dict = {'ids': ids}\n",
    "#     for i, o in enumerate([out]):\n",
    "#         f1 = metrics_fn(target_text, generation_outputs=o, verbose=False)['f1']\n",
    "#         res_dict[f'f1_seg_{i}'] = f1\n",
    "        \n",
    "#         preds = tokenizer.batch_decode(o, skip_special_tokens=True)\n",
    "#         predictions.append(o.cpu().numpy())\n",
    "#         res_dict[f'preds_seg_{i}'] = preds\n",
    "#         res_dict[f'preds_seg_{i}_tokens'] = str(o.cpu().numpy())\n",
    "        \n",
    "#     res_dict['target_text'] = target_text\n",
    "#     res_dict['labels'] = str(labels.cpu().numpy())\n",
    "\n",
    "#     res_df = res_df.append(pd.DataFrame(res_dict), ignore_index=True)\n",
    "\n",
    "\n",
    "#     it += 1\n",
    "#     if it > max_it:\n",
    "#         break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_df.to_csv('tables/qasper-train-rm-t5-499-10.csv', index=False)\n",
    "# res_df.to_csv('tables/qasper-valid-rm-t5-499-10.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt_df = pd.read_csv('tables/qasper-train-rm-t5-1002-10.csv').sort_values(['ids']).reset_index()\n",
    "rmt_df = pd.read_csv('tables/qasper-train-rm-t5-499-10.csv').sort_values(['ids']).reset_index()\n",
    "rmt_df['input'] = rmt_df.ids.apply(lambda x: id2text_train[x])\n",
    "\n",
    "baseline_df = pd.read_csv('tables/qasper-train-t5-base.csv').sort_values(['ids']).reset_index()\n",
    "baseline_df['input'] = baseline_df.ids.apply(lambda x: id2text_train[x])\n",
    "baseline_df['f1_seg_1'] = baseline_df.f1_seg_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmt-t5-seg-1</th>\n",
       "      <th>t5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rmt-t5-seg-1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              rmt-t5-seg-1    t5\n",
       "rmt-t5-seg-1          0.00  0.23\n",
       "t5                    0.26  0.00"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_cols = {'rmt-t5-seg-1': rmt_df.f1_seg_0, 't5': baseline_df.f1_seg_1}\n",
    "comp_df = pd.DataFrame(index = f1_cols.keys(), columns = f1_cols.keys())\n",
    "\n",
    "for better_name, better_f1 in f1_cols.items():\n",
    "    for worse_name, worse_f1 in f1_cols.items():\n",
    "        num_occ = (better_f1 > worse_f1).sum()\n",
    "        comp_df.loc[worse_name, better_name] = num_occ\n",
    "        \n",
    "comp_df = (comp_df / rmt_df.shape[0]).astype(float).round(2)\n",
    "comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt_df.pred_seg_0.value_counts(), rmt_df.pred_seg_1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f707cb58d00>"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rmt_df.f1_seg_0.hist()\n",
    "baseline_df.f1_seg_2.hist()\n",
    "# rmt_df.f1_seg_1.hist()\n",
    "plt.legend(['seg 0', 'baseline',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rmt_df.f1_seg_0 == 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rmt_df.f1_seg_1 - rmt_df.f1_seg_0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt_df[rmt_df.f1_seg_1 >  0].target_text.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt_df[rmt_df.f1_seg_0 >  0].target_text.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt_df = pd.read_csv('tables/qasper-valid-rm-t5-1002-10.csv').sort_values(['ids']).reset_index()\n",
    "rmt_df = pd.read_csv('tables/qasper-valid-rm-t5-499-10.csv').sort_values(['ids']).reset_index()\n",
    "rmt_df['input'] = rmt_df.ids.apply(lambda x: id2text_valid[x])\n",
    "\n",
    "baseline_df = pd.read_csv('tables/qasper-valid-t5-base.csv').sort_values(['ids']).reset_index()\n",
    "baseline_df['input'] = baseline_df.ids.apply(lambda x: id2text_valid[x])\n",
    "baseline_df['f1_seg_1'] = baseline_df['f1_seg_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f707e3f6940>"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "baseline_df.f1_seg_2.hist()\n",
    "rmt_df.f1_seg_0.hist()\n",
    "# rmt_df.f1_seg_1.hist()\n",
    "plt.legend(['baseline', 'seg 0', 'seg 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14.848953476245654, 15.213263557358053)"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rmt_df.f1_seg_0).mean(), (baseline_df.f1_seg_1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmt_df[(rmt_df.f1_seg_1 - rmt_df.f1_seg_0) < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unanswerable                                               54\n",
       "Yes                                                        18\n",
       "No                                                         10\n",
       "accuracy                                                   10\n",
       "BIBREF14                                                    4\n",
       "                                                           ..\n",
       "BiDAF, R-NET, and QANet                                     1\n",
       "Bi-directional, multi-lingual                               1\n",
       "pooling function                                            1\n",
       "CoNLL 2014                                                  1\n",
       "a LSTM with a corresponding knowledge base, a LSTM with     1\n",
       "Name: preds_seg_0, Length: 202, dtype: int64"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt_df[(baseline_df.f1_seg_1 - rmt_df.f1_seg_0) > 0].preds_seg_0.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Yes                                                                       27\n",
       "Unanswerable                                                              20\n",
       "No                                                                        14\n",
       "English                                                                    7\n",
       "LSTM                                                                       7\n",
       "                                                                          ..\n",
       "vulgarity, gender, proximity to target, proximity to target                1\n",
       "corpus of news articles labelled as propaganda, trusted, hoax, or sati     1\n",
       "in a conversational fashion by imitating human apprehensions and in        1\n",
       "by calculating variance from the outputs                                   1\n",
       "a LSTM with a corresponding knowledge base, a LSTM with                    1\n",
       "Name: preds_seg_0, Length: 180, dtype: int64"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt_df[(baseline_df.f1_seg_1 - rmt_df.f1_seg_0) < 0].preds_seg_0.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_segments = 2\n",
    "def split(text, n_segments=n_segments):\n",
    "    premise = text.split('?')[0]\n",
    "    encoded = tokenizer.encode(text, **encode_plus_kwargs, add_special_tokens=False)\n",
    "    segments = np.split(np.array(encoded), n_segments)    \n",
    "    texts = [tokenizer.decode(s) for s in segments]\n",
    "    \n",
    "    \n",
    "    return [premise] + texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_i = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analysis table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmt-t5-seg-1</th>\n",
       "      <th>t5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rmt-t5-seg-1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              rmt-t5-seg-1    t5\n",
       "rmt-t5-seg-1          0.00  0.21\n",
       "t5                    0.17  0.00"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_cols = { 'rmt-t5-seg-1': rmt_df.f1_seg_0, 't5': baseline_df.f1_seg_1}\n",
    "comp_df = pd.DataFrame(index = f1_cols.keys(), columns = f1_cols.keys())\n",
    "\n",
    "for better_name, better_f1 in f1_cols.items():\n",
    "    for worse_name, worse_f1 in f1_cols.items():\n",
    "        num_occ = (better_f1 > worse_f1).sum()\n",
    "        comp_df.loc[worse_name, better_name] = num_occ\n",
    "        \n",
    "comp_df = (comp_df / rmt_df.shape[0]).astype(float).round(2)\n",
    "comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unanswerable                                                                                                    0.076087\n",
       "Yes                                                                                                             0.040761\n",
       "No                                                                                                              0.024457\n",
       "English                                                                                                         0.010870\n",
       "accuracy                                                                                                        0.010870\n",
       "                                                                                                                  ...   \n",
       "the well-studied phenomenon by which the perception of what we hear can be influenced by                        0.002717\n",
       "they draw from their use of emotion detection methods to detect similarities in the emotional appeal of ISIS    0.002717\n",
       "they combine a deep learning model with a knowledge base                                                        0.002717\n",
       "20,000                                                                                                          0.002717\n",
       "They use a combination of ML and ML to determine the similarity of answer sentences                             0.002717\n",
       "Name: preds, Length: 233, dtype: float64"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rmt_df[baseline_df.f1_seg_1 > rmt_df.f1_seg_0].preds_seg_0.value_counts() / sum(baseline_df.f1_seg_1 > rmt_df.f1_seg_0)\n",
    "baseline_df[baseline_df.f1_seg_1 > rmt_df.f1_seg_0].preds.value_counts() / sum(baseline_df.f1_seg_1 > rmt_df.f1_seg_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 0 better than baseline: 302\n",
      "\n",
      "\n",
      "What datasets are used to assess the performance of the system\n",
      "\n",
      "What datasets are used to assess the performance of the system? Introduction Topic identification (topic ID) on speech aims to identify the topic(s) for given speech recordings, referred to as spoken documents, where the topics are a predefined set of classes or labels. This task is typically formulated as a three-step process. First, speech is tokenized into words or phones by automatic speech recognition (ASR) systems BIBREF0, or by limited-vocabulary keyword spotting BIBREF1. Second, standard text-based processing techniques are applied to the resulting tokenizations, and produce a vector representation for each spoken document, typically a bag-of-words multinomial representation, or a more compact vector given by probabilistic topic models BIBREF2, BIBREF3. Finally, topic ID is performed on the spoken document representations by supervised training of classifiers, such as Bayesian classifiers and support vector machines (SVMs). However, in the first step, training the ASR system required for tokenization itself requires transcribed speech and pronunciations. In this paper, we focus\n",
      "\n",
      "on a difficult and realistic scenario where the speech corpus of a test language is annotated only with a minimal number of topic labels, i.e., no manual transcriptions or dictionaries for building an ASR system are available. We aim to exploit approaches that enable topic ID on speech without any knowledge of that language other than the topic annotations. In this scenario, while previous work demonstrates that the cross-lingual phoneme recognizers can produce reasonable speech tokenizations BIBREF4, BIBREF5, the performance is highly dependent on the language and environmental condition (channel, noise, etc.) mismatch between the training and test data. Therefore, we focus on unsupervised approaches that operate directly on the speech of interest. Raw acoustic feature-based unsupervised term discovery (UTD) is one such approach that aims to identify and cluster repeating word-like units across speech based around segmental dynamic time warping (DTW) BIBREF6, BIBREF7. BIBREF8 shows that using the word-like units from UTD for spoken document classification can work well; however, the results in \n",
      "seg_0: WSJ and WSJX,\n",
      "baseline: SST-1, SST-2, SST-3, SST-4\n",
      "target: LORELEI datasets of Uzbek, Mandarin and Turkish\n",
      "\n",
      "\n",
      "what were the evaluation metrics\n",
      "\n",
      "what were the evaluation metrics? Introduction Grammar induction is the task of inducing hierarchical syntactic structure from data. Statistical approaches to grammar induction require specifying a probabilistic grammar (e.g. formalism, number and shape of rules), and fitting its parameters through optimization. Early work found that it was difficult to induce probabilistic context-free grammars (PCFG) from natural language data through direct methods, such as optimizing the log likelihood with the EM algorithm BIBREF0, BIBREF1. While the reasons for the failure are manifold and not completely understood, two major potential causes are the ill-behaved optimization landscape and the overly strict independence assumptions of PCFGs. More successful approaches to grammar induction have thus resorted to carefully-crafted auxiliary objectives BIBREF2, priors or non-parametric models BIBREF3, BIBREF4, BIBREF5, BIBREF6, and manually-engineered features BIBREF7, BIBREF8 to encourage the desired structures to emerge. We revisit these aforementioned issues in light\n",
      "\n",
      "of advances in model parameterization and inference. First, contrary to common wisdom, we find that parameterizing a PCFG's rule probabilities with neural networks over distributed representations makes it possible to induce linguistically meaningful grammars by simply optimizing log likelihood. While the optimization problem remains non-convex, recent work suggests that there are optimization benefits afforded by over-parameterized models BIBREF9, BIBREF10, BIBREF11, and we indeed find that this neural PCFG is significantly easier to optimize than the traditional PCFG. Second, this factored parameterization makes it straightforward to incorporate side information into rule probabilities through a sentence-level continuous latent vector, which effectively allows different contexts in a derivation to coordinate. In this compound PCFGcontinuous mixture of PCFGsthe context-free assumptions hold conditioned on the latent vector but not unconditionally, thereby obtaining longer-range dependencies within a tree-based generative process. To utilize this approach, we need to efficiently optimize the log marginal likelihood of observed sentences. While compound PCFGs break efficient inference, if the latent vector\n",
      "seg_0: BLEU and TER scores,\n",
      "baseline: F1 score\n",
      "target: INLINEFORM0 scores\n",
      "\n",
      "\n",
      "What is the attention module pretrained on\n",
      "\n",
      "What is the attention module pretrained on? Introduction Speech-to-Text translation (ST) is essential for a wide range of scenarios: for example in emergency calls, where agents have to respond emergent requests in a foreign language BIBREF0; or in online courses, where audiences and speakers use different languages BIBREF1. To tackle this problem, existing approaches can be categorized into cascaded method BIBREF2, BIBREF3, where a machine translation (MT) model translates outputs of an automatic speech recognition (ASR) system into target language, and end-to-end method BIBREF4, BIBREF5, where a single model learns acoustic frames to target word sequence mappings in one step towards the final objective of interest. Although the cascaded model remains the dominant approach due to its better performance, the end-to-end method becomes more and more popular because it has lower latency by avoiding inferences with two models and rectifies the error propagation in theory. Since it is hard to obtain a large-scale ST dataset, multi-task learning BIBREF5, BIBREF6 and pre-\n",
      "\n",
      "training techniques BIBREF7 have been applied to end-to-end ST model to leverage large-scale datasets of ASR and MT. A common practice is to pre-train two encoder-decoder models for ASR and MT respectively, and then initialize the ST model with the encoder of the ASR model and the decoder of the MT model. Subsequently, the ST model is optimized with the multi-task learning by weighing the losses of ASR, MT, and ST. This approach, however, causes a huge gap between pre-training and fine-tuning, which are summarized into three folds: Subnet Waste: The ST system just reuses the ASR encoder and the MT decoder, while discards other pre-trained subnets, such as the MT encoder. Consequently, valuable semantic information captured by the MT encoder cannot be inherited by the final ST system. Role Mismatch: The speech encoder plays different roles in pre-training and fine-tuning. The encoder is a pure acoustic model in pre-training, while it has to\n",
      "seg_0: attention module is pre-trained on a set of English-English topics.,\n",
      "baseline: attention module is pre-trained on the following topics: (1) English to Chinese (ES),\n",
      "target: the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# max_i = 10\n",
    "mask = (rmt_df.f1_seg_0 - baseline_df.f1_seg_1) > 0\n",
    "print(f'Segment 0 better than baseline: {sum(mask)}\\n\\n')\n",
    "slice = rmt_df[mask].reset_index()\n",
    "slice_bl = baseline_df[mask].reset_index()\n",
    "for i, row in slice.iterrows():\n",
    "    inp = row['input']\n",
    "    spl = '\\n\\n'.join(split(row['input']))\n",
    "    # bl_pred = baseline_df[baseline_df.ids == row['ids']].preds.iloc[0]\n",
    "    bl_pred = slice_bl.iloc[i].preds\n",
    "    # print(slice.iloc[i])\n",
    "    # print(slice_bl.iloc[i])\n",
    "    print(f\"{spl}\\nseg_0: {row['preds_seg_0']},\\nbaseline: {bl_pred}\\ntarget: {row['target_text']}\\n\\n\")\n",
    "          \n",
    "    if i > max_i:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_i = 5\n",
    "mask = (rmt_df.f1_seg_0 - baseline_df.f1_seg_1) < 0\n",
    "print(f'Segment 0 worse than baseline: {sum(mask)}\\n\\n')\n",
    "slice = rmt_df[mask].reset_index()\n",
    "for i, row in slice.iterrows():\n",
    "    inp = row['input']\n",
    "    spl = '\\n\\n'.join(split(row['input']))\n",
    "    bl_pred = baseline_df[baseline_df.ids == row['ids']].preds.iloc[0]\n",
    "    # bl_pred = baseline_df[baseline_df.ids == row['ids']].preds\n",
    "    print(f\"{spl}\\nseg_0: {row['preds_seg_0']},\\nseg_1: {row['preds_seg_1']}\\nbaseline: {bl_pred}\\ntarget: {row['target_text']}\\n\\n\")\n",
    "          \n",
    "    if i > max_i:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_i = 20\n",
    "mask = (rmt_df.f1_seg_1 - baseline_df.f1_seg_1) > 0\n",
    "print(f'Segment 1 better than baseline: {sum(mask)}\\n\\n')\n",
    "slice = rmt_df[mask].reset_index()\n",
    "for i, row in slice.iterrows():\n",
    "    inp = row['input']\n",
    "    spl = '\\n\\n'.join(split(row['input']))\n",
    "    bl_pred = baseline_df[baseline_df.ids == row['ids']].preds.iloc[0]\n",
    "    print(f\"{spl}\\nseg_0: {row['preds_seg_0']},\\nseg_1: {row['preds_seg_1']}\\nbaseline: {bl_pred}\\ntarget: {row['target_text']}\\n\\n\")\n",
    "          \n",
    "    if i > max_i:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15ebdd31b1273fe4d2b1fe1822219a570cf61693f7cab545dbe286c10cf9691f"
  },
  "kernelspec": {
   "display_name": "hvdenv",
   "language": "python",
   "name": "hvdenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
