learning_rate: 1e-4
model_name: meta-llama/Llama-3.2-1B-Instruct
num_sensory: 32
segment_length: 1024
bptt_depth: 6
num_seg_save: 8
batch_size: 1
test_length: 3000
load_from_ckpt: /home/yingqi/scratch/hmt_pretrained/llama-3.2-1b-instruct/model_weights_800.pth
read_token_file: /home/yingqi/repo/HMT-pytorch/huggingface_token.txt
write_token_file: write_token.txt